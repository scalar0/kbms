An Introduction to
Knowledge
Graphs
Umutcan Serles Dieter Fensel


An Introduction to Knowledge Graphs


Umutcan Serles • Dieter Fensel
An Introduction
to Knowledge Graphs


Umutcan Serles Department of Computer Science Semantic Technology Institute, University of Innsbruck Innsbruck, Austria
Dieter Fensel Department of Computer Science Semantic Technology Institute, University of Innsbruck Innsbruck, Austria
ISBN 978-3-031-45255-0 ISBN 978-3-031-45256-7 (eBook) https://doi.org/10.1007/978-3-031-45256-7
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed. The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use. The publisher, the authors, and the editors are safe to assume that the advice and information in this book are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give a warranty, expressed or implied, with respect to the material contained herein or for any errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
This Springer imprint is published by the registered company Springer Nature Switzerland AG The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland
Paper in this product is recyclable.


Foreword
The standard narrative about the recent history of AI is that a combination of rapidly growing compute power and an immense increase in available data have caused a scaling explosion in AI that has led to the results in Machine Learning in the past decade. Indeed, this narrative is true, and the results of the ML explosion are clear for all to see, in the popular press, in scientific publications, and in real world applications, ranging from product recommendation to fraud detection and from chatbots to face recognition. What is rather less known to the general public, the popular press, and indeed in AI itself is that a similar scaling explosion has taken place in another area of AI. By the end of the 1990s, a knowledge base of a few thousand facts and rules was considered large. But nowadays, we routinely manipulate knowledge bases that contain billions of facts, describing hundreds of millions of entities, using an ontology of many thousands of statements. The main driver for this “other explosion” of size in AI has been the adoption of the knowledge graph model, combined with ontology languages that carefully balance expressivity against computational cost. Knowledge graphs now form the biggest knowledge bases ever built, and without a doubt, languages like RDF Schema and OWL are by far the most widely used knowledge representation languages in the history of AI. And these knowledge graphs have come of age. They are used in science, in public administration, in cultural heritage, in healthcare, and in a broad range of industries, ranging from manufacturing to financial services and from search engines to pharmaceuticals. It is important to realize that this explosion in the size of symbolic representations did not come out of the blue. It stands in a long tradition of research into symbolic representations and their corresponding calculi. This long tradition includes rulebased languages, non-monotonic logics, temporal logics, epistemic logics, Bayesian networks, causal networks, constraint programming, logic programming, the event calculus, and many many others. So, what was it about knowledge graphs and
v


ontologies that made them so successful, as compared to all the others? Let me put forward three reasons. Facts matter. The (implicit) assumption in pretty much all of KR has been that what matters most are the universally valid sentences, the “rules” that describe how the world works: “If t1 is before t2, and t2 is before t3, then t1 is before t3,” “All birds fly,” “If patient X suffers from disease Y, and Y is a viral infection then X will have a fever,” “Every country has precisely one capital,” etc. But the Semantic Web research program showed us that actually the facts matter perhaps even more. Much of intelligent behavior does not arise out of universally quantified sentences (“Every country has precisely one capital.”), but rather from ground predicates: “Paris is the capital of France,” “COVID is a viral infection,” etc. It is not (or: not only) the inferences we can draw that enable intelligent behavior, it is also (or: mainly) the huge amount of rather mundane facts that we know about the world that allows us to navigate that world and solve problems in it. Or to quote from the introduction of this book: “How useful is an intelligent assistant, if it does not know about the address of the restaurant you are looking for?”. This came as rather a shock to KR researchers (and maybe a bit of a disappointment), but it was one of the lessons that Semantic Web research taught us. Most facts are binary. A further insight (and perhaps further disappointment to KR researchers) was that most facts are simple binary relations. Yes, of course, n-ary relations with n>2 exist, and they are sometimes needed, but by far the large majority of facts have the form of “triples”: <object1, hasRelationTo, object2>. Together, these two insights say that a large volume of binary ground predicates are a crucial ingredient for successful KR, and together they directly lead to knowledge graphs as a natural knowledge model. “A little semantics goes a long way.” (In the immortal words of Jim Hendler). Whereas the instinct of KR researchers had always been to ask, “what is the maximum amount of expressivity I can get before the computational costs become unacceptable,” a more interesting question turned out to be “what is the minimum amount of expressivity that I actually need?” And the answer to this question turned out to be “surprisingly little!” RDF Schema, by far the most used KR language in the history of AI, only allows for monotonic type inheritance, simple type inference based on domain and range declarations, and very limited forms of quantification. No non-monotonicity, no uncertainty, not even negation or disjunction. And of course, there are many use cases where some or all of these would be useful or even necessary, but the surprise was an 80/20 trade-off (or even a 99/1 trade-off, who can say) between the large volume of simple things we want to say and the small volume of remaining complicated things. Among all the other books on knowledge graphs (on formal foundations, on methodology, on tooling, on use-cases), this book does an admirable job of placing knowledge graphs in the wider context of the research that they emerged from, by giving a kaleidoscopic overview of the wide variety of fields that have had a direct or indirect influence on the development of knowledge graphs as a widely adopted storage and retrieval model, ranging from AI to Information Retrieval and from the World Wide Web to Databases. That approach makes it clear to the reader that
vi Foreword


knowledge graphs were not just some invention “out of the blue,” but that instead they stand in a long research tradition, and they make specific choices on the conceptual, the epistemological, and the logical level. And it is this set of choices that is ultimately the reason for the success and wide adoption of knowledge graphs.
Vrije Universiteit Amsterdam, Amsterdam, The Netherlands April 2023
Frank van Harmelen
Foreword vii


Preface
The overall goal of this book is to give a deep understanding of various aspects of knowledge graphs:
• Fundamental technologies that inspired the emergence of knowledge graphs • Semantics and logical foundations and • A methodology to create and maintain knowledge graphs as a valuable resource for intelligent applications
Knowledge graphs1 can provide significant support for application types such as:
• Applications are being made more ubiquitous every day. Who does not own a smartphone with Cortana, Google Assistant, or Siri? • Applications are only as powerful as their knowledge. How helpful is an intelligent assistant if it recognizes your speech perfectly but does not know the address of the restaurant you are looking for?
Knowledge graphs, as a flexible way of integrating knowledge from heterogenous sources, will power these intelligent applications. How to build knowledge graphs? We will cover the following major topics:
• Introduction: motivation, related fields, definition, application types • The holy grail: machine-understandable semantics • The underlying base for this: logic • Set up a knowledge graph: knowledge creation (static, dynamic, and active data) • How to store a knowledge graph: knowledge hosting • How good is your knowledge graph: knowledge assessment • How to fix your buggy knowledge graph: knowledge cleaning
1Bonatti et al. (2019), Chen et al. (2016), Croitoru et al. (2018), d’Amato and Theobald (2018), Ehrig et al. (2015), Fensel et al. (2005, 2020), Hogan et al. (2021), Li et al. (2017), Pan et al. (2017a, b), Van Erp et al. (2017).
ix


• How to extend the coverage of your knowledge graph: knowledge enrichment • How to turn knowledge graphs into applications: knowledge deployment
Knowledge graph technology is a continuation of the Semantic Web initiative (see Berners-Lee et al. (2001), Fensel et al. (2005)). Here, for the first time, vast amounts of content and data were semantically annotated on a worldwide scale. Therefore, introducing knowledge graph technology is incomplete without referring to the underlying principles and techniques from the Semantic Web. Besides discussing other related fields, the Semantic Web will be a designated focus of our book. The Semantic Web was invented for two main reasons (Fensel and Musen 2001):
• The traditional web of documents got too big and heterogeneous to be consumed by humans in a scalable manner. However, if the Web has machineunderstandable semantics, then automated agents and search engines can consume it on behalf of humans.
• Solving the knowledge acquisition bottleneck: Artificial Intelligence has failed initially because people realized that it is hard and expensive to acquire knowledge in a form that intelligent applications can understand. Semantic Web crowdsources the knowledge acquisition task to billions of users through machine-understandable annotations.
Therefore, we aim to provide a deep understanding of the Semantic Web and semantic technologies. We introduce:
• The limitations of the traditional Web • The architecture and principles of the Web and the Semantic Web • The stack of technologies enabling the Semantic Web • Semantic Web of content, data, and services and • Semantic Web applications
Since the Semantic Web is based on other research fields, we will also provide an insight and overview of those research areas that directly lie under the Semantic Web or knowledge graph technology. During the following, we will shortly introduce the need for knowledge graphs to enable the age of intelligent agents:
• Intelligent search and query answering (Fig. 1) • Intelligent personal assistants (e.g., Amazon Alexa,2 Apple Siri,3 Google Assistant, Microsoft Cortana) and • Autonomous agents (Fig. 2)
Intelligent Search and Query Answering. Google started in the late 1990s as a search engine for the Web.4 They were actually a bit late in this but quickly managed
2 https://alexa.amazon.com/ 3 https://www.apple.com/siri/ 4 https://en.wikipedia.org/wiki/History_of_Google
x Preface


Fig. 1 Intelligent search engines answering with rich snippets
Fig. 2 Autonomous agents (Image by Eschenzweig. Licensed under CC-BY-SA. https://commons. wikimedia.org/wiki/File:Autonomous-driving-Barcelona.jpg)
Preface xi


to outrun their competitors. Now search on the Web is called “googling” on the Web. Their original approach was based on the PageRank algorithm (Page et al. 1999) (see later Chap. 3 on information retrieval), which helped them to show the relevant links first. This made them very successful and turned them into an opponent of semantic technologies. You simply did not need them to provide a proper Web search (see Fig. 3). However, there was a specific limitation for the advertisement business model attached to it. You find interesting links for the user, and then they leave you the next moment (and also have to extract the wanted information manually from that Web site). What if you try to provide not only valuable links to the users but extract the information from that source and provide them to the users as potential answers (Fig. 4)? You increase the interaction with them; they are not leaving your Web page and you can even try to begin e-commerce with them. For example, when searching for a specific hotel, you could offer them directly a booking possibility. This turned Google from a “simple” search engine into a query-answering engine. Suddenly semantics and semantic annotations of the Web became a strategic issue, as without understanding the content on a Web site but only its overall score in PageRank was no longer enough. Instead, as more Web site providers became willing to annotate their Web pages semantically, the more this new approach could work and scale.
Fig. 3 Google as a search engine
xii Preface


Bots are virtual agents that interact with human users, searching for and integrating information on the Web and in other sources on behalf of a human user. Alexa, Siri, and Google Assistant are examples of this. Their natural language understanding capabilities are impressive and based on Big Data and statistical analysis. Also, they can respond in natural language quite well. Still, there is a severe bottleneck. For example, how can a bot know which restaurants are out there (see Fig. 5)? For a helpful answer, it must know the restaurants, their menus, their opening times, etc. Even the most recent applications like ChatGPT that are trained on a significant portion of the Web may have trouble answering questions accurately. Without such knowledge, all NLP understanding is of little use. Autonomous driving In March 2018, Elaine Herzberg was the first victim of a fully autonomously driving car.5 Besides many software bugs, a core issue was that the car assumed that pedestrians cross streets only on crosswalks. Obviously, such assumptions should have been made explicit and confronted with world knowledge captured by a knowledge graph. In that case, she still would be alive!
Fig. 4 Google as an intelligent search engine
5 https://en.wikipedia.org/wiki/Death_of_Elaine_Herzberg
Preface xiii


What are the conclusions from the requirements of these fields? Explicit knowledge is crucial for intelligent agents to help users to achieve certain goals. Statistical methods can bring us a long way, e.g., text and voice processing is becoming mainstream thanks to the advances in Machine Learning. However, more than just multiplying vectors and matrices are required. Intelligent personal assistants are limited by their lack of knowledge. Autonomous cars may avoid killing people if they have more explicit knowledge about their environments and common sense. Knowledge graphs are the most recent answer to the challenge of providing explicit knowledge. They contain entities’ references and their relationships. They integrate heterogeneous sources and may contain billions and higher numbers of facts. Summary Our book is about how to build knowledge graphs and make them useful resources for intelligent applications. We focus on the following aspects:
• Part I will provide the overall context of knowledge graph technology. • Part II will provide a deep understanding of logic-based semantics as the technical core of knowledge graph technology. • Part III focuses on the building process of knowledge graphs. We focus on the phases of knowledge generation, knowledge hosting, knowledge assessment, knowledge cleaning, knowledge enrichment, and knowledge deployment to provide a complete life cycle for this process. • Part IV provides application types and actual applications as well as an outlook on additional trends that will make the need for knowledge graphs even stronger.
Fig. 5 A bot without world knowledge (The human depiction is taken from https://www.flaticon. com/free-icon/teacher_194935. Licensed with Flaticon license)
xiv Preface


This book is intended as a textbook for courses covering knowledge graphs. It introduces the theoretical foundations of the technologies essential for knowledge graphs but also covers practical examples, applications, and tools. We very much invite you to work with this book, and we hope we do not lose you in Part II on semantics and logic. No pain, no gain.
Innsbruck, Austria Umutcan Serles Dieter Fensel
References
Berners-Lee T, Hendler J, Lassila O (2001) The semantic web. Sci Am 284(5): 34–43 Bonatti PA, Decker S, Polleres A, Presutti V (eds) (2019) Knowledge graphs: new directions for knowledge representation on the semantic web (Dagstuhl seminar 18371). Dagstuhl Reports 8(9) Chen H, Ji H, Sun L, Wang H, Qian T, Ruan T (eds) (2016) Knowledge graph and semantic computing: semantic, knowledge, and linked big data: First China Conference, CCKS 2016, Beijing, China, September 19–22, 2016, Revised Selected Papers, vol 650. Springer Croitoru M, Marquis P, Rudolph S, Stapleton G (eds) (2018) Graph structures for knowledge representation and reasoning: 5th International Workshop, GKR 2017, Melbourne, VIC, Australia, August 21, 2017, Revised Selected Papers, LNCS, vol 10775. Springer d’Amato C, Theobald M (eds) (2018) Reasoning web. In: Learning, uncertainty, streaming, and scalability: 14th International Summer School 2018, EschsurAlzette, Luxembourg, September 22–26, 2018, Tutorial Lectures, LNCS, vol 11078. Springer Ehrig H, Ermel C, Golas U, Hermann F (2015) Graph and model transformation. Monographs in theoretical computer science, Springer Fensel D, Musen MA (2001) The semantic web: a brain for humankind. IEEE Intell Syst 16(2):24–25 Fensel D, Hendler JA, Lieberman H (eds) (2005) Spinning the Semantic Web: bringing the World Wide Web to its full potential. MIT Press Fensel D, Simsek U, Angele K, Huaman E, Kärle E, Panasiuk O, Toma I, Umbrich J, Wahler A (2020) Knowledge graphs. Springer Hogan A, Blomqvist E, Cochez M, d’Amato C, Melo Gd, Gutierrez C, Kirrane S, Gayo JEL, Navigli R, Neumaier S, et al. (2021) Knowledge graphs. ACM Comput Surv 54(4):1–37 Li J, Zhou M, Qi G, Lao N, Ruan T, Du J (eds) (2018) Knowledge graph and semantic computing. Language, knowledge, and intelligence: Second China Conference, CCKS 2017, Chengdu, China, August 26–29, 2017, Revised Selected Papers, vol 784. Springer
Preface xv


Page L, Brin S, Motwani R, Winograd T (1999) The PageRank citation ranking: bringing order to the web. Technical report, Stanford InfoLab Pan JZ, Calvanese D, Eiter T, Horrocks I, Kifer M, Lin F, Zhao Y (eds) (2017a) Reasoning web: logical foundation of knowledge graph construction and query answering: 12th International Summer School 2016, Aberdeen, UK, September 5–9, 2016, Tutorial Lectures, LNCS, vol 9885. Springer Pan JZ, Vetere G, Gomez-Perez JM, Wu H (eds) (2017b) Exploiting linked data and knowledge graphs in large organisations. Springer Van Erp M, Hellmann S, McCrae J, Chiarcos C, Choi K, Gracia J, Hayashi Y, Koide S, Mendes P, Paulheim H, et al. (eds) (2017) Knowledge graphs and language technology. In: Proceedings of the 15th International Semantic Web Conference (ISWC2016): International Workshops: KEKI and NLP & DBpedia, Kobe, Japan, October 17–21, 2016
xvi Preface


Acknowledgments
Writing such a comprehensive textbook for knowledge graphs is not easy, and we would like to acknowledge the support of various people who helped us. The core of this book is the lecture slides we prepared for our Knowledge Graph and Semantic Web courses. We would like to thank Anna Fensel, who prepared the Semantic Web course for many years and provided us with a good basis for our course materials. We would also like to thank the PhD students in our team, Kevin Angele, Elwin Huaman, Juliette Opdenplatz, and Dennis Sommer. Their PhD work in the areas of knowledge deployment, knowledge assessment, knowledge cleaning, and knowledge enrichment provided the foundation for the chapters of this book. We use several real-world examples in the book, especially ones from the tourism domain. Many thanks to Onlim GmbH for giving us early access to the German Tourism Knowledge Graph and for hosting some of the tools we present in this book for the knowledge graph life cycle. Editing a book this size is a challenging task. We are grateful to our student assistants Muhammed Umar, Shakeel Ahmed, and Elbaraa Elsaadany for their support in putting the book together and Ina O’Murchu for working hard to turn our German/Turkish version of English into the English copy. Finally, we would like to also thank Ralf Gerstner from Springer for being very understanding and supportive during the publication process.
xvii


Contents
Part I Knowledge Technology in Context
1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2 Artificial Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.1 General Problem Solver and Limited Rationality . . . . . . . . . . . . 5 2.2 Knowledge Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.3 Reasoning with Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.4 Knowledge Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.5 Ontologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
3 Information Retrieval and Hypertext . . . . . . . . . . . . . . . . . . . . . . . 27 3.1 Boolean Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 3.2 Vector Space Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 3.3 Evaluation of Information Retrieval Systems . . . . . . . . . . . . . . . 31 3.4 PageRank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 3.5 Applications and Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 3.6 Hypertext . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 3.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
4 The Internet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
5 The World Wide Web . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 5.1 Uniform Resource Identifiers (URIs) . . . . . . . . . . . . . . . . . . . . 46 5.2 REST and HTTP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 5.3 HTML and XML . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 5.4 Is XML Schema an Ontology Language? . . . . . . . . . . . . . . . . . 51 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
xix


6 Natural Language Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 6.1 An Example: The GATE System . . . . . . . . . . . . . . . . . . . . . . . 57 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
7 Semantic Web: Or AI Revisited . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
8 Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
9 Web of Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 9.1 Linked Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 9.2 Linked Open Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
10 Knowledge Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
Part II Knowledge Representation
11 Introduction to Knowledge Representation . . . . . . . . . . . . . . . . . . . 91 Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
12 The Five Levels of Representing Knowledge . . . . . . . . . . . . . . . . . . 93 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
13 Epistemology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97 13.1 Data Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98 13.1.1 The Resource Description Framework (RDF) . . . . . . . . 98 13.1.2 RDF Schema (RDFS) . . . . . . . . . . . . . . . . . . . . . . . . . 105 13.1.3 RDF(S) Serialization . . . . . . . . . . . . . . . . . . . . . . . . . 107 13.1.4 Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 13.1.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114 13.2 Data Retrieval and Manipulation . . . . . . . . . . . . . . . . . . . . . . . 114 13.2.1 SPARQL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 13.2.2 SHACL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128 13.2.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151 13.3 Reasoning over Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151 13.3.1 OWL and OWL2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151 13.3.2 Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168 13.3.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176 13.4 SKOS: A Lightweight Approach for Schema Integration . . . . . . 177 13.4.1 Knowledge Organization Systems . . . . . . . . . . . . . . . . 178 13.4.2 Simple Knowledge Organization System (SKOS) . . . . . 181 13.4.3 Tools and Applications . . . . . . . . . . . . . . . . . . . . . . . . 186 13.4.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187 13.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
xx Contents


14 The Logical Level . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191 14.1 Logics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192 14.1.1 Propositional Logic . . . . . . . . . . . . . . . . . . . . . . . . . . 192 14.1.2 First-Order Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203 14.1.3 Description Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209 14.1.4 Herbrand Model Semantics . . . . . . . . . . . . . . . . . . . . . 215 14.1.5 Second-Order Logic and Further Logical Variants . . . . 220 14.1.6 Reasearch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223 14.1.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224 14.2 RDF(S) Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225 14.2.1 RDF(S) Layering . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226 14.2.2 RDF(S) Interpretations . . . . . . . . . . . . . . . . . . . . . . . . 227 14.2.3 RDF(S) Entailment . . . . . . . . . . . . . . . . . . . . . . . . . . . 234 14.2.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240 14.2.5 Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241 14.3 SPARQL Query Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . 246 14.3.1 Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246 14.3.2 SPARQL Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . 247 14.3.3 Example Abstract Query Evaluation . . . . . . . . . . . . . . 250 14.3.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255 14.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
15 Analysis of Schema.org at Five Levels of KR . . . . . . . . . . . . . . . . . 259 15.1 Schema.org at the Conceptual Level . . . . . . . . . . . . . . . . . . . . . 259 15.2 Schema.org at the Epistemological Level . . . . . . . . . . . . . . . . . 260 15.2.1 Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261 15.2.2 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262 15.2.3 Inheritance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263 15.2.4 Instantiation Relationship . . . . . . . . . . . . . . . . . . . . . . 264 15.2.5 Multityped Entities . . . . . . . . . . . . . . . . . . . . . . . . . . . 264 15.3 Schema.org at the Logical Level . . . . . . . . . . . . . . . . . . . . . . . 265 15.3.1 Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265 15.3.2 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265 15.3.3 Inheritance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267 15.3.4 Instantiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268 15.3.5 Multityped Entities . . . . . . . . . . . . . . . . . . . . . . . . . . . 268 15.4 Schema.org at the Implementation Level . . . . . . . . . . . . . . . . . 268 15.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
16 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
Contents xxi


Part III Knowledge Modeling
17 Introduction: The Overall Model . . . . . . . . . . . . . . . . . . . . . . . . . . 275 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
18 Knowledge Creation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279 18.1 Ontologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279 18.1.1 Domain-Independent Ontologies . . . . . . . . . . . . . . . . . 280 18.1.2 Domain Ontologies . . . . . . . . . . . . . . . . . . . . . . . . . . 281 18.1.3 Ontology Engineering and Methodologies . . . . . . . . . . 282 18.1.4 Ontology Manifoldness . . . . . . . . . . . . . . . . . . . . . . . . 284 18.1.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287 18.2 ABox Creation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287 18.2.1 Domain Specification, a Connector for ABox and TBox . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288 18.2.2 Bottom-Up and Top-Down Domain Specification . . . . . 289 18.2.3 Manual, Semi-Automatic, and Automatic Creation of Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291 18.2.4 Handling of Dynamic and Active Data . . . . . . . . . . . . 295 18.2.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298
19 Knowledge Hosting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303 19.1 Challenges in Hosting Knowledge Graphs . . . . . . . . . . . . . . . . 304 19.2 Knowledge Hosting Paradigms . . . . . . . . . . . . . . . . . . . . . . . . 305 19.2.1 Relational Databases . . . . . . . . . . . . . . . . . . . . . . . . . 305 19.2.2 Document Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312 19.2.3 Graph Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315 19.3 RDF Triplestores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316 19.4 Illustration: German Tourism Knowledge Graph in GraphDB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317 19.4.1 Storing an RDF Graph . . . . . . . . . . . . . . . . . . . . . . . . 317 19.4.2 Querying . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319 19.4.3 Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319 19.4.4 Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322 19.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325
20 Knowledge Assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327 20.1 Quality Dimensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329 20.2 Calculating Quality Score . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333 20.3 Approaches and Illustration . . . . . . . . . . . . . . . . . . . . . . . . . . . 334 20.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337
xxii Contents


21 Knowledge Cleaning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339 21.1 Error Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340 21.1.1 Wrong Instance Assertions . . . . . . . . . . . . . . . . . . . . . 341 21.1.2 Wrong Equality Assertions . . . . . . . . . . . . . . . . . . . . . 342 21.1.3 Wrong Property Value Assertions . . . . . . . . . . . . . . . . 343 21.2 Error Detection and Correction . . . . . . . . . . . . . . . . . . . . . . . . 345 21.2.1 Syntactical Processing . . . . . . . . . . . . . . . . . . . . . . . . 345 21.2.2 Statistical Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 347 21.2.3 Logical and Knowledge-Based Approaches . . . . . . . . . 348 21.2.4 A Few Frameworks for Error Detection and Correction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351 21.3 Illustration: Cleaning the German Tourism Knowledge Graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353 21.3.1 Error Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354 21.3.2 Error Correction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354 21.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
22 Knowledge Enrichment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359 22.1 Identification of Additional Data and Knowledge Sources . . . . . 361 22.2 Data Lifting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361 22.3 TBox Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362 22.4 ABox Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368 22.4.1 Entity Resolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . 369 22.4.2 Data Fusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374 22.5 Illustration: Enriching the German Tourism Knowledge Graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376 22.5.1 Duplicate Detection . . . . . . . . . . . . . . . . . . . . . . . . . . 376 22.5.2 Data Fusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377 22.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 379
23 Tooling and Knowledge Deployment . . . . . . . . . . . . . . . . . . . . . . . . 383 23.1 Tooling for the Knowledge Graph Life Cycle . . . . . . . . . . . . . . 384 23.1.1 Knowledge Creation . . . . . . . . . . . . . . . . . . . . . . . . . . 384 23.1.2 Knowledge Hosting . . . . . . . . . . . . . . . . . . . . . . . . . . 385 23.1.3 Knowledge Assessment . . . . . . . . . . . . . . . . . . . . . . . 388 23.1.4 Knowledge Cleaning . . . . . . . . . . . . . . . . . . . . . . . . . 389 23.1.5 Knowledge Enrichment . . . . . . . . . . . . . . . . . . . . . . . 391 23.1.6 Knowledge Deployment . . . . . . . . . . . . . . . . . . . . . . . 391 23.2 Knowledge Access and Representation Layer . . . . . . . . . . . . . . 391 23.2.1 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394 23.2.2 Knowledge Activators . . . . . . . . . . . . . . . . . . . . . . . . 395 23.2.3 Illustration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397 23.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 406
Contents xxiii


24 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409
Part IV Applications
25 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415 25.1 Migration from Search to Query Answering . . . . . . . . . . . . . . . 415 25.2 Virtual Assistants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417 25.3 Enterprise Knowledge Graphs . . . . . . . . . . . . . . . . . . . . . . . . . 422 25.3.1 Data and Knowledge Integration Inside an Enterprise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422 25.3.2 Data and Knowledge Exchange in Enterprise Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424 25.4 Cyber-Physical Systems and Explainable AI . . . . . . . . . . . . . . . 430 25.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 432 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433
xxiv Contents


Part I Knowledge Technology in Context


Chapter 1 Introduction
The developments that led to knowledge graphs go all the way back to the 1940s and earlier. We will cover these related fields briefly for several scientific areas (see also Harth 2019, Gutierrez and Sequeda 2020, Pavlo 2020, Sanderson and Croft 2012):
• Genuine areas: artificial intelligence (AI), particularly symbolic AI as well as Semantic Web and Linked Open Data (LOD), i.e., Web of Data • Heavily related areas: information retrieval (IR), hypertext systems, natural language processing (NLP), the Internet, the World Wide Web (WWW), and databases (DB)
Figure 1.1 provides a roadmap of these various fields, which we will discuss in the following. The purpose is to provide an understanding of technologies essential for building knowledge graphs. They did not fall from the sky but are instead a result of a large collection of related fields.
Disclaimer We do not try to provide a complete survey on all these fields. This would be far beyond our capabilities and purpose. We instead focus on aspects of these fields that were essential, enabling technologies for knowledge graphs. Therefore, our view on these other fields is biased, and we would like to beg the pardon of our colleagues working in these areas.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 U. Serles, D. Fensel, An Introduction to Knowledge Graphs, https://doi.org/10.1007/978-3-031-45256-7_1
3


References
Gutierrez C, Sequeda JF (2020) Knowledge graphs: A tutorial on the history of knowledge graph’s main ideas. In: Proceedings of the 29th ACM international conference on information & knowledge management (CIKM ’20), Virtual Event Ireland, October 19–23, pp 3509–3510 Harth A (2019) Introduction to linked data, Part I. Preview Edition Pavlo A (2020) 01 - History of databases (CMU databases/Spring 2020). https://www.youtube. com/watch?v=SdW5RKUboKc Sanderson M, Croft WB (2012) The history of information retrieval research. In: Proceedings of the IEEE 100 (Special Centennial Issue):1444–1451. doi: https://doi.org/10.1109/JPROC.2012. 2189916
Fig. 1.1 The tree of evolution of knowledge graph technology (Robot graphic by Vectorportal.com licensed with CC-BY 4.0 https://creativecommons.org/licenses/by/4.0/)
4 1 Introduction


Chapter 2 Artificial Intelligence
We first describe the concept of general problem solvers and infer from their limits the need for limited rationality and heuristic search. Then we introduce the concept of “knowledge is power,” which led to new research fields such as knowledge representation, reasoning engines following various paradigms, expert systems, as well as methods and techniques to build them based on engineering principles.
2.1 General Problem Solver and Limited Rationality
Gottfried Leibniz, a mathematician and philosopher who lived in the seventeenth century, believed that every rational reasoning of humans could be reduced to calculations. He theorized a calculus ratiocinator,1 a machine that can solve any problem written in a “universal language.” There are different views on what he meant by this:
• One view sees it as an inference engine, • The other view as a calculating machine, and • For him, inference and calculation were identical.
Leibniz later prototyped the Stepped Reckoner, a machine that solves differential equations, which is often attributed to the calculus ratiocinator idea. A significant event that “started” artificial intelligence (AI) is identified as the Dartmouth Workshop (McCarthy et al. 2006). It was an 8-week workshop organized and participated by scientists and industrial practitioners from various fields, such as computer science, cognitive science, mathematics, and natural sciences, which was held in 1956 at Dartmouth College in New Hampshire, USA. The main objective was to discuss the ideas around “thinking computers.” At this workshop, the idea to
1 https://en.wikipedia.org/wiki/Calculus_ratiocinator
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 U. Serles, D. Fensel, An Introduction to Knowledge Graphs, https://doi.org/10.1007/978-3-031-45256-7_2
5


solve all problems that require intelligence with a General Problem Solver was (re)born (Ernst and Newell 1969; Newell and Shaw 1959; Newell and Simon 1972; Newell et al. 1959). A General Problem Solver is a system that aims to solve all problems with deduction as long as these problems can be represented formally (e.g., with formal logic). Given an initial and a goal state, it searches the path from the initial state to the goal.2 It was developed in 1959 following the Dartmouth Workshop by A. Newell, J. C. Shaw, and Herbert A. Simon. Actually, it worked well for many “toy” problems that also can be formalized straightforwardly. The general algorithm is given a state, and an operator is applied that takes you closer to your goal state. Even a “simple” problem like playing chess gets lost in a combinatorial explosion. Shannon conservatively estimated the lower bound number of possible games as 10120 (Shannon 1950). For a complete and correct search, you need to evaluate all of them.3 Newell and Simon realized that the combinatorial explosion would be a problem. All relevant and interesting problems come with search spaces that are incredibly large, if not infinite. A blind brute-first search of general problem solvers does not scale at all. They introduced the concept of limited rationality and heuristic search.
• Limited rationality is giving up on the goal of finding an optimal solution by considering the costs of finding it (Simon 1957; Newell and Simon 1972). • Heuristic search implements this in a way that you limit your search space and use heuristics to find acceptable semi-optimal solutions (local instead of global optima).4
Hill climbing is an example of a heuristic search algorithm.5 Given a solution state space of a problem and a heuristic function, the hill climbing algorithm traverses from its current state to a neighboring state with the aim of reaching the goal state. The complete state space is not necessarily known in advance. The heuristic function calculates the “heuristic value” of a state. A state is visited only if it appears to bring us closer to the goal state (decided with the help of having the highest heuristic value from all neighborhood nodes). A typical example is the traveling salesman problem, which is about visiting a number of interconnected places with a minimal total path length. The complete algorithm would compute all possible paths and select the shortest one. However, with a complete search, we run into a combinatorial explosion. With hill climbing, we cannot guarantee that we will find the best solution but we will find a local optimum much faster. We always
2The search can start from the goal state backward, from the start state forward or from an intermediate state. 3https://electronics.howstuffworks.com/chess.htm#pt1 and https://medium.com/hackernoon/ machines-that-play-building-chess-machines-7feb634fad98 4 https://en.wikipedia.org/wiki/Heuristic_(computer_science) 5 https://en.wikipedia.org/wiki/Hill_climbing
6 2 Artificial Intelligence


would move to the next place in the neighborhood that is close to our current state. Obviously, we cannot guarantee that we found the shortest path in the end. Up to now, we discussed how difficult it is to find the global optimum. Actually, the problem may start even earlier: It may be difficult to decide what the global optimum is. And even worse, it may change over time. A nice example is provided by climbing to the top of a mountain.6 The following problems may appear:
• It is hard to correctly identify a mountain’s peak, especially with snow cover on the ground and bad weather conditions. Many mountain climbers who are assumed to be on the highest point of a mountain may have missed this point by some meters. So finding the global optima is extremely hard.
• Even worse, the highest point of a mountain changes over time. So you may have been at the highest point in the past but not in the present. Global optima may change quickly over time.
General problem solvers searching the entire search space do not scale. The concept of the incomplete heuristic search was a response to it. However, they are not complete and, therefore, may not find an optimal solution for the general case. For example, the algorithm may terminate at a local optimum. We can implement backtracking, generate more (or all!) states, and so on with the hope of reaching a global optimum. However, then we are just back to a complete search. The question is whether the time and effort are worth it or whether a local optimum may be good enough. Recall limited rationality and the idea of including the costs for finding a solution into consideration.
Knowledge Is Power In essence, you need knowledge to guide your search properly. Feigenbaum formulated this as the knowledge principle (Lenat and Feigenbaum 1987):
A system exhibits intelligent understanding and action at a high level of competence primarily because of the specific knowledge that it contains about its domain of endeavor.
The question is how to represent this knowledge and how to work with this knowledge to solve problems. The proof of the pudding is in the eating: expert systems were built to prove his point. The three major pillars for implementing this slogan are knowledge representation, reasoning with knowledge, and knowledge modeling.
2.2 Knowledge Representation
How is knowledge represented? Soon a number of alternative paradigms arose. Since this was a parallel development, many systems and representation formalisms had features of several paradigms. Still, we can distinguish the following main
6Frankfurter Allgemeine Zeitung, 13.10.2021, Nr. 238, S. 7: Das ist doch der Gipfel.
2.2 Knowledge Representation 7


streams: semantic networks that led to description-logic-based representation formalisms. The network character also reflected the later structure of the World Wide Web. Frames took a more local view of an object with defined properties and property values and became a predecessor of object-oriented programming. A less dominant approach was prototypes, where knowledge is described by characterizing properties of sets of entities. “A semantic network is a structure for representing knowledge as a pattern of interconnected nodes and arcs” (Sowa 1991). It was first used in 1960 for natural language processing (NLP) tasks, particularly to help the machine translation of different languages. Over the years, many different versions of it have been developed. Common characteristics are listed in Sowa (1991) as follows (see also Fig. 2.1):
• Nodes in the net represent concepts. • Arcs in the net represent the relationships between concept nodes. • Concepts are hierarchically organized. • And relationships are inherited across the concept hierarchy.
Frames (Minsky 1975) are also networks of nodes and relationships. Knowledge is stored in a structure called a “frame.” These frames can be organized in a hierarchy and linked via slots. Constraints and conditions can be defined on a slot, as well as actions can be defined based on certain triggers. They also allow the representation of “stereotypical knowledge.” The top levels of a frame are always true for a given situation. Lower levels have multiple slots whose values are filled with specific instances of data. That is, lower-level frames may override the default value inherited from a higher-level frame (see Fig. 2.2). Semantic nets and frame-based systems both represent nodes and their relationships; however, they use different representation paradigms (Grimm et al. 2007). The semantic net paradigm represents arbitrary nodes and arbitrary binary
Fig. 2.1 The term knowledge graph was first used to describe a semantic net whose edge labels are restricted to a certain vocabulary (James 1992). In the figure, you see an example of a semantic network (Adapted from https://en.wikipedia.org/wiki/Semantic_network)
8 2 Artificial Intelligence


relationships between them. The relationships are represented as edges between two nodes. The frame-based paradigm sees nodes as frames and relationships as slots defined on frames filled by other frames (akin to the object-oriented paradigm). As a result, frames provide more structure to nodes. Frames also have a more natural way to represent exceptions in the inheritance of relationships, i.e., the value of an inherited slot can be easily overridden by the inheritor. In semantic nets, consuming applications need to decide on a strategy to solve such inheritance conflicts. Prototypes (Cochez et al. 2016) are a variation of frames. Usually, they have no notion of classes; instead, there is a prototype frame. The instances are defined based on the prototype frame and inherit properties from a base prototype. It can add/change/remove property values. A variation of fuzzy logic or a similarity function can be applied to retrieve similar instances.
Fig. 2.2 An example of frame-based class, slot, and instance definitions [Example adapted from https://en.wikipedia.org/wiki/Frame_(artificial_intelligence)]
2.2 Knowledge Representation 9


2.3 Reasoning with Knowledge
Besides representing knowledge, one would like a computer to process this knowledge drawing new conclusions from it. We identify three major paradigms for this kind of engine:
• Description logics that base their semantics on standard first-order logic (FOL) • Rule systems that base their semantics on variations of minimal model semantics of Horn logic (logic programming) • Production rule systems that follow a more procedural style, often recalling the assembly’s go-to style
A description logic (DL) knowledge base consists of two parts: TBox and ABox. A terminological box (TBox) contains axioms about concept hierarchies and roles (relationships). An assertional box (ABox) contains ground facts, i.e., instances of concepts and relationships. There is a strict separation between these two parts, mostly in order to provide scalable reasoners. The main conceptual building blocks are concepts (unary predicates), roles (binary predicates), and individuals (constants). Like first-order logic, a DL uses the open-world assumption (OWA) and does not have a unique name assumption (UNA). Open-World Assumption (OWA) The nonexistence of a statement does not mean it is false. The implication of OWA is that if a statement is false, it has to be explicitly stated. Imagine you are in a train station and there are only the following entries in the timetable:
Innsbruck – Munich 13:00 Innsbruck – Kufstein 13:10
Is there a train connection between Innsbruck and Munich at 15:00? Intuitively, your answer would be “no.” Under OWA, the answer cannot be “no” as we do not know if the given connection does not exist. Train station schedules work under the closed-world assumption, which makes our lives easier. If you do not see a train connection in the timetable, it does not exist. Because of this, FOL, and therefore DL, cannot express the transitive closure of a relation (see Part II for more details).
Unique Name Assumption (UNA) Things that have different names (identifiers) are different. This is the implication of having UNA. Take the following axioms: a person can only be married to another unique person.7 Now, consider we have the following ground facts: married(john,sally) and married(john,sam). A knowledge base with UNA would be inconsistent as sally and sam are different individuals. A DL system not having the UNA would infer that sally and sam refer to the same individuals. Nothing stops different names from referring to the same individual
7Obviously, this constraint does not hold in all cultures; also, there is the problem that people marry again in time, and finally, the constraint we were taught at university that each spouse must have a different gender is gone, too.
10 2 Artificial Intelligence


unless it is explicitly stated that they are different. An important implication would be on the resolution algorithm for proving a conclusion from premises. A crucial step in resolution is unification (more on this in Sect. 14.1). Only expressions with the same term symbols can be unified. Normally, this is a simple linear syntactic term comparison. Without a unique name assumption, we need to “reason” whether f and g are the same terms, even if they look syntactically different. Usually, DL systems focus on a decidable subset of first-order logic, but reasoning can still be very expensive. The most basic DL language that allows negation, conjunction, and disjunction is an attributive language with complements (ALC). For example, the satisfiability problem of a TBox for ALC has exponential time complexity w.r.t size of the TBox. It becomes a polynomial space complete if no cycles in the TBox are allowed (Martinez et al. 2016; Ortiz 2010). A small DL knowledge base is shown in Fig. 2.3.
The description logic community is a vibrant community with many developed systems (e.g., Hermit, Pellet, RACER, etc.) and applications in areas such as software engineering, configuration tasks, medical informatics, Web-based information systems, natural language processing, etc.; see Baader et al. (2003). For example, GALEN (Rector and Nowlan 1994) was a large European project to provide a common reference model for medical terminology. The model contains modules for diseases, causes, human anatomy, genes, etc. The model is formalized with DL and aims to support applications like decision support systems and the natural language processing of medical text. Finally, the Web ontology language (OWL) development was a big boost for it. Prolog stands for PROgramming in LOGic, which is a logic programming language (framework) invented in 1972.8 It is based on Horn clauses written in implication form. The program tries to answer a query (the goal) via logical deduction (see also Coppin (2004) for a brief introduction to Prolog and Horn clauses).
TBox
h ≡ ¬∃ . ⊤ ⊓ Bachelors are unmarried men and vice versa
≡ −1 Being married to someone is reflexive
∃≥2 . ⊑ ⊥ One can be married to at most one person ABox
Man(umut) Umut is a Man Woman(miriam) Miriam is a Woman married(umut, miriam) Umut is married to Miriam
Fig. 2.3 An example of a specification in description logic [Adapted from Rodriguez (2019)]
8 https://en.wikipedia.org/wiki/Prolog
2.3 Reasoning with Knowledge 11


A Horn clause in logical programming is a formula that looks like a rule (IF. . . Then . . .). It is a disjunction of literals with at most one positive literal.
Øp _ Øq _ . . . _ Øt _ u, which can be written in implication form as u ← p ^ q ^ ... ^ t
It uses minimal model semantics and its closed-world assumption by negation as failure, i.e., nonderivable facts are false. A program in Prolog is a database of facts and rules:
• A rule consists of a head and a body:
– The body of a rule is a sequence of compound terms separated by a comma (,) or a semicolon (;). – The body implies the head. – The rules are written as implications: head: - body.
– For example, sibling(X, Y): - parent_child(Z, X), parent_child(Z, Y).
• Facts are rules without a body:
– For example, father_child(mike, tom).
A program is run by giving a goal (a query). The program tries to answer the query via logical deduction. Prolog uses a proof process called resolution in order to answer queries based on facts and rules. It is actually a refutation process. We first include the negation of the goal to the premises and then prove that the set of formulas is unsatisfiable. Resolution works on the principle that a literal and its negation can be “unified” under certain conditions:
• Can we make these two literals identical by substituting their variables with other terms? • If yes, then we can resolve them. • If we can reach the empty clause, then the query is answered as true.
The example in Fig. 2.4 shows the resolution algorithm in action (Williams 2005). First, the query ancestor(P, juliette) is resolved with the rule (1). This is done by the substitution θ1 = {P/X, Y/ juliette} to obtain the subgoal parent(X, juliette), where X is mapped to P and juliette is mapped to Y. Then resolve subgoal parent(X, juliette) with the fact (4). This is done with the substitution θ2 = {X/ george} to yield the empty clause. The answer is P = george}, i.e., george is an ancestor of juliette. Prolog provides the not operator that can be used in the body of the rule to indicate negation. The negation in Prolog works with the negation as failure (NaF) principle. The literal not(P(x)) holds true if P(X) cannot be derived; otherwise, it is false (Basic and Snajder 2019). Negation as failure (NaF) implements the closedword assumption.
12 2 Artificial Intelligence


The example in Fig. 2.5 explains NaF (adapted from Basic and Snajder (2019)). We cannot derive has_feathers(plato)); therefore, not(has_feathers(plato)) would be true. As a consequence, from the fact (2) and rule (1), we can infer that human(plato) holds true.
Production Rule Systems Many expert systems were implemented as production rule systems. A production rule system consists of a rule base in the form of a set of condition-action rules, a working memory that holds the state of the system, and a rule engine that orchestrates the triggering of the rules. MYCIN9 was an expert system implemented as a production rule system in the 1970s. It contained about 350 production rules that encoded clinical diagnosis
(1) ancestor(X,Y) :- parent(X,Y).
(2) ancestor(X,Y) :- parent(X,Z), ancestor(Z,Y).
(3) parent(morty,jerry).
(4) parent(george, juliette).
(5) parent(jerry,sue).
(6) parent(jerry,alan).
(7) parent(frank,george).
(8) parent(sue,toby).
(9) parent(sue,juliette).
(Query) :- ancestor(P,juliette).
Fig. 2.4 An example Prolog program and a query
(1) human(X) :- speaks(X), not(has_feathers(X)).
(2) speaks(plato).
(3) speaks(polynesia).
(4) has_feathers(polynesia).
? – human(plato) .
Fig. 2.5 An example of negation as failure
9 https://en.wikipedia.org/wiki/Mycin
2.3 Reasoning with Knowledge 13


criteria from infectious disease experts. Its main purpose was to help physicians with their decisions on bacterial disease diagnosis. The rule system was written in a listprocessing language (LISP) (van Melle 1978).10 A MYCIN rule and its English translation are shown below in Fig. 2.6.
Based on the data in the working memory, the rules whose premises are fulfilled can be triggered. The data are represented in a triple format and can be acquired:
• As static factual data • As dynamic patient data (collected via consultation), and • As example patient data
MYCIN has a backward-chaining rule engine. At any given moment, MYCIN is trying to prove a goal (e.g., identify a bacteria). It brings the rules whose conclusion contains the goal and creates subgoals based on the premises. Each conclusion updates the working memory, which would lead to the firing of other rules, forming a rule chain. MYCIN also contains rules about rules, i.e., metarules that enable the prioritization of rules and conflict resolution.11
2.4 Knowledge Modeling
“In artificial intelligence, an expert system is a computer system emulating the decision-making ability of a human expert” (Jackson 1998). Early knowledgebased systems like expert systems were developed not with a methodology but rather with rapid prototyping approaches. The main problem with first-generation expert systems was the lack of reusability of knowledge. This made them very hard
PREMISE: ($AND (SAME CNTXT GRAM GRAMNEG) (SAME CNTXT MORPH ROD) (SAME CNTXT AIR ANAEROBIC)) ACTION: (CONCLUDE CNTXT IDENTITY BACTEROIDES TALLY .6)
If 1) The gram stain of the organism is gramneg, and 2)The morphology of the organism is rod, and 3) The aerobicity of the organism is anaerobic
Then:
Fig. 2.6 A MYCIN rule (Van Melle 1978)
10See also Coppin (2004). 11See Buchanan and Shortliffe (1984) for a book about MYCIN that is compiled based on various related publications.
14 2 Artificial Intelligence


to develop and, in turn, quite costly. It is comparable to programming from scratch. In response to this, knowledge engineering was developed as a modeling task to structure the building and maintenance process of experts or knowledge-based systems. The need for a methodology for getting the knowledge from the expert’s head and formalizing it in a computer system (i.e., knowledge acquisition) has led to the development of a set of methodologies. A prominent approach in this area of knowledge engineering methodologies is KADS (Wielinga et al. 1992) and its successor, CommonKADS (Schreiber et al. 1994). KADS and CommonKADS see knowledge acquisition as a modeling task and not as a task of transferring expert knowledge as a flat list of rules. A major principle is to first concentrate on conceptual modeling, then in a later step on the implementation details. Knowledge and how it can be used within a knowledgebased system are modeled from different aspects. CommonKADS provides knowledge-based system developers with a set of model templates that can be configured and filled during the project. The original situation was very comparable to the situation in computer engineering in the 1970s. Small programs were written quickly and successfully. Larger programs failed horribly in terms of the time of delivery, development costs, and functionality mismatched requirements, and the error rate of programs was much too high. It was especially NASA that was to launch initiatives to improve this situation and help found the field of software engineering. Just take the Mars Climate Orbiter12 as an example. It was a robotic space probe launched by NASA. However, communication with the spacecraft was permanently lost as it went into orbital insertion. The spacecraft encountered Mars on a trajectory that brought it too close to the planet, and it was either destroyed in the atmosphere or escaped the planet’s vicinity and entered an orbit around the sun. An investigation attributed the failure to a measurement mismatch: metric units by NASA and US Customary (imperial or ‘English’) units by spacecraft builder Lockheed Martin. Central means of software engineering are various models that guide the software development process. KADS/CommonKADS does this for a specific type of system: knowledge-based systems. The core models are shown in Fig. 2.7 and are defined by Schreiber et al. (1994) as follows:
• The organization model supports the analysis of an organization. The goal is to identify problems, opportunities, and potential impacts of the KBS (knowledgebased system) development. • The task model describes tasks that are performed or will be performed in the organization. • The agent model describes the capabilities, norms, preferences, and permissions of agents. An agent is the executor of tasks. • The knowledge model gives an implementation-independent description of the knowledge involved in a task.
12 https://en.wikipedia.org/wiki/Mars_Climate_Orbiter
2.4 Knowledge Modeling 15


• The communication model describes the communicative transactions between agents. • The design model describes the structure of the system that needs to be constructed.
We will go a bit deeper into the knowledge model as it is the most significant one in the context of knowledge graphs. The knowledge model distinguishes between domain, inference, and task knowledge.13 Domain knowledge represents the relevant domain knowledge and information, mostly static by nature. See the example for car components in Fig. 2.8. Inference knowledge models inference actions that are basic reasoning steps that can be made with the domain knowledge and are applied by tasks. From a functional point of view, it describes the lowest level of decomposition. Its basic informationprocessing units are inference functions that enable reasoning and the transfer function that facilitates communication with other agents. See the inference action cover that tries to derive a cause for a malfunction in Fig. 2.9. Task knowledge is goal oriented and provides a higher level of functional decomposition. It describes goals such as:
• Assessing a loan application to reduce the risk • Finding the reason for a malfunction of a device and restoring it • Designing an elevator for a skyscraper
It is typically described in a hierarchical fashion based on reusable design patterns. It is finally mapped on reusable problem-solving methods by refining the latter in task-specific terms and adaptations (see Fig. 2.10).
Fig. 2.7 CommonKADS models, adapted from Fig. 1 in Schreiber et al. (1994)
13See also https://commonkads.org/knowledge-model-basics/
16 2 Artificial Intelligence


KADS/CommonKADS provides a methodological framework for developing knowledge-based systems. It approaches KBS development as a modeling task, avoiding the drawbacks of seeing such development as mining knowledge from experts’ heads. Its focus is on reusability. It considers many aspects, such as domain, task, organization, and agent models. Here, we focused on domain and task knowledge, but for further details about the methodology, see Schreiber et al. (1994). The core principles of these methodologies are:
• Reusable domain knowledge: ontologies • Reusable task knowledge: problem-solving methods
This differentiation generalizes the distinction between data and programs but is also aiming at keeping pieces of both aspects reusable. We will focus here on the aspect of data.
gas dial
value: gas-dial-value
CONCEPT gas dial; ATTRIBUTES: value: gas-dial-value; END CONCEPT gas-dial;
VALUE-TYPE gas-dial-value; VALUE-LIST: {zero, low, normal}; TYPE: ORDINAL; END VALUE-TYPE gas-dial-value;
fuel tank
status: {full, almostempty, empty}
CONCEPT fuel-tank; ATTRIBUTES: status: {full, almost-empty, empty}; END CONCEPT fuel-tank;
Fig. 2.8 Domain knowledge for car components (Adapted from https://commonkads.org/ knowledge-model-basics/)
Fig. 2.9 An inference action (Adapted from https://commonkads.org/knowledge-model-basics/)
2.4 Knowledge Modeling 17


2.5 Ontologies
Ontology14 is the branch of philosophy that studies concepts such as existence, being, becoming, and reality. It includes the questions of how entities are grouped into basic categories and which of these entities exist on the most fundamental level. Ontology is sometimes referred to as the science of being. Aristotle, 384–322 BC, is often referred to as the founder of this area of science. Actually, the goal is to build a model of the world. Unfortunately, nature is not very keen on being classified into fixed schemas. On the contrary, it is full of transitions, overlaps, exceptions, and changes. These are intrinsic features of self-evolving systems that necessarily develop through gradual self-modification rather than falling from the sky with a given blueprint. Let us take an example from Wikipedia. Mammals15 (from the Latin word mamma) are characterized by the presence of mammary glands, which in females produce milk for feeding (nursing) their young. They reproduce with viviparity, i.e., the embryo is developed inside the body of the parent.16 These characteristics distinguish them from Sauropsida (reptiles and birds), from which they diverged over 300 million years ago. They are oviparous, a reproductive mode in which females lay developing eggs that complete their development and hatch externally from the mother.17 And
knowledge intensive task
analytic synthetic
classification diagnosis prediction
assessment monitoring
design planning assignment
modelling scheduling
configuration designing
Fig. 2.10 The task model, adapted from Schreiber et al. (1994)
14 https://en.wikipedia.org/wiki/Ontology 15 https://en.wikipedia.org/wiki/Mammal 16 https://en.wikipedia.org/wiki/Viviparity 17 https://en.wikipedia.org/wiki/Oviparity
18 2 Artificial Intelligence


then biologists traveled upside down. They found a strange animal, which they called a duck-billed platypus, in Australia. This animal brings the offspring to life as eggs but later starts to feed them with milk.18 The clear separation was gone. Fish live in water, and then we meet flying fish, fish living on land, moving forward and backward between land and water, etc. Vice versa, crocodiles live most of their time in the water but are not fish. Birds can fly, and then you run into a Struthio camelus. Classifying nature is a Sisyphus task. A second fundamental objection against an objective model of the world is that we also see it from a specific perspective. We are part of it, and our perception has to focus on a small aspect of it. Plato (428/427 or 424/423–348/347 BC) raised the question of whether we can really see the real world in its essence or only its shadow in its appearance. A concrete example is the movement of the stars on the firmament. What we see are stars rotating around the earth. Meanwhile, we know that more realistic models are that this movement only reflects the rotation of the earth. Kant (1724–1804 AC), a German philosopher, came in a principled way back to this question: Can we really see the real world in its essence or only its shadow in our perceptions? Kant argued that the sum of all objects, the empirical world, is a complex of appearances whose existence and connection occur only in our representations:
And we indeed, rightly considering objects of sense as mere appearances, confess thereby that they are based upon a thing in itself, though we know not this thing as it is in itself, but only know its appearances, viz., the way in which our senses are affected by this unknown something. (Kant 1783 paragraph 32)
In general, perception is selection and construction. Only tiny aspects of the world can be perceived, depending on the perceptive system and the attendance of the observer. In addition, the receiver builds a model to organize and interpret otherwise random signals into a “meaningful” picture that helps him find his pathway to reality (see Fig. 2.11). Kant goes even so far as to postulate space and time as categories of perception and not of the world as such. If we cannot access – according to Kant – the thing itself, why should we assume there exists something at all beyond our cognition? What is it that is in our perception? Postmodernism takes this indeed a step further by assuming that there is no reality beyond our perception, and there is no sense in talking about it in scientific terms since we have no access to it. We recently saw in the United States where this leads in political debates. There are no facts, and all news I do not like is fake news! You just need to repeat a lie as often as you can till people believe it and it becomes true.
18 https://en.wikipedia.org/wiki/Platypus
2.5 Ontologies 19


How did Kant fix this obvious problem of the correspondence between perception and reality? He (reasonably) assumed that there is a world as such, but how can he explain that what is in our perception meaningfully corresponds with some aspect of the world? He assumed a common ground between both. God created our perceptional system and the world in a way that both fit each other. Meanwhile, we would no longer accept this as a scientific answer but as a way to hide away the issue. However, we also have an answer to this question: “It is the purpose, stupid.” Perception and cognition are a means to ensure survival in evolution. They must be able to produce fast and reliable models of the world (but not the world as such) to help feed and for survival.19 It is actually always the purpose that defines the specific way of modeling the real world. For example, in the river and coastal navigation, your ontology shows a flat world; see Fig. 2.12. And when changing to offshore sailing (or flying), suddenly everything changes. The earth is suddenly no longer flat, and the shortest path is no longer a straight line (see Fig. 2.13).20 The difficulty of producing a two-dimensional projection of this three-dimensional sphere became obvious with two-dimensional projection that can either be angle preserving or area size equivalent but never both (see Fig. 2.14). Most of these models are from the first view to support navigation, but they work for certain purposes only.
Fig. 2.11 Different
perspectives in viewing an object (Image by Nevit Dilmen, licensed with CCBY-SA 4.0 https:// commons.wikimedia.org/ wiki/File:Face_or_vase_ ata_01.svg)
19 https://en.wikipedia.org/wiki/Charles_Darwin 20 https://en.wikipedia.org/wiki/Great-circle_distance
20 2 Artificial Intelligence


Fig. 2.12 Different perspectives in viewing an object. An ontology for river and coastal navigation models a flat world (Image by OpenSeaMap published by Markus Bärlocher, CC-BY-SA 2.0 https://commons.wikimedia.org/wiki/File:OpenSeaMap-Warnem%C3%BCnde.png)
Fig. 2.13 Only in a flat world is the straightway the shortest one. A straight route on earth appears as an arc in a two-dimensional projection like a map (The image on the left by CheCheDaWaff, distributed under CC BY-SA 4.0 https://commons.wikimedia.org/wiki/File:Illustration_of_greatcircle_distance.svg. The image on the right derived from the work of MixoMiso27 and distributed under CC-BY-SA 3.0 https://commons.wikimedia.org/wiki/File:A_large_blank_world_map_with_ oceans_marked_in_blue.PNG)
2.5 Ontologies 21


Let us use the Newton’s laws of gravity as a final example. These laws perfectly describe with a small set of differential equations the movement of all bodies in our solar systems. They are so accurate that the existence and location of Neptune could be predicted by the orbital disturbances Uranus has through it. In the same way, researchers predicted a planet called Vulcan to explain the perihelion movement of Mercury. In difference to Neptune, this planet could never be found. In fact, it can only be explained by a more complex theory based on field equations as a mathematical formulation of the space-time continuum. This general relativity theory of Einstein is, for example, also essential to making GPS work.
Fig. 2.14 The Mercator projection with the wrong size continents (Image by Strebe, distributed under CC BY-SA 3.0 https://commons.wikimedia.org/wiki/File:Mercator_projection_Square.JPG)
22 2 Artificial Intelligence


Summary of Ontologies An ontology always reflects the point of view of an observer. The more this is made explicit by modeling the underlying assumptions, the better it can be aligned with different ontologies. Ontologies can only be partially reused for cases with similar points of view (purpose) in related domains. The idea of one right ontology is misleading. It creates pointless arguments and does hamper reuse and alignment. In computer science, ontologies are used to model things in a certain part of the world from a certain point of view. Ontologies for computer science were defined in the first half of the 1990s (Gruber 1995; Studer et al. 1998) as a formal, explicit specification of a shared conceptualization:
• Conceptualization: an abstract model of some part of the world • Formal: machine-understandable (e.g., formalized with logic) • Explicit: concepts and constraints are explicitly defined, and • Shared: captures the knowledge on which a group of people have a consensus
2.6 Summary
Still, with all these developments, knowledge acquisition remained a hard and expensive task. The costs related to it often significantly outnumber the gains using an expert system in production. The term knowledge acquisition bottleneck was coined. The reasons for the knowledge acquisition bottleneck were:
• Too high costs to acquire knowledge • The low quality of the acquired knowledge • Brittleness of the acquired knowledge, and • The velocity of the acquired knowledge
As a result, this led to the so-called AI winter in the 1990s.
References
Baader F, Calvanese D, McGuinness D, Patel-Schneider P, Nardi D (eds) (2003) The description logic handbook: Theory, implementation and applications. Cambridge University Press Basic BD, Snajder J (2019) Lecture 7: logic programming in prolog. University of Zagreb Artificial Intelligence Lecture Slides. https://www.fer.unizg.hr/_download/repository/AI-7LogicProgramming.pdf Buchanan BG, Shortliffe EH (1984) Rule-based expert systems: the MYCIN experiments of the Stanford heuristic programming project (the Addison-Wesley series in artificial intelligence). Addison-Wesley Longman Publishing Co., Inc.
References 23


Cochez M, Decker S, Prud’Hommeaux E (2016) Knowledge representation on the web revisited: the case for prototypes. In: The Semantic Web–ISWC 2016: 15th international semantic web conference, Kobe, Japan, October 17–21, 2016, Proceedings, Part I 15, Springer, pp 151–166 Coppin B (2004) Artificial intelligence illuminated. Jones & Bartlett Learning Ernst GW, Newell AG (1969) A case study in generality and problem-solving. Academic, New York Grimm S, Hitzler P, Abecker A (2007) Logic, ontologies and semantic web languages. In: Studer R, Grimm S, Abecker A (eds) Semantic web services: concepts, technologies, and applications. Springer Gruber TR (1995) Toward principles for the design of ontologies used for knowledge sharing? International Journal of Human-computer Studies 43(5-6):907–928 Jackson P (1998) Introduction to expert systems, 3rd edn. Addison Wesley James P (1992) Knowledge graphs. In: 1991 Workshop on linguistic instruments in knowledge engineering, Tilburg, January 18–19, Elsevier, pp 97–117 Kant I (1783) Prolegomena zu einer jeden künftigen Metaphysik die als Wissenschaft wird auftreten können. Project Gutenberg. https://www.projekt-gutenberg.org/kant/prolegom/prolegom.html Lenat DB, Feigenbaum EA (1987) On the thresholds of knowledge. In: Proceedings of the tenth international joint conference on artificial intelligence, August, pp 23–28 Martinez M, Rohrer E, Severi P (2016) Complexity of the description logic ALCM. In: Proceedings of the fifteenth international conference on principles of knowledge representation and reasoning, Cape Town South Africa, April 26–29, AAAI Press, KR’16, pp 585–588 McCarthy J, Minsky ML, Rochester N, Shannon CE (2006) A proposal for the Dartmouth summer research project on artificial intelligence, August 31, 1955. AI Mag 27(4):12–12 Minsky M (1975) A framework for representing knowledge. In: Winston PH (ed) The psychology of computer vision. McGraw-Hill, New York, pp 211–277 Newell A, Shaw J (1959) A variety of intelligent learning in a general problem solver. RAND Report P-1742, July 6 Newell A, Simon HA (1972) Human problem solving, vol 104. Prentice-Hall, Englewood Cliffs, NJ Newell A, Shaw JC, Simon HA (1959) Report on a general problem-solving program. In: Proceedings of the first international conference on information processing congress, UNESCO, Paris, June 15–20, Pittsburgh, PA, pp 256–264 Ortiz M (2010) Lecture 5: Complexity of reasoning in ALC. Declarative knowledge processing lecture slides. http://www.kr.tuwien.ac.at/education/deklslides/ws12/dkpWS10 6.1nup.pdf Rector AL, Nowlan WA, & Galen Consortium (1994) The GALEN project. Comput Methods Prog Biomed 45(1–2):75–78 Rodriguez ND (2019) Introduction to description logics and ontologies. Symbolic artificial intelligence lecture slides. https://perso.telecomparitech.fr/bloch/OptionIA/IA301-Lecture1IntroDL-OWL.pdf Schreiber G, Wielinga B, de Hoog R, Akkermans H, Van de Velde W (1994) CommonKADS: a comprehensive methodology for KBS development. IEEE Expert 9(6):28–37 Shannon CE (1950) Programming a computer for playing chess. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science 41(314):256–227 Simon HA (1957) Models of man: social and rational-mathematical essays on rational human behavior in a social setting. Wiley Sowa JF (ed) (1991) Principles of semantic networks: explorations in the representation of knowledge. Morgan Kaufmann Studer R, Benjamins VR, Fensel D (1998) Knowledge engineering: principles and methods. Data Knowl Eng 25(1-2):161–197
24 2 Artificial Intelligence


Van Melle W (1978) MYCIN: a knowledge-based consultation program for infectious disease diagnosis. International Journal of Man-Machine Studies 10(3):313–322 Wielinga BJ, Schreiber AT, Breuker JA (1992) KADS: a modelling approach to knowledge engineering. Knowl Acquis 4(1):5–53 Williams A (2005) Prolog, resolution and logic programming. CS612 automated reasoning lecture slides. http://www.cs.man.ac.uk/schmidt/CS612/2005-2006/resolution-slides.pdf
References 25


Chapter 3 Information Retrieval and Hypertext
Information retrieval (IR) is about finding material (documents, graphics, voice, video) of an unstructured nature that satisfies an information need from within large collections [see Manning et al. (2008), Teufel (2014a), and Teufel (2014c)] that are usually stored on computers. In this chapter, we focus on the retrieval of text. Searching for information in human storage systems has always been a challenge for humanity. It goes as far as the third century BC and earlier. The main challenge is how do I find the relevant information sources given a query? Information retrieval research led to the development of various methods and techniques for representing queries and documents and finding the most relevant documents:
• Boolean models consider documents as a set of words. • Vector space models consider documents as word vectors. • Probabilistic models consider the similarity of a query to a document as probabilities. This can be used as part of the Boolean and vector space models.
We will give a bit more detail about the Boolean model, the vector space models, and the PageRank algorithm used on the Web. We add a few applications and discuss the specific area of hypertext-based systems, given their relevance for the Web.
3.1 Boolean Model
The initial work on IR focused on converting user queries to Boolean expressions and looking for exact matches among the set of words comprising each document. Consider the following query as an example: Which plays of Shakespeare are about Brutus and Caesar but not Calpurnia (Teufel 2014a)? The main idea is to record for each play whether it contains these words (see Fig. 3.1).
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 U. Serles, D. Fensel, An Introduction to Knowledge Graphs, https://doi.org/10.1007/978-3-031-45256-7_3
27


The Boolean model was mainly used by large commercial information providers until the 1990s, e.g., IR systems for lawyers. The main advantages of the Boolean model are straightforward implementation and knowing exactly what you will get since a word either matches the document or does not match. The disadvantages are that the results are not ranked in any way and the term weights are not considered. The approach is also semantically highly brittle as they cannot handle, for example, synonyms or negation. For example, a query to retrieve the documents about the word “birds” would retrieve a document containing the sentence “This document is not about birds.” Therefore, extended Boolean models were developed to address some shortcomings (Teufel 2014a). For example, a proximity operator is introduced. This means a document is matched only if some of the words in the query appear in the document at a given proximity (Manning et al. 2008). Also, some implementations of the Boolean model allowed weighting of the terms in the query and provided fuzzy matching. For example, in a conjunctive query, the document that does not contain all words still receives a matching score based on the weights of the terms in the query.1
3.2 Vector Space Model
The standard Boolean model returns the documents matching the query exactly. What happens if there are 1000 documents matching a query? Results are unordered: a user must go through all documents. What happens if there are no exact matches?
Fig. 3.1 An excerpt from the Boolean representation of unique words in Shakespeare’s plays, adapted from Manning et al. (2008) and Teufel (2014a)
1See also: https://courses.cs.vt.edu/~cs5604/cs5604cnIF/IF3.html
28 3 Information Retrieval and Hypertext


Maybe there are some documents that match to some extent. The vector space model allows representing documents and queries as N-dimensional vectors (N = number of unique words in a document). The main idea is to calculate the similarity between two vectors (Teufel 2014b). Cosine similarity2 is a good way to calculate the degree of overlap between two vectors. It is based on calculating the angle between two vectors in the vector space. A and B in Fig. 3.2 are two vectors representing the query and a potential answer. The most important choice is to decide how to represent documents as vectors. Binary incidence vectors represent documents in a way similar to the Boolean method; however, this does not provide a way to rank terms in a document. What we can do is assign a weight (wt, d) for each word (t) for each document (d). Term frequency uses the frequency of a word in a document instead of binary values (Fig. 3.3). We say tft, d is the term frequency of term t in document d, and it is defined as the number of times that t occurs in d (Teufel 2014b). Raw term frequency is a start but not the best metric for calculating wt, d to build vectors. A document with tf = 100 of a term is more relevant than a document with tf = 1 of that term but not 100 times more relevant. Relevance does not depend proportionally on term frequency. Therefore, log frequency is used instead to alleviate this effect of proportion (Teufel 2014b):3
∑
∑∑
Fig. 3.2 Vector-based calculation of similarity
Fig. 3.3 An excerpt from the representation of unique words in Shakespeare’s plays as term frequency vectors, adapted from Teufel (2014b)
2 https://en.wikipedia.org/wiki/Cosine_similarity 3The addition of “1” is a normalization step to prevent log frequency from producing zero for the words that occur only once in a document since log1 would be zero.
3.2 Vector Space Model 29


wt, d = 1 þ logtf t, d if tf t,d > 0
0 otherwise
Most frequently occurring words do not necessarily distinguish documents in terms of relevance to a query. Take the word “the” as an example, which occurs in practically every English document. Rare words are more significant for deciding the relevance of a document to a query. Therefore, we want to weigh rare terms more than frequent terms across documents. In addition to the term frequency, we also want to use the inverse document frequency of these terms. If a word appears in a document frequently but rarely in the collection of documents, it indicates that this document is more relevant for that term. The inverse document frequency of a term t (idft) is calculated as follows (Teufel 2014b). Let us say that dft is the document frequency of t, i.e., the number of documents in which t occurs, and N is the number of documents in the collection. Then idft is calculated as:
idf t = log N
df t
The log of the ratio is used to alleviate the proportion effect, as we discussed when we talked about term frequency. wt, d based on TF-IDF is then calculated as follows:
wt, d = 1 þ logtf t, d idf t if tf t, d > 0
0 otherwise
Note that intuitively, there are two ways the weight of a term for a document can be 0. Either the word does not appear in that document (i.e., its term frequency is 0), or the word appears in every document (i.e., its logarithmic inverse document frequency is 0). Figure 3.4 shows an excerpt from the unique words that occur in
Fig. 3.4 An excerpt from the TF-IDF vector representation of the words in Shakespeare’s plays, adapted from Teufel (2014b)
30 3 Information Retrieval and Hypertext


Shakespeare’s plays. The words are represented as vectors where each vector contains the TF-IDF weights of the corresponding word.
3.3 Evaluation of Information Retrieval Systems
Information retrieval systems are evaluated by means such as precision, recall, and F-score. Precision and recall4 are the most common measures for evaluating IR systems (Teufel 2014c). Precision is the ratio of the number of retrieved documents that are relevant to the query to the number of retrieved documents. The following terminology is typically used while calculating precision, recall, and F-score:
precision = relevant documents
f g \ retrieved documents
j f gj retrieved documents
jf gj
Recall is the ratio of the number of retrieved documents that are relevant to the query to the number of all relevant documents:
recall = relevant documents
f g \ retrieved documents
j f gj relevant documents
jf gj
F-score5 is the harmonic mean of precision and recall evaluating the effectiveness of an IR system:
F = 2 precision recall
precision þ recall
Note that there is a trade-off between precision and recall (Fig. 3.5). A system can simply return all documents to have 100% recall; however, the precision will be low. In contrast, high precision means that recall will be low. For IR at a large scale, for example, in Web search, precision and recall may be impractical measures. Who cares about general precision if the search engine returns 1,000,000 results? Users are generally interested in just checking a dozen results. What is the precision among the top 10 results? The importance of ranking increases significantly: try to get the top k results as relevant as possible to the user’s information need. Originally, the query-dependent ranking was dominant, e.g., TF-IDF vector similarity between query and document. Meanwhile, a prominent alternative has become a query-independent ranking, e.g., the PageRank algorithm.
4https://en.wikipedia.org/wiki/Precision_and_recall - distributed under CC-BY-SA 3.0 5 https://en.wikipedia.org/wiki/F-score
3.3 Evaluation of Information Retrieval Systems 31


3.4 PageRank
PageRank6 (Page et al. 1999) has been an algorithm used by Google for many years since its founding. Its main goal is to identify how important a Web page is. The importance of a Web page is identified mainly based on the incoming links from other pages. Pages “vote” for the importance of a page by providing links to it. How much of a vote is given by a page depends on the page rank of that page. Actually, its value is recursively defined by the Web pages that point to it. And how are their values defined? It is by the value of the Web pages that point to each of those Web pages and so on. To be precise, a Web page distributes its value on all the Web pages it points to. That is, if it points to five Web pages, then each of them receives 20% of its value. So even Web pages with a high value through ingoing links may give only little value to their outgoing links when there are many of them. The calculation must happen recursively: but the Web is huge; where and when do we stop with the iterations? However, at some point, the changes in the calculated page rank value will get very small; then we can stop the calculation. Assume page A has incoming links from pages T1. . .Tn. Parameter d is a damping factor that can be assigned a value between 0 and 1 (usually 0.85).7 PR(Ti) is the page rank of the ith page that has an outgoing link to A. C(Ti) is the total number of outgoing links from the ith page. The PageRank of page A is calculated as follows (Brin and Page 1998)8:
Fig. 3.5 Trade-off between precision and recall
6 http://ianrogers.uk/google-page-rank/. 7“PageRank can be thought of as a model of user behavior. We assume there is a ‘random surfer’ who is given a web page at random and keeps clicking on links, never hitting ‘back’ but eventually gets bored and starts on another random page. The probability that the random surfer visits a page is its PageRank. And the d damping factor is the probability at each page the ‘random surfer’ will get bored and request another random page”—(Brin and Page 1998). 8See also http://ianrogers.uk/google-page-rank/ for a more detailed explanation and example.
32 3 Information Retrieval and Hypertext


PRðAÞ = ð1 - dÞ þ d PRðT1Þ
CðT1Þ þ . . . þ PRðTnÞ
CðTnÞ
At the first iteration, the PageRank of each page can be initialized as PR(T1) = PR(T2) = PR(Tn) = 1. The PageRank values of each page are updated after each iteration until they do not change (or change very little). The iterative calculation process becomes clearer from a linear algebraic perspective. The PageRank calculation is simply finding an eigenvector r of a matrix L that represents the links between the Web pages on the Web:9
L r=r
As the formula shows, the calculation is self-referential and must be solved iteratively. Figure 3.6 shows the iterative computation of an eigenvector r of matrix L. Now, consider a subset of the Web that consists of pages A, B, C, and D, as shown in Fig. 3.7. We can represent this Web as an n × n link matrix (L ). Each column represents a page and its outgoing links to another page, while rows represent the incoming links.
⋮= ⋮
∑
⋮ =⋮
Fig. 3.6 Iterative eigenvector computation
Fig. 3.7 A small Web of documents
9For simplicity, we ignore the damping factor d.
3.4 PageRank 33


The values are normalized by the total number of outgoing links from a page to convert them into probabilities of a user reaching a page from another page. For example, L21 is the probability of a user reaching from A to B, and L31 represents the probability of a user reaching from A to C. Given a link matrix L and a rank vector r0 (each row corresponds to the rank of pages A to D), we will calculate the final rank vector r that contains the ranking for the Web pages A to D. See Fig. 3.8 for the initial stage of the calculation. The tricky part is that at the beginning, we do not know the rank vector r. So the initial step of the iteration starts with 1/n as the rank of each page, where n is the number of pages. In our example, r would be a 4×1 vector with 0.25 at each row (n = 4).
0 0.33 0.5 1 0.5 0 0 0 0.5 0.33 0 0
0.25 0.25 0.25
Fig. 3.8 The small Web of documents represented as the link matrix L and r0 rank vector initialized
0 0.33 0.5 1 0.5 0 0 0 0.5 0.33 0 0 0 0.33 0.5 0
*
0.25 0.25 0.25 0.25
=
0.46 0.13 0.21 0.21
0 0.33 0.5 1 0.5 0 0 0 0.5 0.33 0 0 0 0.33 0.5 0
*
0.46 0.13 0.21 0.21
=
0.35 0.23 0.27 0.15
...
0 0.33 0.5 1 0.5 0 0 0 0.5 0.33 0 0 0 0.33 0.5 0
*
0.37 0.18 0.24 0.19
=
0.37 0.18 0.24 0.18
0 0.33 0.5 1 0.5 0 0 0 0.5 0.33 0 0 0 0.33 0.5 0
*
0.37 0.18 0.24 0.18
=
0.37 0.18 0.25 0.18
Fig. 3.9 The iterative eigenvector calculation process
34 3 Information Retrieval and Hypertext


Figure 3.9 demonstrates the calculation of the vector r. We make the matrix multiplication iteratively until it converges to the desired eigenvector. At each step, we use the rank vector obtained from the previous stage. We know we found that vector when r stops changing at each iteration. As shown in Fig. 3.9, the calculation converges around the 10th–11th iteration, which indicates that we found vector r. Note that the actual values in the vector is not particularly important, but it is how they are ordered in terms of magnitude. In our example, the final vector r obtained from the 11th iteration shows that the highest ranked page is A, followed by C, which is followed by equally ranked pages B and D. Obviously, one can ask many questions:
• Is there always an eigenvector? • Is the eigenvector unique? • Does the iteration always converge? • What is the mathematical role of the damping factor?
However, we would need to add more on linear algebra, which is beyond our scope.10
3.5 Applications and Systems
The explosion of the information available toward the second half of the twentieth century motivated people to think about effective and efficient methods of information retrieval (Sanderson and Croft 2012). The first computerized machine for information retrieval was invented in 1948 (Holmstrom 1948) and was implemented with a Univac computer. It searches for text references on magnetic steel tape, given a subject code as a query. It was able to process 120 words per minute. A breakthrough for IR happened toward the end of the twentieth century with the invention of the Internet and the Web. Search engines appeared as large-scale IR systems on the Web. Early search engines crawled a (significant) portion of the Web, created indexes of the crawled pages, and retrieved documents (i.e., Web pages) given a query.
3.6 Hypertext
The theoretical Memex machine from Vannevar Bush (1945) is considered the inspiration for hypertext systems. The Memex machine is a large desk with a mechanism that can store large information sources in microfilms. The user can
10We recommend watching https://www.coursera.org/lecture/linear-algebra-machine-learning/ introduction-to-pagerank-hExUC and https://www.youtube.com/watch?v=-RdOwhmqP5s
3.6 Hypertext 35


enter a code and retrieve the relevant microfilm magnified on a translucent screen. The system also allows linking content on microfilms to create “trails of information.” Inspired by the Memex idea, in 1962, Doug Engelbart started to work on text processing software (oN-Line System (NLS)) that has the capability of linking other peoples’ work within a text. The term hypertext was coined and later expanded to hypermedia by Ted Nelson in 1965 (Rayward 1994; Nielsen 1995). In the 1960s, 1970s, and 1980s, many other hypertext systems were developed (see Nielsen (1995) for a comprehensive list). Among those, HyperCard was one of the most successful commercial hypermedia products (Fig. 3.10). It was developed by Bill Atkinson from Apple in 1987. The concept of the system is based on a stack of virtual cards. Each card can contain text enhanced with links and other interactive elements, like textboxes, checkboxes, and buttons. Documents can be written in a language called Hypertalk, which also provides a graphical development interface. Programming with Hypertalk was somewhat similar to form-based application development with Visual Basic. These concepts and techniques got a significant increase in importance with the World Wide Web, which opened these closed systems and connected them via the Internet.
Fig. 3.10 The interface of HyperCard (Image by 37 Hz, distributed under CC BY 2.0 license. https://www.flickr.com/photos/37hz/9842400043)
36 3 Information Retrieval and Hypertext


3.7 Summary
Information retrieval (IR) deals with the task of retrieving relevant documents from a collection of documents given a query. The development of IR systems was an answer to the information explosion starting in the 1940s. At the core of IR systems lies deciding “to what extent a document is relevant for a query.” Initial approaches used a Boolean model. With this model, only the documents that match the entire query were retrieved without any indication of the level of relevance (i.e., ranking). Various extensions of the Boolean model and approaches using the vector space model and probabilistic models allowed incorporating a ranking mechanism. For example, approaches using the vector space model could tell how relevant a document to a query is by measuring the cosine distance between the query and document vectors. IR found many commercial applications in the 1990s in domains like banking and law. Toward the end of the twentieth century, the invention of the World Wide Web brought a new challenge to IR: the collection of documents was now much larger and distributed. Moreover, the documents were linked to each other as the Web is built on hypertext. The proper ranking of relevant documents became more important as the users were interested in a small number of the most relevant and important pages across the large network of documents. Google developed the PageRank algorithm that benefits from the principles of hypertext. Simply put, the more a page was linked from other pages, the higher the rank it was assigned. Information retrieval, particularly in the context of the Web, undoubtedly changed our lives. As discussed in Chap. 1, traditional IR methods worked well for many years for search engines; however, they are almost purely syntactic, which means they are susceptible to errors when negation and synonyms are involved. IR supported with knowledge graphs opened new doors for users. The queries were not only matched with documents but also with the actual information they were looking for.
References
Brin S, Page L (1998) The anatomy of a large-scale hypertextual web search engine. Computer Networks and ISDN Systems 30(1–7):107–117 Bush V (1945) As we may think. The Atlantic Monthly 176(1):101–108 Holmstrom J (1948) Section III. Opening plenary session. In: The royal society scientific information conference, June 21–July 2, London Manning CD, Raghavan P, Schütze H (2008) Introduction to information retrieval. Cambridge University Press Nielsen J (1995) Multimedia and hypertext: the Internet and beyond. Morgan Kaufmann Page L, Brin S, Motwani R, Winograd T (1999) The PageRank citation ranking: bringing order to the web. Technical reports, Stanford InfoLab Rayward WB (1994) Visions of Xanadu: Paul Otlet (1868–1944) and hypertext. J Am Soc Inf Sci 45(4):235–250
References 37


Sanderson M, Croft WB (2012) The history of information retrieval research. Proceedings of the IEEE 100 (Special Centennial Issue), pp 1444–1451. doi:https://doi.org/10.1109/JPROC.2012. 2189916 Teufel S (2014a) Introduction and overview. Information retrieval. https://www.cl.cam.ac.uk/ teaching/1314/InfoRtrv/lecture1.pdf Teufel S (2014b) Term weighting and the vector space model. URL https://www.cl.cam.ac.uk/ teaching/1314/InfoRtrv/lecture4.pdf Teufel S (2014c) Evaluation. https://www.cl.cam.ac.uk/teaching/1314/InfoRtrv/lecture5.pdf
38 3 Information Retrieval and Hypertext


Chapter 4 The Internet
The Internet is one of the things the Cold War brought to humanity. At that time, the Soviets were shooting Sputnik into space, and the USA did not want to stay behind in technological advances. So they founded the Advanced Research Projects Agency (ARPA) in the 1960s. At that time, ARPA started to support work on the communication of computers on large networks. This research led to the Advanced Research Projects Agency Network (ARPANET). The first communication was done between a computer at Menlo Park and the University of California, Los Angeles. It was a node-to-node communication to transfer the message “LOGIN” between two computers.1 The initial way of connecting different physical networks was problematic. They had rigid routing structures and were fragile as they had a single point of failure. Both American and English researchers were meanwhile working on a more robust way of transferring data on a network. In the end, the concept of packet switching was adopted. Packet switching allows messages to be transported as small packages in arbitrary order over more flexible routes2 (Fig. 4.1). First, there is the Internet Protocol (IP),3 which provides the network layer communication protocol of the Internet. Its task is to deliver the data packages across various networks based on IP addresses in the header of the data and encapsulate the data to be delivered. On top of IP (responsible for the lower layer transfer of packages and containing the IP addresses) is the higher-level transport protocol TCP.4 Soon, there were many application-level protocols developed, for example:
1 https://www.bbc.com/news/business-49842681 2 https://en.wikipedia.org/wiki/History_of_the_Internet 3https://en.wikipedia.org/wiki/Internet_Protocol licensed under CC-BY-SA 3.0. 4 https://en.wikipedia.org/wiki/Transmission_Control_Protocol
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 U. Serles, D. Fensel, An Introduction to Knowledge Graphs, https://doi.org/10.1007/978-3-031-45256-7_4
39


• File Transfer Protocol (FTP): transferring files between server and client. • Domain Name System (DNS): a naming system that maps human-friendly domain names to IP addresses.
• Simple Mail Transfer Protocol (SMTP) provides a protocol for e-mail communication. • Hypertext Transfer Protocol (HTTP): the Internet protocol for exchanging hypertext objects.
The Internet connects the edges retaining no state and aims for speed and simplicity. To that end, an important principle is the robustness principle:
In general, an implementation should be conservative in its sending behavior, and liberal in its receiving behavior. That is, it should be careful to send well-formed datagrams, but should accept any datagram that it can interpret (e.g., not object to technical errors where the meaning is still clear). – Internet Protocol Specification, August 19795
The Internet uses the Open System Interconnection (OSI) model. The OSI model layers are (Fig. 4.2):6
1. The physical layer provides the physical interface between the device and the transmission media. 2. The data link layer provides the transmission protocol controlling the data flow between network devices.
Fig. 4.1 Prevent the central point of failure! (The nuclear explosion photo is the courtesy of the National Nuclear Security Administration Nevada Field Office. Licensed under Public Domain.)
5 https://www.postel.org/ien/txt/ien111.txt 6 https://insights.profitap.com/osi-7-layers-explained-the-easy-way
40 4 The Internet


3. The network layer provides the necessary routing and switching technologies. It routes data packets of variable length from a source to a destination network. This is provided by the Internet protocol (IP). 4. The transport layer transfers data between end users, ensuring that the data transfer is error- and congestion free. This is provided by the Transmission Control Protocol (TCP). 5. The session layer takes care of the management, establishment, and termination of connections between two end users of a network. 6. The presentation layer translates data for the application layer. It also takes care of encryption and authentication. 7. Finally, the application layer provides functionality for applications.
The main standards used by the Internet are Unicode, IP, and TCP. In the following, we will briefly introduce these standards. Unicode standard provides an international character set, e.g., that is used to encode the data in the data package.
Fig. 4.2 The OSI layer model
4 The Internet 41


The Internet Protocol (IP)7 is the network layer communication protocol, e.g., that enables internetworking and establishes the Internet. Its routing function enables internetworking, which establishes the Internet. It delivers packets from source to destination based on IP addresses in packet headers.
The Transmission Control Protocol (TCP)8 is one of the main protocols of the Internet, enabling applications to exchange messages over a network. It is part of the Internet Protocol Suite (TCP/IP) located at the transport layer (layer 4). TCP enables reliable, ordered, and error-checked transmission of a stream of bytes between applications running on an IP network. TCP is connection oriented, i.e., a connection between client and server must be built before data can be sent. Flags are important for connection handling (Fig. 4.3):9
• ACK—indicates that the Acknowledgment field is significant. • FIN—last packet from the sender. • SYN—synchronize sequence numbers; only the first packet sent from each end should have this flag set.
Based on this, a client can establish a connection with a server via a TCP handshake as follows:10
Fig. 4.3 Flags during a TCP handshake between client and server (Image by Snubcube, distributed under CC-BY-SA 3.0 https://de. wikipedia.org/wiki/Datei: Tcp-handshake.svg)
7Reprinted from https://en.wikipedia.org/wiki/Internet_Protocol licensed under CC-BY-SA 3.0. 8 https://en.wikipedia.org/wiki/Transmission_Control_Protocol 9Reprinted from https://en.wikipedia.org/wiki/Transmission_Control_Protocol#TCP_segment_ structure licensed under CC-BY-SA 3.0. 10Reprinted from https://en.wikipedia.org/wiki/Transmission_Control_Protocol#Connection_estab lishment licensed under CC-BY-SA 3.0.
42 4 The Internet


1. SYN: the active open is performed by the client sending an SYN to the server. The client sets the segment’s sequence number to a random value, x. 2. SYN-ACK: in response, the server replies with an SYN-ACK. The acknowledgment number is set to one more than the received sequence number, i.e., x + 1, and the sequence number that the server chooses for the packet is another random number, y. 3. ACK: finally, the client sends an ACK back to the server. The sequence number is set to the received acknowledgment value, i.e., x + 1, and the acknowledgment number is set to one more than the received sequence number, i.e., y + 1.
Even though development and progress on the Internet are achieved in a decentralized way, there is a need for governance and standardization. The organizations are the Internet Engineering Task Force (IETF), the Internet Corporation for Assigned Names and Numbers (ICANN), and the Internet Assigned Numbers Authority (IANA).
4 The Internet 43


Chapter 5 The World Wide Web
Many people use the Internet and WWW as synonyms. However, the Internet is an infrastructure to interconnect various heterogeneous networks, and the Web is one of the most important application layers on top of this infrastructure, the Internet provides. The World Wide Web (WWW or simply the Web) is “a system of interlinked, hypertext documents that runs over the Internet. With a Web browser, a user views Web pages that may contain text, images, and other multimedia and navigates between them using hyperlinks.”1 Its essence is easy to understand: it combines a hypertext infrastructure with the Internet. The end of the 1980s and the beginning of the 1990s marked an important point that shaped developments in artificial intelligence (AI), the Internet, databases, information retrieval, hypertext, and natural language processing. The World Wide Web was invented as a combination of Internet and hypertext systems by Sir Tim Berners-Lee. The amount of content, data, and services published on the Web rapidly came to an enormous size within the following decades. In the Web, links can refer to content stored at external computers and systems in extension to traditional hypertext systems. The Web is built on many technologies, but the following are arguably at the center:
• A common naming and addressing schema: Uniform Resource Identifiers (URIs) described in Request for Comments (RFC) 3986.2 • A common protocol for accessing resources and exchanging their various representations based on the REST principles: Hypertext Transfer Protocol (HTTP) described in RFC 2616.3 • A markup language for allowing applications like Web browsers to render hypermedia content: Hypertext Markup Language (HTML). The first versions
1 http://en.wikipedia.org/wiki/World_Wide_Web 2 https://en.wikipedia.org/wiki/Uniform_Resource_Identifier 3 https://www.rfc-editor.org/rfc/rfc9110.html
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 U. Serles, D. Fensel, An Introduction to Knowledge Graphs, https://doi.org/10.1007/978-3-031-45256-7_5
45


were described in RFC 1866. Later versions were standardized by W3C and enriched toward XML.4
All these technologies are standardized by bodies like the Internet Engineering Task Force (IETF) and the World Wide Web Consortium (W3C). “The World Wide Web Consortium (W3C) is an international community where Member organizations, a full-time staff, and the public work together to develop Web standards.”5 The W3C aims for the worldwide availability of Web access and envisions Web progress in terms of interaction, data, services, and security through standardization. Comparable to IETF’s standardization through RFCs, W3C publishes reports that pass different maturity levels until they are officially recommended:
1. Working Draft (WD) 2. Candidate Recommendation (CR) 3. Proposed Recommendation (PR) 4. W3C Recommendation (REC)
5.1 Uniform Resource Identifiers (URIs)
The Web assumes that resources are anything we want to talk about. Uniform Resource Identifiers (URIs) denote (‘are names for’) these resources. Obviously, it is necessary to distinguish between the name of a thing (URI) and the thing itself (resource) and its representation (Fig. 5.1). A Uniform Resource Identifier (URI) is a string of characters used to identify a resource on the Internet. Such a URI can be a URL or a URN (see Fig. 5.2). A Uniform Resource Name (URN) defines an item’s identity: the URN urn: isbn:0-395-36,341-1 is a URI that specifies the identifier system, i.e., the International Standard Book Number (ISBN), as well as the unique reference within that system, and allows one to talk about a book but does not suggest where and how to obtain an actual copy of it. A Uniform Resource Locator (URL) provides a location and a method for finding the resource. For example, the URL http://www.sti-innsbruck.at/ identifies a resource (STI’s home page) and implies that a representation of that resource (such as the home page’s current HTML code as encoded characters) is obtainable via HTTP from a network host named www.sti-innsbruck.at.6 The URI syntax has the following high-level structure:7
URI = scheme ":" path [ "?" query ] [ "#" fragment ]
4 https://www.w3.org/XML/ 5 https://www.w3.org/Consortium 6Adapted from https://wiki.eclipse.org/File_URI_Slashes_issue 7 https://www.ietf.org/rfc/rfc3986.txt
46 5 The World Wide Web


5.2 REST and HTTP
The Web is based on a simple and robust architecture, i.e., the Representational State Transfer (REST), which is the theoretical foundation for Web architecture principles (Fielding 2000). Requests and responses are based on the transfer of “representations” of “resources.” A simple, stateless, and uniform protocol to access information chunks, i.e., the Hypertext Transfer Protocol (HTTP) for client/server
Fig. 5.1 The distinction between a URI, the thing itself, and its representation (https://www.w3. org/wiki/HttpUrisAreExpensive Copyright © 2010 World Wide Web Consortium. https://www. w3.org/Consortium/Legal/2023/doc-license)
Fig. 5.2 URIs, URLs, and URN and their relationship (Image by David Torres, published under CC-BY-SA 3.0 license https://commons.wikimedia.org/wiki/File:URI_Euler_Diagram_no_lone_ URIs.svg)
5.2 REST and HTTP 47


communication, implements this architecture. Therefore, HTTP is a good example of a REST-style implementation; however, it should not be confused with REST. REST is a style of software architecture for distributed systems such as the World Wide Web providing a client/server architectural style (Fig. 5.3).8 Requirements supported by REST-enabled systems stem from the requirements addressed by any system following the Web architecture (Fielding and Taylor 2002):
• Simplicity: low-entry barrier, rapid Web Application Programming Interface (API) adoption • Extensibility: fostering easy growth. • Distributed hypermedia: depends on the well-established notions of hypermedia. • Scalability at the Web level: should rely on technologies/protocols that allow scaling on a large distributed system like the Web. • Independent deployment: old and new solutions must be able to exist together.
Information is organized in the form of resources that correspond to specific information and are referenced with a global identifier (e.g., a URI in HTTP). Components of the network (user agents and origin servers) communicate via a standardized protocol (e.g., HTTP) by exchanging resource representations (e.g., a JSON object representing the resource). Any number of connectors (e.g., clients, servers, caches, tunnels, etc.) can mediate the request, but each does so without knowing anything other than their own request. An application only needs to know the following about a resource to interact with it: its identifier, representation formats, and operation needed to be taken. The key assumption is stateless communication for scalability and reliability. The server does not store any client state
Fig. 5.3 The architectural style of the Internet and the Web (Image adapted from https://www.w3. org/People/Frystyk/thesis/WWW.html)
8 https://www.w3.org/People/Frystyk/thesis/WWW.html
48 5 The World Wide Web


between requests. Each request from a client contains all the necessary information to respond to the request. Any state about the interaction is stored in the client. The Hypertext Transfer Protocol (HTTP) relies on the URI naming mechanism and provides a protocol for client/server communication. It is a very simple request/ response protocol where the client sends a request message, and the server replies with a response message providing a way to publish and retrieve, e.g., HTML pages. Following the REST architecture, it is stateless. An HTTP request consists of an HTTP request method (e.g., GET, PUT, POST, DELETE), the request URI, and HTTP protocol version information, optionally a list of HTTP headers consisting of name/value pairs, and optionally a message body. An HTTP response consists of an HTTP protocol version information and an HTTP status code, optionally a list of HTTP headers consisting of name/value pairs, and optionally a message body.
5.3 HTML and XML
The Hypertext Markup Language (HTML)9 can be used to encode hypertext documents. In 1995, HTML 2.0 was specified as an IETF RFC 1866 (http://tools.ietf.org/ html/rfc1866). The next version, HTML 3.2, reached a W3C recommendation status in 1997. HTML 5 was initiated in 2004, and ultimately, it was finalized within the W3C process (http://www.w3.org/TR/html5/). It facilitates a hypermedia environment. Documents use elements to “markup” or identify sections of content for different purposes or display characteristics. HTML markup consists of several types of entities, including elements, attributes, data types, and character references. Markup elements are not seen by the user when a page is displayed. The documents are rendered by browsers that interpret HTML. Hereby, a user agent (Web browser) uses links to enable users to navigate to other pages and to display additional information. The eXtensible Markup Language (XML) is a language for creating other languages (dialects) and data exchange on the Web. For example, HTML can be expressed in XML as XHTML. It can be used for describing structured and semistructured data. It is platform independent and has wide support providing interoperability. It is a W3C Recommendation (standard). The structure of XML documents consists of elements, attributes, and content. It has one root element in a document. Characters and child elements form the content. An XML element has the following syntax: <name>contents</name> with <name> is called the opening tag, and </name> is called the closing tag. Element names are case sensitive.
9Adapted from https://en.wikipedia.org/wiki/HTML
5.3 HTML and XML 49


Example. XML tags
<gender>Female</gender> <story>Once upon a time there was. . . . </story>
XML can add attributes to XML elements. The syntax is <name attribute_name = “attribute_value”> contents</name>. Values are surrounded by single or double quotes.10
Example. XML attributes
<temperature unit="F">48</temperature> <swearword language='fr'>con</swearword>
XML namespaces allow combined content from various XML sources. In general, documents use different vocabularies. Assume a scenario where the data from a store about products and an e-commerce system about product orders will be integrated. Merging multiple documents together can lead to name collisions. For example, the XML documents describing products and customers can both have <name> XML elements. Namespaces provide a solution for name collision. Namespaces are a syntactic way to differentiate similar names in an XML document by bundling them using Uniform Resource Identifiers (URI), e.g., http://example.com/NS, which can be bound to a named or “default” prefix. The binding syntax uses the “xmlns” attribute to define a namespace.
• Named prefix: <a:foo xmlns:a = “http://example.com/NS”/>. • Default prefix: <foo xmlns = “http://example.com/NS”/>. • Element and attribute names are prefix – local part (or “local name”) pairs, e.g., “http://example.com/NS”, becomes “foo.”
Example. XML namespaces
<?xml version=‘1.0’ encoding=‘UTF-8’?> <order> <item code=‘BK3’> <name>FLATLAND: A Romance of Many Dimensions</name> <desc xmlns:html=‘http://www.w3.org/1999/xhtml’> The <html:b>best</html:b> book ever! </desc> </item> </order>
10Using XML attributes was the first way on the Web to include semantic information describing the content by the system Ontobroker (Fensel et al. 1998). It was called HTML-A as it was already applied to the HTML tag attribute Finally with RDFa this approach became a W3C standard, https:// www.w3.org/MarkUp/2004/rdf-a
50 5 The World Wide Web


XML schema specifies to describe the elements in an XML document formally and is a W3C Recommendation. It can define a structure for XML documents and the names used by its XML schema language. Then an XML document can be verified according to these definitions. The main feature of an XML schema is the definition of types. An XML schema offers a set of built-in datatypes. They can be primitive datatypes (e.g., xs:anyURI, xs:boolean, xs:date, xs:double); simple types, which are typically derived from primitive datatypes; and complex types, which define the structure of elements. In general, a datatype can be derived from another datatype.
Example. A simple type called MyInteger that restricts the primitive integer type to the (1234, 5678] range <xsd:simpleType name=‘MyInteger’> <xsd:restriction base=‘xsd:integer’> <xsd:minExclusive value=‘1234’/> <xsd:maxInclusive value=‘5678’/> </xsd:restriction> </xsd:simpleType>
Example. A complex type of “person” <xs:complexType name="person"> <xs:sequence> <xs:element name="firstname" type="xs:string"/> <xs:element name="lastname" type="xs:string"/> <xs:element name="age" type="xs:integer"/> </xs:sequence> </xs:complexType>
5.4 Is XML Schema an Ontology Language?
XML schema was developed as a means to specify a structure for documents (on the Web) (Klein et al. 2001). Therefore, it has a very specific inheritance relationship. A subdocument can refine but also extend the range of document properties. The subdocument is more specific. On one hand, it could provide less properties for describing the document, which is fine in terms of set-based inheritance. One the other hand, it may offer additional ways (ranges) to existing properties. This breaks any set-based interpretation of inheritance because an instance of a subdocument class is not a proper instance of its superdocument classes anymore. But it may make sense for documents. See the following example of inheritance as a means to extend a range definition:
5.4 Is XML Schema an Ontology Language? 51


Example. The broaderPerson type is a restriction of the person type. It uses a different range for the “age” element <xs:complexType name="person"> <xs:sequence> <xs:element name="firstname" type="xs:string"/> <xs:element name="lastname" type="xs:string"/> <xs:element name="age" type="xs:integer"/> </xs:sequence> </xs:complexType> <xs:complexType name="broaderPerson"> <xs:complexContent> <xs:restriction base="person"> <xs:sequence> <xs:element name="firstname" type="xs:string"/> <xs:element name="lastname" type="xs:string"/> <xs:element name="age" type="extendedAge"/> </xs:sequence> </xs:restriction> </xs:complexContent> </xs:complexType> <xs:simpleType name="extendedAge"> <xs:union memberTypes="xs:integer xs:string" /> </xs:simpleType>
The range of age in the subtype is broader than the range of its supertype. The age of broaderPerson allows integer and string; person allows integer only. This means the documents of type broaderPerson violate the definition of person, and an instance of type broaderPerson is not necessarily an instance of person. This is a violation of set-based inheritance, where a subset must be a real subset of the superset (see Fig. 5.4).
Fig. 5.4 Set-based inheritance (left) and its violation (right). XML violates the set-based interpretation of inheritance since the ranges of the properties on a subtype can be broader than the ones of the supertype
52 5 The World Wide Web


References
Fensel D, Decker S, Erdmann M, Studer R (1998) Ontobroker: the very high idea. In: Proceedings of the eleventh international florida artificial intelligence research society conference, May 18–20, 1998, AAAI Press, Sanibel Island, FL, pp 131–135 Fielding RT (2000) REST: architectural styles and the design of network-based software architectures. Doctoral dissertation, University of California Fielding RT, Taylor RN (2002) Principled design of the modern web architecture. ACM Transactions on Internet Technology (TOIT) 2(2):115–150 Klein M, Fensel D, Van Harmelen F, Horrocks I (2001) The relation between ontologies and XML schemas. Electronic Transactions on Artificial Intelligence (ETAI) 6(4)
References 53


Chapter 6 Natural Language Processing
Information retrieval (IR) deals with the retrieval of relevant information sources (typically documents) given a query. Information extraction (IE) (Pazienza 1997) goes one step further to obtain information from unstructured sources (e.g., documents). It is mostly concerned with processing unstructured text to “understand” the content and extract data following a certain structure. Natural Language Processing (NLP) is a field that addresses the challenge of processing, analyzing, and understanding natural language (Clark et al. 2012). Naturally, it plays an important role in information extraction, given the definition above. Actually, one of the first uses of the term “knowledge graph” was in the context of NLP. In the PhD thesis of Bakker (Bakker 1987), knowledge graphs were referred to as a way of structuring and representing knowledge extracted from a scientific text (see also the content about semantic nets in Chap. 2) (Maynard et al. 2016). Summing up, the major difference between information retrieval and NLP is that IR retrieves documents for a human to inspect, whereas NLP dives into the content of documents and tries to answer human queries directly. An NLP application such as information extraction typically deals with three main tasks (Maynard et al. 2016)1:
• Linguistic processing • Named entity recognition (NER), and • Relation extraction
A pipeline of low-level linguistic tasks that prepare a given text for the next steps is provided in Fig. 6.1.
Tokenization It is the task of splitting an input text into atomic units called tokens. Tokens generally correspond to words, numbers, and symbols typically separated
1The explanations regarding the given NLP pipeline are mainly based on Maynard et al. (2016). See there for a more detailed description.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 U. Serles, D. Fensel, An Introduction to Knowledge Graphs, https://doi.org/10.1007/978-3-031-45256-7_6
55


with whitespace. Tokenization is typically the first step in any linguistic processing pipeline since more complex steps use tokens as input.
Sentence Splitting It is the task of separating a text into sentences. The main challenge is to decide if punctuation is at the end of a sentence or has another purpose. For instance, sentence splitters typically benefit from a list of abbreviations to decide if a full stop is at the end of a sentence or an abbreviation like Ms.
Part-of-Speech (POS) Tagging It is the task of labeling words with their linguistic categories, also known as part-of-speech (e.g., noun, verb). Several tag classifications exist, like Penn Treebank (PTB), Brown Corpus, and Lancaster-Oslo/Bergen Corpus.
Morphological Analysis and Stemming2 Morphological analysis is the task of identifying and classifying the linguistic units of a word. For example, the verb “talked” consists of a root “talk” and a suffix “-ed.” The stemming task strips a word of all its suffixes.
Parsing/Chunking It is the task of building the syntactic structure of sentences given a grammar (e.g., which verb connects to which nouns) and building a parse tree. Parsing shows how different parts of a sentence are related to each other. Parsers give very granular information about the words in a sentence; however, they may be computationally very expensive. These preprocessing steps are needed to support higher-level tasks such as named entity recognition and relation extraction.
Named Entity Recognition Based on this linguistic processing, named entity recognition (NER) can be provided. NER is an annotation task helping to identify the semantics of people, organizations, locations, dates, and times mentioned in the text (e.g., Nelson Mandela, Amazon, New York . . .). The information obtained from the linguistic processing is valuable at this stage, e.g., nouns are good candidates for named entities. Ambiguity is a typical challenge, i.e., “Amazon” can refer to a rainforest or an organization. Gazetteer lookup is a step involving a simple lookup from a list of known entities. Again, gazetteer lookup can be ambiguous. London can be a location but also part of an organization. Combined with the result of POS tagging, the ambiguity may be solved relatively more easily. Some rule-based grammar matching can be combined with gazetteer lookup to improve effectiveness, e.g., a pattern might tell that “University of” is followed by a city. Finally, the coreference resolution aims to identify coreferences between named entities, e.g., the
Fig. 6.1 The linguistic processing pipeline (Figure adapted from Maynard et al. (2016))
2This step may look significantly different for different languages.
56 6 Natural Language Processing


linking of pronouns: “I voted for Trump, because he was the best candidate,” John said.
Relation Extraction It is typically considered a slot-filling task. Given a relation schema, what are the relations between named entities in the text? Such a relation schema can be based on an ontology. A relation extractor takes as input an annotated text with named entities, relationship occurrences, and linguistic features extracted by preprocessing for training, as well as testing instances for prediction. Its output is extracted relations.
As evident from the proposal of the Dartmouth AI workshop (McCarthy et al. 2006), one of the major parts of the AI vision was to enable computers to understand and generate natural language. This had also implications on information retrieval, particularly for answering (controlled) natural language queries based on unstructured text. SIR, a computer program for semantic information retrieval by Bertram Raphael,3 was developed in 1964 (Raphael 1964). It was one of the first programs with “understanding” capabilities. It could learn word associations and property lists based on conversations in a controlled form of the English language. It could then answer questions based on the knowledge it gained from those conversations.
6.1 An Example: The GATE System
We introduce the General Architecture for Text Engineering (GATE),4 which is a Java suite of tools, as an example of an NLP tool set. It was originally developed at the University of Sheffield beginning in 1995 and is now used worldwide by a wide community of scientists, companies, teachers, and students. It can be used for many natural language processing tasks, including information extraction in many languages.5 It became commercialized by Ontotext/Sirma via KIM.6
GATE Tokenizer The tokenizer splits the text into tokens such as numbers, punctuations, and different types of words; see Fig. 6.2.7
GATE Sentence Splitter The splitter annotates the sentences in a text. It uses a gazetteer list of abbreviations to determine whether a punctuation marks a sentence or has another purpose. In Fig. 6.3, the text is split into two sentences.
3 https://en.wikipedia.org/wiki/Bertram_Raphael 4 https://gate.ac.uk/
5https://en.wikipedia.org/wiki/General_Architecture_for_Text_Engineering, content distributed under CC-BY-SA 3.0 6The content is based on the documentation in https://gate.ac.uk/sale/tao/splitch6.html#x9-1190006 7The example used in this figure and all the upcoming figures is from https://www.bbc.com/news/ world-europe-65215576
6.1 An Example: The GATE System 57


GATE Part-of-Speech Tagger (POS-Tagger) It annotates tokens in the tokenized text with a part-of-speech tag. Figure 6.4 shows the tagged tokens (annotated with the start- and end-character positions). The “category” feature specifies the POS-Tag.
GATE Gazetteer The gazetteer provides a list of entity names that are used to match named entities in the text. A gazetteer lookup can be seen as a preliminary named entity recognition. Each gazetteer list represents a set of names, such as names of cities, organizations, and so on. In Fig. 6.5, we use the default GATE gazetteer. The token Evan is tagged as a male person.
GATE Semantic Tagger The semantic tagger is a named entity recognition module that uses certain rules on previously annotated text to make more advanced named entity recognition in comparison to the gazetteer-based step. This module can
Fig. 6.2 The GATE tokenizer
Fig. 6.3 The GATE sentence splitter. Two sentences are marked on the bottom of the figure with their start and end characters
58 6 Natural Language Processing


recognize entities like people, organizations, locations, monetary amounts, dates, percentages, and some types of addresses (see Fig. 6.6).
GATE – OrthoMatcher (Orthographic Coreference) This module creates identity relations between the recognized named entities to perform a coreference resolution (e.g., the mention of the same name at multiple places in the text or the usage of aliases and synonyms). Figure 6.7 shows that the OrthoMatcher matches the occurrences of “Wall Street Journal” and “WSJ.”
GATE—Pronominal Coreference Resolution This module also does coreference resolutions like the OrthoMatcher, but it links named entities and the pronouns
Fig. 6.4 The GATE POS-tagger
Fig. 6.5 The GATE gazetteer results
6.1 An Example: The GATE System 59


referring to them. Figure 6.8 shows an excerpt from the results of this module. The pronoun his is matched with the named entity Evan Gershkovich.
Almost all GATE components can be configured extensively with new rules and grammar. A plug-in system gives flexibility for NLP pipeline development. For example, the OntoGazetteer plugin can be used in the NER task to match entities with classes in an ontology.
Fig. 6.6 The GATE—NER module recognizing people and organizations
Fig. 6.7 The GATE— OrthoMatcher module recognizing coreferences
60 6 Natural Language Processing


References
Bakker R (1987) Knowledge graphs: representation and structuring of scientific knowledge (doctoral dissertation). University Twente Clark A, Fox C, Lappin S (eds) (2012) The handbook of computational linguistics and natural language processing, vol 118. Wiley Maynard D, Bontcheva K, Augenstein I (2016) Natural language processing for the semantic web. In: Ding Y, Groth P (eds) Synthesis lectures on the semantic web: Theory and technology, vol 15. Morgan & Claypool Publishers, pp 1–184 McCarthy J, Minsky ML, Rochester N, Shannon CE (2006) A proposal for the dart-mouth summer research project on artificial intelligence, August 31, 1955. AI Magazine 27(4):12–12 Pazienza MT (ed) (1997) Information extraction: A multidisciplinary approach to an emerging information technology, vol LNAI 1299. Springer Raphael B (1964) SIR: a computer program for semantic information retrieval. Doctoral Dissertation, MIT
Fig. 6.8 The GATE—pronominal coreference resolution module recognizing pronouns
References 61


Chapter 7 Semantic Web: Or AI Revisited
When we left AI, we were talking about the so-called AI winter. Building knowledge-based systems was too costly to justify their construction and usage besides in some niche applications. However, the world did not stop rotating. With the Web, a new, extremely large information source appeared. And the content was provided for free by a worldwide crowd activity. It just required turning this information into machine-understandable knowledge by adding semantics. Then the so-called knowledge acquisition bottleneck would be a memory from the far past. And precisely, this happened with the Semantic Web. The Semantic Web started in 1996 for two reasons. First was the dramatic growth of the World Wide Web. The semantic annotations of content were used to improve search by applying machine-understandable semantics. Figure 7.1 shows an early initiative (Fensel et al. 1997; Benjamins et al. 1999) and system (Fensel et al. 1998) using the semantic annotation of Web content based on an ontology to support information retrieval and extraction, i.e., direct query answering. The Web was designed to bring a piece of information to a human user. The user has to manually extract and interpret the provided information and may need to integrate it with information from other sources. This can turn into a huge human effort, especially when the information comes from different Web sites and needs to be carefully and properly aligned. For example, I want to travel from Innsbruck to Rome, where I want to stay in a hotel and visit the city. Many Web sites have to be visited, manually checked, aligned, and backtracked when one aspect provides a bottleneck. By adding semantics, a virtual agent can do all these tasks on behalf of the human user. It can also automatically provide links to additional information, e.g., this image is about Innsbruck, Dieter Fensel is a professor, etc. Syntactic structures are converted to knowledge structures (Fig. 7.2). Obviously, one could argue that platforms such as Booking.com provide such a service for traveling based on backend integration, too. However, then you leave the
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 U. Serles, D. Fensel, An Introduction to Knowledge Graphs, https://doi.org/10.1007/978-3-031-45256-7_7
63


open Web and deal with a large provider through a single Web site, i.e., you are locked into a walled garden.1 The second reason to work on the Semantic Web (mostly for researchers) was to solve the knowledge acquisition bottleneck. The vision of the Semantic Web was to build a brain of/for humankind (Fensel and Musen 2001). Billions of humans put information on this global network. Through this, the Web mirrors large fractions of human knowledge. Empowered by semantics, computers can access and understand this knowledge. The knowledge acquisition problem would be solved when the entire humanity would join this task for free. Like annotating content with structural info such as HTML, it just requires annotating content with semantic information. The second half of the 1990s witnessed the initial efforts to allow the Web to scale by enabling machines to consume it (see Fensel et al. 2005):
Fig. 7.1 The knowledge acquisition initiative
Fig. 7.2 Semantic enrichment of content
1Tim Berners-Lee Warns of ‘Walled Gardens’ for Mobile Internet, https://archive.nytimes.com/ www.nytimes.com/idg/IDG_002570DE00740E1800257394004818F5.html
64 7 Semantic Web: Or AI Revisited


• SHOE (Heflin et al. 2003) is an early system for semantically annotating Web content in a distributed manner. • Ontobroker and On2Broker (Fensel et al. 1998, 1999) are an architecture for consuming distributed and heterogeneous semi-structured sources on the Web (Fig. 7.3).
With ontologies at its center, On2Broker consists of four main components: a query interface, a repository, an info agent, and an inference engine. The Query Engine provides a structured input interface that enables users to define their queries. Input queries are then transformed into the query language (e.g., SPARQL). The DB Manager decouples query answering, information retrieval, and reasoning and provides support for the materialization of inferred knowledge. The Info Agent extracts knowledge from different distributed and heterogeneous data sources. HTML-A2 pages and Resource Description Framework (RDF) repositories can be included directly. HTML and XML data sources require processing provided by wrappers to derive RDF data. The Inference Engine relies on knowledge imported from the crawlers and axioms contained in the repository to support advanced query answering (Fensel et al. 1999). These early academic systems triggered a significant standardization effort by the W3C to develop common standards for building such Semantic Web systems. The Semantic Web Stack (Fig. 7.4) has been developed that contains a set of specifications that enables the Semantic Web based on existing Web recommendations. Core
Fig. 7.3 The architecture of On2broker from a bird’s-eye view (Fensel et al. 1999)
2A predecessor of RDFa.
7 Semantic Web: Or AI Revisited 65


technologies are RDF as a data model, SPARQL as a query language, and RDFS and OWL as ontology languages.
The Resource Description Framework (RDF) provides a triple-based data model for exchanging (meta)data on the Web. Resources can be identified with Internationalized Resource Identifiers (IRIs). Shared IRIs provide the means for linking resources and forming a directed labeled graph (Fig. 7.5).
RDF Schema (RDFS) and the Web Ontology Language (OWL)3 are ontology languages for defining the meaning of RDF data. RDFS provides the means for defining type and property taxonomies and domains/ranges for properties. OWL extends RDFS with universal and existential quantifiers; inverse, functional, and
Fig. 7.4 The W3C
Semantic Web layer cake (Image by Mhermans, distributed under CC-BYSA license https://commons. wikimedia.org/wiki/File: SW_layercake_2006.svg)
subject object predicate
Fig. 7.5 The RDF data model
3Meanwhile further developed to OWL2. https://www.w3.org/TR/owl2-primer/
66 7 Semantic Web: Or AI Revisited


transitive properties and cardinality restrictions; and more using description logic as underlying logical formalism. SPARQL is a query language for manipulating RDF(S) data. It resembles the Structured Query Language (SQL), which is used for relational databases. It mainly works by matching graph patterns to the triples in a graph (Fig. 7.6). Still, it took longer than expected before these techniques took over the Web at full scale. Google was excellent at finding information based on simple syntactic means, and semantic annotations did not seem to be necessary at all. However, this changed drastically when search engines (finding links people follow and immediately leaving the search page) tried to turn into query-answering engines that want to engage with their users. Here, formal understanding and, therefore, semantics are a must. Suddenly, large search engines turned from strong opponents into enthusiastic supporters of the Semantic Web. Schema.org was started in 2011 by Bing, Google, Yahoo!, and Yandex to provide vocabularies for annotating Web sites. Meanwhile, it has become a de facto standard for annotating content and data on the Web with around 797 types, 1457 properties, 14 datatypes, 86 enumerations, and 462 enumeration members (in March 2023). It provides a corpus of types (e.g., LocalBusiness, SkiResort, Restaurant) organized hierarchically), properties (e.g., name, description, address), range restrictions (e.g., Text, URL, PostalAddress), and enumeration values (e.g., DayOfWeek, EventStatusType, ItemAvailability) and covers a large number of different domains. The use of semantic annotations has experienced a tremendous surge in activity since the introduction of schema.org. According to WebDataCommons, 50% of all crawled Web pages contain annotations (Fig. 7.7).
SELECT ?event ?name WHERE { ?event :name ?name . ?event :location ?location . ?location :city :Innsbruck . }
Fig. 7.6 An excerpt from a simple SPARQL query that returns the event instance identifiers and their names for events in Innsbruck
7 Semantic Web: Or AI Revisited 67


References
Benjamins VR, Fensel D, Decker S, Perez AG (1999) KA2: building ontologies for the internet: a mid-term report. International Journal of Human-Computer Studies 51(3):687–712 Fensel D, Musen MA (2001) The semantic web: a brain for humankind. IEEE Intell Syst 16(2): 24–25 Fensel D, Erdmann M, Studer R (1997) Ontology groups: semantically enriched subnets of the www. In: Proceedings of the 1st international workshop intelligent information integration during the 21st German annual conference on artificial intelligence, Freiburg, September Fensel D, Decker S, Erdmann M, Studer R (1998) Ontobroker: the very high idea. In: Proceedings of the eleventh international Florida artificial intelligence research society conference, May 18–20, 1998, Sanibel Island, FL, AAAI Press, pp 131–135 Fensel D, Angele J, Decker S, Erdmann M, Schnurr H, Staab S, Studer R, Witt A (1999) On2broker: Semantic-based access to information sources at the WWW. In: Proceedings of the IJCAI-99 workshop on intelligent information integration, held on July 31, 1999, in conjunction with the sixteenth international joint conference on Artificial Intelligence City Conference Center, Stockholm, CEUR-WS.org, CEUR Workshop Proceedings, vol 23. https://ceur-ws.org/Vol-23/fensel-ijcai99-iii.pdf Fensel D, Hendler JA, Lieberman H (eds) (2005) Spinning the Semantic Web: bringing the World Wide Web to its full potential, Paperback edition. MIT Press Heflin J, Hendler JA, Luke S (2003) SHOE: a blueprint for the semantic web, pp 29–63. In: Fensel et al. (2005)
Fig. 7.7 WebDataCommons statistics from October 2022 (http://webdatacommons.org/ structureddata/2022-12/stats/stats.html)
68 7 Semantic Web: Or AI Revisited


Chapter 8 Databases
While the AI community was being built in the 1960s, there have been important developments on the data side of things, too.1 NASA needed a system to keep track of parts and suppliers for rocket parts in the Apollo project. So they developed a hierarchical file system for it. Then IBM built a database based on this file system, called the Information Management System (IMS),2 in 1966. Hierarchical models store data in a tree-like form. Around the same time, Charles Bachman from General Electric came up with the network model and implemented it in the Integrated Data Store (IDS) (Bachman 2009), which stores data in a graph-like form. Both the hierarchical model and network model had some crucial characteristics. Both databases were navigational. Programs were accessing database tuple at a time. This means writing many nested loops. In consequence, the performance of the query is in the hands of the programmer. For example, IMS even did not have any abstraction of the storage implementation. You need to write different loops for different data structures. This coupling opened the possibility for accessing data very efficiently but also meant that a slight change in the database structure required the applications to be reprogrammed. A mathematician at IBM named Edgar F. Codd saw this situation and wanted to achieve data independence. In 1970, Codd produced a model that decouples a logical representation of a database from its implementation (physical vs logical). He created the relational model for databases (Codd 1990). Data are stored in simple data structures (relations), and operations on the database are not at the tuple level but at the relation level. This allows for working on many tuples at once. Operations are formalized with relational algebra, which is based on set operations and can be queried with a high-level declarative language. The query language SQL (aka SEQUEL) has become an American National Standards Institute (ANSI)
1The content of this chapter is partially based on (Pavlo 2020). 2 https://www.ibm.com/docs/en/zos-basic-skills?topic=now-history-ims-beginnings-nasa
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 U. Serles, D. Fensel, An Introduction to Knowledge Graphs, https://doi.org/10.1007/978-3-031-45256-7_8
69


standard, and the relational model dominated the field instead of navigational models like hierarchical and network-based models. The relational model stores data in tuples in a structure called relation (table). A tuple is a partial function that maps attribute names to values, and a relation consists of a header (a finite set of attribute names (columns)) and a body (a set of tuples that all have a header as their domain; see Fig. 8.1).3 However, there were always alternative developments in this area. Meanwhile, object-oriented programming was becoming popular, and developers realized an issue between the object-oriented paradigm and the relational model. Actually, there is a relation-object mismatch: complex objects are not straightforwardly stored in a relational database, e.g., a customer with multiple phone numbers can be represented with a customer object with an array of phone numbers. In relational databases, this would ideally require two relations. For object-oriented databases, queries can be written natively with the programming language of the application, most of the time at the expense of declarative querying. Object-oriented databases4 were able to store more complex data models with the help of features like the native definition of custom types and inheritance. Deductive databases (Ullman and Zaniolo 1990; Ramakrishnan and Ullman 1995) tried to provide logic as an access layer for databases quite in parallel to the developments of knowledge representations on the AI side. As a result of the efforts in that direction, the Datalog language was created (Ceri et al. 1989). It is aligned with the relational model formalism but is still more expressive. Later, SQL implemented some features of Datalog-like recursion. Datalog is a subset
Fig. 8.1 The relational model (Adapted from https://en.wikipedia.org/wiki/Relational_model)
3Based on https://en.wikipedia.org/wiki/Relational_model, licensed under CC-BY-SA 4 https://en.wikipedia.org/wiki/Object_database
70 8 Databases


of Prolog: expressivity traded off for computational efficiency. Pure Datalog has no function symbols and no negation. Without function symbols and negation, a Datalog program always guarantees to terminate. Also, it is fully declarative, i.e., the order of statements does not matter as it does for Prolog. Finally, it has efficient query optimization methods like Magic sets (Bancilhon et al. 1985). The strong connection between object-oriented databases and procedural languages was hindering data independence. F-logic (Kifer and Lausen 1989) emerged as a way to build deductive object-oriented databases combining object orientation with logical languages. F-logic is a declarative language for creating, accessing, and manipulating object-oriented database (OODB) schemas and provides a higherorder knowledge representation formalism that combines features of frames and object-oriented languages. F-logic supports overriding in inheritance and “statements about properties”5 (in F-logic, they are referred to as attributes); properties are defined locally on classes and have closed-world and unique name assumptions. Although it was initially targeting databases, it also has many use cases in AI, especially in frame-based systems. F-logic and description logic (DL) represent two camps in knowledge representation (de Bruijn et al. 2005). We have already covered that description logic uses the open-world assumption and does not have a unique name assumption.
• F-logic adopts the closed-world assumption: the train timetable example does not require an explicit statement of nonexisting connection as the facts that do not exist are considered false. • F-logic adopts the unique name assumption: remember the example from description logic – if the same individual is married to more than one thing, DL says these things are the same (unless they are explicitly stated as different). F-logic says there is an inconsistency.
The relational model has certain limitations for working with knowledge graphs. Relational databases have rigid schemas, which are good for data quality and optimization (query and storage) but work poorly for integrating heterogeneous and dynamic sources. Let us view some approaches on how to host a knowledge graph with a relation database (Ben Mahria et al. 2021):
• The most straightforward approach is using a statement table. The graph is stored in one table with three columns (subject, predicate, object). • The property-class table provides one table for each type. • Vertical partitioning provides one table per property. • Finally, one can provide virtual RDF graphs over a relational database. It is a relatively popular way to convert relational databases to knowledge graphs because it allows the integration of the knowledge graph into already-existing IT environments.
5Without moving into Second Order Logic semantically.
8 Databases 71


However, each of these approaches comes with various issues, which we will discuss in more detail in Chap. 19 on knowledge graph hosting. With rapidly growing and dynamic data, traditional relational model solutions reached similar limitations, e.g., for geospatial data, graphical data, the Internet of Things (IoT), social networks, etc. Big tech companies like Amazon, eBay, Facebook, and Google developed their own ad hoc solutions to scale up. This showed that for many modern cases, answering user queries fast is more important than ensuring immediate consistency/integrity. NoSQL solutions have appeared (e.g., document stores, key-value databases, graph databases). The typical features of such databases are not having rigid schemas, nonrelational models, and mostly custom application programming interfaces (APIs) for data access and manipulation. In the context of knowledge graphs, one important NoSQL data model is the graph model (Angles and Gutierrez 2008). Various graph data models have been around for a long time, but as a commercial success, two models seem to have won the race:
• RDF graph databases that support directed labeled edge models (e.g., GraphDB) • Property graphs that take edges as first-class citizens (e.g., Neo4j)
RDF databases (aka triple stores) have been around longer as a result of the Semantic Web effort and are much more standardized than property graphs. For example, GraphDB6 is an enterprise-level graph database with RDF and SPARQL support. Like many other RDF graph databases, it has three main functionalities: storage, querying, and reasoning. It supports Named Graphs (Carroll et al. 2005) and RDF-Star (Hartig and Champin 2021) for reification and various indexing mechanisms, including adapters for external indexing systems like Lucene and Elasticsearch. A strength is its customizable and modular reasoning support. It supports various rule sets for reasoning with different expressivity and complexity:
• Standard RDFS. • RDFS+: RDFS extended with symmetric and transitive properties. • Various OWL dialects: OWL-Lite, OWL-QL, OWL-RL, and OWL Horst (ter Horst 2005). • Customized rule sets for reasoning can be defined. • Finally, constraint checking with SHACL is provided.
Summarizing the discussion, there are two main options. The first is using a virtual graph on top of a well-established relational database. This brings the advantage that the handling of the knowledge graph is directly integrated into the existing standard infrastructure. However, access and manipulation require mappings to an ontology. Such mappings can be complex, and many things can go wrong. Also, the work with the graph is limited as no built-in reasoning is provided. The second option is to use an RDF repository. They are directly customized for the
6 https://graphdb.ontotext.com/
72 8 Databases


data model and semantics of a knowledge graph and provide interchangeability by relying solely on open W3C standards.
References
Angles R, Gutierrez C (2008) Survey of graph database models. ACM Computing Surveys (CSUR) 40(1):1–39 Bachman CW (2009) The origin of the integrated data store (IDS): the first direct-access DBMS. IEEE Ann Hist Comput 31(4):42–54 Bancilhon F, Maier D, Sagiv Y, Ullman JD (1985) Magic sets and other strange ways to implement logic programs. In: Proceedings of the fifth ACM SIGACT-SIGMOD symposium on Principles of database systems, Cambridge, MA, March 24–26, pp 1–15 Ben Mahria B, Chaker I, Zahi A (2021) An empirical study on the evaluation of the RDF storage systems. Journal of Big Data 8:1–20 Carroll JJ, Bizer C, Hayes P, Stickler P (2005) Named graphs. J Web Semantics 3(4):247–267 Ceri S, Gottlob G, Tanca L et al (1989) What you always wanted to know about Datalog (and never dared to ask). IEEE Trans Knowl Data Eng 1(1):146–166 Codd EF (1990) The relational model for database management: version 2. Addison-Wesley Longman Publishing Co., Inc. De Bruijn J, Lara R, Polleres A, Fensel D (2005) Owl dl vs owl flight: conceptual modeling and reasoning for the semantic web. In: Proceedings of the 14th international conference on World Wide Web, Chiba, May 10–14, pp 623–632 Hartig O, Champin PA (2021) Metadata for RDF statements: the RDF-Star approach. Lotico Kifer M, Lausen G (1989) F-logic: a higher-order language for reasoning about objects, inheritance and schema. In: SIGMOD/PODS04: international conference on management of data and symposium on principles database and systems, Portland, OR, June 1, pp 134–146 Pavlo A (2020) 01 - History of databases (CMU databases/Spring 2020). https://www.youtube. com/watch?v=SdW5RKUboKc Ramakrishnan R, Ullman JD (1995) A survey of deductive database systems. J Log Program 23(2): 125–149 ter Horst HJ (2005) Combining RDF and part of OWL with rules: semantics, decidability, complexity. In: The Semantic Web–ISWC 2005: 4th International Semantic Web Conference, ISWC 2005, Galway, Ireland, November 6–10, 2005. Proceedings 4, Springer, pp 668–684 Ullman JD, Zaniolo C (1990) Deductive databases: achievements and future directions. ACM SIGMOD Rec 19(4):75–82
References 73


Chapter 9 Web of Data
We will introduce the Linked Data concept and its extension to Linked Open Data.
9.1 Linked Data
Starting around 2006, Linked Data is a set of principles to publish interlinked data on the Web, less focused on the complex formalisms to describe the data. It is mostly an adaptation of the traditional Web principles extending them from content to data. How do we extend this Web of documents into a Web of Data? Typically, the data are published in an isolated fashion, for example, embedded into Web pages or Resource Description Framework (RDF) datasets. Assume that one data silo contains movies, another one contains reviews, and, again, another one contains actors. Many common things are represented in multiple datasets. Linking identifiers (i.e., Uniform Resource Identifiers (URIs)) would link these datasets (Bizer et al. 2008). The Web of Data is envisioned as a global database consisting of objects and their descriptions; objects are linked with each other and have a high degree of object structure with explicit semantics for links and content. Finally, it should be designed for humans and machines (Bizer et al. 2008) and was defined by Tim Berners-Lee in 2006 with the aim of providing a unified method for describing and accessing resources (Käfer 2020):
The Semantic Web isn’t just about putting data on the Web. It is about making links, so that a person or machine can explore the Web of Data. With Linked Data, when you have some of it, you can find other, related data.—Sir Tim Berners-Lee
Recall the traveling example used in Chap. 7 to illustrate the need for automatic data integration. Data integration involves combining data residing in different sources and providing users with a unified view of these data. Data integration over the Web can be implemented as follows (Herman 2012):
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024 U. Serles, D. Fensel, An Introduction to Knowledge Graphs, https://doi.org/10.1007/978-3-031-45256-7_9
75


• Export the datasets to be integrated as RDF graphs. • Merge identical resources (i.e., resources having the same URI) from different datasets. • Start making queries on the integrated data, queries that were not possible on the individual datasets.
Linked Data should follow certain principles:1,2
1. Uniform Resource Identifiers (URIs) should be used to name and identify individual things. 2. Hypertext Transfer Protocol (HTTP) URIs should be used to allow these things to be looked up, interpreted, and subsequently “dereferenced.”3 3. Useful information about what a name identifies should be provided through open standards such as RDF. 4. When publishing data on the Web, other things should be referred to using their HTTP URI-based names.
Principle 1: Use URIs as names for things This principle implies the use of URIs not only for documents but also for any resource inside RDF graphs. This allows for addressing a unique resource while exchanging data. For example, https://www. wikidata.org/wiki/Q1735 is a URI for the city of Innsbruck in Austria, and so is https://dbpedia.org/resource/Innsbruck.
Principle 2: Use HTTP URIs so that users can look up those names The users can be both humans and automated agents. The server delivers a suitable representation of the requested resource via HTTP with the help of content negotiation (Fig. 9.1). A resource can be anything, an image, a document, a person. . . An HTTP URI (more precisely a URL) both identifies and locates a resource and also specifies how to access it.
Principle 3: When someone looks up a URI, provide useful information using the following standards RDF is the standard data model for both Semantic Web and Linked Data. The provided information via a URI should be in RDF. There are
1http://www.w3.org/DesignIssues/LinkedData.html See also Linked Data Platform 1.0. W3C Recommendation 1.0. 2See also a nice tutorial about Linked Data and Knowledge Graphs in Käfer (2020).
3Note that this principle implies that URLs must be used for identifying resources. The main purpose of URLs is to “locate” resources and determine how they should be accessed (i.e., via a protocol like HTTP, FTP. . .). Using URLs for both identification and access may be problematic, as the way we access things may change over time. A good example of such a scenario is the shift from HTTP to HTTPS, which may mean that many published resources on the Semantic Web become inaccessible. The “same” URI with HTTP and HTTPs can locate the same resource but are not inherently treated as the same identifier. See also a discussion about this in the Semantic Web mailing list: https://lists.w3.org/Archives/Public/semantic-web/2023Jun/0028.html
76 9 Web of Data


various serializations for RDF, such as RDF/XML, Turtle, Notation 3, and JSON LD.
Principle 4: Include links to other URIs Resources across datasets must be linked. In principle, we can talk about three different kinds of links (Heath and Bizer 2011):
• Relationship links: links to external entities related to the original entity, e.g., <https://elias.kaerle.com> schema:knows <https://umutcanserles.com>
• Identity links: links to external entities referring to the same object or concept, e.g., dbpedia:Innsbruck owl:sameAs wikidata:Innsbruck, e.g., dbpedia: Innsbruck skos:exactMatch http://globalwordnet.org/ili/i83317
• Vocabulary links: links to definitions of the original entity, e.g., dbo:City owl: equivalentClass schema:City
In principle, any property could be used to create links across datasets; there are some properties commonly used for various linking purposes:
• owl:sameAs—for connecting individuals
• owl:equivalentClass—for connecting classes
• rdfs:seeAlso—provides additional information about resources
Fig. 9.1 Linked data principle 2 (Human icon by freepik. https://www. flaticon.com/free-icon/ computer_44357)
9.1 Linked Data 77


• skos:closeMatch, skos:narrowMatch, skos:broaderMatch, skos:relatedMatch, and skos:exactMatch—for linking “concepts” across datasets with weak formal semantics.4
Linked Open Data is an extension to Linked Data principles, which will be discussed next.
9.2 Linked Open Data
Linked Data is not necessarily open. The definition of Linked Data has been extended, through the openness principle by Tim Berners Lee, thereby coining the term Linked Open Data in 2010. It aims to encourage institutions (especially governments) to provide “good” Linked Data:
Linked Open Data (LOD) is Linked Data which is released under an open licence, which does not impede its reuse for free.—Sir Tim Berners-Lee5
Linked Open Data essentially implies that the information must be published with an open license. An example of an open license is Creative Commons CC-BY. For calling them Linked Open Data, a few more aspects should be considered. These aspects are provided on a scale of 5 stars (Fig. 9.2):6
Fig. 9.2 The 5-star linked open data rating scheme (https://5stardata.info)
4See Sect. 13.4 about Simple Knowledge Organization System (SKOS). 5 https://www.w3.org/DesignIssues/LinkedData.html 6 https://5stardata.info/
78 9 Web of Data


• 1-star: Make your data available on the Web in some format under an open license. • 2-star: Make it available as structured data (e.g., Excel instead of PDF). • 3-star: Make it available in a nonproprietary open format (e.g., CSV instead of Excel). • 4-star: Use URIs to denote things so that people can point at your data. • 5-star: Link your data to other data to provide context.
In the following, we will discuss a possible process for publishing Linked Open Data.
LOD: A Linked Data Publication Scenario in Seven Steps (Heath and Bizer 2011)
1. Select vocabularies. Important hereby is the reuse of existing vocabularies to increase the value of your dataset and align your own vocabularies to increase interoperability. 2. Partition the RDF graph into “data pages” (for example, a data page can contain the RDF representation of a specific person). 3. Assign a URI to each data page. 4. Create Hypertext Markup Language (HTML) variants of each data page (to allow the rendering of pages in browsers). It is important to set up content negotiation between RDF and HTML versions. 5. Assign a URI to each entity (cf. “Cool URIs for the Semantic Web”). 6. Add page metadata. It is important to make data pages understandable for consumers, i.e., add metadata such as publisher, license, topics, etc. 7. Add a semantic sitemap.
Data need to be prepared (e.g., extracted from text), links and the proper usage of URIs need to be defined, and these data need to get stored in an appropriate storage system and published as data on the Web (see Fig. 9.3 for an architecture for publishing Linked Data on the Web) (Heath and Bizer 2011). For example, the RDF graph in Fig. 9.4 contains information about the book “The Glass Palace” by Amitav Ghosh.7 Information about the same book, but in French this time, is modeled in the RDF graph in Fig. 9.5. We can merge identical resources (i.e., resources having the same URI) from different datasets (Figs. 9.6 and 9.7). Finally, one can make queries on the integrated data. A user of the second dataset may ask queries like “give me the title of the original book.” This information is not in the second dataset. However, this information can be retrieved from the integrated dataset, in which the second dataset is connected with the first dataset (Herman 2012). For example, DBpedia Mobile (Becker and Bizer 2009) combines maps on mobile devices with information about places from DBpedia, pictures from Flickr, reviews from Revyu, etc.8
7Figures 9.4, 9.5, 9.6, and 9.7 are taken from W3C (Herman 2012)—last accessed on 04.04.2023. The content is distributed under CC-BY 3.0. 8Herman (2012) http://wiki.dbpedia.org/DBpediaMobile
9.2 Linked Open Data 79


Starting from the early years of development, special browsers, data mashups, and search engines have been developed for consuming Linked Data (Heath and Bizer 2011):
• Linked Data browsers: to explore things and datasets and to navigate between them, several browsers have been developed. For example, the Tabulator Browser (MIT, USA) (Berners-Lee et al. 2006), Marbles (FU Berlin, DE),9 and OpenLink RDF Browser10 (OpenLink, UK) have been developed. • Linked Data mashups: sites that mash up (thus combine) Linked Data were developed, for example, Revyu.com (KMI, UK), DBpedia Mobile (Becker and
Fig. 9.3 An architecture for publishing linked open data (Figure adapted from Heath and Bizer (2011))
9 https://mes.github.io/marbles/ 10 https://www.w3.org/wiki/OpenLinkDataExplorer
80 9 Web of Data


Bizer 2009) (FU Berlin, DE), and Semantic Web Pipes (Le-Phuoc et al. 2009) (DERI, Ireland).
• Search engines for searching Linked Data: examples are Falcons (Cheng and Qu 2009) (IWS, China), Sindice (Tummarello et al. 2007) (DERI, Ireland), and Swoogle (Ding et al. 2004) (UMBC, USA).
In May 2020, the LOD cloud contained 1300 datasets containing subclouds in the following categories (Fig. 9.8):
Fig. 9.4 The RDF graph contains information about the book “The Glass Palace” by Amitav Ghosh
Fig. 9.5 The same book is modeled in a French dataset
9.2 Linked Open Data 81


• Cross-domain: e.g., DBpedia, WebIsALOD, Linked Open Colors • Geography: weather stations, Smart Points of Interest • Government: UNESCO statistics, Italian immigration statistics • Life Science: several BioPortal and Bio2RDF datasets • Linguistics: WordNet, Ontologies of Linguistic Annotation • Others
Fig. 9.6 Two datasets with identical resources
Fig. 9.7 Merging identical resources from Figs. 9.4 and 9.5
82 9 Web of Data


These interconnected datasets of Linked Data are the predecessor and enabler for knowledge graphs.
References
Becker C, Bizer C (2009) Exploring the geospatial semantic web with DBpedia mobile. Journal of Web Semantics 7(4):278–286 Berners-Lee T, Chen Y, Chilton L, Connolly D, Dhanaraj R, Hollenbach J, Lerer A, Sheets D (2006) Tabulator: exploring and analyzing linked data on the semantic web. In: Proceedings of the 3rd international semantic web user interaction workshop, Athens, November 6, vol 2006, p 159 Bizer C, Heath T, Berners-Lee T (2008) Linked data: principles and State of the Art. 17th International World Wide Web Conference W3C Track @ WWW2008, Beijing, China, April 23–24
Fig. 9.8 The linked open data cloud from May 2020 (Image by https://lod-cloud.net/, distributed under CC-BY)
References 83


Cheng G, Qu Y (2009) Searching linked objects with falcons: approach, implementation and evaluation. International Journal on Semantic Web and Information Systems (IJSWIS) 5(3): 49–70 Ding L, Finin T, Joshi A, Pan R, Cost RS, Peng Y, Reddivari P, Doshi V, Sachs J (2004) Swoogle: a search and metadata engine for the semantic web. In: Proceedings of the 13th ACM international conference on information and knowledge management, Washington, DC, November 8–13, pp 652–659 Heath T, Bizer C (2011) Linked data: evolving the web into a global data space. Synthesis lectures on the semantic web: theory and technology, vol 1(1), pp 1–136 Herman I (2012) SW tutorial. http://www.w3.org/People/Ivan/CorePresentations/SWTutorial Käfer T (2020) Distributed knowledge graphs: knowledge graphs and linked data, AI4Industry summer school. https://ai4industry.sciencesconf.org/data/DistributedKnowledgeGraphsPt.1.pdf Le-Phuoc D, Polleres A, Hauswirth M, Tummarello G, Morbidoni C (2009) Rapid prototyping of semantic mashups through semantic web pipes. In: Proceedings of the 18th international conference on World Wide Web, Madrid, April 20–24, pp 581–590 Tummarello G, Delbru R, Oren E (2007) Sindice. com: Weaving the open linked data. In: The Semantic Web: 6th international semantic web conference, 2nd Asian semantic web conference, ISWC 2007+ ASWC 2007, Busan, November 11–15, 2007. Proceedings, Springer, pp 552–565
84 9 Web of Data