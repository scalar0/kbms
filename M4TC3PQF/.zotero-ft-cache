C
M
&
Morgan Claypool Publishers
&
SYNTHESIS LECTURES ON ARTIF ICIAL
INTELLIGENCE AND MACHINE LEARNING
Ronald J. Brachman, William W. Cohen, and Peter Stone, Series Editors
A Concise Introduction to Models and Methods for Automated Planning
Hector Geffner Blai Bonet
C
M
&
Morgan Claypool Publishers
&
SYNTHESIS LECTURES ON ARTIF ICIAL
INTELLIGENCE AND MACHINE LEARNING
Ronald J. Brachman, William W. Cohen, and Peter Stone, Series Editors
A Concise Introduction to Models and Methods for Automated Planning
Hector Geffner Blai Bonet
C
M
&
Morgan Claypool Publishers
&
SYNTHESIS LECTURES ON ARTIF ICIAL
INTELLIGENCE AND MACHINE LEARNING
Ronald J. Brachman, William W. Cohen, and Peter Stone, Series Editors
A Concise Introduction to Models and Methods for Automated Planning
Hector Geffner Blai Bonet




A Concise Introduction to
Models and Methods for
Automated Planning




Synthesis Lectures on Artificial Intelligence and Machine Learning
Editor
Ronald J. Brachman, Yahoo! Labs
William W. Cohen, Carnegie Mellon University Peter Stone, University of Texas at Austin
A Concise Introduction to Models and Methods for Automated Planning Hector Geffner and Blai Bonet 2013
Essential Principles for Autonomous Robotics Henry Hexmoor 2013
Case-Based Reasoning: A Concise Introduction Beatriz López 2013
Answer Set Solving in Practice Martin Gebser, Roland Kaminski, Benjamin Kaufmann, and Torsten Schaub 2012
Planning with Markov Decision Processes: An AI Perspective Mausam and Andrey Kolobov 2012
Active Learning Burr Settles 2012
Computational Aspects of Cooperative Game eory Georgios Chalkiadakis, Edith Elkind, and Michael Wooldridge 2011


iv
Representations and Techniques for 3D Object Recognition and Scene Interpretation Derek Hoiem and Silvio Savarese 2011
A Short Introduction to Preferences: Between Artificial Intelligence and Social Choice Francesca Rossi, Kristen Brent Venable, and Toby Walsh 2011
Human Computation Edith Law and Luis von Ahn 2011
Trading Agents Michael P. Wellman 2011
Visual Object Recognition Kristen Grauman and Bastian Leibe 2011
Learning with Support Vector Machines Colin Campbell and Yiming Ying 2011
Algorithms for Reinforcement Learning Csaba Szepesvári 2010
Data Integration: e Relational Logic Approach Michael Genesereth 2010
Markov Logic: An Interface Layer for Artificial Intelligence Pedro Domingos and Daniel Lowd 2009
Introduction to Semi-Supervised Learning XiaojinZhu and Andrew B.Goldberg 2009
Action Programming Languages Michael ielscher 2008
Representation Discovery using Harmonic Analysis Sridhar Mahadevan 2008


v
Essentials of Game eory: A Concise Multidisciplinary Introduction Kevin Leyton-Brown and Yoav Shoham 2008
A Concise Introduction to Multiagent Systems and Distributed Artificial Intelligence Nikos Vlassis 2007
Intelligent Autonomous Robotics: A Robot Soccer Case Study Peter Stone 2007


Copyright © 2013 by Morgan & Claypool
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or by any means—electronic, mechanical, photocopy, recording, or any other except for brief quotations in printed reviews, without the prior permission of the publisher.
A Concise Introduction to Models and Methods for Automated Planning
Hector Geffner and Blai Bonet
www.morganclaypool.com
ISBN: 9781608459698 paperback ISBN: 9781608459704 ebook
DOI 10.2200/S00513ED1V01Y201306AIM022
A Publication in the Morgan & Claypool Publishers series
SYNTHESIS LECTURES ON ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING
Lecture #22 Series Editors: Ronald J. Brachman, Yahoo! Labs
William W. Cohen, Carnegie Mellon University Peter Stone, University of Texas at Austin Series ISSN Synthesis Lectures on Artificial Intelligence and Machine Learning Print 1939-4608 Electronic 1939-4616


A Concise Introduction to
Models and Methods for
Automated Planning
Hector Geffner
ICREA and Universitat Pompeu Fabra, Barcelona, Spain
Blai Bonet
Universidad Simón Bolívar, Caracas, Venezuela
SYNTHESIS LECTURES ON ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING #22
C
&M Morgan cLaypool publishers
&


ABSTRACT
Planning is the model-based approach to autonomous behavior where the agent behavior is derived automatically from a model of the actions, sensors, and goals. e main challenges in planning are computational as all models, whether featuring uncertainty and feedback or not, are intractable in the worst case when represented in compact form. In this book, we look at a variety of models used in AI planning, and at the methods that have been developed for solving them. e goal is to provide a modern and coherent view of planning that is precise, concise, and mostly self-contained, without being shallow. For this, we make no attempt at covering the whole variety of planning approaches, ideas, and applications, and focus on the essentials. e target audience of the book are students and researchers interested in autonomous behavior and planning from an AI, engineering, or cognitive science perspective.
KEYWORDS
planning, autonomous behavior, model-based control, plan generation and recognition, MDP and POMDP planning, planning with incomplete information and sensing, action selection, belief tracking, domain-independent problem solving


ix
Contents
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi
1 Planning and Autonomous Behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.1 Autonomous Behavior: Hardwired, Learned, and Model-based . . . . . . . . . . . . . . . . . 1 1.2 Planning Models and Languages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.3 Generality, Complexity, and Scalability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.4 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.5 Generalized Planning: Plans vs. General Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . 11 1.6 History . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2 Classical Planning: Full Information and Deterministic Actions . . . . . . . . . . . . 15
2.1 Classical Planning Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.2 Classical Planning as Path Finding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2.3 Search Algorithms: Blind and Heuristic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2.4 Online Search: inking and Acting Interleaved . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 2.5 Where do Heuristics come from? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 2.6 Languages for Classical Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 2.7 Domain-Independent Heuristics and Relaxations . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 2.8 Heuristic Search Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 2.9 Decomposition and Goal Serialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 2.10 Structure, Width, and Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
3 Classical Planning: Variations and Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
3.1 Relaxed Plans and Helpful Actions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 3.2 Multi-Queue Best-First Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 3.3 Implicit Subgoals: Landmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 3.4 State-of-the-Art Classical Planners . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 3.5 Optimal Planning and Admissible Heuristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 3.6 Branching Schemes and Problem Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 3.7 Regression Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 3.8 Planning as SAT and Constraint Satisfaction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 3.9 Partial-Order Causal Link Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 3.10 Cost, Metric, and Temporal Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 3.11 Hierarchical Task Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49


x
4 Beyond Classical Planning: Transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
4.1 Soft Goals and Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 4.2 Incomplete Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 4.3 Plan and Goal Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 4.4 Finite-State Controllers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 4.5 Temporally Extended Goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
5 Planning with Sensing: Logical Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
5.1 Model and Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 5.2 Solutions and Solution Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 5.3 Offline Solution Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 5.4 Online Solution Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 5.5 Belief Tracking: Width and Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 5.6 Strong vs. Strong Cyclic Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
6 MDP Planning: Stochastic Actions and Full Feedback . . . . . . . . . . . . . . . . . . . . 79
6.1 Goal, Shortest-Path, and Discounted Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 6.2 Dynamic Programming Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 6.3 Heuristic Search Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 6.4 Online MDP Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 6.5 Reinforcement Learning, Model-based RL, and Planning . . . . . . . . . . . . . . . . . . . . 95
7 POMDP Planning: Stochastic Actions and Partial Feedback . . . . . . . . . . . . . . . 97
7.1 Goal, Shortest-Path, and Discounted POMDPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97 7.2 Exact Offline Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 7.3 Approximate and Online Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102 7.4 Belief Tracking in POMDPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 7.5 Other MDP and POMDP Solution Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
8 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
8.1 Challenges and Open Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109 8.2 Planning, Scalability, and Cognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
Author’s Biography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129


xi
Preface
Planning is a central area in Artificial Intelligence concerned with the automated generation of behavior for achieving goals. Planning is also one of the oldest areas in AI with the General Problem Solver being the first automated planner and one of the first AI programs [Newell et al., 1959]. As other areas in AI, planning has changed a great deal in recent years, becoming more rigorous, more empirical, and more diverse. Planners are currently seen as automated solvers for precise classes of mathematical models represented in compact form, that range from those where the state of the environment is fully known and actions have deterministic effects, to those where the state of the environment is partially observable and actions have stochastic effects. In all cases, the derivation of the agent behavior from the model is computational intractable, and hence a central challenge in planning is scalability. Planning methods must exploit the structure of the given problems, and their performance is assessed empirically, often in the context of planning competitions that in recent years have played an important role in the area. In this book, we look at a variety of models used in AI planning and at the methods that have been developed for solving them. e goal is to provide a modern and coherent view of planning that is precise, concise, and mostly self-contained, without being shallow. For this, we focus on the essentials and make no attempt at covering the whole variety of planning approaches, ideas, and applications. Moreover, our view of the essentials is not neutral, having chosen to emphasize the ideas that we find most basic in a model-based setting. A more comprehensive treatment of planning, circa 2004, can be found in the planning textbook by Ghallab et al. [2004]. Planning is also covered at length in the AI textbook by Russell and Norvig [2009]. e book is organized into eight chapters. Chapter 1 is about planning as the model-based approach to autonomous behavior in contrast to appproaches where behaviors are learned, evolved, or specified by hand. Chapters 2 and 3 are about the most basic model in planning, classical planning, where a goal must be reached from a fully known initial state by applying actions with deterministic effects. Classical planners can currently find solutions to problems over huge state spaces, yet many problems do not comply with these restrictions. e rest of the book addresses such problems in two ways: one is by automatically translating non-classical problems into classical ones; the other is by defining native planners for richer models. Chapter 4 focuses thus on reductions for dealing with soft goals, temporally extended goals, incomplete information, and a slightly different task: goal recognition. Chapter 5 is about planning with incomplete information and partial observability in a logical setting where uncertainty is represented by sets of states. Chapters 6 and 7 cover probabilistic planning where actions have stochastic effects, and the state is either fully or partially observable. In all cases, we distinguish between offline solution methods that derive the complete control offline, and online solution methods that derive the control as needed, by interleaving planning and execution, thinking and doing. Chapter 8 is about open problems. We are grateful to many colleagues, co-authors, teachers, and students. Among our teachers, we would like to mention Judea Pearl, who was the Ph.D. advisor of both of us at different times, and always a role model as a person and as a scientist. Among our students, we thank in particular Hector


xii PREFACE
Palacios, Emil Keyder, Alex Albore, Miquel Ramírez, and Nir Lipovetzky, on whose work we have drawn for this book. e book is based on tutorials and courses on planning that one of us (Hector) has been giving over the last few years, more recently at the ICAPS Summer School (essaloniki, 2009; São Paulo, 2012; Perugia, 2013), the International Joint Conference on AI (IJCAI, Barcelona, 2011), La Sapienza, Università di Roma (2010), and the Universitat Pompeu Fabra (2012). We thank the students for the feedback and our colleagues for the invitations and their hospitality. anks also to Alan Fern who provided useful and encouraging feedback on a first draft of the book.
A book, even if it is a short one, is always a good excuse for remembering the loved ones.
A los chicos, caminante no hay camino, a Lito, la llama eterna, a Marina, mucho más que dos, a la familia toda; a la memoria del viejo, la vieja, la bobe, y los compañeros tan queridos – Hector
A Iker y Natalia, por toda su ayuda y amor, a la familia toda, por su apoyo. A la memoria de Josefina Gorgal Caamaño y la iaia Francisca Prat – Blai
Hector Geffner, Barcelona Blai Bonet, Caracas June 2013


1
CHAPTER 1
Planning and Autonomous Behavior
Planning is the model-based approach to autonomous behavior where the agent selects the action to do next using a model of how actions and sensors work, what is the current situation, and what is the goal to be achieved. In this chapter, we contrast programming, learning, and model-based approaches to autonomous behavior, and present some of the models in planning that will be considered in more detail in the following chapters. ese models are all general in the sense that they are not bound to specific problems or domains. is generality is intimately tied to the notion of intelligence which requires the ability to deal with new problems. e price for generality is computational: planning over these models when represented in compact form is intractable in the worst case. A main challenge in planning is thus the automated exploitation of problem structure for scaling up to large and meaningful instances that cannot be handled by brute force methods.
1.1 AUTONOMOUS BEHAVIOR: HARDWIRED, LEARNED, AND MODEL-BASED
At the center of the problem of intelligent behavior is the problem of selecting the action to do next. In Artificial Intelligence (AI), three different approaches have been used to address this problem. In the programming-based approach, the controller that prescribes the action to do next is given by the programmer, usually in a suitable high-level language. In this approach, the problem is solved by the programmer in his head, and the solution is expressed as a program or as a collection of rules or behaviors. In the learning-based approach, the controller is not given by a programmer but is induced from experience as in reinforcement learning. Finally, in the model-based approach, the controller is not learned from experience but is derived automatically from a model of the actions, sensors, and goals. In all these approaches, the controller is the solution to the model. e three approaches to the action selection problem are not orthogonal, and exhibit different virtues and limitations. Programming agents by hand, puts all the burden on the programmer that cannot anticipate all possible contingencies, and often results in systems that are brittle. Learning methods have the greatest promise and potential, but their flexibility is often the result of learning a model. Last, model-based methods require a model of the actions, sensors, and goals, and face the computational problem of solving the model—a problem that is computationally intractable even for the simplest models where information is complete and actions are deterministic. e Wumpus game, shown in Figure 1.1 from the standard AI textbook [Russell and Norvig, 2009], is an example of a simple scenario where an agent must process information arriving from the sensors to decide what to do at each step. e agent, initially at the lower left corner, must obtain the gold while avoiding deadly pits and a killer wumpus. e locations of the gold, pits, and wumpus are


2 1. PLANNING AND AUTONOMOUS BEHAVIOR
1-"//*/( "/% "650/0.064 #&)"7*03
PIT
PIT
PIT
Breeze
Breeze
Breeze
Breeze
Breeze
Breeze
Stench
Stench
Stench
'JHVSF "VUPOPNPVT #FIBWJPS JO UIF 8VNQVT 8PSME 8IBU UP EP OFYU
OPU LOPXO UP UIF BHFOU CVU FBDI FNJUT B TJHOBM UIBU DBO CF QFSDFJWFE CZ UIF BHFOU XIFO JO UIF TBNF DFMM HPME PS JO B DPOUJHVPVT DFMM QJUT BOE XVNQVT ɩF BHFOU DPOUSPM NVTU TQFDJGZ UIF BDUJPO UP CF EPOF CZ UIF BHFOU BT B GVODUJPO PG UIF PCTFSWBUJPOT HBUIFSFE ɩF UISFF CBTJD BQQSPBDIFT GPS PCUBJOJOH TVDI B DPOUSPMMFS BSF UP XSJUF JU CZ IBOE UP MFBSO JU GSPN JOUFSBDUJPOT XJUI B 8VNQVT TJNVMBUPS PS UP EFSJWF JU GSPN B NPEFM SFQSFTFOUJOH UIF JOJUJBM TJUVBUJPO UIF BDUJPOT UIF TFOTPST BOE UIF HPBMT 8IJMF QMBOOJOH JT PGUFO EFmOFE BT UIF CSBODI PG "* DPODFSOFE XJUI UIF iTZOUIFTJT PG QMBOT PG BDUJPO UP BDIJFWF HPBMTw QMBOOJOH JT CFTU DPODFJWFE BT UIF NPEFM CBTFE BQQSPBDI UP BDUJPO TFMFDUJPO B WJFX UIBU EFmOFT NPSF DMFBSMZ UIF SPMF PG QMBOOJOH JO JOUFMMJHFOU BVUPOPNPVT TZTUFNT ɩF EJTUJODUJPO UIBU UIF QIJMPTPQIFS %BOJFM %FOOFUU NBLFT CFUXFFO A%BSXJOJBO A4LJOOFSJBO BOE A1PQQFSJBO DSFB UVSFT <%FOOFUU > NJSSPST RVJUF DMPTFMZ UIF EJTUJODUJPO CFUXFFO IBSEXJSFE QSPHSBNNFE BHFOUT BHFOUT UIBU MFBSO BOE BHFOUT UIBU VTF NPEFMT SFTQFDUJWFMZ ɩF DPOUSBTU CFUXFFO UIF mSTU BOE UIF MBU UFS DPSSFTQPOET BMTP UP UIF EJTUJODUJPO NBEF JO "* CFUXFFO SFBDUJWF BOE EFMJCFSBUJWF TZTUFNT BT MPOH BT EFMJCFSBUJPO JT OPU SFEVDFE UP MPHJDBM SFBTPOJOH *OEFFE BT XF XJMM TFF UIF JOGFSFODFT DBQUVSFE CZ NPEFM CBTFE NFUIPET UIBU TDBMF VQ BSF OPU MPHJDBM CVU IFVSJTUJD BOE GPMMPX GSPN SFMBYBUJPOT BOE BQQSPYJNBUJPOT PG UIF QSPCMFN CFJOH TPMWFE
1-"//*/( *4 .0%&- #"4&% "650/0.064 #&)"7*03
.PEFM CBTFE BQQSPBDIFT UP UIF BDUJPO TFMFDUJPO QSPCMFN BSF NBEF VQ PG UISFF QBSUT UIF NPEFMT UIBU FYQSFTT UIF EZOBNJDT GFFECBDL BOE HPBMT PG UIF BHFOU UIF MBOHVBHFT UIBU FYQSFTT UIFTF NPEFMT JO DPNQBDU GPSN BOE UIF BMHPSJUINT UIBU VTF UIF SFQSFTFOUBUJPO PG UIF NPEFMT GPS HFOFSBUJOH UIF CFIBWJPS " SFQSFTFOUBUJPO PG UIF NPEFM GPS UIF 8VNQVT QSPCMFN GPS FYBNQMF XJMM GFBUVSF WBSJBCMFT GPS UIF MPDBUJPOT PG UIF BHFOU UIF HPME UIF XVNQVT UIF QJUT BOE B CPPMFBO WBSJBCMF GPS XIFUIFS UIF BHFOU JT BMJWF ɩF MPDBUJPO WBSJBCMFT DBO UBLF EJĊFSFOU WBMVFT DPSSFTQPOEJOH XJUI UIF DFMMT JO UIF HSJE FYDFQU GPS UIF HPME UIBU DBO BMTP CF IFME CZ UIF BHFOU BOE IFODF IBT QPTTJCMF WBMVFT " TUBUF GPS UIF QSPCMFN JT B WBMVBUJPO PWFS UIFTF TFWFO WBSJBCMFT ɩF OVNCFS PG QPTTJCMF TUBUFT JT UIVT
*G UIF OVNCFS PG QJUT BOE XVNQVT JT OPU LOPXO B QSJPSJ BO BMUFSOBUJWF SFQSFTFOUBUJPO XPVME CF OFFEFE XIFSF FBDI DFMM JO UIF HSJE XPVME DPOUBJO B XVNQVT B QJU PS OFJUIFS
Figure 1.1: Autonomous Behavior in the Wumpus World: What to do next?
not known to the agent, but each emits a signal that can be perceived by the agent when in the same cell (gold) or in a contiguous cell (pits and wumpus). e agent control must specify the action to be done by the agent as a function of the observations gathered. e three basic approaches for obtaining such a controller are to write it by hand, to learn it from interactions with a Wumpus simulator, or to derive it from a model representing the initial situation, the actions, the sensors, and the goals. While planning is often defined as the branch of AI concerned with the “synthesis of plans of action to achieve goals,” planning is best conceived as the model-based approach to action selection—a view that defines more clearly the role of planning in intelligent autonomous systems. e distinction that the philosopher Daniel Dennett makes between “Darwinian,” “Skinnerian,” and “Popperian” creatures [Dennett, 1996], mirrors quite closely the distinction between hardwired (programmed) agents, agents that learn, and agents that use models respectively. e contrast between the first and the latter corresponds also to the distinction made in AI between reactive and deliberative systems, as long as deliberation is not reduced to logical reasoning. Indeed, as we will see, the inferences captured by model-based methods that scale up are not logical but heuristic, and follow from relaxations and approximations of the problem being solved.
PLANNING IS MODEL-BASED AUTONOMOUS BEHAVIOR
Model-based approaches to the action selection problem are made up of three parts: the models that express the dynamics, feedback, and goals of the agent; the languages that express these models in compact form; and the algorithms that use the representation of the models for generating the behavior. A representation of the model for the Wumpus problem, for example, will feature variables for the locations of the agent, the gold, the wumpus, the pits, and a boolean variable for whether the agent is alive. e location variables can take 16 different values, corresponding with the cells in the 4 4 grid, except for the gold that can also be held by the agent and hence has 17 possible values.1 A state for the problem is a valuation over these seven variables. e number of possible states is thus 165 17 2, which is slightly more than 35 million. Initially, the agent is alive and knows its location
1If the number of pits and wumpus is not known a priori, an alternative representation would be needed where each cell in the grid would contain a wumpus, a pit, or neither.


1.2. PLANNING MODELS AND LANGUAGES 3
but not the value of the pit and wumpus variables. e state of the system is thus not fully observable. e agent gets partial knowledge about the hidden variables through each of its three sensors that relate the true but hidden state of the world with observable tokens. e agent receives the observation token “stench” in the states where the wumpus is in one of the (at most) four cells adjacent to the agent, the token “breeze” in the states where a pit is adjacent to the agent, and the token “bright” when the gold and the agent are in the same cell. e actions available to the agent are to move to an adjacent cell, and to pick up the gold if known to be in the same cell. e actions change the state of the system in the expected way, affecting the location of the agent or the location of the gold. Yet the agent dies if it enters a cell with a wumpus or a pit, and a dead agent cannot execute any of the actions, and hence cannot achieve the goal of getting the gold. In this problem, an intelligent agent should notice first that there is no wumpus or pit in cells .1; 2/ or .2; 1/ as there is no stench or breeze at the initial agent location .1; 1/. It is then safe to move either up or right. If it moves up, it’ll sense a stench at .1; 2/ and conclude that the wumpus is at either .1; 3/ or .2; 2/. Likewise, since it senses no breeze, it can conclude that neither of these cells contains a pit. e only safe move is then to get back to .1; 1/ where it can move safely to .2; 1/. From the sensed breeze, it can conclude that there is a pit at .3; 1/ or .2; 2/, or one pit at each, and from sensing no stench, that there is no wumpus at either .3; 1/ or .2; 2/. At this point, it should conclude that cell .2; 2/ is safe as it cannot contain either a wumpus or a pit. It should then move up to .2; 2/, from which the process of visiting new cells that are safe is repeated until the gold is found. Writing a program for solving any instance of the Wumpus domain, for any (solvable) initial situation and grid size, is interesting enough. Yet, the task in planning is quite different. We want a program that can take a representation of any problem exhibiting a certain mathematical structure, not limited to the Wumpus domain, and find a solution to it. A number of planning models will make these mathematical structures explicit. Other problems that have a number of features in common with the Wumpus domain include the familiar Battleship game or the popular PC game Minesweeper. ese are all problems where a goal is to be achieved by acting and sensing in a world where the state of the system, that may change or not, is partially observable. While a program that has been designed to play the Wumpus can be deemed as intelligent, a program that can play the Wumpus without having been designed specifically for it will be intelligent in a much broader sense. e first contains the recipes for playing the Wumpus; the latter contains “recipes” for playing an infinite collection of domains, known or unknown to the programmer, that share a general mathematical structure. e formulation of these mathematical structures and the general “recipes” for solving them is what planning is about.
1.2 PLANNING MODELS AND LANGUAGES
A wide range of models used in planning can be understood as variations of a basic state model featuring:
• a finite and discrete state space S,
• a known initial state s0 2 S,
• a non-empty set SG S of goal states,
• actions A.s/ A applicable in each state s 2 S,


4 1. PLANNING AND AUTONOMOUS BEHAVIOR
• a deterministic state transition function f .a; s/ such that s0 D f .a; s/ stands for the state resulting of applying action a in s, a 2 A.s/, and
• positive action costs c.a; s/.
is is the model underlying classical planning where it is normally assumed that action costs c.a; s/ do not depend on the state, and hence c.a; s/ D c.a/. A solution or plan in this model is a sequence of applicable actions that map the initial state into a goal state. More precisely, a plan D a0; : : : ; an 1 must generate a state sequence s0; : : : ; sn such that ai 2 A.si /, siC1 D f .ai ; si /, and sn 2 SG, for i D 0; : : : ; n 1. e cost of the plan is the sum of the action costs c.ai ; si /, and a plan is optimal if it has minimum cost over all plans. Classical planners accept a compact description of models of this form in languages featuring variables, where the states are the possible valuations of the variables. A classical plan D a0; : : : ; an represents an open-loop controller where the action to be done at time step i depends just on the step index i. e solution of models that accommodate uncertainty and feedback, produce closed-loop controllers where the action to be done at step i depends on the actions and observations collected up to that point. ese models can be obtained by relaxing the assumptions in the model above displayed in italics.
e model for partially observable planning, also called planning with sensing or contingent planning, is a variation of the classical model that features both uncertainty and feedback—namely, uncertainty about the initial and next possible state, and partial information about the current state of the system. Mathematically such a model can be expressed in terms of the following ingredients:
• a finite and discrete state space S,
• a non-empty set S0 of possible initial states, S0 S,
• a non-empty set SG S of goal states,
• a set of actions A.s/ A applicable in each state s 2 S,
• a non-deterministic state transition function F .a; s/ for s 2 S and a 2 A.s/, where F .a; s/ is non-empty and s00 2 F .a; s/ stands for the possible successor states of state s after action a is done, a 2 A.s/,
• a set of observation tokens O,
• a sensor model O.s; a/ O, where o 2 O.s; a/ means that token o may be observed in the (possibly hidden) state s if a was the last action done, and
• positive action costs c.a; s/.
In the model for the Wumpus problem, the state space S is given by the set of possible valuations over the problem variables, S0 is the set of states where the agent is initially alive and at location .1; 1/, SG is the set of states where the agent is holding the gold, and A stands for the actions of moving and picking up the gold, provided that the agent can’t leave the grid and can’t pick the gold if not in the same cell. Likewise, the state transitions F .a; s/ associated with these actions is deterministic, meaning that F .a; s/ contains a single state s0 so that jF .a; s/j D 1. e same is true for the sensor


1.2. PLANNING MODELS AND LANGUAGES 5
Planning Problem Planner Controller Environment
Figure 1.2: A planner takes a compact representation of a planning problem over a certain class of models (classical, conformant, contingent, MDP, POMDP) and automatically produces a controller. For fully and partially observable models, the controller is closed-loop, meaning that the action selected depends on the observations gathered. For non-observable models like classical and conformant planning, the controller is open-loop, meaning that it is a fixed action sequence.
model O.s; a/, which does not depend on a but just on the hidden state s. Namely, O contains nine observation tokens o, corresponding to the possible combinations of the three booleans stench, breeze, and bright, so that if s is a state where the agent is next to a pit and a wumpus but not in the same cell as the gold, then o 2 O.s; a/ iff o represents the combination where stench and breeze are true, and bright is false. e action costs for the problem, c.a; s/, can be all assumed to be 1, and in addition, no action can be done by the agent when he is not alive. A partially observable planner is a program that accepts compact descriptions of instances of the model above, like the one for the Wumpus, and automatically outputs the control (Figure 1.2). As we will see, planners come in two forms: offline and online. In the first case, the behavior specifies the agent response to each possible situation that may result; in the second case, the behavior just specifies the action to be done in the current situation. ese types of control, unlike the control that results in classical planning, are closed-loop: the actions selected usually depend on the observation tokens received. Offline solutions of partially observable problems are not fixed action sequences as in classical planning, as observations need to be taken into account for selecting actions. Mathematically, thus, these solutions are functions mapping the stream of past actions and observations into actions, or more conveniently, functions mapping belief states into actions. e belief state that results after a given stream of actions and observations represents the set of states that are deemed possible at that point, and due to the Markovian state-transition dynamics, it summarizes all the information about the past that is relevant for selecting the action to do next. Moreover, since the initial belief state b0 is given, corresponding to the set of possible initial states S0, a solution function , called usually the control policy, does not need to be defined over all possible beliefs, but just over the beliefs that can be produced from the actions determined by the policy from the initial belief state b0 and the observations that may result. Such partial policies can be represented by a directed graph rooted at b0, where nodes stand for belief states, edges stand for actions ai or observations oi , and the branches in the graph from b0, stand for the stream of actions and observations a0; o0; a1; o1; : : :, called executions, that are possible. e policy solves the problem when all these possible executions end up in belief states where the goal is true.2 e models above are said to be logical as they only encode and keep track of what is possible or not. In probabilistic models, on the other hand, each possibility is weighted by a probability measure. A probabilistic version of the partially observable model above can be obtained by replacing the set of possible initial states S0, the set of possible successor states F .a; s/, and the set of possible observation tokens O.s; a/, by probability distributions: a prior P .s/ on the states s 2 S0 that are initially possible,
2We will make this all formal and precise in Chapter 5.


6 1. PLANNING AND AUTONOMOUS BEHAVIOR
transition probabilities Pa.s0js/ for encoding the likelihood that s0 is the state that follows s after a, and observation probabilities Pa.ojs/ for encoding the likelihood that o is the token that results in the state s when a is the last action done. e model that results from changing the sets S0, F .a; s/, and O.s; a/ in the partially observable model, by the probability distributions P .s/, Pa.s0js/, and Pa.ojs/, is known as a Partially Observable Markov Decision Process or POMDP [Kaelbling et al., 1998]. e advantages of representing uncertainty by probabilities rather than sets is that one can then talk about the expected cost of a solution as opposed to the cost of the solution in the worst case. Indeed, there are many meaningful problems that have infinite cost in the worst case but perfectly well-defined expected costs. ese include, for example, the problem of preparing an omelette with an infinite collection of eggs that may be good or bad with non-zero probabilities, but that can be picked up and sensed one at a time. Indeed, while the scope of probabilistic models is larger than the scope of logical models, we will consider both, as the latter are simpler, and the computational ideas are not all that different. A fully observable model is a partially observable model where the state of the system is fully observable, i.e., where O D S and O.s; a/ D fsg. In the logical setting such models are known as Fully Observable Non-Deterministic models, abbreviated FOND. In the probabilistic setting, they are known as Fully Observable Markov Decision Processes or MDPs [Bertsekas, 1995].
Finally, an unobservable model is a partially observable model where no relevant information about the state of the system is available. is can be expressed through a sensor model O containing a single dummy token o that is “observed” in all states, i.e., O.s; a/ D O.s0; a/ D fog for all s, s0, and a. In planning, such models are known as conformant, and they are defined exactly like partially observable problems but with no sensor model. Since there are no (true) observations, the solution form of conformant planning problems is like the solution form of classical planning problems: a fixed action sequence. e difference between classical and conformant plans, however, is that the former must achieve the goal for the given initial state and unique state-transitions, while the latter must achieve the goal in spite of the uncertainty in the initial situation and dynamics, for any possible initial state and any state transition that is possible. As we will see, conformant problems make up an interesting stepping stone in the way from classical to partially observable planning. In the book, we will consider each of these models in turn, some useful special cases, and some variations. is variety of models is the result of several orthogonal dimensions: uncertainty in the initial system state (fully known or not), uncertainty in the system dynamics (deterministic or not), the type of feedback (full, partial or no state feedback), and whether uncertainty is represented by sets of states or probability distributions.
1.3 GENERALITY, COMPLEXITY, AND SCALABILITY
Classical planning, the simplest form of planning where actions have deterministic effects and the initial state is fully known, can be easily cast as a path-finding problem over a directed graph where the nodes are the states, the initial node and target nodes are the initial and goal states, and a directed edge between two nodes denotes the existence of an action that maps one state into the other. Classical planning problems can thus be solved in theory by standard path-finding algorithms such as Dijkstra’s that run in time that is polynomial in the number of nodes in the graph [Cormen et al., 2009, Dijkstra, 1959]. Yet in planning, this is not good enough as the nodes in the graph stand for the problem states, whose number is exponential in the number of problem variables. If these variables have at least two


1.3. GENERALITY, COMPLEXITY, AND SCALABILITY 7
(&/&3"-*5: $0.1-&9*5: "/% 4$"-"#*-*5:
A
BC
ABC
A
B
C
A
BC
··· ···
AB
C
AC
AB
BC
A
BC A
B
C AB
C
AB
C
A ABC
C
B
··· ··· ··· ··· ···
Init
Goal
'JHVSF ɩF HSBQI DPSSFTQPOEJOH UP B TJNQMF QMBOOJOH QSPCMFN JOWPMWJOH UISFF CMPDLT XJUI JOJUJBM BOE HPBM TJUVBUJPOT BT TIPXO ɩF BDUJPOT BMMPX UP NPWF B DMFBS CMPDL PO UPQ PG BOPUIFS DMFBS CMPDL PS UP UIF UBCMF ɩF TJ[F PG UIF DPNQMFUF HSBQI GPS UIJT EPNBJO JT FYQPOFOUJBM JO UIF OVNCFS PG CMPDLT " QMBO GPS UIF QSPCMFN JT TIPXO CZ UIF QBUI JO SFE
QPTTJCMF WBMVFT UIF OVNCFS PG OPEFT JO UIF HSBQI UP TFBSDI DBO CF JO UIF PSEFS PG XIFSF JT UIF OVNCFS PG WBSJBCMFT *O QBSUJDVMBS JG UIF QSPCMFN JOWPMWFT WBSJBCMFT UIJT NFBOT OPEFT BOE JG UIF QSPCMFN JOWPMWFT WBSJBCMFT JU NFBOT NPSF UIBO OPEFT *O PSEFS UP HFU B DPODSFUF JEFB PG XIBU FYQPOFOUJBM HSPXUI NFBOT JG JU UBLFT TFDPOE UP HFOFSBUF OPEFT B SFBMJTUJD FTUJNBUF HJWFO DVSSFOU UFDIOPMPHZ JU XPVME UBLF NPSF UIBO TFDPOET UP HFOFSBUF OPEFT ɩJT JT IPXFWFS BMNPTU POF NJMMJPO UJNFT UIF FTUJNBUFE BHF PG UIF VOJWFSTF " NPSF WJWJE JMMVTUSBUJPO PG UIF DPNQMFYJUZ JOIFSFOU UP UIF QMBOOJOH QSPCMFN DBO CF PCUBJOFE CZ DPOTJEFSJOH B XFMM LOPXO EPNBJO JO "* UIF #MPDLT 8PSME 'JHVSF TIPXT BO JOTUBODF PG UIJT EPNBJO XIFSF CMPDLT " # BOE $ JOJUJBMMZ BSSBOHFE TP UIBU " JT PO # BOE # BOE $ BSF PO UIF UBCMF NVTU CF SFBSSBOHFE TP UIBU # JT PO $ BOE $ JT PO " ɩF BDUJPOT BMMPX UP NPWF B DMFBS CMPDL B CMPDL XJUI OP CMPDL PO UPQ PO UPQ PG BOPUIFS DMFBS CMPDL PS PO UIF UBCMF ɩF QSPCMFN DBO CF FBTJMZ FYQSFTTFE BT B DMBTTJDBM QMBOOJOH QSPCMFN XIFSF UIF WBSJBCMFT BSF UIF CMPDL MPDBUJPOT CMPDLT DBO CF PO UIF UBCMF PS PO UPQ PG BOPUIFS CMPDL ɩF mHVSF TIPXT UIF HSBQI BTTPDJBUFE UP UIF QSPCMFN XIPTF TPMVUJPO JT B QBUI DPOOFDUJOH UIF OPEF SFQSFTFOUJOH UIF JOJUJBM TJUVBUJPO XJUI B OPEF SFQSFTFOUJOH B HPBM TJUVBUJPO ɩF OVNCFS PG TUBUFT JO B #MPDLT 8PSME QSPCMFN XJUI CMPDLT JT FYQPOFOUJBM JO BT UIF TUBUFT JODMVEF
ɩF BHF PG UIF VOJWFSTF JT FTUJNBUFE BU ZFBST BQQSPYJNBUFMZ 7JTJUJOH OPEFT BU OPEFT B TFDPOE XPVME UBLF JO UIF PSEFS PG ZFBST BT
Figure 1.3: e graph corresponding to a simple planning problem involving three blocks with initial and goal situations as shown. e actions allow to move a clear block on top of another clear block or to the table. e size of the complete graph for this domain is exponential in the number of blocks. A plan for the problem is shown by the path in red.
possible values, the number of nodes in the graph to search can be in the order of 2n, where n is the number of variables. In particular, if the problem involves 30 variables, this means 1; 073; 741; 824 nodes, and if the problem involves 100 variables, it means more than 1030 nodes. In order to get a concrete idea of what exponential growth means, if it takes one second to generate 107 nodes (a realistic estimate given current technology), it would take more than 1023 seconds to generate 1030 nodes. is is however almost one million times the estimated age of the universe.3 A more vivid illustration of the complexity inherent to the planning problem can be obtained by considering a well known domain in AI: the Blocks World. Figure 1.3 shows an instance of this domain where blocks A, B, and C, initially arranged so that A is on B, and B and C are on the table, must be rearranged so that B is on C, and C is on A. e actions allow to move a clear block (a block with no block on top) on top of another clear block or on the table. e problem can be easily expressed as a classical planning problem where the variables are the block locations: blocks can be on the table or on top of another block. e figure shows the graph associated to the problem whose solution is a path connecting the node representing the initial situation with a node representing a goal situation. e number of states in a Blocks World problem with n blocks is exponential in n, as the states include all the nŠ possible towers of n blocks plus additional combinations of lower towers. us, a planner
3e age of the universe is estimated at 13:7 109 years approximately. Visiting 2100 nodes at 107 nodes a second would take in the order of 1015 years, as 2100=.107 60 60 24 365/ D 4:01969368 1015.


8 1. PLANNING AND AUTONOMOUS BEHAVIOR
able to solve arbitrary Blocks World instances should be able to search for paths over huge graphs. is is a crisp computational challenge that is very different from writing a domain-specific Blocks World solver—namely, a program for solving any instance of this specific domain. Such a program could follow a domain-specific strategy, like placing all misplaced blocks on the table first, in order, from top to bottom, then moving these blocks to their destination in order again, this time, from the bottom up. is program will solve any instance of the Blocks World but will be completely useless in other domains. e challenge in planning is to achieve both generality and scalability. at is, a classical planner must accept a description of any problem in terms of a set of variables whose initial values are known, a set of actions that change the values of these variables deterministically, and a set of goals defined over these variables. e planner is domain-general or domain-independent in the sense that it does not know what the variables, actions, and domain stand for, and for any such description it must decide effectively which actions to do in order to achieve the goals. For classical planning, as for the other planning models that we will consider, the general problem of coming up with a plan is NP-hard [Bylander, 1994, Littman et al., 1998]. In Computer Science, an NP-hard problem (non-deterministic polynomial-time hard) is a problem that is at least as hard as any NP-complete problem; these are problems that can be solved in polynomial time by a nondeterministic Turing Machine but which are widely believed not to admit polynomial-time solutions on deterministic machines [Sipser, 2006]. e complexity of planning and related models has been used as evidence for contesting the possibility of general planning and reasoning abilities in humans or machines [Tooby and Cosmides, 1992]. e complexity of planning, however, just implies that no planner can efficiently solve every problem from every domain, not that a planner cannot solve an infinite collection of problems from seen and unseen domains, and hence be useful to an acting agent. is is indeed the way modern AI planners are empirically evaluated and ranked in the AI planning competitions, where they are tried over domains that the planners” authors have never seen. us, far from representing an insurmountable obstacle, the twin requirements of generality and scalability have been addressed head on in AI planning research, and have resulted in simple but powerful computational principles that make domain-general planning feasible. e computational challenge aimed at achieving both scalability and generality over a broad class of intractable models, has actually come to characterize a lot of the research work in AI, that has increasingly focused on the development of effective algorithms or solvers for a wide range of tasks and models (Figure 1.4); tasks and models that include SAT and SAT-variants like Weighted-Max SAT and Weighted Model Counting, Bayesian Networks, Constraint Satisfaction, Answer Set Programming, General Game Playing, and Classical, MDP, and POMDP Planning. is is all work driven by theory and experiments, with regularly held competitions used to provide focus, to assess progress, and to sort out the ideas that work best empirically [Geffner, 2013a].
1.4 EXAMPLES
We consider next a simple navigation scenario to illustrate how different types of planning problems call for different planning models and different solution forms. e general scenario is shown in Figure 1.5 where the agent marked as A has to reach the goal marked as G. e four actions available let the agent move one unit in each one of the four cardinal directions, as long as there is no wall. Actions that lead the agent to a wall have no effect. e question is how should the agent select the actions for achieving the goal with certainty under different knowledge and sensing conditions. In all


1.4. EXAMPLES 9
Model Instance Solver Solution
Figure 1.4: Models and Solvers: Research work in AI has increasingly focused on the formulation and development of solvers for a wide range of models. A solver takes the representation of a model instance as input, and automatically computes its solution in the output. Some of the models considered are SAT, Bayesian Networks, General Games Playing, and Classical, MDP, and POMDP Planning. All of these models are intractable when represented in compact form. e main challenge is scaling up.
cases, we assume that the agent knows the map, including where the walls and the goals are. In the simplest case, the actions are assumed to be deterministic and the initial agent location known. In this case, the agent faces a classical planning problem whose solution is a path in the grid joining the initial agent location and the goal. On the other hand, if the actions have effects that can only be predicted probabilistically but the state of the problem—the agent location—is always observable, the problem becomes an MDP planning problem. e solution to this problem is no longer an action sequence, that cannot guarantee that the goal will be achieved with certainty, but a policy assigning one of the four possible actions to each one of the states. e number of steps to reach the goal can no longer be determined with certainty but there is then an expected number of steps to reach the goal that can be determined, as the policy and the action model induce a probability distribution over all the possible paths in the grid. Policies that ensure that the goal is eventually achieved with certainty are called proper policies. Interestingly, we will see that the exact transition probabilities are not relevant for defining or computing proper policies; all we need to know for this are which state transitions are possible (probability different than zero) and which ones are not (probability equal to zero). e problem variation in which the actions have stochastic effects but the location of the agent cannot be fully observed is a POMDP planning problem, whose general solution is neither a fixed action sequence, that ignores the observations, nor a policy prescribing the action to do in each state, that assumes that the state is observable. It is rather a policy that maps belief states into actions. In the POMDP setting, a belief state is a probability distribution over the states that are deemed possible. ese probability distributions summarize all the information about the past that is relevant for selecting the action to do next. e initial belief state has to be given, and the current belief state is determined by the actions done, the observations gathered, and the information in the model. On the other hand, if uncertainty about the initial situation, the system dynamics and the feedback are represented by sets of states as opposed to probability distributions over states, we obtain a partially observable planning problem which is the logical counterpart of POMDPs. In such problems, the number of possible belief states (sets of states deemed possible) is finite, although exponential in the number of states, and hence doubly exponential in the number of problem variables. Finally, if the agent must reach the goal with certainty but there is uncertainty about the initial state or about the next state dynamics, and there is no feedback of any type, the problem that the agent faces is a conformant problem. For the problem shown in the figure, if the actions are deterministic and the agent knows that it is initially somewhere on the room on the left, a conformant solution can be obtained as follows: the agent moves up five times, until it knows with certainty that it is somewhere on the top row, then it moves right three times until it knows with certainty that it is exactly at the top right corner of the left room. With all uncertainty gone, the agent can then find a path to the goal from that corner.


10 1. PLANNING AND AUTONOMOUS BEHAVIOR
A
G
Figure 1.5: Variations on a planning problem: Agent, marked as A, must reach the goal marked G, by moving one cell at a time under different knowledge and sensing conditions.
A completely different planning example is shown in Figure 1.6 for a problem inspired on the use of deictic representations [Ballard et al., 1997, Chapman, 1989], where a visual-marker or “eye” (the circle on the lower left) must be placed on top of a green block by moving it one cell at a time. e location of the green block is not known initially, and the observations are just whether the cell with the mark contains a green block (G), a non-green block (B), or neither (C), and whether such cell is at the level of the table (T) or not (–). e problem is a partially observable planning problem and the solution to it can be expressed by means of control policies mapping beliefs into actions. An alternative way to represent solutions to these types of problems is by means of finite-state controllers, such as the one shown on the right of Figure 1.6. is finite-state controller has two internal states, the
initial controller state q0, and a second controller state q1. An arrow q o=a
! q0 in the controller indicates that, when obtaining the observation o in the controller state q, the action a should be performed, moving to the controller state q0, that may be equal to q or not, and where the same action selection mechanism is applied. e reader can verify that the finite-state controller searches for a tower with a green block from left to right, going all the way up to the top block in each tower, then going all the way down to the table, and iterating in this manner until the visual marker appears on top of a block that is green. Finite-state controllers provide a very compact and convenient representation of the actions to be selected by an autonomous system, and for this reason they are commonly used in practice for controlling robots or non-playing characters in video games [Buckland, 2004, Mataric, 2007, Murphy, 2000]. While these controllers are normally written by hand, we will show later that they they can be obtained automatically using planners. Indeed, the controller shown in the figure has been derived in this way using a classical planner over a suitable transformation of the partially observable problem shown on the left [Bonet et al., 2009]. It is actually quite remarkable that the finite-state controller that has been obtained in this manner is not only good for solving the original problem on the left, but also an infinite number of variations of it. It can actually be shown that the controller will successfully solve any modification in the problem resulting from changes in either the dimensions of the grid, the number of blocks or their configuration. us, in spite of appearances, the power


1.5. GENERALIZED PLANNING: PLANS VS. GENERAL STRATEGIES 11
'JHVSF 7BSJBUJPOT PO B QMBOOJOH QSPCMFN "HFOU NBSLFE BT " NVTU SFBDI UIF HPBM NBSLFE ( CZ NPWJOH POF DFMM BU B UJNF VOEFS EJĊFSFOU LOPXMFEHF BOE TFOTJOH DPOEJUJPOT
q0 q1
–C/Down
TB/Right
TC/Right –B/Up TB/Up –B/Down
'JHVSF -FGU 1SPCMFN XIFSF B WJTVBM NBSLFS NBSL PO UIF MPXFS MFGU DFMM NVTU CF QMBDFE PO UPQ PG B HSFFO CMPDL CZ KVTU PCTFSWJOH XIBU JT PO UIF NBSLFE DFMM 3JHIU 'JOJUF TUBUF DPOUSPMMFS PCUBJOFE XJUI B DMBTTJDBM QMBOOFS GSPN TVJUBCMF USBOTMBUJPO ɩF DPOUSPMMFS TPMWFT UIF QSPCMFN BOE BOZ WBSJBUJPO PG UIF QSPCMFN UIBU SFTVMUT GSPN DIBOHFT JO UIF OVNCFS PS DPOmHVSBUJPO PG CMPDLT
" DPNQMFUFMZ EJĊFSFOU QMBOOJOH FYBNQMF JT TIPXO JO 'JHVSF GPS B QSPCMFN JOTQJSFE PO UIF VTF PG EFJDUJD SFQSFTFOUBUJPOT <#BMMBSE FU BM $IBQNBO > XIFSF B WJTVBM NBSLFS PS AFZF UIF DJSDMF PO UIF MPXFS MFGU NVTU CF QMBDFE PO UPQ PG B HSFFO CMPDL CZ NPWJOH JU POF DFMM BU B UJNF ɩF MPDBUJPO PG UIF HSFFO CMPDL JT OPU LOPXO JOJUJBMMZ BOE UIF PCTFSWBUJPOT BSF KVTU XIFUIFS UIF DFMM XJUI UIF NBSL DPOUBJOT B HSFFO CMPDL ( B OPO HSFFO CMPDL # PS OFJUIFS $ BOE XIFUIFS TVDI DFMM JT BU UIF MFWFM PG UIF UBCMF 5 PS OPU o ɩF QSPCMFN JT B QBSUJBMMZ PCTFSWBCMF QMBOOJOH QSPCMFN BOE UIF TPMVUJPO UP JU DBO CF FYQSFTTFE CZ NFBOT PG DPOUSPM QPMJDJFT NBQQJOH CFMJFGT JOUP BDUJPOT "O BMUFSOBUJWF XBZ UP SFQSFTFOU TPMVUJPOT UP UIFTF UZQFT PG QSPCMFNT JT CZ NFBOT PG mOJUF TUBUF DPOUSPMMFST TVDI BT UIF POF TIPXO PO UIF SJHIU PG 'JHVSF ɩJT mOJUF TUBUF DPOUSPMMFS IBT UXP JOUFSOBM TUBUFT UIF
JOJUJBM DPOUSPMMFS TUBUF BOE B TFDPOE DPOUSPMMFS TUBUF "O BSSPX JO UIF DPOUSPMMFS JOEJDBUFT
Figure 1.6: Left: Problem where a visual-marker (mark on the lower left cell) must be placed on top of a green block by just observing what is on the marked cell. Right: Finite-state controller obtained with a classical planner from suitable translation. e controller solves the problem and any variation of the problem that results from changes in the number or configuration of blocks.
of classical planners shouldn’t be underestimated. Often we will be able to solve non-classical planning problems using classical planners by means of feasible, well-defined transformations.
1.5 GENERALIZED PLANNING: PLANS VS. GENERAL STRATEGIES
e visual-marker problem illustrates two differences that are crucial in planning. One is the difference between a solution to a problem instance and a solution to a family of instances. e second is the difference between expressing the solution to a problem and finding a solution to the problem in the first place. For example, a solution to a particular Blocks World instance, with blocks A, B, and C, on the table, that must be stacked in order with C on top, may be the action sequence pick(B), stack(B,A), pick(C), stack(C,B). On the other hand, the general strategy of putting all blocks on the table, in order from top to bottom, followed by putting all blocks on their destination in order from the bottom to the top, works for all Blocks World instances, regardless of the number and names of the blocks. Planning in AI, and in particular what is called domain-independent planning, has been mostly focused on models and methods for expressing and solving single planning instances. On the other hand, the work in planning driven by applications over certain specific domains, has usually focused on languages like Hierarchical Task Networks [Erol et al., 1994] for expressing by hand the strategies for solving any problem in the given domain. e problem of computing general domain strategies has not been tackled by automated methods, because the problem appears to be too hard in general. Yet, ideally, this is where we would like to get, at least on domains that admit compact general solutions [Srivastava et al., 2011a]. In a recent formulation, a form of generalized planning of this type has been shown to be EXPSPACE-Complete [Hu and de Giacomo, 2011]. In this formulation, all instances are assumed to share the same set of actions and observations, and a general solution is a function mapping streams of observations into actions. In some cases, such functions can be conveniently expressed as policies that map suitable combination of observables, called features, into actions. e crucial question is how to get such features and policies effectively, in particular over domains that admit compact solutions over the right features. An early approach that does this in the blocks world constructs a pool of possible


12 1. PLANNING AND AUTONOMOUS BEHAVIOR
features from the primitive domain predicates and a simple grammar (a description logic), and then looks for compact rule-based policies over some of such features using supervised learning algorithms [Fern et al., 2003, Martin and Geffner, 2000]. Another approach, used for playing a challenging game in real-time, represents the policies that map observables into actions by means of neural networks whose topology and weights are found by a form of evolutionary search [Stanley et al., 2005]. Both of these approaches are aimed at general domain policies that map state features into actions, which are not tied to specific instances. In a famous exchange during the 80s about “universal plans,” Matthew Ginsberg attacked the value of the idea of general plans on computational grounds [Ginsberg, 1989]. A universal plan is a strategy for solving not just one planning instance, but many instances, and more specifically, all those instances that can be obtained by just changing the initial state of the problem [Schoppers, 1987]. While the solutions to such generalized planning problems can be expressed as policies mapping states into actions, as for MDPs, Ginsberg argued that the size of such universal plans would be often just too large; exponential at least for families of problems that are NP-hard to solve. In order to illustrate this point, Ginsberg conjectured that no compact universal plan could be defined for a specific problem that he called the fruitcake problem, where blocks were to be placed on a tower to spell the word “fruitcake.” e challenge, however, was answered by David Chapman who came up with an elegant reactive architecture, basically a circuit in line with his “Pengi” system [Agre and Chapman, 1987], that in polynomial time solved the problem [Chapman, 1989]. Chapman went on to claim that Blockhead, his system, solved the fruitcake problem, and solved it easily, with no search or planning, thus raising doubts not only about the value of “universal plans” but also of the need to plan itself. is was an interesting debate, and if we bring it here it is because it relates to the two key distinctions mentioned at the beginning of this section: one between a solution to a problem instance and a solution to a family of instances, the other between expressing a solution and finding a solution. From this perspective, Chapman is right that general strategies for solving many classes of interesting problems can often be encoded through compact representations, like Pengi-like circuits, yet the challenge is in coming up with such strategies automatically. Without this ability, it cannot be said that Blockhead is solving the problemit’s Chapman. Blockhead simply executes the policy crafted by Chapman that is not general and doesn’t apply to other domains.
1.6 HISTORY
e first AI planner and one of the first AI programs was introduced by Newell and Simon in the 50s [Newell and Simon, 1961, Newell et al., 1959]. is program called GPS, for General Problem Solver, introduced a technique called means-ends analysis where differences between the current state and the goal were identified and mapped into operators that decreased those differences. e STRIPS system [Fikes and Nilsson, 1971] combined means-ends analysis with a convenient declarative action language. Since then, the idea of means-ends analysis has been refined and extended in many ways, in the formulation of planning algorithms that are sound (only produce plans), complete (produce a plan if one exists), and effective (scale up to large problems). By the early 90s, the state-of-the-art planner was UCPOP [Penberthy and Weld, 1992], an implementation of an elegant planning method known as partial-order planning where plans are not searched either forward from the starting state or backward from the goal, but are constructed from a decomposition scheme in which joint goals are decomposed into subgoals that create as further subgoals the preconditions of the actions used to establish them


1.6. HISTORY 13
[McAllester and Rosenblitt, 1991, Sacerdoti, 1975, Tate, 1977]. e actions that are incorporated into the plan are partially ordered as needed in order to resolve possible conflicts among them. Partial-order planning algorithms are sound and complete, but do not scale up well, as there are too many choices to make and too little guidance on how to make those choices; yet see [Nguyen and Kambhampati, 2001, Vidal and Geffner, 2006]. e situation in planning changed drastically in the mid 90s with the introduction of Graphplan [Blum and Furst, 1995], an algorithm that appeared to have little in common with previous approaches but scaled up much better. Graphplan builds a plan graph in polynomial time reasoning forward from the initial state, which is then searched backward from the goal to find a plan. It was shown later that the reason Graphplan scaled up well was due to a powerful admissible heuristic implicit in the plan graph [Haslum and Geffner, 2000]. e success of Graphplan prompted other approaches. In the SAT approach [Kautz and Selman, 1996], the planning problem for a fixed planning horizon is converted into a general satisfiability problem expressed as a set of clauses (a formula in Conjunctive Normal Form or CNF) that is fed into state-of-the-art SAT solvers, which currently manage to solve huge SAT instances even though the SAT problem is NP-complete. Currently, the formulation of classical planning that appears to scale up best is based on heuristic search, with heuristic values derived from the delete-relaxation [Bonet et al., 1997, McDermott, 1996]. In addition, state-of-the-art classical planners use information about the actions that are most “helpful” in a state [Hoffmann and Nebel, 2001], and implicit subgoals of the problem, called landmarks, that are also extracted automatically from the problem with methods similar to those used for deriving heuristics [Hoffmann et al., 2004, Richter and Westphal, 2010]. Since the 90s, increasing attention has been placed on planning over non-classical models such as MDPs and POMDPs where action effects are not fully predictable, and the state of the system is fully or partially observable [Dean et al., 1993, Kaelbling et al., 1998]. We will consider all these variations and others in the rest of the book. We will have less to say about Hierarchical Task Planning or HTN planning, which, while widely used in practice, is focused on the representation of general strategies for solving problems rather than in representing and solving the problems themselves. For a comprehensive planning textbook, see Ghallab et al. [2004], while for a modern AI textbook covering planning at length, see Russell and Norvig [2009].




15
CHAPTER 2
Classical Planning: Full Information and Deterministic Actions
In classical planning, the task is to drive a system from a given initial state into a goal state by applying actions whose effects are deterministic and known. Classical planning can be formulated as a pathfinding problem over a directed graph whose nodes represent the states of the system or enviroment, and whose edges capture the state transitions that the actions make possible. e computational challenge in classical planning results from the number of states, and hence the size of the graph, which are exponential in the number of problem variables. State-of-the-art methods in classical planning search for paths in such graphs by directing the search toward the goal using heuristic functions that are automatically derived from the problem. e heuristic functions map each state into an estimate of the distance or cost from the state to the goal, and provide the search for the goal with a sense of direction. In this chapter, we look at the model and languages for classical planning, and at the heuristic search techniques that have been developed for solving it. Variations and extensions of these methods, as well as alternative methods, will be considered in the next chapter.
2.1 CLASSICAL PLANNING MODEL
Classical planning is concerned with the selection of actions in environments that are deterministic and whose initial state is fully known. e model underlying classical planning can be described as the state model S D hS; s0; SG; A; f; ci where
• S is a finite and discrete set of states,
• s0 2 S is the known initial state,
• SG S is the non-empty set of goal states,
• A.s/ A represents the set of actions in A that are applicable in each state s 2 S,
• f .a; s/ is the deterministic transition function where s0 D f .a; s/ is the state that follows s after doing action a 2 A.s/, and
• c.a; s/ is a positive cost for doing action a in the state s.
A solution or plan in this model is a sequence of applicable actions a0; : : : ; an that generates a state sequence s0; s1; : : : ; snC1 where snC1 is a goal state. More precisely, the action ai is applicable


16 CLASSICAL PLANNING
in the state si if ai 2 A.si /, the state siC1 follows the state si if siC1 D f .ai ; si /, and snC1 is a goal state if snC1 2 SG. e cost of the plan is the sum of the action costs c.ai ; si /, i D 0; : : : ; n. A plan is optimal if it has minimum cost, and the cost of the model is the cost of an optimal plan. A common cost structure arises when all action costs c.a; s/ are equal to 1. en the cost of the plan is given by its length, and the optimal plans are the shortest ones. Domain-independent classical planners accept a compact description of the above models, and automatically produce a plan. e problem is intractable in the worst case [Bylander, 1994], yet currently large classical problems can be solved very quickly. Optimal planners produce optimal plans, while satisficing planners are aimed at producing good plans which are not necessarily optimal. Computing optimal plans is harder than computing plans, as the former involves a universal claim about the space of all plans—namely, that they all have a cost that is not smaller than the cost of the plan found.
2.2 CLASSICAL PLANNING AS PATH FINDING
ere is a direct correspondence between classical planning models and directed graphs, and between classical plans and certain paths over these graphs. Recall that directed graphs are structures G D .V; E/ made up of a set of nodes V and a set of directed edges represented by ordered pairs of nodes .n; n0/, n; n0 2 V . Directed graphs or digraphs are a fundamental model in Computer Science, and there is a wide range of algorithms for solving tasks over graphs [Cormen et al., 2009]. A weighted digraph is a digraph G D .V; E; w/ where every edge .n; n0/ in E comes with a real weight w.n; n0/. One of the basic tasks over digraphs is finding a (directed) path from a given source node n0 to a node n in a target set, and similarly, one of the basic tasks over weighted digraphs is finding one such path with minimum cost. A classical planning model S D hS; s0; SG; A; f; ci defines a weighted directed graph G D .V; E; w/ where the nodes in V represent the states in S, the edges .s; s0/ in E represent the presence of an action a in A.s/ such that s0 is the state that follows a in s, and the weight w.s; s0/ is the minimum cost over such actions in s. It follows from this correspondence that an action sequence D a0; : : : ; am is a plan for the state model S iff generates a sequence of states s0; : : : ; smC1 that represents a directed path in the weighted digraph G D .V; E; w/. Clearly, if one such path can be found in the graph, the corresponding plan can be obtained by retrieving ai as the action that makes the transition .si ; siC1/ possible with least cost. us, in principle any path-finding algorithm over weighted directed graphs can be used for finding plans for the classical planning model. We look at some of these algorithms below. All these algorithms are incremental in the sense that none of them requires the graph to be explicit in memory at the beginning; rather they all explicate an implicit graph incrementally as the search to the goal proceeds. is is crucial in planning where the size of the graphs is exponential in the number of problem variables.
2.3 SEARCH ALGORITHMS: BLIND AND HEURISTIC
Path-finding algorithms come in two main varieties: those in which the goal plays an active role in the search, and those in which the goal sits passively until encountered. e standard way in which goals can bias the search is by means of heuristic functions; these are functions h.s/ that provide a quickand-dirty estimate of the cost to reach the goal from the state s, making the search goal-directed. Algorithms that use these heuristics are called heuristic search algorithms; those in which the goals


2.3. SEARCH ALGORITHMS: BLIND AND HEURISTIC 17
S.Nodes/
while Nodes ¤ ; do
Let n WD S-N.Nodes/ Let Rest WD Nodes n fng if n is a goal node then return E-S.n/ else
Let Children WD E-N.n/
Set Nodes WD A-N.Children; Rest/ end if end while
return Unsolvable
Figure 2.1: General search schema invoked with Nodes containing the source node only. A number of familiar search algorithms are obtained by suitable choices of the S-N and A-N functions.
play no active role during the search are called brute force or blind search algorithms [Edelkamp and Schrödl, 2012, Pearl, 1983]. e latter include algorithms like Depth-First Search, Breadth-First Search, and Uniform Cost Search, also called Dijkstra’s algorithm [Cormen et al., 2009]. e former include Best-First Search, A*, and Hill Climbing. e algorithms search for paths in different ways, and have different properties concerning completeness, optimality, and time and memory complexity. We will make a quick overview of them before reviewing some useful variants. e algorithms can all be understood as particular instances of the general schema shown in Figure 2.1, where a search frontier called Nodes, initialized with the root node of the graph, shifts incrementally as the graph is searched. In each iteration, two steps are performed: a selected node is removed from the search frontier, and if the selected node is not a goal node, its children are added to the search frontier, else the search terminates and the path to the last node selected is returned. e nodes in the search represent the states of the problem and contain in addition bookkeeping information like a pointer to the parent node, and the weight of the best path to the node so far, called the accumulated cost and denoted by the expression g.n/ where n is the node. e various algorithms arise from the representation of the search frontier Nodes, the way nodes are selected from this frontier, and the way the children of these nodes are added to the search frontier. Depth-First Search is the algorithm that results from implementing the search frontier Nodes as a : the node that is selected is the top node in the stack, and the children nodes are added to the top of the stack. It can be shown that if the graph is acyclic, the nodes in Nodes will be selected indeed in depth-first order. Similarly, Breadth-First Search is the algorithm that results from implementing the search frontier Nodes as a : nodes are selected from one end, and their children are added to the other end. It can be shown then that the nodes in Nodes will be selected depth-last, and more precisely, shallowest-first, which is the characteristic of Breadth-First Search. Finally, Best-First Search is the algorithm that results when the search frontier is set up as a -, so that the nodes selected are the ones that minimize a given evaluation function f .n/. Best-First Search reduces to the well-known A* algorithm [Hart et al., 1968] when the evaluation function is defined as the sum


18 CLASSICAL PLANNING
f .n/ D g.n/ C h.n/ of the accumulated cost g.n/ and the heuristic estimate of the cost-to-go h.n/. Other known variations of Best-First Search are Greedy Best-First where f .n/ D h.n/, and WA* where f .n/ D g.n/ C W h.n/ and W is a constant larger than 1. Finally, Uniform-Cost Search or Dijkstra’s algorithm corresponds basically to a Best-First Search with evaluation function f .n/ D g.n/, or alternatively to the A* algorithm with the heuristic function h.n/ set to 0. Certain optimizations are common. In particular, Depth-First Search prunes paths that contain cycles—namely, pairs of nodes that represent the same state (duplicate nodes), while Breadth-First and Best-First Search keep track of the nodes that have been already selected and expanded in a CLOSED list. Duplicates of these nodes can be pruned except when the new node has a lower accumulated cost g.n/ and the search is aimed at returning a minimum-cost path. Similarly, duplicate nodes in the search frontier, also called the OPEN list, are avoided by just keeping in OPEN the node with the least evaluation function. It can be shown that all these algorithms are complete, meaning that if there is a path to a goal node, the algorithms will find a path in finite time.1 Furthermore, some of these algorithms are optimal, meaning that the paths returned upon termination will be optimal. ese include Dijkstra’s algorithm, Breadth-First Search when action costs are uniform, and A* when the heuristic h.n/ is admissible, i.e., it doesn’t overestimate the true optimal cost h .n/ from n to the goal for any node n. e complexity in space of these algorithms can be described in terms of the length d of the solutions and the average number of children per node, the so-called branching factor b. e space requirement of Breadth-First search is exponential and grows with O.bd /, where d is the length of the optimal solution, as a b-ary tree of depth d has bd leafs all of which can make it into the search frontier in the worst case. e space complexity of A* with admissible heuristics is in turn O.bC =cmin /, where C is the optimal cost of the problem and cmin is the minimum action cost. is is because A* may expand in the worst case all nodes n with evaluation function f .n/ C and such nodes can be at depth C =cmin.2 e same bounds apply for the time complexity of the algorithms. On the other hand, the space requirements for Depth-First Search are minor: they grow linearly with d as O.bd /, as the search frontier in DFS just needs to keep track of the path to the last selected node along with the children of the ancestors nodes that have not yet been expanded. e difference between linear and exponential memory requirements can be crucial, as algorithms that require exponential memory may abort after a few seconds or minutes with an “insufficient memory” message. Since DFS is the only linear space algorithm above, extensions of DFS have been developed that use linear memory and yet return optimal solutions. e core of some of these algorithms is a bounded-cost variant of DFS where selected nodes n whose evaluation function f .n/ D g.n/ exceeds a given bound B are immediately pruned. Bounded-Cost DFS remains complete provided that the bound B is not smaller than the cost C of an optimal solution, but it is not optimal unless B is equal to C . Two iterative variants of Bounded-Cost DFS achieve optimality by performing several successive trials with increasing values for the bound B, until B D C . Iterative Deepening (ID) is a sequence of Bounded-Cost DFS searches with the bound B0 for the first iteration set to 0, and the bound Bi for iteration i > 0 set to minimum evaluation function f .n/ over the nodes pruned in the previous iteration so that at least a new node is expanded in each iteration. In its most standard form, when all action costs are equal to 1, the bound Bi in the iteration i is Bi D i. ID
1For DFS to be complete, paths containing cycles must be pruned to avoid getting trapped into a loop. 2is bound assumes that the heuristic is consistent, as otherwise, nodes may have to be reopened an exponential number of times in the worst case [Pearl, 1983].


2.3. SEARCH ALGORITHMS: BLIND AND HEURISTIC 19
combines the best elements of Depth-First Search (memory) and Dijkstra’s algorithm (optimality). ID achieves this combination by performing multiple searches where the same node may be expanded multiple times. Yet, asymptotically this does not affect the time complexity that is dominated by the worst-case time of the last iteration. Iterative Deepening A* (IDA*) is a variant of ID that uses the evaluation function of A*—namely, f .n/ D g.n/ C h.n/. As long as the heuristic is admissible, the first solution encountered by IDA* will be optimal too, while usually performing fewer iterations than ID and pruning many more nodes [Korf, 1985]. e number of nodes expanded by heuristic search algorithms like A* and IDA* depends on the quality of the heuristic h. A* is no better than Breadth-First Search or Dijkstra’s algorithm when h.n/ is uniformly 0, and IDA* is no better then than ID. Yet if the heuristic is optimal, i.e., h.n/ D h .n/ where h stands for the optimal cost from n to the goal, then both A* and IDA* will find an optimal path to the goal with no search at all, just expanding the nodes in one optimal path only (for this though, A* must break ties in the evaluation function by favoring the nodes with the smaller heuristic, else if the problem has multiple optimal solutions, A* may keep switching from one optimal path to another). In the middle, A* and IDA* will expand fewer nodes using an admissible heuristic h1 than using an admissible heuristic h2 when h1 is higher than h2.3 A more common situation is when h1 is higher than h2 over some states, and equal to h2 over the other states. In such cases, h1 will produce no more expansions than h2 provided that ties are broken in the same way in the two cases. In the first case, the heuristic h1 is said to be more informed than h2 or to dominate h2, in the second, that it is at least as informed as h2 [Edelkamp and Schrödl, 2012, Pearl, 1983]. Admissible heuristics are crucial in algorithms like A* and IDA* for ensuring that the solutions returned are optimal, yet they are not crucial for finding solutions fast, when there is no need for optimality. Indeed, these algorithms will often find solutions faster by multiplying an admissible heuristic h by a constant W > 1 as in WA*. WA* can be thought as an A* algorithm but with heuristic W h which is not necessarily admissible even when h is. e reason that WA* will find solutions faster than A* can be seen by considering the nodes selected for expansion given an OPEN list that contains one node n that is deep in the graph but close to the goal, e.g., g.n/ D 10 and h.n/ D 1, and another node n0 that is shallow and far from the goal; e.g., g.n0/ D 2 and h.n0/ D 6. Among the two nodes, A* will choose n0 for expansion as f .n0/ D g.n0/ C h.n0/ D 2 C 6 D 8, while f .n/ D g.n/ C h.n/ D 10 C 1 D 11. On the other hand if W D 2, WA* chooses the node n instead as f .n0/ D g.n0/ C W h.n0/ D 2 C 2 6 D 14 and f .n/ D g.n/ C W h.n/ D 10 C 2 1 D 12.
e optimality of A* with admissible heuristics can be shown by contradiction. First, it is not hard to verify that until termination, the OPEN list always contains a node n in an optimal path to the goal if the problem is solvable. en, if A* selects a goal node n0 with cost f .n0/ that is higher than the optimal cost C , then f .n0/ > C g.n/ C h.n/ D f .n/ as the heuristic h is admissible. erefore, A* did not select the node with minimum f -value from OPEN which is a contradiction. WA* is not optimal, even when the heuristic h is admissible, yet the same argument can be used to show that the solution returned by WA* will not exceed the optimal cost C by more than a factor of W . is is important in practice, as for example, with W D 1:2 the algorithm may turn out to run an order-of-magnitude faster and with much less memory than A*, yet the loss in optimality is then at most 20%. Anytime WA* [Hansen and Zhou, 2007] is an anytime optimal algorithm which basically works exactly like WA* until a solution is found with cost C , not necessarily optimal. Rather than stopping
3Technically speaking, this result assumes that both heuristics are consistent [Pearl, 1983].


20 CLASSICAL PLANNING
there, however, anytime WA* uses the amount of time available for improving the quality of this solution, continuing the WA* search, while pruning nodes n with accumulated costs g.n/ greater than C , and updating the bound C to C 0 when solutions with cost C 0 less than C are found. Anytime WA* can thus produce solutions more quickly, and if given enough time, produce better and better solutions until finding an optimal solution. is, however, can only be verified when the search terminates, i.e., when the OPEN list becomes empty. Another interesting anytime optimal algorithm obtained as a variation of WA* is Restarting WA* or RWA* that performs iterated WA* searches but with decreasing weights, while keeping in memory nodes expanded in previous iterations that are re-expanded only when a cheaper path to the node is found [Richter et al., 2010]. is is the search algorithm used in the state-of-the-art heuristic search planner called LAMA [Richter and Westphal, 2010]. Many other heuristic search planners use Greedy Best-First (GBFS) which is a best-first search with evaluation function f .n/ D h.n/, where the accumulated cost term g.n/ has been dropped. GBFS can be seen as an WA* algorithm with a very large constant, and a tie breaking rule that favors nodes n with smallest accumulated costs g.n/.
2.4 ONLINE SEARCH: THINKING AND ACTING INTERLEAVED
Often there is no time for machines to compute a complete solution offline before performing an action. is is clearly the situation in Chess where programs do not compute a complete solution to the game before choosing a move, but rather focus iteratively on the move to do next. Of course, Chess is a two-player game that cannot be described in terms of classical state models, yet the situation is similar for people in games such as Rubik’s Cube, the 15-Puzzle, or Sokoban. People do not have the patience or the computational resources to compute complete solutions offline. Rather, they think a bit, explore the possible options and consequences, and then move. Search algorithms that interleave thinking and acting in this way are called online search algorithms. While online search algorithms come often with fewer guarantees than the offline algorithms, they are often more practical and have a wider scope. In particular, online search algorithms can deal with classical state models that are not completely accurate, provided that the state of the system is fully observable. Indeed, when the model predicts the state s0 after performing the action a in the state s, but the state s00 is observed instead, online algorithms can continue the search from s00. us, for example, a Blocks World problem where there is a chance for blocks to fall down accidentally from the gripper, can be described in terms of the standard, deterministic Blocks World state model. en an online search algorithm that keeps track of the state of the system can replan from the new state when it is not the state that was predicted from the model (e.g., where a block fell unexpectedly from the gripper). In the presence of feedback, thus, online search algorithms can be used to provide a form of closed-loop control where a bounded search over the model is used to select the action to do in the observed state, which is then applied to the real or simulated system, and so on, until the goal is reached. Online search algorithms are also called planning and execution algorithms, as they interleave planning and execution, as opposed to offline search algorithms that just plan. e simplest heuristic online algorithm is also the simplest heuristic algorithm of all: from the current state s, the action a that is selected is the one that minimizes the estimated cost to the goal defined as:


2.4. ONLINE SEARCH: THINKING AND ACTING INTERLEAVED 21
Q.a; s/ D c.a; s/ C h.s0/ (2.1)
where h is the heuristic function and s0 is the state that is predicted to follow the action a in the state s, i.e., s0 D f .a; s/. e minimization is done over the actions a applicable in s, i.e., a 2 A.s/. is algorithm is known as the greedy algorithm or policy, and also as hill climbing search. It’s called greedy because it selects the action to be done by trusting the heuristic function h completely, and hill climbing, because when action costs are uniform it behaves as if Q.a; s/ D h.s0/, selecting thus actions that minimize the heuristic function toward goal states that should have a zero heuristic.4 e main positive property of the greedy algorithm is that it is optimal if the heuristic h is optimal, i.e., if h D h . In addition, the algorithm uses constant memory; it doesn’t keep track of a search frontier at all, just the current state and its children. is is however where the good news for the greedy algorithm end. e algorithm in general is neither optimal nor complete; in fact, it can get trapped into a loop, selecting actions that take it from a state s into state s0 and then back from s0 to s.
A common way to improve the greedy algorithm is by looking ahead from the current state s, not just one level as done by the minimization of the Q.a; s/ expression in Eq. 2.1, but several levels. A depth-first search from s that prunes nodes that are deeper than a given bound H can be used to perform this lookahead where H is the number of levels, or planning horizon. is form of lookahead is time exponential in the horizon, O.bH /, where b is the branching factor of the problem. After this lookahead, the action that is selected is the one on the path to the best leaf, with “best” defined in terms of the evaluation function f .n/ D g.n/ C h.n/. is form of lookahead ensures that the action selected is the one that is best within the planning horizon H for the given heuristic, yet this horizon must be kept small, else the local lookahead search cannot be completed in real time. A useful alternative lookahead scheme over small time windows is the combination of a larger horizon H along with a heuristic search algorithm that operates within this horizon (deeper nodes are still pruned) as an anytime optimal algorithm. For example, the lookahead search can be done with the A* algorithm from the current state s. en, when time is up, whether the search is finished or not, the action selected in s is taken as the one leading to the best leaf, yet with the leaves being both the nodes at depth H that have been generated plus the nodes that have been generated at any other level that have not been yet expanded. Algorithms like Anytime WA* can also be convenient for this type of anytime optimal lookahead search. While a depth-first or best-first lookahead can improve the quality of the actions selected in the greedy online search algorithm, neither approach guarantees completeness and optimality, except in the trivial case where a solution and an optimal solution are within an horizon H of the seed state. On the other hand, there is a simple fix to the greedy search that delivers both completeness and optimality. e fix is due to Richard Korf, and the resulting algorithm is known as Learning Real Time A* or LRTA* [Korf, 1990]. LRTA* is an extremely simple, powerful, and adaptable algorithm, that as we will see generalizes naturally to MDPs. LRTA* is the online greedy search algorithm with one change: once the action a that minimizes the estimated cost-to-go term Q.a; s/ from s is applied, the heuristic value h.s/ is updated to Q.a; s/. e code for LRTA* is shown in Figure 2.2 where the dynamically changing
4In the planning setting, the algorithm actually does hill descending. e name of the algorithm, however, comes from contexts where states that maximize a given function are sought.


22 CLASSICAL PLANNING
LRTA*
% h is the initial value function and V is the hash table that stores the updated % values. When fetching a value for s in V , if V does not contain an entry for s, % an entry is created with value h.s/
Let s WD s0 while s is not a goal state do
Evaluate each action a 2 A.s/ as: Q.a; s/ WD c.a; s/ C V .f .a; s// Select best action a that minimizes Q.a; s/ Update value V .s/ WD Q.a; s/ Set s WD f .a; s/ end while
Figure 2.2: Single Trial of Learning Real Time A* (LRTA*)
heuristic function, initially set to h.s/, is denoted as V .s/. For the implementation of LRTA*, the estimates V .s/ are stored in a hash table that initially contains the heuristic value h.s0/ of the initial state s0 only. en, when the value of a state s that is not in the table is needed, a new entry for s with value V .s/ D h.s/ is allocated. ese entries V .s/ are updated as
V .s/ WD mina2A.s/ Q.a; s/ D mina2A.s/ Œc.a; s/ C V .s0/ç (2.2)
where s0 D f .a; s/, when the action a D argmina2A.s/ Q.a; s/ is applied in the state s. is simple
greedy algorithm combined with these updates delivers the two key properties provided that the heuristic h.s/ is admissible and that there are no dead-ends (states from which the goal cannot be reached). First, LRTA* will not be trapped into a loop and will eventually reach the goal. Second, if upon reaching the goal, the search is restarted from the same initial state while keeping the current heuristic function V , and this process is repeated iteratively, eventually LRTA* converges to an optimal path to the goal. is convergence will be achieved in a finite number of iterations, and the convergence is achieved when the updates V .s/ WD mina2A.s/ Q.a; s/ do not change the value V .s/ of any of the states encountered in the way to the goal, which are then optimal. ese are two remarkable properties that follow from a simple change in the greedy algorithm that adjusts the value of the heuristic according to Eq. 2.2 over the states that are visited in the search. Of course, LRTA*, unlike the greedy algorithm, does not run in constant space, as the updates to the heuristic function take space in the hash table that in the worst case can become as large as the number of states in the problem. e value of the initial heuristic is critical in the performance of LRTA*, both in terms of time and space, as better heuristic values mean a more focused search, and a more focused search means more updates on the states that matter. When LRTA* is to be run once and not until convergence, a lookahead can improve the quality of the actions selected and boost the heuristic values of the visited states (which remain admissible if they are initially admissible). e latter can be achieved if the states that are expanded in the lookahead search are also updated using Eq. 2.2, and the new values are propagated up to their parents. In this way, a move from s will leave a heuristic


2.5. WHERE DO HEURISTICS COME FROM? 23
8)&3& %0 )&63*45*$4 $0.& '30.
14 12 15 13
5879
6 2 11 10
1 34
'JHVSF ɩF TMJEJOH QV[[MF XIFSF UIF HPBM JT UP HFU UP B DPOmHVSBUJPO XIFSF UIF UJMFT BSF PSEFSFE CZ OVNCFS XJUI UIF FNQUZ TRVBSF MBTU ɩF BDUJPOT BMMPXFE BSF UIPTF UIBU TMJEF B UJMF JOUP UIF FNQUZ TRVBSF 8IJMF UIF QSPCMFN JT OPU TJNQMF UIF IFVSJTUJD UIBU TVNT UIF IPSJ[PO BOE WFSUJDBM EJTUBODFT PG FBDI UJMF UP JUT UBSHFU QPTJUJPO JT TJNQMF UP DPNQVUF BOE QSPWJEFT JOGPSNBUJWF FTUJNBUFT UP UIF HPBM *O QMBOOJOH IFVSJTUJD GVODUJPOT BSF PCUBJOFE BVUPNBUJDBMMZ GSPN UIF QSPCMFN SFQSFTFOUBUJPO
8)&3& %0 )&63*45*$4 $0.& '30.
)FVSJTUJD TFBSDI BMHPSJUINT FYQSFTT B GPSN PG HPBM EJSFDUFE TFBSDI XIFSF IFVSJTUJD GVODUJPOT BSF VTFE UP HVJEF UIF TFBSDI UPXBSET UIF HPBM " LFZ RVFTUJPO JT IPX TVDI IFVSJTUJDT DBO CF PCUBJOFE GPS B HJWFO QSPCMFN " VTFGVM IFVSJTUJD JT POF UIBU QSPWJEFT HPPE FTUJNBUFT PG UIF DPTU UP UIF HPBM BOE DBO CF DPNQVUFE SFBTPOBCMZ GBTU )FVSJTUJDT IBWF CFFO USBEJUJPOBMMZ EFWJTFE BDDPSEJOH UP UIF QSPCMFN BU IBOE <&EFMLBNQ BOE 4DISÚEM 1FBSM > UIF &VDMJEFBO EJTUBODF JT B HPPE IFVSJTUJD GPS SPVUF mOEJOH UIF TVN PG UIF .BOIBUUBO EJTUBODFT PG FBDI UJMF UP JUT EFTUJOBUJPO JT B HPPE IFVSJTUJD GPS UIF TMJEJOH QV[[MFT UIF BTTJHONFOU QSPCMFN IFVSJTUJD JT HPPE GPS 4PLPCBO <+VOHIBOOT BOE 4DIBFĊFS > CPUI UIF BTTJHONFOU QSPCMFN BOE TQBOOJOH USFF IFVSJTUJDT IBWF CFFO VTFE GPS UIF 5SBWFMMJOH 4BMFTNBO 1SPCMFN <-BXMFS FU BM > BOE TP PO ɩF HFOFSBM JEFB UIBU FNFSHFT GSPN UIF WBSJPVT QSPCMFNT JT UIBU IFVSJTUJDT DBO CF TFFO BT FODPEJOH UIF DPTU PG SFBDIJOH UIF HPBM GSPN UIF TUBUF JO B QSPCMFN UIBU JT TJNQMFS UIBO UIF PSJHJOBM POF <.JOTLZ 1FBSM 4JNPO > 'PS FYBNQMF UIF TVN PG .BOIBUUBO EJTUBODFT JO UIF TMJEJOH QV[[MFT 'JHVSF DPSSFTQPOET UP UIF PQUJNBM DPTU PG B TJNQMJmDBUJPO PG UIF QV[[MF XIFSF UJMFT DBO CF NPWFE UP BEKBDFOU QPTJUJPOT XIFUIFS UIFTF QPTJUJPOT BSF FNQUZ PS OPU 4JNJMBSMZ UIF &VDMJEFBO IFVSJTUJD GPS SPVUF mOEJOH JT UIF DPTU PG B TJNQMJmDBUJPO PG UIF QSPCMFN XIFSF TUSBJHIU SPVUFT BSF BEEFE CFUXFFO BOZ QBJS PG DJUJFT JO UIF NBQ ɩF TJNQMJmFE QSPCMFNT BSF OPSNBMMZ SFGFSSFE UP BT SFMBYBUJPOT PG UIF PSJHJOBM QSPCMFN *G JT UIF PSJHJOBM QSPCMFN JT JUT SFMBYBUJPO BOE BOE SFGFS UP UIF QSPCMFN BOE SFMBYBUJPO XIFO UIF JOJUJBM TUBUF JT TFU UP UIF TUBUF UIF HFOFSBM JEFB JT UP TFU UIF IFVSJTUJD WBMVF BTTPDJBUFE XJUI UIF QSPCMFN UP UIF PQUJNBM DPTU PG UIF SFMBYFE QSPCMFN *U JT FBTZ UP TIPX UIBU JG UIF TPMVUJPOT UP UIF PSJHJOBM QSPCMFN BSF BMTP TPMVUJPOT PG UIF SFMBYFE QSPCMFN TPNFUIJOH XIJDI JT OBUVSBM GPS NPTU SFMBYBUJPOT UIFO UIF IFVSJTUJD UIBU SFTVMUT GSPN UIF SFMBYBUJPO JT BDUVBMMZ BENJTTJCMF ɩJT JT CFDBVTF BOZ PQUJNBM TPMVUJPO GPS NVTU CF BMTP B TPMVUJPO UP UIF SFMBYBUJPO XIPTF PQUJNBM WBMVF DBOOPU FYDFFE UIFO UIF PQUJNBM WBMVF PG 0O UIF PUIFS IBOE JG UIF IFVSJTUJD GPS JT
Figure 2.3: e sliding 15-puzzle where the goal is to get to a configuration where the tiles are ordered by number with the empty square last. e actions allowed are those that slide a tile into the empty square. While the problem is not simple, the heuristic that sums the horizon and vertical distances of each tile to its target position is simple to compute and provides informative estimates to the goal. In planning, heuristic functions are obtained automatically from the problem representation.
value V .s/ for s that would be more informed than the value of s computed from its children using Eq. 2.2. LSS-LRTA* is a version of LRTA* with a lookahead of this type [Koenig and Sun, 2009].
2.5 WHERE DO HEURISTICS COME FROM?
Heuristic search algorithms express a form of goal-directed search where heuristic functions are used to guide the search toward the goal. A key question is how such heuristics can be obtained for a given problem. A useful heuristic is one that provides good estimates of the cost to the goal and can be computed reasonably fast. Heuristics have been traditionally devised according to the problem at hand [Edelkamp and Schrödl, 2012, Pearl, 1983]: the Euclidean distance is a good heuristic for route finding, the sum of the Manhattan distances of each tile to its destination is a good heuristic for the sliding puzzles, the assignment problem heuristic is good for Sokoban [Junghanns and Schaeffer, 2001], both the assignment problem and spanning tree heuristics have been used for the Travelling Salesman Problem [Lawler et al., 1985], and so on. e general idea that emerges from the various problems is that heuristics h.s/ can be seen as encoding the cost of reaching the goal from the state s in a problem that is simpler than the original one [Minsky, 1961, Pearl, 1983, Simon, 1955]. For example, the sum-of-Manhattan distances in the sliding puzzles (Figure 2.3) corresponds to the optimal cost of a simplification of the puzzle where tiles can be moved to adjacent positions, whether these positions are empty or not. Similarly, the Euclidean heuristic for route finding is the cost of a simplification of the problem where straight routes are added between any pair of cities in the map. e simplified problems are normally referred to as relaxations of the original problem. If P is the original problem, P 0 is its relaxation, and P .s/ and P 0.s/ refer to the problem and relaxation when the initial state is set to the state s, the general idea is to set the heuristic value hP .s/ associated with the problem P .s/ to the optimal cost hP 0.s/ of the relaxed problem P 0.s/. It is easy to show that if the solutions to the original problem P .s/ are also solutions of the relaxed problem P 0.s/, something which is natural for most relaxations, then the heuristic hP .s/ that results from the relaxation is actually admissible. is


24 CLASSICAL PLANNING
is because any optimal solution for P .s/ must be also a solution to the relaxation P 0.s/, whose optimal value cannot exceed then the optimal value of P .s/. On the other hand, if the heuristic hP .s/ for P is obtained from a solution to the relaxation P 0.s/ that is not necessarily optimal, the resulting heuristic hP .s/ would not be necessarily admissible. A key development in modern planning research was the realization that useful heuristics could be derived automatically from the representation of the problem in a domain-independent planning language [Bonet et al., 1997, McDermott, 1996]. It does not matter what the problem P .s/ is about, an automated relaxation P 0.s/ yielding informative heuristics can be obtained directly and effectively from the representation of P .s/. e result is a domain-general heuristic h.s/, i.e., a heuristic that makes the search goal-driven, no matter what the problem is about, as long as it is a problem where deterministic actions expressed in compact form have to be used to drive the system from a known initial state into a goal state.
2.6 LANGUAGES FOR CLASSICAL PLANNING
e languages for expressing classical planning models in compact form come in two main varieties. In one, the state variables are all boolean, i.e., they can take just one of two values, true or false. In the other, they are multivalued and can take values from a finite domain. In either case, the states in the resulting model are the valuations over the variables, where a valuation assigns to each variable a value from its domain. e simplest and possibly oldest classical planning language in use is STRIPS, a language based on boolean variables, which was originally developed in a different form for controlling the Robot Shakey at SRI during the late 60s [Fikes and Nilsson, 1971]. A planning problem in the current version of STRIPS is a tuple P D hF; I; O; Gi where
• F represents the set of atoms or propositions of interest,
• O represents the set of actions,
• I F represents the initial situation, and
• G F represents the goal.
In STRIPS, the actions o 2 O are represented by three sets of atoms over F called the Add, Delete, and Precondition lists, denoted as Add.o/, Del.o/, P re.o/. e first describes the atoms that the action o makes true, the second, the atoms that o makes false, and the third, the atoms that must be true in order for the action to be applicable. A STRIPS problem P D hF; I; O; Gi encodes implicitly, in compact form, the classical state model S.P / D hS; s0; SG; A; f; ci where
• the states s 2 S are the possible collections of atoms over F , each defining a truth valuation where an atom p 2 F is true in s iff p 2 s,
• the initial state s0 is I ,
• the set SG of goal states comprises the states s for which G s,
• the actions a in A.s/ are the ones in O with P rec.a/ s,


2.6. LANGUAGES FOR CLASSICAL PLANNING 25
• the state transition function is f .a; s/ D .s n Del.a// [ Add.a/, so that the state s0 that results from action a in s is s but with the atoms in Del.a/ deleted, and the atoms in Add.a/ added, and
• the action costs c.a; s/ are equal to 1 by default.
Given that the STRIPS problem P represents the state model S.P /, the plans for P are defined as the plans for S.P /, namely, the action sequences that map the initial state s0 that corresponds to I into a goal state where the goals G are true. Since the states in S.P / are represented as collections of atoms from F , the number of states in S.P / is 2jF j where jF j is the number of atoms in P , usually called fluents. e state representation that follows from a planning language such as STRIPS is domainindependent. us, while a specialized solver for a Blocks World problem may represent the state of the problem by a set of lists, each one representing a tower of blocks, in the state representation that follows from STRIPS there are just atoms, and the same is true of any other domain. As an illustration, a domain that involves three locations l1, l2, and l3, and three tasks t1, t2, and t3, where ti can be performed only at location li , can be modeled with a set F of fluents at.li / and done.ti /, and a set O of actions go.li ; lj / and do.ti /, i; j D 1; : : : ; 3, with precondition, add, and delete lists
P re.a/ D fat .li /g ; Ad d.a/ D fat .lj /g ; Del.a/ D fat .li /g
for a D go.li ; lj /, and
P re.a/ D fat .li /g ; Ad d.a/ D fd one.ti /g ; Del.a/ D fg
for a D do.ti /. e problem of doing tasks t1 and t2 starting at location l3 can then be modeled by the tuple P D hF; I; O; Gi where
I D fat .l3/g and G D fd one.t1/; d one.t2/g :
A solution to P is an applicable action sequence that maps the state s0 D I into a state where the goals in G are all true. In this case one such plan is the action sequence
D hgo.l3; l1/; d o.t1/; go.l1; l2/; d o.t2/i :
e number of states in the problem is 26 as there are six boolean variables. Still, it can be shown that many of these states are not reachable from the initial state. Indeed, the atoms at.li / for i D 1; 2; 3 are mutually exclusive and exhaustive, meaning that every state reachable from s0 by applying the available actions makes one and only one of these atoms true. ese boolean variables encode indeed the possible values of the multivalued variable that represents the agent’s location. Planning languages featuring non-boolean variables and richer syntactic constructs than STRIPS are also common in planning [Bäckström and Nebel, 1995, Gerevini et al., 2009]. In particular, if X is a multivalued variable with domain DX , then the initial situation can be characterized by a set of literals of the form ‘X D x” for each variable X where x 2 DX , the actions can be described in terms of pre and postconditions expressed through these literals, and the same for goals. In principle, a planning problem expressed through multivalued variables can be compiled automatically into a problem with boolean variables only, by simple transformations such as replacing each literal X D x


26 CLASSICAL PLANNING
by the proposition p.X D x/ throughout, and by including the propositions p.X D x0/ in the delete list of every action that adds p.X D x/. Planning problems expressed over boolean variables such as STRIPS can be similarly expressed in multivalued form by mapping atoms p into literals Xp D t rue throughout, except in delete lists when they are mapped into postconditions Xp D f alse. Often it is possible to derive a more compact multivalued encoding of a planning problem, e.g., like when a set of atoms at.l1/, ..., at.ln/ is used to represent the possible locations of an object. Such a location can be encoded through a multivalued variable L with domain DL D fl1; : : : ; lng. Programs that automatically infer invariants from a STRIPS encoding, such as sets of exhaustive and mutually exclusive atoms, are used to transform one representation into another automatically [Helmert, 2009]. While some of the representation languages are more natural for users, they are not necessarily more efficient for planning as the extra features can often be compiled away at no cost. For example, STRIPS does not accommodate negation or negated atoms, yet when it is convenient to introduce a negated atom :p in the initial situation, preconditions, or goals of a problem, it is possible to introduce a new atom pN for capturing :p. e atom pN has to be part of I when p 62 I , has to be included in the Add list of an action when p is included in the Delete list, and vice versa, has to be included in the Delete list when p is in Add list. en the atom pN can be used in preconditions and goals, as indeed, pN represents :p over all the states that are reachable from I using the available actions, where the logical formula pN :p can be shown to hold. In other words, the formula pN :p is an invariant in the problem. One important syntactic construct that extends STRIPS and is not convenient to compile away in general, because of a potential exponential blow up in the size of the problem, is conditional effects [Gazen and Knoblock, 1997, Nebel, 2000]. While Add and Delete lists represent sets of atoms that become true and false unconditionally after an action is done, a conditional effect C ! C 0 associated with an action, where C and C 0 are sets of literals (atoms or negated atoms), says that C 0 will be true right after the action if C was true right before the action. In other words, unlike an action precondition, C does not have to be true for the action to be applicable, yet if it is true, then C 0 will become true as a result of the action. Figure 2.4 shows a description of the Blocks World domain in PDDL. PDDL is the Planning Domain Definition Language, a language and syntax that has been used in the planning competitions [McDermott et al., 1998]. PDDL accommodates the STRIPS language along with a number of additional syntactic constructs in a notation that originates in the Lisp programming language. Problems in PDDL are expressed in two parts: one about the general domain; the other about a particular domain instance. In the domain part, the actions are described by means of schemas over generic atoms defined using predicates names like clear, variables like ‹x, and possibly constants. In the instance part, the object names that will replace the variables are declared, along with the atoms describing the initial state, and the formula describing the goal states. e “requirement” flag in the domain definition describes the PDDL fragment used by the encoding which can include STRIPS, ADL extensions featuring negation, conditional effects, function symbols, and various forms of quantification [Pednault, 1989], a hierarchy of types for controlling how variables can be substituted by object names, the equality predicate, and so on. ere are currently tens of classical planners that can be downloaded free from the Internet and hundreds of planning problems expressed in PDDL for use with such planners.


2.7. DOMAIN-INDEPENDENT HEURISTICS AND RELAXATIONS 27
(define (domain BLOCKS) (:requirements :strips) (:predicates (clear ?x) (on-table ?x) (arm-empty) (holding ?x) (on ?x ?y)) (:action stack :parameters (?x ?y) :precondition (and (holding ?x) (clear ?y)) :effect (and (not (holding ?x)) (not (clear ?y)) (clear ?x) (on ?x ?y) (handempty))) (:action unstack :parameters (?x ?y) :precondition (and (clear ?x) (on ?x ?y) (handempty)) :effect (and (not (clear ?x)) (not (on ?x ?y)) (not (handempty)) (holding ?x) (clear ?y))) (:action put_down :parameters (?x) :precondition (holding ?x) :effect (and (not (holding ?x)) (clear ?x) (ontable ?x) (handempty))) (:action pick_up :parameters (?x) :precondition (and (clear ?x) (ontable ?x) (handempty)) :effect (and (not (clear ?x)) (not (ontable ?x)) (not (handempty)) (holding ?x))) )
(define (problem BLOCKS_6) (:domain BLOCKS) (:objects A B C D E F) (:init (clear B) (clear C) (clear E) (ontable C) (ontable D) (on A D) (on B A) (ontable F) (on E F) (handempty)) (:goal (and (on E F) (on F C) (on C B) (on B A) (on A D))) )
Figure 2.4: A Blocks World instance described in PDDL.
2.7 DOMAIN-INDEPENDENT HEURISTICS AND RELAXATIONS
A STRIPS planning problem P D hF; I; O; Gi defines a state model and a directed graph so that the plans for P corresponds to paths connecting a source node to a target node in the graph. e size of the graph, however, is exponential in the number of atoms in P , and this prevents the use of blind search methods in general. As mentioned, a key development in modern planning research was the realization that this search could be guided by heuristics extracted automatically from the problem [Bonet and Geffner, 2001, McDermott, 1999]. e heuristics are derived from relaxations, and the most common and useful domain-independent relaxation in planning is the delete-relaxation, that maps a problem P D hF; I; O; Gi into a problem P C D hF; I; OC; Gi that is exactly like P but with


28 CLASSICAL PLANNING
the actions in OC set to the actions in O with empty delete lists. at is, the delete-relaxation is a domain-independent relaxation that takes a planning problem P and produces another problem P C where atoms are added exactly as in P but they are never deleted. e relaxation implies, for example, that objects and agents can be in “multiple places” at the same time as when an object or an agent moves into a new place, the atom representing the old location is not deleted. Relaxations, however, are not aimed at providing accurate models of the world; quite the opposite, simplified and even meaningless models of the world that while not accurate yield useful heuristic guidance. e domain-independent delete-relaxation heuristic is obtained as an approximation of the optimal cost of the delete-relaxation P C obtained from the cost of a plan that solves P C not necessarily optimally. e reason that an approximation is needed is because finding an optimal plan for a deletefree planning problem like P C.s/ is still a computationally intractable task (also NP-hard). On the other hand, finding just one plan for the relaxation whether optimal or not, can be done quickly and efficiently. e property that allows for this is decomposability: a problem without deletes is decomposable in the sense that a plan for a joint goal G1 and G2 can always be obtained from a plan 1 for the goal G1 and a plan 2 for the goal G2. e concatenation of the two plans D 1; 2 in either order is indeed one such plan. is property allows for a simple method for computing plans for the relaxation from which the heuristics are derived. e main idea behind the procedure for computing the heuristic h.s/ for an arbitrary planning problem P can be explained in a few lines. For this, let P .s/ refer to the problem that is like P but with the initial situation set to the state s, and let P C.s/ stand for the delete-relaxation of P .s/, i.e., the problem that is like P .s/ but where the delete-lists are empty. e heuristic h.s/ is computed from a plan for the relaxation P C.s/ that is obtained using the decomposition property and a simple iteration. Basically, the plans for achieving the atoms p that are already true in the state s, i.e., p 2 s, are the empty plans. en if 1, 2, ..., m are the plans for achieving each of the preconditions p1; p2; : : : ; pm of an action a that has the atom q in the add list, D 1; 2; : : : ; m followed by the action a is a plan for achieving q from s. It can be shown that this iteration yields a plan in the relaxation P C.s/ for each atom p that has a plan in the original problem P .s/ in a number of steps that is bounded by the number of atoms in the problem. A plan for the actual goal G of P .s/ in the relaxation P C.s/ can then be obtained in a similar manner by concatenating the plans for each of the atoms q in G in any order. Such a plan for the relaxation P C.s/, denoted as C.s/, is called a relaxed plan. e heuristic h.s/ can then be set to the cost of such a plan. A better estimate can be obtained if duplicate actions in the resulting relaxed plan are removed, since no STRIPS action needs to be done twice for solving a delete-free problem, as the effects of the first action occurrence stay true until the end of the plan.5 Below we will formalize the domain-independent planning heuristic sketched above, and explore some variations. As an illustration, Figure 2.5 displays a fragment of the directed graph corresponding to a blocks world problem P with the automatically derived heuristic values next to some of the nodes. e heuristic values shown are computed very fast, in low polynomial time, using an algorithm similar to the one described above, with h.s/ representing an approximation of the number of actions needed (cost) to solve the relaxed problem P C.s/. Actually, the instance shown can be solved without any search at all by just selecting in each node, starting from the source node, the action that leads to the node with a lower heuristic value (closer to the goal). e resulting plan is shown as a red path in the figure.
5is is not true, however, in planning languages that extend STRIPS with negation and conditional effects, where the same action may have to be applied multiple times for solving the relaxation.


2.7. DOMAIN-INDEPENDENT HEURISTICS AND RELAXATIONS 29
%0."*/ */%&1&/%&/5 )&63*45*$4 "/% 3&-"9"5*0/4
A
BC
ABC
A
B
C
A
BC
··· ···
AB
C
AC
AB
BC
A
BC A
B
C AB
C
AB
C
A ABC
C
B
··· ··· ··· ··· ···
Init
Goal
h=3
h=3 h=2 h=3
h=3
h=3 h=2 h=1 h=2 h=2
h=0 h=2 h=2
'JHVSF " GSBHNFOU PG UIF HSBQI DPSSFTQPOEJOH UP B CMPDLT XPSME QMBOOJOH QSPCMFN XJUI UIF BVUPNBUJDBMMZ EFSJWFE IFVSJTUJD WBMVFT TIPXO OFYU UP TPNF PG UIF OPEFT ɩF IFVSJTUJD WBMVFT BSF DPNQVUFE JO MPX QPMZOPNJBM UJNF BOE QSPWJEF UIF TFBSDI XJUI B TFOTF PG EJSFDUJPO ɩF JOTUBODF DBO BDUVBMMZ CF TPMWFE XJUIPVU BOZ TFBSDI CZ KVTU QFSGPSNJOH JO FBDI TUBUF UIF BDUJPO UIBU MFBET UP UIF OPEF XJUI B MPXFS IFVSJTUJD WBMVF DMPTFS UP UIF HPBM ɩF SFTVMUJOH QMBO JT TIPXO JO SFE IFMQGVM BDUJPOT TIPXO JO CMVF
NPWJOH $ UP " JT OFFEFE ɩF SFTVMU JT B IFVSJTUJD WBMVF BT TIPXO XIJDI BDUVBMMZ DPJODJEFT JO UIJT DBTF XJUI UIF DPTU PG UIF CFTU QMBO UP BDIJFWF UIF KPJOU HPBM GSPN JO UIF OPO SFMBYFE QSPCMFN /POFUIFMFTT UIJT JT KVTU B DPJODJEFODF BOE JOEFFE UIF CFTU QMBOT JO UIF SFMBYBUJPO DBO CF RVJUF EJĊFSFOU UIBO UIF CFTU QMBOT JO UIF PSJHJOBM QSPCMFN ɩF CFTU QMBO GPS JT JOEFFE VOJRVF NPWJOH " UP UIF UBCMF UIFO $ PO " BOE mOBMMZ # PO $ 0O UIF PUIFS IBOE B QPTTJCMF PQUJNBM QMBO JO UIF SFMBYBUJPO JT UP NPWF mSTU $ PO " UIFO " PO UIF UBCMF BOE mOBMMZ # PO $ 0G DPVSTF UIJT QMBO EPFT OPU NBLF BOZ TFOTF JO UIF SFBM QSPCMFN XIFSF " DBOU CF NPWFE XIFO DPWFSFE CZ $ ZFU UIF SFMBYBUJPO JT OPU BJNFE BU DBQUVSJOH UIF SFBM QSPCMFN PS UIF SFBM QIZTJDT JU JT BJNFE BU QSPEVDJOH JOGPSNBUJWF CVU RVJDL FTUJNBUFT PG UIF DPTU UP UIF HPBM ɩF SFBEFS DBO WFSJGZ UIBU GPS UIF MFGUNPTU DIJME PG UIF JOJUJBM TUBUF UIF DPTUT PG UIF QSPCMFN BOE UIF SFMBYBUJPO OP MPOHFS DPJODJEF ɩF GPSNFS JT XIJMF UIF MBUUFS JT UIF EJĊFSFODF BSJTJOH GSPN UIF HPBM A$ PO " UIBU JO UIF PSJHJOBM QSPCMFN NVTU CF VOEPOF BOE UIFO SFEPOF *O UIF SFMBYBUJPO UIJT JT OFWFS OFFEFE BT OP BUPN JT FWFS EFMFUFE ɩF IFVSJTUJDT GPS DMBTTJDBM QMBOOJOH UIBU IBWF CFFO EFWFMPQFE TP GBS BMM BTTVNF UIBU BDUJPOT DPTUT BSF PS EFQFOE BU NPTU PO UIF BDUJPO CVU OPU PO UIF TUBUF J F *O QSJODJQMF UIFSF JT OP QSPCMFN JO FYQSFTTJOH BSCJUSBSZ BDUJPO DPTUT JO DPNQBDU GPSN MJLF GPS
Figure 2.5: A fragment of the graph corresponding to a blocks world planning problem with the automatically derived heuristic values shown next to some of the nodes. e heuristic values are computed in low polynomial time and provide the search with a sense of direction. e instance can actually be solved without any search by just performing in each state the action that leads to the node with a lower heuristic value (closer to the goal). e resulting plan is shown in red; helpful actions shown in blue.
In order to get a more vivid idea of where the heuristic values shown in the figure come from, consider the heuristic h.s/ for the initial state where block A is on B, and both B and C are on the table. In order to get the goal “B on C” in the relaxation from the state s, two actions are needed: one to get A out of the way to achieve the preconditions for moving B, the second to move B on top of C. On the other hand, in order to achieve the second goal “C on A” in the relaxation from s, just the action of moving C to A is needed. e result is a heuristic value h.s/ D 3 as shown, which actually coincides in this case with the cost of the best plan to achieve the joint goal from s in the non-relaxed problem P .s/. Nonetheless, this is just a coincidence, and indeed, the best plans in the relaxation P C.s/ can be quite different than the best plans in the original problem P .s/. e best plan for P .s/ is indeed unique: moving A to the table, then C on A, and finally B on C. On the other hand, a possible optimal plan in the relaxation P C.s/ is to move first C on A, then A on the table, and finally B on C. Of course, this plan does not make any sense in the real problem where A can’t be moved when covered by C, yet the relaxation is not aimed at capturing the real problem or the real physics; it is aimed at producing informative but quick estimates of the cost to the goal. e reader can verify that for the leftmost child s0 of the initial state s, the costs of the problem P .s0/ and the relaxation P C.s0/ no longer coincide. e former is 4, while the latter is 3, the difference arising from


30 CLASSICAL PLANNING
the goal “C on A” that in the original problem must be undone and then redone. In the relaxation this is never needed as no atom is ever deleted. e heuristics for classical planning that have been developed so far, all assume that actions costs c.a; s/ are 1 or depend at most on the action a but not on the state s, i.e., c.a; s/ D c.a/. In principle, there is no problem in expressing arbitrary action costs c.a; s/ in compact form, like for instance saying that c.a; s/ is 1 except that when s makes both p and q true where it is 100. Yet, while such cost structures are important and are often needed, they have not been addressed systematically in the literature so far, and thus there are not yet good heuristics for handling them in planning.
ADDITIVE AND MAX HEURISTICS
One of the first heuristics developed for domain-independent planning operate on the deleterelaxation but doesn’t involve the computation of relaxed plans. We review this heuristic below along with some variations that actually do. In order to simplify the definition, we introduce a new dummy End action with zero cost, whose preconditions G1; : : : ; Gn are the goals of the problem, and whose effect is a dummy atomic goal G. e heuristics h.s/ simply estimates the cost of achieving this “dummy” goal G from the state s. Since the heuristic hC that represents the optimal cost function of the delete-relaxation is intractable, the additive heuristic hadd introduces a polynomial approximation in which subgoals are assumed to be independent in the sense that they are achieved with no “side effects” [Bonet et al., 1997]. is assumption is normally false, but results in a simple heuristic function
hadd .s/ def
D hadd .P re.End /I s/ (2.3)
that can be computed quite efficiently in every state s visited in the search, where hadd .P re.a/I s/ is an estimate of the cost of achieving the preconditions of action a from s, defined from the expressions:
hadd .pI s/ def
D
0 if p 2 s
mina2O.p/ Œcost .a/ C hadd .P re.a/I s/ç otherwise (2.4)
and
hadd .P r e.a/I s/ def
D
P
q2P re.a/ hadd .qI s/ : (2.5)
In these expressions, hadd .pI s/ stands for the estimated cost of achieving the atom p from s, O.p/ stands for the actions in the problem that add p, and hadd .P re.a/I s/ stands for the estimated cost of achieving the preconditions of the actions a from s. Versions of the additive heuristic appear in several planners [Bonet and Geffner, 2001, Do and Kambhampati, 2001, Smith, 2004], where the cost of the joint condition in action preconditions (and goals) is set to the sum of the costs of each condition in isolation. e additive heuristic hadd is neither a lower bound nor an upper bound on the optimal cost function h over the original problem. e reason is that the cost of achieving two atoms jointly from a state s can be lower or higher than the sum of the costs of achieving each one of them individually. In particular, if a is a unit-cost action with preconditions that are true in s and atoms p and q in the Add list that are not true in s, then the heuristic hadd .s/ for the goal G D fp; qg will be 2, while clearly the optimal cost h .s/ of achieving G in the problem from s is 1. Likewise, if there is


2.7. DOMAIN-INDEPENDENT HEURISTICS AND RELAXATIONS 31
instead just one action that adds p and deletes q, and one action that adds q and deletes p, then the heuristic hadd .s/ would still be 2, while the optimal cost h .s/ of achieving G in the problem would be infinity. G is indeed unachievable in such a problem where the formula :.p ^ q/ is an invariant. If the estimated cost of the joint condition in Eq. 2.5 is changed from the sum to the maximum,
hmax.P r e.a/I s/ def
D maxq2P re.a/ hmax.qI s/ ; (2.6)
a different heuristic is obtained by setting hmax.s/ to hmax.P re.End /I s/, and replacing hadd by hmax in Eq. 2.4. Since the cost of achieving several atoms from a state s can never be lower than the cost of achieving one of them, the max-heuristic hmax unlike the additive heuristic hadd is admissible, and hence, potentially useful for computing optimal plans in combination with algorithms such as A* or IDA*. Still, the max-heuristic is not informative enough in general, as it ignores all but one of the atoms of each action precondition. In this sense, the heuristic hadd is not admissible but is better at discriminating good from bad actions, as no precondition is left out from the computation. e equations for hadd and hmax basically define a path-finding problem over atom space as opposed to the planning problem that is a path-finding problem over the exponentially larger state space. Indeed, any shortest-path algorithm can be used for computing these heuristics, including Dijkstra’s algorithm, Bellman and Ford’s, or Value Iteration [Bertsekas, 1995, Cormen et al., 2009]. A single change is needed though: while the nodes of the graph are the problem atoms, the edges, that correspond to problem actions are actually hyperedges rather than normal edges, as they link the set of atoms appearing in an action precondition with each of the atoms appearing in the Add list. So, while an edge .n; n0/ in a normal directed graph induces a cost c.n0/ c.n/ C w.n; n0/ on the target node n0, a (directed) hyperedge .fn1; : : : ; nkg; n0/ associated with an action a induces a cost
c.n0/ P
iD1;k c.ni / C c.a/ instead, in the additive heuristic. For any of the algorithms, the costs c.n/ can be initialized to 0 for the nodes n corresponding to atoms p that are true in s, and to 1
for all other atoms. All the algorithms use the inequalities c.n0/ P
iD1;k c.ni / C c.a/ as updates
of the form c.n0/ WD min.c.n0/; P
iD1;k c.ni / C c.a//, and differ in the order in which these updates are performed and the conditions under which they are terminated. Dijkstra’s algorithm for example updates nodes n0, once, in order, according to the value of the right-hand side expression, lowest first. Bellman and Ford’s algorithm, and Value Iteration, do not pay the overhead required for following this order, but end up updating nodes many times. In all cases, the computation is polynomial and finishes in Dijkstra’s algorithm when there are no more nodes to update, while in Bellman and Ford’s algorithm and in Value Iteration, when the updates produce no changes. For the max heuristic, the update expression needs to be changed to c.n0/ WD min.c.n0/; maxiD1;k c.ni / C c.a//. An analysis of these various methods for computing the additive heuristic is given by Liu et al. [2002].
RELAXED PLAN HEURISTIC
e heuristic search planners UNPOP and HSP use the additive heuristic in the context of standard heuristic search algorithms [Bonet and Geffner, 2001, McDermott, 1999]. e planner FF that built on these planners introduced two important changes [Hoffmann and Nebel, 2001]: one in the heuristic, and one in the search procedure. We focus here on the heuristic, and consider the search procedure in the next chapter. FF’s heuristic hFF.s/ is set to the cost of a plan FF.s/ for the relaxation P C.s/ that is not necessarily optimal, and which is obtained by running a Graphplan-like procedure [Blum and Furst, 1995] that exploits the decomposability of the delete-free problem in line with the algorithm


32 CLASSICAL PLANNING
sketched above. Basically, a graph made of successive layers P0; A0; P1; A1; : : : where Pi is a set of atoms, and Ai is a set of actions, is constructed for a problem P .s/ D hF; I D s; O; Gi as:
P0 D fp 2 sg
Ai D fa 2 O j P re.a/ Pi g
PiC1 D Pi [ fp 2 Ad d.a/ j a 2 Ai g
until a fixed point is reached; i.e., a layer Pn for which PnC1 D Pn. Here P0 contains all the atoms in s, Ai contains all the actions whose preconditions are true in Pi , and PiC1 contains the positive effects of these actions along with the atoms appearing in previous layers Pk, k i C 1. e resulting layered graph cannot contain more than jF j layers, which happens only when P0 is empty, and PiC1 just contains one more atom than Pi . Moreover, the construction can be stopped when the goal G first appears in a layer PmC1, i.e., G PmC1, as the plan FF.s/ for the relaxation P C.s/ is extracted then backward from that layer. For this, it’s convenient to conceive FF.s/ as a “parallel” plan made up of actions B0 done in “parallel” at time 0, actions B1 done in “parallel” at time 1, and so on until Bm. By actions done in parallel, we mean that the actions can be done in any order, as they will necessarily have the same effect in the relaxation. In addition, during the construction of the graph, each atom p that makes it for the first time in a layer PiC1 is tagged with one of the actions in Ai that adds p which is called the best supporter for p and is denoted by ap. Clearly, there must be one such action, else the atom p would not make it into the layer PiC1. e sets of actions Bi in the relaxed plan FF.s/ is then obtained recursively backward from the set GiC1 of atoms, initially set to G for i D m. en for i D m; m 1; : : : ; 0, and starting with Bi D ;, we add to Bi , the best supporter ap of each of the atoms p in GiC1 that made into the layer PiC1 for the first time, and recursively set Gi to .GiC1 n Ad d.Bi // [ P re.Bi /, where Ad d.Bi / and P re.Bi / are respectively the union of the Add and Precondition lists of the actions in Bi . It is easy to see that the resulting plan FF.s/ made up of this sequence of action sets Bi , where actions in a set can be done in any order, is a plan for the relaxation P C.s/. is is because FF.s/ contains actions that add each of the goals in G, and in addition, every action in FF.s/ has preconditions that are true in the state s or are added by previous actions in the sequence. e heuristic hFF.s/ is defined as the size j FF.s/j of the plan, namely, the number of actions that it contains, thus assuming implicitly that action costs are all 1. Of course, hFF.s/ could be defined instead as the sum of the action costs c.a/ for a in FF.s/, yet this does not address the fact that the relaxed plan was constructed assuming that costs were uniform. An advantage of FF’s heuristic over the additive heuristic is that it is less likely to overcount actions. For example, if there is an action a whose preconditions hold in s with effects p and q that do not, then the additive heuristic for the goal G D fp; qg is 2, while FF’s heuristic will be 1. Still, FF’s heuristic can also overcount if there is a second action that adds q and whose preconditions hold in s. In such a case, FF can produce a relaxed plan where the atom p is supported by the first action, and the atom q is supported by the second action. An additional limitation of the FF heuristic is that it assumes that all action costs are uniform. ere is however a simple way to combine the benefits of the additive and FF’s heuristic [Keyder and Geffner, 2008a]. For this, all that is required is to change the definition of the best support action ap for each atom p in the computation of the layered graph to
ap
def
D argmina2O.p/ Œc.a/ C hadd .P re.a/I s/ç (2.7)


2.8. HEURISTIC SEARCH PLANNING 33
when p is not true in the state s. ese best supports obtained from the additive heuristic can then be used to build a relaxed plan that no longer ignores action costs. e backward procedure for extracting a plan from these best supports proceeds like above: we collect in the relaxed plan the best supports for the atoms in the goal, and recursively, the best supports of their preconditions, in all cases skipping the atoms that are true in the state s. Actually, the same construction can be used also with the max heuristic. Interestingly, the relaxed plan that would be obtained in this way from the max heuristic is equivalent to the one obtained from FF’s procedure, provided that ties in the selection of best supports are broken in the same way. is is because there is a tight correspondence between the max heuristic and the layered graph constructed by FF. Indeed, it can be easily shown that the heuristic hmax.s/ is equal to the index i of the first propositional layer Pi that contains the goal G of the problem [Haslum and Geffner, 2000]. is also implies that the construction of the layered graph provides an alternative way for computing the hmax heuristic that is still polynomial, although taking more space than the shortest-path formulation over atom space described above.
2.8 HEURISTIC SEARCH PLANNING
e first generation of heuristic search planners in the late 90s looked for plans by plugging one of these heuristics into a standard heuristic search algorithm. HSP, in particular, used the additive heuristic inside a WA* search. e planner FF, on the other hand, used the relaxed planning heuristic hFF.s/ and the relaxed plan FF.s/ itself in a search architecture that works in two phases. In the first phase, an incomplete but fast search called Enforced Hill Climbing (EHC) is performed. If a plan is found, it is reported, else a second phase is triggered consisting of a complete Greedy Best First Search guided by the hFF heuristic. is second phase is not too different from HSP as the search algorithm and the heuristics are similar to the ones used in HSP. e main novelty of FF is in the first phase. First, the EHC search ignores all the actions a that are not “helpful” in a given state, where an action is deemed as helpful in a state s when it is applicable in s and adds a goal or the precondition of an action in the relaxed plan FF.s/ that is not true in s. Second, with the actions pruned in this way, thus reducing significantly the branching factor of the problem, a Breadth-First Search is triggered from s until a state s0 is found with a heuristic value hFF.s0/ that is strictly smaller than hFF.s/. e actions leading from s to s0 are then applied, and the process is repeated from s0 until a state s00 is found with hFF.s/ D 0, which is necessarily a goal state. e EHC search is thus a Greedy search with a lookahead restricted to the actions that are helpful over an horizon that is defined implicitly by the presence of a state with a smaller heuristic value. e EHC search is extremely fast in problems with high branching factors where standard best-first search is unfeasible or too slow. More recent planners like Fast Downward and LAMA [Helmert, 2006, Richter and Westphal, 2010] have managed to improve on FF by incorporating “helpful actions” inside a complete search scheme. In FF, on the other hand, if the incomplete EHC search fails, the useful distinction between “helpful” and “non-helpful” actions is lost. We will come back to these issues in the next chapter. ese planners can scale up to very large problems, but they make no attempt at computing provable optimal solutions. eir performance is assessed empirically by comparing the number of problems that they solve, the time that they take, and the quality of the solutions that they find. Additional information, like the number of nodes that are expanded in the search, provides an indication of how informative are the heuristics and how selective is the search.


34 CLASSICAL PLANNING
2.9 DECOMPOSITION AND GOAL SERIALIZATION
e domain-independent heuristics developed for planning often yield quite impressive results, but can also fail miserably sometimes. It is not surprising when the heuristics fail on domains that are inherently hard [Hoffmann et al., 2007], yet the heuristics can fail on trivial domains too. - is a domain from the 2011 Int. Planning Competition [Coles et al., 2012] where an agent in the middle of a square n n grid must visit all the cells in the grid. is is an extremely simple problem to solve when there are no optimality requirements, yet planners such as HSP and FF do not scale up to values of n greater than 20. Indeed, a Greedy Best-First search guided by the additive heuristic does a memory out for n D 15, and produces plans that are much longer than optimal, at cost 883 for n D 10. e planner LAMA, in the default configuration which uses landmarks—to be explained below—does much better in this domain, solving grids with n up to 30, with relatively good plans. Indeed, for n D 10, the solution found by LAMA has cost 138, much lower than 883. e problem with the additive heuristic in this domain is that when the agent gets closer to one goal, visiting one particular cell, it gets away from other goals. us, huge plateaus must be traversed in the search before the additive heuristic can be decreased. is causes problems both in best-first search algorithms, and in the otherwise quite effective Enforced Hill Climbing (EHC) search. Actually, - is a domain that is simple because it’s easy to decompose and solve, just requiring to visit the closest unvisited cell, one at a time, until all cells have been visited. Indeed, there is a very simple domain-independent heuristic that does not look at the actions in the domain at all, and yet does better than heuristics like hadd .s/ and hFF.s/ that do: it’s the number of unachieved goals heuristic hug .s/ that simply counts the number of top goals in the problem, i.e., atoms in G, that are not true in the state s. A Greedy Best-First search planner with this heuristic does actually better than LAMA and the other planners, solving easily problems with n greater than 50, producing good plans too. For n D 10, the resulting plan has cost 104, smaller than LAMA’s 138, and much smaller than the 883 steps that result from a GBFS search guided by additive heuristic. ere are several lessons to draw from this simple example. First, that heuristics like hadd and hFF are not necessarily better than simpler heuristics like hug , which simply counts the number of unachieved goals, without trying to estimate the cost of achieving them. Second, and related to the first point, heuristics like hadd and hFF fail often to decompose a problem into subproblems, even when this appears trivial and effective. Of course, the number of unachieved goals heuristics is not good enough for itself, because it provides no guidance at all for achieving any of the goals, yet implicitly manages to yield a goal decomposition scheme that in many domains pays off, as indeed, most of the planning problems that are not inherently hard, are decomposable or nearly decomposable problems, where goals can be tackled one at a time, probably undoing slightly and reconstructing previously achieved goals [Korf, 1987, Simon, 1996]. In such cases, a best-first search guided by the number of unachieved subgoals heuristics, in which the additive heuristic is used as a tie-breaker, can do much better than either heuristic alone.
2.10 STRUCTURE, WIDTH, AND COMPLEXITY
ere is a wide gap between the complexity of planning [Bylander, 1994], and the ability of current planners to solve most existing benchmarks in a few seconds. Work on tractable planning has been devoted to the identification of planning fragments that due to syntactic or structural restrictions can be solved in polynomial time; fragments that include for example problems with single atom


2.10. STRUCTURE, WIDTH, AND COMPLEXITY 35
preconditions and goals [Bylander, 1994, Jonsson and Bäckström, 1994]. On the other hand, work on factored planning has appealed instead to mappings of planning problems into Constraint Satisfaction Problems, and the notion of width over CSPs [Amir and Engelhardt, 2003, Brafman and Domshlak, 2006]. e width of a CSP is related to the number of variables that have to be collapsed to ensure that the induced graph underlying the CSP becomes a tree [Dechter, 2003, Freuder, 1982]. e complexity of a CSP is exponential in the problem width, and hence CSP-trees, for example, can be solved in linear time. A notion of width for classical planning using a form of Hamming distance was introduced by Chen and Giménez [2007], where the distance is set to the number of problem variables whose value needs to be changed in order to increase the number of goals achieved. ese proposals all identify planning fragments that can be solved efficiently, yet few of the existing benchmarks fall into these fragments even though they can be solved easily too. A related thread of research has aimed at understanding the performance of modern heuristic search planners by analyzing the characteristics of the optimal delete-relaxation heuristic hC that planners approximate for guiding the search for plans [Hoffmann, 2005, 2011]. For instance, the lack of local minima for hC implies that the search for plans (and hence the global minimum of hC) can be achieved by local search, and this local search is tractable when the distance to the states that decrement hC is bounded by a constant. More recently, a novel width notion for classical planning has been introduced, along with a simple algorithm that solves problems in time and space exponential in their width [Lipovetzky and Geffner, 2012]. Basically, for a STRIPS planning problem P D hF; I; O; Gi, the authors define chains of tuples (sets) of atoms ti , t0 ! t1 ! : : : ! tn that obey two conditions: t0 holds in the initial situation I , and for every optimal plan that achieves jointly all the atoms in ti from I , there is an action a such that followed by a is an optimal plan for tiC1. e sets of atoms ti are like stepping stones in the construction of optimal plans for tiC1, but stepping stones of a special type, where all optimal plans for ti can be extended into optimal plans for tiC1. A chain t0 ! t1 ! : : : ! tn implies a formula W if all the optimal plans for tn are also optimal plans for W . e size of the chain is the size of the largest set ti in the chain, and the width of the problem P , w.P /, is the size of the min-size chain that implies the goal G of P . It is then shown that many of the existing benchmark domains have a bounded and low width when goals are restricted to single atoms, and an algorithm called Iterated Width (IW) search is presented that can solve planning problems in time exponential in their width. e algorithm is very simple; it is just a sequence of pruned breadth-first searches where a novelty bound used for pruning is increased from 0 until the problem is solved or the number of variables in the problem is exceeded. Basically, the novelty of a newly generated state s is defined as the size of the smallest tuple of atoms that is true in s and false in all the states generated in the search before s. us, for example, if s makes an atom p true that was false in all previously generated states, then the novelty of s is 1; if this is not true for any atom but is true for a pair of atoms .p; q/, then the novelty of s is 2, and so on. If the problem has width w.P /, then it is shown that IW solves the problem in at most w.P / iterations, in time and space exponential in w.P /. e blind-search algorithm IW, which does not look at the goal in any way, is shown to be quite effective in solving the standard domains when goals are single atoms, where it is competitive with the best heuristic-search planners. e blindsearch algorithm IW is extended into another algorithm, Serialized IW (SIW), that uses IW for both decomposing a joint goal into atomic subgoals, and for solving each individual subgoal. Interestingly, while SIW is an incomplete blind-search algorithm, its performance over the benchmark domains is not far from the state of the art. While further work is required along this line, one tentative lesson that can be drawn from these results is that planners do well on most benchmarks because the benchmark


36 CLASSICAL PLANNING
domains are easy when containing single atomic goals, and because simple methods appear to work for decomposing joint goals into atomic goals. is account also suggests that problems with joint goals that are hard to decompose, and problems with atomic goals that have a high width, may constitute two sources of hard problems for classical planning.


37
CHAPTER 3
Classical Planning: Variations and Extensions
Most of the current state-of-the-art classical planners are based on the heuristic search formulation where plans are searched forward from the initial state using heuristics derived from the problem. is basic idea, however, has been extended in a number of ways, like in the use of structural information, also obtained automatically from the problem, in the form of “helpful actions” and “landmarks.” In this chapter, we look at extensions and variations of the basic framework, at the heuristics developed for optimal planning, and at other computational approaches to classical and related forms of planning such as temporal and hierarchical task network planning.
3.1 RELAXED PLANS AND HELPFUL ACTIONS
e planner FF [Hoffmann and Nebel, 2001] that followed the first generation of heuristic search planners, used the relaxed plan heuristic inside a novel search architecture comprised of two phases: an incomplete but fast Enforced Hill Climbing (EHC) search, followed if needed by a complete but slower Greedy Best-First Search (GBFS). e gap in performance between FF and previous planners results mainly from the EHC search which can be particularly effective in problems where the branching factor is high. As explained in Section 2.8, the EHC search is a greedy search that uses a breadth-first lookahead where non-helpful actions are ignored and where the lookahead horizon is given implicitly by the first state that improves the value of the heuristic. It is the pruning of nonhelpful actions that makes the EHC search incomplete. Recall that the helpful actions in a state s are defined as the actions applicable in the state s that add a goal or a precondition for an action in the relaxed plan FF.s/ that is not part of the state s. e reason that this lookahead is effective is because this notion of “helpful actions” often accounts for the actions that are most relevant to the goal, at least on domains that are not inherently hard. is, however, is an empirical observation; we don’t understand yet why and when this is so. A single state s in the EHC search where the action required is not helpful is sufficient to make the EHC search fail (assuming that this state can’t be avoided in the way to the goal). e problem with this is that FF then switches to a complete GBFS where the information contained in the helpful actions is not used at all. e EHC will fail for example in a problem where an agent has to bring a suitcase to a destination, and there is a short route, say of 10 steps such that the agent cannot get past the fifth step with the suitcase, and a longer, disjoint route, say of 100 steps, with no such impediment. If the agent can drop the suitcase at any location and pick it up under the obvious preconditions, the relaxation will drive the EHC search along the short route, yet as long as the non-helpful action leading to the longer route is not taken in the initial state, the goal won’t be achieved. e “helpful actions” are


38 3. CLASSICAL PLANNING: VARIATIONS AND EXTENSIONS
the actions that most directly drive the agent to the goal in the relaxation, yet these are not always the actions that serve best the agent in the original problem.
3.2 MULTI-QUEUE BEST-FIRST SEARCH
A key innovation in the planner Fast Downward (FD) that followed on FF [Helmert, 2006], was a way to use the information contained in the actions deemed as “helpful” in a complete search. For this, FD introduced a novel version of a Best-First Search algorithm that uses not just one OPEN list but multiple ones. e lists do not have to be disjoint and each one can be ordered by a different evaluation function. In particular, the complete algorithm that incorporates FF’s heuristic along with FF’s helpful actions in FD contains two OPEN lists, both using FF’s heuristic as the evaluation function. Yet, the children nodes that result from the application of a helpful action are placed in the “helpful” OPEN list, while all the children, resulting either from helpful or non-helpful actions, are placed in the “non-helpful” OPEN list. e search algorithm then alternates between expansions of the best nodes according to the heuristic hFF in the two OPEN lists [Röger and Helmert, 2010]. Of course, a node selected for expansion can be pruned when a duplicate of that node has been expanded already, and moreover, it makes sense to expand the “helpful” OPEN lists more often that the “non-helpful” list. Yet, as long as the selection between the two lists remains fair and one list is never left waiting forever, the algorithm remains complete and able to exploit information about helpful actions. Fast Downward complements this novel search architecture with a technique for dealing with the very large branching factors that result from not pruning the non-helpful actions as in the EHC search. e idea, called delayed evaluation, is to compute the heuristic of nodes only when they are expanded and not when they are generated. In the meantime, the heuristic of the node is set to the heuristic of its parent node (which must have been expanded). While this simplification leaves out useful information that could be derived, it avoids the overhead of evaluating many nodes that will never be expanded. Indeed, under delayed evaluation, the number of heuristic computations is equal to the number of nodes expanded rather than the possibly much larger number of nodes generated. Since the computation of the heuristic often represents the bulk of the computation in heuristic search planners, for problems with average branching factors of 100, for example, this technique can achieve time savings that can approach two orders of magnitude.
3.3 IMPLICIT SUBGOALS: LANDMARKS
We have seen in Section 2.9 that problem decomposition can pay off in some planning problems where a joint goal G D fG1; : : : ; Gng can often be achieved by dealing with one subgoal Gi at a time, in an ordering that doesn’t have to be fixed a priori. We have also seen that some of the standard planning heuristics such as hadd and hFF do not deliver such a decomposition, while the simpler heuristic hug that counts the number of goals Gi not yet achieved, does. e Multi-Queue Best-First search algorithm introduced in Fast Downward integrates multiple heuristics in a natural way by ordering different queues with different heuristics. e more recent planner LAMA [Richter and Westphal, 2010], which followed on Fast Downward, uses a similar search architecture for combining FF’s heuristic with a more sophisticated version of the number of unachieved goals heuristic that counts not only the explicit top goals of the problem, but also the implicit subgoals. ese implicit subgoals are called landmarks, which are defined as formulas that must achieved in the way to the goal by any plan that solves the problem [Hoffmann et al., 2004]. In


3.4. STATE-OF-THE-ART CLASSICAL PLANNERS 39
particular, an atomic landmark is an atom that all plans for the problem must achieve at some point. LAMA uses both atomic landmarks and certain types of disjunctive landmarks. As an example, the atom clear.A/ is a landmark in any Blocks World problem where block A has to be placed onto a different block but its top is not clear. Similarly, if B is the block sitting on A, then the disjunction ontable.B/ _
W
X on.B; X/, where X ranges over the blocks X different than A, is also a landmark. While the problem of determining if a formula is a landmark is computationally intractable, there are very efficient algorithms for identifying some, although not necessarily all landmarks. In particular, one can identify landmarks in the delete-relaxation, as landmarks in the delete-relaxation are landmarks of the original problem too (this is because all plans are plans for the relaxation). An atom p will be a landmark of the problem when any of the delete-relaxation heuristics like hadd , hmax, or hFF yield an infinite heuristic value in the problem that results from excluding the atom p from all action effects. Since all these heuristics yield an infinite value only when the delete-relaxation, and hence the original problem, is unsolvable, this means that if the problem is solvable at all, all the plans will have to contain an action that adds p. Since for this computation we are just interested in whether the value of the heuristic is infinite or not, for performance it pays off to set all action costs to zero when using either the additive or max heuristic for this purpose. While this computation has to be done jF j times for identifying the atomic landmarks, where F is the set of atoms in the problem, this can be done just once as preprocessing. ere are also algorithms that compute all atomic landmarks more efficiently in one pass, as well as algorithms that compute atomic and certain classes of disjunctive landmarks [Hoffmann et al., 2004, Keyder et al., 2010, Richter et al., 2008, Zhu and Givan, 2005]. e LAMA planner which has been the top performing planner in the last two International Planning Competitions [Coles et al., 2012, Helmert et al., 2008], uses the same Multi-Queue BestFirst Search architecture as Fast Downward with two heuristics and four queues. e two heuristics are FF’s and the number of unachieved landmark heuristic, which as we have seen, is an extension of the traditional but less informed number of unachieved top-goals heuristic. Two of the queues are then ordered by FF’s heuristic, and two by the landmark heuristic. As in Fast Downward, one of the two queues for each heuristic is for the “helpful” children only; the other, for the non-helpful. e definition of “helpful” children is the standard one for the queue ordered by FF’s heuristic (children of helpful actions); on the other hand, the helpful children in the landmark queue are defined in a different manner, as the children of actions that add an unachieved landmark, and when there are no such actions, as the actions that would be helpful in FF’s sense for achieving the nearest unachieved landmark. ese are somewhat arbitrary definitions that follow from performance considerations over the wide range of planning benchmarks, and there is not yet a good theory justifying these choices.
3.4 STATE-OF-THE-ART CLASSICAL PLANNERS
Table 3.1 shows the performance of a number of classical planners over benchmarks from past competitions, some of which feature hundreds of atoms and actions, and result in very long plans. We include the planners FF [Hoffmann and Nebel, 2001], Fast Downward [Helmert, 2006], LAMA [Richter and Westphal, 2010], Probe [Lipovetzky and Geffner, 2011], and BFS(f ) [Lipovetzky and Geffner, 2012]. We have discussed most of these planners before. Probe is a GBFS planner guided by the additive heuristic that throws a carefully designed probe to the goal from each state that is expanded. e probe does not involve any search and either reaches the goal quickly or fails. In the first case, a plan is returned, in the second, the search continues. BFS(f ) is also a GBFS planner but uses


40 3. CLASSICAL PLANNING: VARIATIONS AND EXTENSIONS
Table 3.1: Some recent classical planners and their performance over competition benchmarks. Planners are FF, Fast Downward, Probe, LAMA, BFS(f ). I is number of instances per domain, S is number of solved instances, Q and T are the average plan lengths and times in seconds computed over problems solved by all planners.
FF FD PROBE LAMA’11 BFS(f ) Domain I S Q T S Q T S Q T S Q T S Q T 8puzzle 50 49 52.61 0.03 50 52.30 0.18 50 60.94 0.09 49 92.54 0.18 50 45.30 0.20 Barman 20 0 – – 20 197.90 84.00 20 169.30 12.93 20 192.15 8.39 20 174.45 281.28 BlocksW 50 44 39.36 66.67 50 104.24 0.46 50 43.88 0.25 50 89.96 0.41 50 54.24 2.25 Cybersec 30 4 29.50 0.73 28 36.58 859.24 24 50.73 48.29 30 35.27 880.06 28 36.92 63.79 Depots 22 22 51.82 32.72 17 110.25 91.86 22 88.88 1.45 21 43.56 3.58 22 39.56 69.11 Driver 20 16 25.00 14.52 20 50.67 1.26 20 60.17 1.49 20 46.22 1.51 18 48.06 140.93 Elevators 30 30 85.73 1.00 30 92.57 3.20 30 107.97 26.66 30 97.07 4.69 30 129.13 93.88 Ferry 50 50 27.68 0.02 50 30.08 0.09 50 44.80 0.02 50 26.86 0.08 50 31.28 0.03 Floortile 20 5 44.20 134.29 3 39.00 6.91 5 40.50 106.97 5 40.00 8.94 7 36.50 4.15 Freecell 20 20 64.00 22.95 20 61.06 26.55 20 62.44 41.26 19 67.78 27.35 20 64.39 13.00 Grid 5 5 61.00 0.27 5 61.60 4.95 5 58.00 9.64 5 70.60 4.84 5 70.60 7.70 Gripper 50 50 76.00 0.03 50 152.62 0.17 50 152.66 0.06 50 92.76 0.15 50 152.66 0.38 Logistics 28 28 41.43 0.03 28 77.11 0.18 28 55.36 0.09 28 73.64 0.17 28 87.04 0.12 Miconic 50 50 30.38 0.03 50 39.80 0.07 50 44.80 0.01 50 31.02 0.06 50 34.46 0.01 Mprime 35 34 9.53 14.82 35 8.37 9.50 35 12.97 26.67 35 8.60 10.30 35 10.17 19.30 Mystery 30 18 6.61 0.24 19 6.86 1.87 25 7.71 1.08 22 7.29 1.70 27 7.07 0.93 NoMyst 20 4 19.75 0.23 6 22.40 1.96 5 23.20 2.73 11 23.00 1.77 19 22.60 0.78 OpenSt 30 30 155.67 6.86 30 130.11 5.97 30 134.14 64.55 30 130.18 3.49 29 125.89 129.06 OpenSt6 30 30 136.17 0.38 30 222.67 5.39 30 224.00 48.89 30 140.60 4.89 30 139.13 40.19 ParcPr 30 30 42.73 0.06 27 35.79 1.97 28 70.92 0.26 30 70.54 0.28 27 70.42 6.72 Parking 20 3 88.33 945.86 20 74.86 330.76 17 143.36 685.47 19 129.57 361.19 17 83.43 562.39 Pegsol 30 30 25.50 7.61 30 25.97 0.80 30 25.17 8.60 30 26.07 2.76 30 24.20 1.17 Pipes-N 50 35 34.34 12.77 44 75.50 7.94 45 46.73 3.18 44 54.41 11.11 47 58.39 35.97 Pipes-T 50 20 31.45 87.96 40 73.33 99.06 43 54.19 88.47 41 69.83 35.28 40 39.14 216.25 PSR-s 50 42 16.92 63.05 50 14.61 0.27 50 17.20 0.07 50 14.65 0.31 48 18.14 2.57 Rovers 40 40 100.47 31.78 40 153.18 13.69 40 131.20 24.19 40 108.53 17.90 40 126.30 44.20 Satellite 20 20 37.75 0.10 20 40.90 0.78 20 37.05 0.84 20 42.05 0.78 20 36.05 1.26 Scan 30 30 31.87 70.74 28 30.04 7.30 28 25.15 5.59 28 28.04 8.14 27 29.37 7.40 Sokoban 30 26 213.38 26.61 28 204.14 12.44 25 231.52 39.63 28 231.81 184.38 23 218.52 125.12 Storage 30 18 16.28 39.17 20 17.72 3.20 21 14.56 0.07 18 24.56 8.15 20 20.94 4.34 Tidybot 20 15 63.20 9.78 15 66.00 338.14 19 52.67 33.50 16 62.60 102.52 18 63.27 207.85 Tpp 30 28 122.29 53.23 30 127.93 16.95 30 152.53 60.95 30 205.37 18.72 30 110.13 126.03 Transport 30 29 117.41 167.10 30 97.57 12.75 30 125.63 38.87 30 215.90 76.18 30 97.57 46.64 Trucks 30 11 27.09 3.84 17 26.00 0.65 8 26.75 113.54 16 24.75 0.53 15 26.50 8.59 Visitall 20 6 450.67 38.22 7 3583.86 166.35 19 411.71 9.02 20 468.00 4.68 20 339.00 4.58 WoodW 30 17 32.35 0.22 30 57.13 18.40 30 41.13 15.93 30 79.20 12.45 30 41.13 19.12 Zeno 20 20 30.60 0.17 20 37.45 2.68 20 44.90 6.18 20 35.80 4.28 20 37.70 77.56 Summary 1150 909 67.75 51.50 1037 168.60 57.78 1052 83.64 41.28 1065 86.51 48.98 1070 74.32 63.91


3.5. OPTIMAL PLANNING AND ADMISSIBLE HEURISTICS 41
an evaluation function based on width-considerations (Section 2.10) along with tie breakers based on the additive and number-of-unachieved-landmark heuristics. e scalability of planners has improved considerably over the last 15 years with the best planners using and extending the ideas of previous planners, such as heuristic functions, helpful actions, and landmarks. As a reference, a baseline planner such as HSP, based solely on a Greedy Best-First Search guided by the additive heuristic, solves 789 of the problems, while LAMA, the winner of the last two competitions solves 1,065 problems out of a total of 1,150. In the Table, I stands for the number of instances per domain, while S, Q, and T stand for the number of instances solved, and the average plan lengths and times in seconds. e experiments were conducted on a dual-core CPU running at 2.33 GHz and with two GB of RAM, with processes timing out after two hours. All of these domains and planners, including their sources, are available on the Internet.
3.5 OPTIMAL PLANNING AND ADMISSIBLE HEURISTICS
Optimal planners ensure the optimality of the plans found by using admissible heuristics (lower bounds) in the context of search algorithms like A* or IDA*. Most admissible heuristics developed for planning are based either on the delete-relaxation like the heuristic hmax [Bonet and Geffner, 2001], on a notion of critical paths like the heuristics hm [Haslum and Geffner, 2000], on abstractions where certain atoms are dropped from the problem, like pattern-database heuristics [Edelkamp, 2001], or on landmarks, like the LA and LM-Cut heuristics [Helmert and Domshlak, 2009, Karpas and Domshlak, 2009]. Many of these heuristics are general templates that leave some choices open, and some dominance relations among these heuristics have been established as well [Helmert and Domshlak, 2009]. e heuristics hm, where m is a positive integer, are based on the assumption that the estimated cost hm.C; s/ of achieving a (conjunctive) set of C atoms from a state s is given by the estimated cost hm.C 0; s/ of achieving the most costly subset C 0 of at most m atoms in C . Mathematically, the estimate hm.C; s/ is defined inductively as:
hm.C; s/ D
8<
:
0 if C s, mina2R.C / Œc.a/ C hm.Reg.a; C /; s/ç if C a s and jC j m,
maxfhm.C 0; s/ W C 0 C; jC 0j mg otherwise,
(3.1)
where Reg.a; C / stands for the regression of the set of atoms C through the action a, i.e., Reg.a; C / D .C n Add.a// [ P rec.a/, and R.C / stands for set of actions a in the problem that add some atom in C and delete none. e approximation captured by this definition follows from setting the estimated cost hm.C; s/ of achieving sets C of more than m atoms, to the cost of achieving the most costly subset C 0 of C of at most m atoms. e hm heuristic for state s, hm.s/, is hm.s/ D hm.G; s/, where G is the goal of the problem. For m D 1, it is easy to show that hm is equal to hmax, while for a sufficiently large value of m that is less than or equal to the total number of variables in the problem, hm is equal to the optimal heuristic h . Pattern database (PDB) heuristics provide a generalization of the PDB heuristics developed for domain-specific heuristic search [Culberson and Schaeffer, 1998]. A PDB heuristic is a lookup table that stores exact optimal distances for an abstraction of the problem computed by a regression search from the goal. e abstraction is obtained by dropping a sufficient number of atoms from the problem


42 3. CLASSICAL PLANNING: VARIATIONS AND EXTENSIONS
so that the number of reachable states in the reduced problem fits in memory. An atom is removed from a problem P D hF; I; O; Gi by removing it from F , I , O, and G; i.e., the atom is removed from precondition, delete, and add lists, from the goal and the initial situation, and from the set of problem atoms. If s is a state over the original problem, and A is the set of atoms retained in the reduced problem, the heuristic h.s/ is set to the optimal heuristic h .s0/ over the reduced problem where s0 is the state s0 D s \ A. A key challenge in the design of PDBs is deciding which atoms to abstract away from the problem [Haslum et al., 2007]. Two recent variations on the PDB idea for planning are the merge-and-shrink heuristics [Helmert et al., 2007] and structural pattern heuristics [Katz and Domshlak, 2008b]. Multiple admissible heuristics h1; : : : ; hn can be combined into a potentially more informed admissible heuristic by taking their pointwise maximum as h.s/ D maxfh1.s/; : : : ; hn.s/g, or by partitioning the action costs [Haslum et al., 2007, Katz and Domshlak, 2008a]. A cost partitioning  ̆ of problem P with cost function c. / is a collection P1; : : : ; Pn of problems identical to P except on their
cost functions c1; : : : ; cn that must be non-negative and satisfy P
iD1;n ci .a/ c.a/ for every action a in P . en, if h1; : : : ; hn are (arbitrary) admissible heuristics for the problems P1; : : : ; Pn respectively, the additive heuristics h D h1 C C hn is an admissible heuristic for P . One can indeed improve a base heuristic by doing a cost partitioning that applies the same base heuristic to every problem in the partition; the difficulty is in the choice of the cost functions ci , 1 i n, and the number of partitions. e LM-Cut heuristic [Helmert and Domshlak, 2009] is a powerful admissible heuristic that can be thought as either a landmark heuristic or a cost partitioning heuristic based on hmax. For determining the LM-Cut value of a state, a sequence L1; : : : ; Lm of (disjunctive but not necessarily disjoint) action landmarks are computed together with cost functions c1; : : : ; cm providing a cost partitioning. Like (atomic) landmarks, a disjunctive action landmark L for state s is a set of actions such that every plan from the state s must contain one of the actions in the set. In cases when the landmarks computed by LM-Cut are pairwise disjoint, the cost function c1 in the partition defined by LM-Cut assigns costs c1.a/ D c.a/ to every action a 2 L1 and c1.a/ D 0 to a ... L1, costs c2.a/ D c.a/ for every action a 2 L2 n L1, and c2.a/ D 0 for a ... L2, and so on, while the heuristic values in such cases become h1 D mina2L1 c1.a/, h2 D mina2L2 c2.a/, and so on. e value of the LM-Cut heuristic at state s is the sum h1 C h2 C C hm as in any cost partitioning scheme, which is guaranteed to be admissible. e LM-Cut heuristic can be improved by computing and considering more than one landmark at a time, exploiting a connection between action landmarks and hitting sets [Bonet and Helmert, 2010].
3.6 BRANCHING SCHEMES AND PROBLEM SPACES
We have reduced the search for plans for a STRIPS planning problem P D hF; I; O; Gi to the search for paths in the directed graph associated with the state space S.P /, where the root node represents the initial state I , the target nodes represent the states that include the goal G, and the edges express the state transitions that are possible given the actions in O. Yet, the search for plans can be formulated in many other ways, some of which will be explored over the next few sections. As we will see, the search for plans can also be formulated as a path-finding problem over a different graph, so that plans are searched backward from the goal rather than forward from the initial state. We will also see formulations where plans are not constructed directionally either from the initial situation or from


3.7. REGRESSION PLANNING 43
the goal. In all cases, however, the search will involve the traversal of a graph, along with heuristics or inference procedures for selecting or pruning branches. Branches will represent partial plans: either partial plans that have to be refined further, partial plans that are complete and hence encode solutions, or partial plans that can’t be refined into solutions. In the formulation we have considered so far, branches in the graph represent plan prefixes, and the nodes at the end of these branches represent the resulting states. In other formulations, branches represent plan suffixes, partially ordered plans, or simply commitments about actions and propositions that are true or false at different time steps. While the formulation considered so far is the most natural, and the one that underlies the best current planners, the other formulations are also useful, and may in fact be superior in slightly different settings, as in temporal planning, where actions have different durations and may be executed concurrently, and plans that minimize total duration (makespan) are sought. e graph that results from these decisions, and in particular, the way in which children nodes are generated from parent nodes in the graph, are sometimes referred to as the branching scheme, the problem space, or the search graph. While in the standard problem space for planning, the branching factor (number of children) is given by the number of applicable actions, we will see other problem spaces for planning whose branching factor is always 2, regardless of the number of actions or atoms in the problem. As an illustration of this, consider the famous Travelling Salesman Problem (TSP), where a minimum cost tour that visits every node in a graph once is sought [Lawler et al., 1985]. e most natural way for solving the problem is by starting in an arbitrary TSP node n, and by defining the children of the node in the search graph as the TSP nodes that can be reached from n in one step. Branches in this search graph would thus stand for tour prefixes. In principle, if there are n nodes in the TSP graph, this means up to n 1 children per node in the search graph, and hence a large branching factor if the TSP graph is large and dense. A different branching scheme for solving the TSP is to pick one edge .i; j / from the TSP graph, while considering two options: that the edge will be part of the optimal solution, or that the edge will not be part of the optimal solution. e resulting search graph then becomes a tree with a branching factor of 2, where the branches are no longer tour prefixes but tour commitments, namely, that certain edges in the TSP graph have to be part of the final tour and that other edges don’t. Interestingly, in combination with good admissible heuristics for the TSP like the assignment problem heuristic, this less direct form of branching, scales up much better for large TSPs than the most straightforward approach of starting in one city and building the tour forward from it [Lawler et al., 1985]. It is crucial though that the problem space and the heuristics or inference used for guidance or pruning are well matched to each other. Heuristics such as hFF and hadd work well in the forward search for plans because the initial state of the problem can be progressed through the plan prefix (by applying the actions in sequence), and the heuristic for the plan prefix can be computed from the state that results. is property extends also to regression planners that search for plans backward, where a suitable state, obtained by regressing the goal through a plan suffix, is used for computing the heuristic. For this reason, planners that search for plans forward or backward are often called state-space planners. Computing heuristics for planners that search for plans in other ways is more difficult, yet powerful propagation and pruning criteria have been developed for SAT and CSP formulations.
3.7 REGRESSION PLANNING
Planners can search for plans backward from the goal by applying the actions in reverse. Basically, if a set of atoms C is to be achieved, the only actions that could be “last” in a minimal plan for achieving


44 3. CLASSICAL PLANNING: VARIATIONS AND EXTENSIONS
C are actions that add some atom in C and do not delete any. If a is one such action, the set of atoms C 0 that have to be true right before the application of a in order for C be true right after, are the action preconditions of a along with the atoms in C that are not added by a. e set of atoms C 0 is said to be then the regression of C through the action a. Starting this “subgoaling” process with the top goal G of the problem, we obtain a state space that is called the regression state model associated with the STRIPS planning problem P D hF; I; O; Gi, to distinguish it from the forward or progression state model S.P / [Bonet and Geffner, 1999, Nilsson, 1980, Weld, 1994]. In this regression space R.P /:
• the states s are sets of atoms from F ,
• the initial state s0 is G,
• the goal states are the states s for which s I ,
• the set of actions A.s/ applicable in s are the actions a 2 O that are relevant and consistent, namely, for which Add.a/ \ s ¤ ; and Del.a/ \ s D ;,
• the state s0 D f .a; s/ that follows the application of action a 2 A.s/ in s is s0 D .s n Add.a// [ P rec.a/, and
• the action costs are c.a; s/.
e solution of this state space is like the solution of any state model, an applicable action sequence mapping the initial state into a goal state. Yet, notice that in the regression space R.P /, the initial state is the goal of P , and the goal states are the states that must be true in the initial situation of P . Moreover, while the states s in the regression space R.P / are defined syntactically as in the progression space S.P / by sets of atoms, the meaning of these sets is very different in the two cases: states represent complete truth-assignments in the progression space but partial truth-assignments in the regression space. In particular, in the initial state s0 D I of the progression space S.P /, every atom that is not in I is false, while in the initial state s0 D G of the regression space, this is not true; indeed, the goal G can be the single atom on.A; B/ in a blocks world problem with three blocks A, B, and C , and there is no reachable state of the problem where on.A; B/ is true and all other atoms are false (blocks B and C must be somewhere!). It can be shown that every state s in the regression space R.P / stands indeed for a collection of states s0 in the progression space S.P /, namely all the states s0 in the progression space that include s. Alternatively, the states s in the regression can be thought of as goals and subgoals to be achieved, with an action applied in reverse, mapping one goal into another one. e regression space R.P / is sound and complete in the sense that the solutions to R.P / encode the solutions to the problem P but in reverse. One potential advantage of searching for plans backward in R.P / as opposed to forward in S.P / is that it is possible to avoid the computation of the heuristic from scratch in every state. is can represent up to 80% of the total time that heuristic search planners take in solving a problem. In the regression search it is indeed possible to perform the bulk of this computation just once. For example, if h.pI s0/ is the estimated cost of achieving the atom p from the initial problem state s0 according to the additive heuristic, the estimated cost h.s0/ from a state s0 to the goal I in the regression search can be set up to the sum h.s0/ D
P
p2s0 h.pI s0/, where the elements of the sum need to be computed just once from s0 and used then to determine the heuristic h.s0/ of any state s0 in the regression space.


3.8. PLANNING AS SAT AND CONSTRAINT SATISFACTION 45
A potential problem in the regression search is that while the regression is sound and complete, it may contain many more dead-ends than the progression search; i.e., the regression can generate collection of atoms that cannot be achieved jointly by any plan from the initial situation, and hence that will not lead to any solution [Bonet and Geffner, 1999]. In addition, it is not clear that the heuristics as defined above are as informative in the backward search as in the forward search. None of these issues have been studied throughly, though, yet the fact is that there are no regression-based planners these days that can compete with the best progression-based planners examined above.
3.8 PLANNING AS SAT AND CONSTRAINT SATISFACTION
SAT is the problem of determining whether a formula in Conjunctive Normal Form (CNF) is satisfiable. A formula in CNF can be regarded as a set of disjunctions of literals where a literal is an atom or its negation. Such disjunctions are called clauses. For instance, the formula .p _ :q/ ^ .:p _ q/ is in CNF, p _ :q and :p _ q are its clauses, and the formula is satisfiable as the assignment p 7! t rue, q 7! t rue, for example, makes both clauses true. SAT is an NP-Complete problem [Sipser, 2006], which in practice means that all complete algorithms for SAT will run in exponential time in the worst case. Still, very large SAT instances can be solved nowadays due to very effective inference techniques, such as Unit Propagation and Conflict-driven Learning [Biere et al., 2012]. e problem of classical planning can be expressed as a SAT problem provided that a planning horizon is given. e SAT approach to planning has been introduced and shown to be effective by Kautz and Selman in the mid 90s [Kautz and Selman, 1992, 1996, 1999]. e basic idea is very simple. For a STRIPS problem P D hF; I; O; Gi and a planning horizon n, a CNF formula C.P; n/ is produced that is fed into a general SAT solver. e CNF formula C.P; n/ includes propositions p0; p1; : : : ; pn for each atom p 2 F and propositions a0; a1; : : : ; an 1 for each action a 2 O. e formula C.P; n/ is such that C.P; n/ is satisfiable iff there is a plan of at most n steps that solves the problem P . In such a case, the plan can be extracted from the actions that are true in the satisfying assignment. e clauses in the formula C.P; n/ encoding the planning problem P D hF; I; O; Gi with horizon n are given in Figure 3.1. It can be shown that if C.P; n/ is not satisfiable, then there is no plan of length n solving the problem P , while if C.P; n/ is satisfiable, the plan given by the action that is true at time 0, followed by the action that is true at time 1, and so on, with truth evaluated in the satisfying assignment, is a plan that solves P . Moreover for any plan that solves P , there is an assignment satisfying the formula C.P; n/ that makes the plan true. Since the horizon n required to find a plan is not known a priori, the SAT approach to planning increases the horizon one by one from n D 0 until a satisfying assignment for C.P; n/ is found. e approach, as described, works remarkably well, although it doesn’t scale up as well as the best heuristic search planners. In part, this is to be expected, as in this formulation the first plan found is an optimal plan (assuming actions costs are uniform). In order for the SAT approach to scale up, a number of variations have been introduced. One is to allow certain sets of actions to be done in parallel, in particular, the sets of actions that if applicable in a given situation, remain applicable and yield the same overall effect regardless of how they are ordered. A sufficient condition for this is that no action in the set deletes a precondition or add effect of any other action in the set. Actions that do not comply with this condition are called mutex, as they are regarded as mutually exclusive at any one time step [Blum and Furst, 1995]. Other improvements involve the introduction of a NO-OP action for each atom p in the problem, with precondition and effect p, which helps to simplify the


46 3. CLASSICAL PLANNING: VARIATIONS AND EXTENSIONS
Init: p0 for each p 2 I , :q0 for each q 2 F such that q 62 I .
Goal: pn for each p 2 G.
Actions: For i D 0; 1; : : : ; n 1 and each action a 2 O: ai pi for each p 2 P re.a/ ai piC1 for each p 2 Ad d.a/ ai :piC1 for each p 2 Del.a/
Persistence: For i D 0; 1; : : : ; n 1 and each fluent p 2 F , where O.pC/ and O.p / are the actions that add and delete p respectively,
pi ^
V
a2O.p / :ai piC1
:pi ^
V
a2O.pC/ :ai :piC1
Seriality: For each a; a0 2 O such that a ¤ a0, :.ai ^ ai0 / for i D 1; : : : ; n 1.
Figure 3.1: e subformulas that make up the CNF formula C.P; n/ encoding a planning problem P D hF; I; O; Gi with horizon n. Fluents p and actions a are tagged with time indices i. e subformulas can be easily converted into clauses.
clauses encoding the persistence axioms, and the use of lower bounds for initializing the planning horizon [Kautz and Selman, 1999]. More recently, Jussi Rintanen has introduced other refinements that improve the performance of SAT-based planners further, including the use of a special heuristic for variable selection in the otherwise generic SAT solver, an improved search for an adequate planning horizon, and better memory management for dealing with the millions of clauses that often result from planning encodings in CNF [Rintanen, 2012]. While SAT-based planners do not yet scale up as well as the best heuristic-search planners, the gap has narrowed down considerably. Moreover, it is well known that on domains that are inherently difficult, SAT approaches can do much better than optimal and non-optimal heuristic search planners [Hoffmann et al., 2007]. Constraint Satisfaction Problem (CSP) provide a generalization of SAT where the variables are not restricted to be boolean, and constraints are not restricted to be clauses [Dechter, 2003]. General CSP solvers can deal with multivalued variables over arbitrary constraints. In the same way that classical planning problems with a fixed horizon can be cast as SAT problems, they can also be cast as CSP problems [Do and Kambhampati, 2000]. In spite and perhaps because of the additional expressive power afforded by the CSP formulation, CSP-based planners have not been able to keep up with SAT-based planners, that are based on a more restricted task (SAT) over which the technology has moved faster.
3.9 PARTIAL-ORDER CAUSAL LINK PLANNING
Branching schemes have also been developed for searching for plans neither forward from the initial state or backward from the goal, but by extending a set of actions that are partially ordered. e resulting planners are known as partial-order planners [Ghallab et al., 2004, Weld, 1994]. We follow below


3.10. COST, METRIC, AND TEMPORAL PLANNING 47
the formulation of partial order planning known as partial-order causal-link or POCL planning, where structures known as causal links are used in the plan representation to keep track of temporal precedences and constraints [McAllester and Rosenblitt, 1991]. A partial plan in POCL planning corresponds to a set of commitments represented by a tuple D hSteps; O rd; CL; Openi, where Steps is the set of actions in the partial plan , O rd is a set of precedence constraints on Steps, CL is a set of causal links, and Open is a set of open preconditions. A precedence constraint a a0 states that action a precedes action a0 in the partial plan, a causal link aŒpça0 states that action a supports the precondition p of action a0 in the partial plan, and an open precondition Œpça states that the precondition p of the action a in the partial plan is open, meaning that it is not yet supported by any action. e initial node of the search, 0, is given by the tuple hfS t art; End g; fS t art End g; ;; fŒG1çEnd; : : : ; ŒGmçEnd gi where G1; G2; : : : ; Gm are the top level goals of the problem. Here Start and End are two dummy actions used for encoding the initial situation and goals: the action Start must precede all actions and is assumed to add all the atoms in the initial situation of the problem, and the action End must be the last action, whose preconditions are the goals of the problem and whose effect is the dummy target goal G. Branching in POCL planning proceeds by picking a “flaw” in a non-terminal node (partial plan) and applying the possible repairs [Kambhampati et al., 1995, Weld, 1994]. Flaws are of two types. Open precondition flaws Œpça in are solved by selecting an action a0 that supports p and adding the causal link a0Œpça to CL and the precedence constraint a0 a to O rd (a0 should also be added to Steps if a0 62 Steps). Similarly, threats—which refer to situations in which an action a 2 Steps deletes the condition p in a causal link a1Œpça2 in CL with the ordering a1 a0 a2 consistent with O rd —are solved by placing one of the precedence constraints a0 a1 or a2 a0 in O rd . A node is terminal if it is inconsistent (i.e., the ordering O rd is inconsistent or contains flaws that cannot be fixed) or is a goal (is consistent and contains no flaws). e goal nodes represent partial plans that are complete and solve the problem. A plan can then be obtained from such nodes by any total ordering of Steps that respects the partial order O rd ; an operation that can be done in polynomial time [Dechter et al., 1991]. POCL planning is a clever branching scheme for organizing the search for plans, which is sound and complete. Up to the mid 90s, it represented the main computational approach in planning, but with the advent of Graphplan, SAT, and heuristic-based planners, it lost appeal as it could not scale up as well. One of the main problems is the lack of good heuristics for guiding or pruning the POCL search. POCL planning, however, remains a convenient scheme for some expressive forms of planning, including planning with time, resources, and concurrency [Smith et al., 2000], and several attempts have been made to make its search more informed [Nguyen and Kambhampati, 2001, Vidal and Geffner, 2006].
3.10 COST, METRIC, AND TEMPORAL PLANNING
Optimal and non-optimal planning in the presence of non-uniform action costs c.a/ is direct in the heuristic search approach to planning, where most heuristic estimators can be easily extended to take action costs into account. In SAT or CSP approaches, the handling of such costs is less direct. In all cases, one important challenge that has been left unaddressed is the handling of costs c.a; s/ that depend on both the action and the state where it is applied, or costs c.s/ that depend solely on the state. One could express, for example, that states s where two atoms p and q are true, are to be avoided,


48 3. CLASSICAL PLANNING: VARIATIONS AND EXTENSIONS
by assigning a high cost c.s/ to actions applied in such states. Unfortunately, no informative heuristics have been formulated so far for taking such state-dependent costs into account. Numeric or metric planning refers to planning in the presence of numeric variables that can potentially take an infinite number of values, such as all integer, rational, or real numbers, or whose range cannot be bounded a priori. If this range is finite, the problem can be transformed into a planning problem over a finite collection of multivalued or boolean variables, even if this transformation is not always convenient computationally. In the presence of such numerical variables X, the states are defined in the usual way as assignment of values x to variables X, and the initial situation fully defines one such state. Goals and preconditions, however, can then include atoms like X D Y , X > Y , X Y , and their negations, where Y is a value or another variable, and the effect of these actions can involve expressions of the form X WD f .X1; : : : ; Xn/ where the new value of X is set as a function of the current value of a subset of variables Xi that may include the variable X itself. In common applications of metric-planning, numeric variables refer to resources: money, fuel, time, space, etc. e semantics of metric planning problems is direct, as they easily map into classical planning models where a target state is to be achieved from a given initial state by applying deterministic actions. e sole difference with the standard classical planning model is that the number of states in metric planning may be infinite. While in the case of non-integer variables this may present some subtleties (e.g., no finite plan can achieve X D 0 if the only available action changes X to X WD X=2), the challenge that has received the most attention in metric planning is the search for finite plans (when they exist), and in particular, the formulation and use of heuristics that take numeric variables into account. Metric-FF was one of the first modern planners to tackle this problem by introducing a polynomial relaxation and heuristic that rather than ignoring “negative” effects, as in the delete-relaxation, it ignores either increasing or decreasing effects [Hoffmann, 2003]. e relaxation yields lower and upper bounds for the numeric variables so that values within these bounds are assumed to be all achievable. us, in particular, a relaxed plan that makes the upper bound of X higher than the lower bound of Y is assumed to be also a relaxed plan for an atom like X > Y . e relaxation provides useful guidance in many problems, although there are obvious limitations such as potentially regarding an atom like X > X as achievable. More recent work addressing metric planning can be found in [Coles et al., 2009, Do and Kambhampati, 2001, Edelkamp, 2006, Gerevini et al., 2008, van den Briel and Kambhampati, 2005]. Finally, temporal planning refers to planning in the presence of actions with durations, that under certain conditions can be executed concurrently. Normally, the objective in temporal planning is to achieve the goal as early as possible, often referred to as the minimization of the plan makespan. From a computational point of view, temporal planning raises two challenges. e first has to do with the problem space: the branching factor that results from the explicit consideration of sets of parallel actions in a forward search may be just too large. In general, it is thought that the problem space associated with POCL planning, where actions in the plan are partially ordered is then more convenient [Smith et al., 2000]. is, however, raises the second challenge: the control of the search in temporal POCL planning. CPT is a temporal planner that optimizes makespan by means of a constraint programming formulation of POCL planning where partial plans that cannot be refined into complete plans within a given makespan are detected and pruned early in the search by a form of constraint propagation [Vidal and Geffner, 2006]. A common strategy when formal guarantees on the makespan are not required is to deal with temporal problems as if they were standard sequential


3.11. HIERARCHICAL TASK NETWORKS 49
problems with action costs set to action durations. en different approaches can be used to parallelize the resulting plans for reducing the makespan. is approach, however, is not universal, and can be used only when temporal plans, accommodating concurrent actions, can be serialized. It can be shown that this is true, for example, when the only sets of actions that can be done in parallel, i.e., that may overlap in time, are commutative actions, like non-mutex actions that do not delete a precondition or effect of another action [Blum and Furst, 1995, Smith and Weld, 1999]. Often, however, there are parallel plans that cannot be serialized in this way, as when someone must light a match for inserting a key in a door. Interestingly, the current version of the planning language standard, PDDL 2.1 [Fox and Long, 2003], can express temporal problems of this type, which involve what is called required concurrency [Cushing et al., 2007]. While most existing temporal planners, whether optimal or not, do not handle this type of concurrency, the extensions required for handling it may not be that complex in some formulations. In a constraint-based temporal planner like CPT, this may just require the ability to express and deal with simple constraints on actions, such as that if a plan includes one action a at time t, then it must include another action (event) b at time t C , like “if light is turned on now, it’ll turn off in 30 seconds.” Such action constraints create a new type of “flaws” in partial plans that must be fixed too, e.g., by including b in the partial plan at time t C when a is in the plan at time t. e expressive power afforded by such changes and the necessary ways for dealing with them, however, have not been fully addressed yet.
3.11 HIERARCHICAL TASK NETWORKS
In the forms of planning considered so far, no information is given about which actions to apply or which subgoal to pursue; rather, actions are characterized in terms of their pre and postconditions, and the choice and ordering of the actions for solving a problem is computed automatically. HTN planning, where HTN stands for Hierarchical Task Networks, provide a completely different way of constructing plans [Erol et al., 1994, Ghallab et al., 2004]. In HTN planning, plans are not obtained from a model that describes how the actions change the world, rather actions, called tasks, are described at several levels, with tasks at one level decomposing into tasks at a lower level, with some tasks, called the primitive tasks, standing for real executable actions that do not decompose further. For example, the abstract task of taking a taxi may be decomposed into the tasks of getting to the street, stopping a taxi, getting on board, and so on. Similarly, the task of getting on the taxi can be decomposed into the tasks of opening the door, entering the taxi, and closing the door. Some tasks, like getting to the airport, may admit multiple decompositions called methods, one of which may involve the tasks of taking a taxi to the train station, and then a train from the station to the airport. In this case, the tasks inside the methods are constrained to be one after the other. Other types of restrictions may relate the tasks in a method as well. While the objective in classical planning is to find an action sequence that maps the initial situation into a goal state, in HTN planning, the objective is to find a decomposition that results in a consistent network of primitive tasks. Classical planning is model-based because it’s based on a model of the actions, the initial situation, and goals, from which the plan is derived and with respect to which the computed plan can be proved correct. HTN planning is not model-based in this sense, and indeed, in HTN planning there is no clear separation between the problem that is being solved and the strategies being used for solving it. Actually, HTNs are most commonly used for encoding solution strategies. From a theoretical point of view, this is not good enough, as there is then


50 3. CLASSICAL PLANNING: VARIATIONS AND EXTENSIONS
no assurance that the planning strategy encoded by the HTN leads to plans that are correct.1 Yet, from a practical point of view, this may be a feature rather than a bug: in many applications, humans feel more comfortable describing the solving strategies for the domain than the domains themselves, and place more trust on such strategies than on plans found by domain-independent planners. is may explain why HTN planners are more common in applications than domain-independent planners. is, however, may change, as better ways are found for integrating strategy and domain descriptions, and for coming up automatically with general and transparent strategies.
1Indeed, in some of the knowledge-based planning competitions held so far, teams were given the planning domains in advance, and they were free to determine and encode the strategies for solving them. It was then found that plans obtained from these strategies were not always correct. is is because the HTN encodings were not derived from the domain descriptions but were written by hand.


51
CHAPTER 4
Beyond Classical Planning: Transformations
We have considered models of planning where a goal is to be achieved by performing actions that are deterministic given an initial situation that is fully known. Often, however, planning problems exhibit features that do not fit into this format, features such as goals that are desirable but which are not to be achieved at any cost (soft goals), goals that refer not only to end states but to the intermediate states as well (temporally extended goals), or initial situations that are not fully known (conformant planning). In this chapter, rather than reviewing more powerful algorithms for dealing with such features, we illustrate how features such as these can be handled by off-the-shelf classical planners through suitable transformations that can be performed automatically. Similar transformations will also be introduced for dealing with a different task, plan recognition, where a probability distribution over the possible goals of the agent is to be inferred from partial observations of the agent behavior.
4.1 SOFT GOALS AND REWARDS
Soft goals are used to express desirable outcomes that unlike standard hard goals are subject to a cost-utility tradeoff [Sanchez and Kambhampati, 2005, Smith, 2004].We consider a simple STRIPS planning setting where problems P D hF; I; O; Gi are extended with information about positive action costs c.a/ for every action a 2 O, and non-negative rewards or utilities u.p/ for every atom p 2 F . e soft goals of the problem are the problem atoms with positive utility. It is possible to associate utilities with more complex logical formulas, like disjunctions of atoms or negated literals, yet standard methods can be used to introduce atoms for representing such formulas [Gazen and Knoblock, 1997]. In the presence of soft goals, the target plans are the ones that maximize the utility measure or net-benefit given by the difference between the total utility obtained by the plan and its cost:
u. / D
P
pW ˆp u.p/ c. / (4.1)
where c. / is given by the sum of the action costs in , and ˆ p expresses that p is true in the state that results from applying the action sequence to the initial problem state. A plan for a problem with soft goals is optimal when no other plan 0 has utility u. 0/ higher than u. /. e utility of an optimal plan for a problem with no hard goals is never negative as the empty plan has non-negative utility and zero cost. e International Planning Competition held in 2008 featured a Net-Benefit Optimal track where the objective was to find optimal plans with respect to Eq. 4.1 [Helmert et al., 2008]. Soft goal or net-benefit planning appears to be very different than classical planning as it involves two interrelated problems: deciding which soft goals to adopt, and deciding on the plan for achieving them. Indeed, most of the entries in the competition developed native


52 4. BEYOND CLASSICAL PLANNING: TRANSFORMATIONS
planners for solving these two problems. More recently, however, it has been shown that problems P with soft goals can be compiled into equivalent problems P 0 without soft goals that can then be solved by classical planners able to handle action costs c.a/ only [Keyder and Geffner, 2009]. e plans for P and P 0 are the same, except for the presence of dummy actions, and the utilities of the plans for P are inversely related to the cost of the plans for P 0. us, optimal cost-based planners for P 0 yield optimal net-benefit plans for P , while satisfacing cost-based planners for P 0, that scale up better, yield satisfacing net-benefit plans for P . e idea of the transformation from the problem P with soft goals into the equivalent problem P 0 with hard goals only is very simple. For soft goals p associated with individual atoms, one just needs to add new atoms p0 that are made into hard goals in P 0, that are achievable in one of two ways: by the new actions collect.p/ with precondition p and cost 0, or by the new actions forgo.p/ with precondition p, that stands for the negation of p, and cost equal to the utility u.p/ of p. Additional bookkeeping is needed in the translation so that these new actions can be done only after the normal actions in the original problem. More precisely, for a STRIPS problem P D hF; I; O; Gi with action costs c. / and soft goals u. /, the equivalent, compiled STRIPS problem P 0 D hF 0; I 0; O0; G0i with action costs c0. / and no soft goals has the following components, where Fu D fp j .p 2 F / ^ .u.p/ > 0/g stands for the set of soft goals [Keyder and Geffner, 2009]:
• F 0 D F [ fp0 j p 2 Fug [ fp0 j p 2 Fug [ fnormal-mode; end-modeg,
• I 0 D I [ fp0 j p 2 Fug [ fnormal-modeg,
• O0 D O00 [ fcollect.p/; forgo.p/ j p 2 Fug [ fendg,
• G0 D G [ fp0 j p 2 Fug, and
• c0.a/ D
8<
:
c.a/ if a 2 O00 ; u.p/ if a D forgo.p/ ;
0 if a D collect.p/ or a D end :
If the STRIPS actions a are denoted as pairs hP re; P osti, where P re stands for the preconditions of a, and P ost for its effects (negated atoms indicate atoms in Del.a/), the actions in the new compiled problem P 0 can be expressed as:
• O00 D fhPre.o/ [ fnormal-modeg; Eff.o/i j o 2 Og,
• end D hfnormal-modeg; fend-mode; :normal-modegi,
• collect.p/ D hfend-mode; p; p0g; fp0; :p0gi,
• forgo.p/ D hfend-mode; p; p0g; fp0; :p0gi.
e forgo and collect actions can be used only after the end action that makes the fluent end-mode true, while the actions from the original problem P can be used only when the fluent normal-mode is true prior to the execution of the end action. Moreover, exactly one of fcollect.p/; forgo.p/g can appear for each soft goal p in the plan, as both delete the fluent p0 which appears in their preconditions, and no action makes this fluent true. As there is no way to make normal-mode true again after it is deleted


4.2. INCOMPLETE INFORMATION 53
G
I
Figure 4.1: Deterministic conformant problem where a robot must move from an uncertain location I into the location with G with certainty, one cell at a time, in an n n grid.
by the e nd action, all plans 0 for P 0 have the form 0 D h ; e nd; 00i, where is a plan for P and 00 is a sequence of jS0.P /j collect.p/ and forgo.p/ actions in any order, the former appearing when ˆ p, and the latter otherwise. e two problems P and P 0 are equivalent in the sense that there is a correspondence between the plans for P and P 0, and corresponding plans are ranked in the same way. More specifically, for any plan for P , a plan 0 in P 0 that extends with the end action and a set of collect and forgo actions has cost c. 0/ D u. / C  ̨, where  ̨ is a constant that is independent of and 0. Finding an optimal (maximum utility) plan for P is therefore equivalent to finding an optimal (minimum cost) plan 0 for P 0. is implies that the best plans for P can be obtained from the best plans for P 0, and these can be computed with any optimal classical planner able to handle action costs. From a computational point of view, the transformation above can be made more effective by means of a simple trick. Recall that for a single plan for P , there are many extensions 0 in P 0, all containing the same actions and having the same cost, but differing in the way the collect and forgo actions are ordered. For efficiency purposes, it makes sense to enforce a fixed but arbitrary ordering p1; : : : ; pm on the soft goals in P by adding the dummy hard goal pi0 as a precondition of the actions collect.piC1/ and forgo.piC1/ for i D 1; : : : ; m 1. e result is that there is a single possible extension 0 of every plan in P , and the space of plans to search is reduced. Interestingly, the cost-optimal planners that entered the Optimal Sequential Track of the 2008 IPC, fed with the translations of the problems in the Optimal Net-Benefit Track, do significantly better than the net-benefit planners that entered that track [Keyder and Geffner, 2009].
4.2 INCOMPLETE INFORMATION
Figure 4.1 shows a simple problem where an agent, whose initial state is uncertain and corresponds to one of the four shaded cells, must reach the cell marked G with certainty. For this, the agent can move one cell at a time in each one of the four directions, but cannot get passed the walls, thus any


54 4. BEYOND CLASSICAL PLANNING: TRANSFORMATIONS
move that would take him out of the grid has no effect. e problem is very much like a classical planning problem except for the uncertain information about the initial situation. It is assumed that the problem has uncertainty but no feedback of any type; i.e., the agent does not get to observe the cell where it is located nor the nearby walls. e solution to the problem, if there is a solution, can’t use feedback and hence it must be a fixed action sequence like in classical planning. e difference is that this action sequence must be applicable and achieve the goal for any of the possible initial states. Problems of this type are called conformant problems, as the solutions must conform with each possible initial state. is is a deterministic conformant problem as the actions have all deterministic effects. In non-deterministic conformant problems, solutions are also action sequences but they must conform not only with each possible initial state but with each possible non-deterministic state transition as well [Goldman and Boddy, 1996, Smith and Weld, 1998]. While conformant planning does not appear to be too interesting in itself, it is a special case of the more general problem of planning with sensing, and as we will see, the ideas developed for conformant planning provide indeed the basis for the stateof-the-art planners that sense. e best solution to the problem in the figure is to move left two times, then to move up three times, after which the robot will know with certainty that it is located on the upper left-hand corner of the grid (this is sometimes called localization). From there, the robot can head to the goal G directly as in classical planning as all the uncertainty has been removed. We address next two issues: how to model such problems in general, and how to solve them. We then turn to a solution method for deterministic conformant problems that translates such problems into classical problems. Checking the existence of a valid plan in the conformant setting is EXPSPACE-hard, so in the worst case the translation is exponential in the number of problem variables [Bonet, 2010, Haslum and Jonsson, 1999, Rintanen, 2004a]. Still many problems exhibit a structure that makes the transformation polynomial and practical [Palacios and Geffner, 2009]. A deterministic conformant problem is a tuple P D hF; I; O; Gi where F stands for the fluents or atoms in the problem, O stands for a set of deterministic actions a, I is a set of clauses over F defining the initial situation, and G is a set of literals over F defining the (conjunctive) goal. e difference to classical problems is the uncertainty in the initial situation which is described by means of clauses. Recall that a literal is an atom in F or its negation, and that a (non-empty) clause over F is a disjunction of one or more literals. We will assume that the problem is not purely STRIPS but can feature conditional effects and negation; i.e., every action a is assumed to have a precondition given by a set of fluent literals, and a set of conditional effects a W C ! C 0 where C and C 0 are sets (conjunctions) of literals, meaning that the literals in C 0 become true after the action a if the literals in C were true when the action was done. e states associated with the problem P are valuations over the atoms in F , and the set of possible initial states are the states that satisfy the clauses in I . e problem in the figure can be encoded by a tuple P D hF; I; O; Gi where the atoms xi and yi encode the X and Y position of the robot in the grid, i D 1; : : : ; n, the goal is given by the literals x4 and y4, and the initial situation has clauses expressing that the different xi and yi atoms are mutually exclusive and that both x2 _ x3 and y3 _ y4 are true. e actions are four, and for example, the action move-right is characterized by the conditional effects:
move-right W x1 ! x2; :x1 I move-right W x2 ! x3; :x2 I :::
move-right W x5 ! x6; :x5 :


4.2. INCOMPLETE INFORMATION 55
A (deterministic) conformant problem P D hF; I; O; Gi defines a (deterministic) conformant state model S.P / which is like the state model for a classical problem featuring negation and conditional effects but with one difference: there is no single initial state s0 but a set of possible initial states S0. A solution for P , namely a conformant plan for P , is an action sequence that simultaneously solves all the classical state models S0.P / that result from replacing the set of possible initial states S0 in S.P / by each one of the states s0 in S0. From a computational point of view, conformant planning can also be formulated as a pathfinding problem over a graph, but the nodes in the graph do not represent the states of the problem as in classical planning, but belief states, where a belief state or belief is a set of states deemed possible at one point [Bonet and Geffner, 2000]. us, the root node of the graph is the belief b0 D S0 corresponding to the set of possible initial states, and the goal beliefs bG are the possible non-empty sets of goal states. Likewise, the edges correspond to the belief state transitions .b; ba/ that are possible, where ba is the belief state that results from applying the action a in the belief state b characterized as:
ba D fs0 j s 2 b and s0 2 F .a; s/g (4.2)
where F .a; s/ denotes the set of states that are possible following the action a in s. Recent proposals have advanced new heuristics for guiding the search in belief space and more compact belief state representations [Brafman and Shani, 2012b, Bryce et al., 2006, Cimatti et al., 2004, Hoffmann and Brafman, 2006, Rintanen, 2004b, To et al., 2011]. A different approach to deterministic conformant planning is based on the translation of conformant problems into classical ones [Palacios and Geffner, 2009]. e basic sound but incomplete translation removes the uncertainty in the problem by replacing each literal L in the conformant problem P by two literals KL and K:L, to be read as “L is known to be true” and “L is known to be false,” respectively. If L is known to be true or known to be false in the initial situation, then the translation will contain respectively KL or K:L. On the other hand, if L is not known, then both KL and K:L will be initially false. e result is that there is no uncertainty in the initial situation of the translation which thus represents a classical planning problem. More precisely, the basic translation K0 is such that if P D hF; I; O; Gi is a deterministic conformant problem, the translation K0.P / is the classical planning problem K0.P / D hF 0; I 0; O0; G0i where
• F 0 D fKL; K:L j L 2 F g
• I 0 D fKL j L is a unit clause in I g
• G0 D fKL j L 2 Gg
• O0 D O, but with each precondition L for a 2 O replaced by KL, and each conditional effect a W C ! L replaced by a W KC ! KL and a W :K:C ! :K:L.1
e expressions KC and :K:C for C D fL1; L2; : : :g are abbreviations for the conjunctions fKL1; KL2; : : :g and f:K:L1; :K:L2; : : :g respectively. Recall that in a classical planning problem, atoms that are not part of the initial situation are assumed to be initially false, so if KL is not part of I 0, KL will be initially false in K0.P /.
1A conditional effect a W C ! C 0 is equivalent to a collection of conditional effects a W C ! L, one for each literal L in C 0.


56 4. BEYOND CLASSICAL PLANNING: TRANSFORMATIONS
e only subtlety in this translation is that each conditional effect a W C ! L in P is mapped into two conditional effects in K0.P /: a support effect a W KC ! KL, that ensures that L is known to be true when the condition C is known to be true, and a cancellation effect a W :K:C ! :K:L, that ensures that L is possible when the condition C is possible. e translation K0.P / is sound as every classical plan that solves K0.P / is a conformant plan for P , but is incomplete, as not all conformant plans for P are classical plans for K0.P /. e meaning of the KL literals follows a similar pattern: if a plan achieves KL in K0.P /, then the same plan achieves L with certainty in P , yet a plan may achieve L with certainty in P without making the literal KL true in K0.P /. For completeness, the basic translation K0 is extended into a general translation scheme KT;M where T and M are two parameters: a set of tags t and a set of merges m. A tag t 2 T is a set (conjunction) of literals L from P whose truth value in the initial situation is not known. e tags t are used to introduce a new class of literals KL=t in the classical problem KT;M .P / that represent the conditional statements: “if t is initially true, then L is true.” Likewise, a merge m is a non-empty collection of tags
t in T that stands for the Disjunctive Normal Form (DNF) formula W
t2m t . A merge m is valid when one of the tags t 2 m must be true in I , i.e., when
Iˆ
W
t2m t : (4.3)
A merge m for a literal L in P translates into a “merge action” with effects that capture a simple form of reasoning by cases:
V
t2m KL=t ! KL : (4.4)
We assume that the collection of tags T always includes a tag that stands for the empty collection of literals, called the empty tag and denoted as ;. If t is the empty tag, literals KL=t are denoted as KL. e parametric translation scheme KT;M is the basic translation K0 “conditioned” with the tags in T and extended with the actions that capture the merges in M . If P D hF; I; O; Gi is a deterministic conformant problem, then KT;M .P / is the classical planning problem KT;M .P / D hF 0; I 0; O0; G0i where
• F 0 D fKL=t; K:L=t j L 2 F and t 2 T g,
• I 0 D fKL=t j I; t ˆ Lg,
• G0 D fKL j L 2 Gg,
• O0 D fa W KC =t ! KL=t; a W :K:C =t ! :K:L=t j a W C ! L in P g [
O0 D fam;L W
V
t2m KL=t ! KL j L 2 P; m 2 M g.
As before, the literal KL is a precondition of action a in the translation KT;M .P / if L is a precondition of a in P . e translation KT;M .P / reduces to the basic translation K0.P / when M is empty (no merges) and T contains the empty tag only. Two basic properties of the general translation scheme KT;M .P / are that it is always sound (provided that merges are valid), and for suitable choice of the sets of tags and merges T and M , it is complete. In particular, a complete instance of the general translation KT;M .P / results when the set of tags T is set to the set S0 of possible initial states of P , and


4.3. PLAN AND GOAL RECOGNITION 57
ABC
D
HF E
JX
Figure 4.2: Plan Recognition: Which destination is the agent moving to after observing that he moved twice up?
a merge m is included in M such that m D S0. While the resulting translation KS0.P / is exponential in the number of unknown atoms in the initial situation in the worst case, there is an alternative choice of tags and merges, called the Ki .P / translation, that is exponential in the non-negative integer i, and that is complete for problems P that have a structural parameter w.P /, called the width of P , bounded by i. In problems defined over multivalued variables, this width often stands for the maximum number of variables all of which are relevant to a variable appearing in an action precondition or goal. It turns out that many conformant problems have a bounded and small width, and hence such problems can be efficiently solved by a classical planner after a low polynomial translation [Palacios and Geffner, 2009]. e conformant plans are then obtained from the classical plans by removing the “merge” actions. e translation-based approach, introduced initially for deterministic conformant planning, has been extended to deterministic planning with sensing [Albore et al., 2009, Bonet and Geffner, 2011, Brafman and Shani, 2012a,b]. In Chapter 5, we will look at a related notion of width in the more general setting of non-deterministic planning.
4.3 PLAN AND GOAL RECOGNITION
e need to recognize the goals and plans of an agent from observations of his behavior arises in a number of tasks. Plan recognition is like planning but in reverse: while in planning the goal is given and a plan is sought, in plan recognition, part of a plan is observed, and the agent goal is sought [Geib and Goldman, 2009, Kautz and Allen, 1986, Yang, 2009]. Figure 4.2 shows a simple scenario of plan recognition where an agent is observed to move up twice from cell X. e question is which is the most likely destination among the possible targets A to J. Clearly, A, B, and C appear to be more likely destinations than D, E, or F. e reason is that the agent is moving away from these other targets, while it’s not moving away from A, B, or C. e second question is whether B can be regarded as more likely than A or C. ere are indeed good reasons for this. If we adopt a Bayesian


58 4. BEYOND CLASSICAL PLANNING: TRANSFORMATIONS
formulation, the probability of a hypothesis H given the observation Obs, P .H jObs/ is given by the formula [Pearl, 1988]:
P .H jObs/ D
P .ObsjH / P .H /
P .Obs/ (4.5)
where P .ObsjH / represents how well the hypothesis H predicts the observation Obs, P .H / stands for how likely is the hypothesis H a priori, and P .Obs/, which affects all hypotheses H equally, measures how surprising is the observation. In our problem, the hypotheses are about the possible destinations of the agent, and since there are no reasons to assume that one is more likely a priori than the others, Bayes’ rule yields that P .H jObs/ should be proportional to the likelihood P .ObsjH / that measures how well H predicts Obs. Going back to the figure, and assuming that the agent is reasonably “rational” and hence wants to achieve his goals with least cost, it’s clear that A, B, and C predict Obs better than D, E, F, and also that B predicts Obs better than A and C. is is because there is a single optimal plan for B that is compatible with Obs, but there are many optimal plans for A and for C, some of which are not compatible with Obs (as when the agent moves first left or right, rather than up). We say that a plan is compatible with the observed action sequence Obs when the action sequence Obs is embedded in the action sequence , i.e., when Obs is but with certain actions in omitted (not observed). e reasoning above reduces goal recognition to Bayes’ rule and how well each of the possible goals predicts the observed action sequence. Moreover, how well a goal G predicts the sequence Obs turns out to depend on considerations having to do with costs, and in particular, two cost measures: the cost of achieving G through a plan compatible with the observed action sequence Obs, and the cost of achieving G through a plan that is not compatible with Obs. We will denote the first cost as cP .G C Obs/ and the second cost as cP .G C Obs/, where P along with the observations Obs define the plan recognition problem. at is, P is like a classical planning problem but with the actual goal hidden and replaced by a set G of possible goals G, i.e., P D hF; I; O; Gi. e plan recognition problem is to infer the probability distribution P .GjObs/ over the possible goals G 2 G, where each possible goal G can be a (conjunctive) set of atoms. For the plan recognition problem in Figure 4.2, the measures cP .B C Obs/ and cP .B C Obs/, encoding the costs of getting to B from X through plans compatible and incompatible with the observed action sequence Obs, are 4 and 6 respectively, assuming moves in each one of the four possible directions, each with cost 1. On the other hand, the pairs of measures .cP .G C Obs/; cP .G C Obs// for G equal to A, J, and H, are .8; 8/, .8; 4/, .12; 8/ respectively. e key feature is actually the cost difference .G; Obs/ D cP .G C Obs/ cP .G C Obs/ for each goal G which can range from 1 to C1. It can be argued that the higher the value of .G; Obs/, the better that G predicts Obs, and hence the higher the likelihood P .ObsjG/. In particular, .G; Obs/ is 1 when all the plans for G comply with Obs, and 1 when none of them complies with Obs. Values in the middle reflect how good are the plans that comply and do not comply with the observed action sequence Obs. In our example, .G; Obs/ is 2 for G D B, 0 for G D A and G D C, and 4 for the other possible goals. Hence P .ObsjG/ is largest for G D B, smaller for G D A and G D C, and smallest for the rest. e function used by Ramírez and Geffner [2010] for mapping the cost difference .G; Obs/ D cP .G C Obs/ cP .G C Obs/ into the likelihoods P .ObsjG/ is the sigmoid function:


4.3. PLAN AND GOAL RECOGNITION 59
P .ObsjG/ D
1
1 C e ˇ .G;Obs/ (4.6)
where ˇ is a positive constant. is expression is derived from the assumption that while the observed agent is not perfectly rational, he is more likely to follow cheaper plans, according to a Boltzmann distribution. e larger the value of the constant ˇ, the more rational the agent, and the less likely that he will follow suboptimal plans. e target distribution P .GjObs/ over the possible goals G 2 G given the observation sequence Obs can thus be obtained in three steps. First, the costs cP .G C Obs/ and cP .G C Obs/ of achieving each possible goal G with plans that are compatible and incompatible with the observed action sequence Obs are determined. en, the resulting cost differences .G; Obs/ are plugged into Eq. 4.6 to yield the likelihoods P .ObsjG/. Finally, these likelihoods are plugged into Bayes’ rule (4.5) from which the goal posterior probabilities are obtained. e probabilities P .Obs/ used in Bayes’ rule are obtained by normalization (goal probabilities must add up to 1 when summed over all possible goals). e open question is how to compute the cost measures cP .G C Obs/ and cP .G C Obs/. Ramírez and Geffner [2010] show that these costs correspond to the costs of two classical planning problems, that we will call P .G C Obs/ and P .G C Obs/, defined from the plan recognition problem P D hF; I; O; Gi, where G stands for the set of possible agent goals, and the observed action sequence Obs. If we assume that no action occurs twice in the observed sequence Obs, the problems P .G C Obs/ and P .G C Obs/ are like P but with extra atoms pa for each a 2 Obs, all initially false, such that pa is made into an effect of the action a when a is the first action in Obs, while pb ! pa is made into a conditional effect of a, when b is the action that immediately precedes a in sequence Obs. e cost cP .G C Obs/ is then the cost of this classical problem for the goal G0 D G [ fpag, where a is the last action in the sequence Obs, and the cost cP .G C Obs/ is the cost of the same classical problem but with goal G00 D G [ f:pag where :pa is the negation of pa. In other words, the constraint of achieving a possible goal G in a way that is compatible or incompatible with an observed action sequence Obs, is mapped into the problem of achieving G and a suitable dummy goal associated with Obs in a transformed classical problem. Figure 4.3 shows a slightly different example, where the path followed by the agent is shown on the left as time progresses. e curves on the right show the resulting goal posterior probabilities over each one of the possible targets as a function of time. e account presented is not tied to agents navigating in grids but is completely domain-independent. For computing the posterior probabilities, 2 jGj classical planning problems need to be solved. ese probabilities will be exact if the problems are solved optimally, and will be approximate if they are solved with more scalable non-optimal planners. e use of model-based approaches to behavior generation for the inverse task of behavior recognition has been considered recently for other models such as MDPs [Baker et al., 2009] and POMDPs [Ramírez and Geffner, 2011]. Moreover, the approaches can also be used to recognize both goals and agent beliefs, by just replacing the set of possible goals by a set of possible goals and initial belief pairs.


60 4. BEYOND CLASSICAL PLANNING: TRANSFORMATIONS
A
BC DE
F
I
ƺƝǹŁЀƣ Ł>
ŞҸ ֘Ň ӏ
ֵեւ ֞ ׺ Б Ш п եւհ եւեւ եւ ף ׌ ֵ ֞ եւ
հ
ף֞հՂ
ף հՂ
ף հՂБ
եւ
қ қ қ3 қJ қa қx
Figure 4.3: Left: Red path shows noisy walk of agent Obs
t
as time t progresses. Right: Curves show goal posterior probabilities P .GjObs
t
/ for each possible target as a function of time.
4.4 FINITE-STATE CONTROLLERS
Finite-state controllers represent an action selection mechanism widely used in video games and mobile robotics. In comparison to plans and POMDP policies, to be studied later, finite-state controllers have two advantages: they are often extremely compact, and they are general, applying not just to one problem but to many variations as well. As an illustration, Figure 4.4(a) depicts a simple problem over a 1 5 grid where a robot, initially at one of the two leftmost positions, must visit the rightmost position, marked B, and get back to A. Assuming that the robot can observe the mark in the current cell if any, and that the actions Left and Right deterministically move the robot one unit left and right respectively, the problem can be solved by planners that sense and POMDP planners. A solution to the problem, however, can also be expressed as the finite-state controller shown on the right. Starting in the controller state q
0
, this controller selects the action Right, whether A or no mark (‘ ’) is observed, until observing B. en the controller selects the action Left, switches to state q
1
, and remains in this state selecting the action Left as long as no mark is observed. Later, when a mark is observed, no further actions are taken as the agent must be back at A, having achieved the goal. e finite-state controller displayed in the figure has two appealing features: it is very compact (it involves two states only), and it is very general. Indeed, the problem can be changed in a number of ways and the controller would still work, driving the agent to the goal. For example, the size of the grid can be changed from 1 5 to 1 n, the agent can be placed initially anywhere in the grid (except at B), and the actions can be made non-deterministic by adding “noise” so that the agent can move one or two steps at a time. e controller would work for all these variations. is generality is well beyond the power of plans or policies that are normally tied to a particular state space. Memoryless controllers or policies [Littman, 1994] are widely used as well, and they are nothing but finite-state controllers with a single controller state. Additional states provide controllers with a memory that allows different actions to be selected given the same observation. e benefits of finite-state controllers, however, come at a price: unlike plans, they are usually not derived automatically from a model but are written by hand—a task that is non-trivial even in the simplest cases. Recently, however, the problem of deriving compact and general finite-state controllers using planners has been considered [Bonet et al., 2009]. Once again, this is achieved by using classical planners over suitable transformations. We sketch the main ideas below.


4.4. FINITE-STATE CONTROLLERS 61
AB
q0 q1
B/Left
−/Right A/Right −/Left
(a) (b)
Figure 4.4: (a) A partially observable problem where an agent initially in one of the two leftmost positions has to go to the cell marked B and then back to the cell marked A. ese two marks are observable. (b) A 2-state controller that solves this problem and many variations of it. e circles are the controller states, and an edge q ! q0 labeled o=a means to perform action a when the observation is o in state q, switching then to state q0. e initial controller state is q0.
A finite-state controller CN with N controller states q0; : : : ; qN 1, for a partially observable problem P with possible observations o 2 O is fully characterized by the tuples .q; o; a; q0/ associated
with the edges q o=a
! q0 in the controller graph. ese edges, and hence these tuples, prescribe the action a to do when the controller state is q and the observation is o, switching then to the controller state q0 (which may be equal to q or not). A controller solves P if starting in the distinguished controller state q0, all the executions that are possible given the controller reach a goal state. e key question is how to find the tuples .q; o; a; q0/ that define such a controller. In the approach by Bonet et al. [2009], the problem P is transformed into a problem P 0 whose actions are associated with each one of the possible tuples .q; o; a; q0/, and where extra fluents pq and po for keeping track of the controller states and observations are introduced. e action hti associated with the tuple t D .q; o; a; q0/ behaves then very much like the action a but with two differences: first, the atoms pq and po are added to the body of each conditional effect, so that the resulting action hti behaves like the original action a but only when the controller state is q and the observation is o; second, the action makes the atom pq false and the atom pq0 true, in accordance with the interpretation of the tuple (unless q D q0). Additional bookkeeping is required in the transformed problem P 0 to prevent plans from executing actions hti and ht 0i when t D .q; o; a; q0/, t 0 D .q; o; a0; q00/, and a 6D a0 or q0 6D q00. e reason is that no controller can include such pairs of tuples, as the action and new controller state are always a function of the current controller state and observation. Interestingly, the transformation from P into P 0 eliminates sensing by making the effects of the actions conditional on the current controller state and observation. e result is that while P is a partially observable problem, P 0 is a conformant problem, which as we have seen in Section 4.2, can be further transformed into a classical problem. e actions hti that solve such classical problems encode the tuples that define the controller with up to N states that solves P . As a further illustration of the power of these transformations, Figure 4.5, on the left, shows a problem inspired in the use of deictic representations [Ballard et al., 1997, Chapman, 1989], where a visual-marker (the circle on the lower left) must be placed on top of a green block by moving it one cell at a time. e location of the green block is not known, and the observations are whether the cell currently marked contains a green block (G), a non-green block (B), or neither (C), and whether


62 4. BEYOND CLASSICAL PLANNING: TRANSFORMATIONS
#&:0/% $-"44*$"- 1-"//*/( 53"/4'03."5*0/4
q0 q1
–C/Down
TB/Right
TC/Right –B/Up TB/Up –B/Down
'JHVSF -FGU 1SPCMFN XIFSF B WJTVBM NBSLFS NBSL PO UIF MPXFS MFGU DFMM NVTU CF QMBDFE PO UPQ PG B HSFFO CMPDL XIPTF MPDBUJPO JT OPU LOPXO CZ NPWJOH UIF NBSL POF DFMM BU B UJNF BOE CZ PCTFSWJOH XIBU T JO UIF NBSLFE DFMM 3JHIU 'JOJUF TUBUF DPOUSPMMFS PCUBJOFE XJUI B DMBTTJDBM QMBOOFS GSPN TVJUBCMF USBOTMBUJPO ɩF DPOUSPMMFS TPMWFT UIF QSPCMFN BOE BOZ WBSJBUJPO SFTVMUJOH GSPN DIBOHFT JO FJUIFS UIF OVNCFS PS DPOmHVSBUJPO PG CMPDLT
UIJT DFMM JT BU UIF MFWFM PG UIF UBCMF 5 PS OPU o ɩF mOJUF TUBUF DPOUSPMMFS TIPXO PO UIF SJHIU IBT CFFO DPNQVUFE CZ SVOOJOH B DMBTTJDBM QMBOOFS PWFS B USBOTMBUJPO PCUBJOFE GPMMPXJOH UIF UXP TUFQT BCPWF POF GSPN UIF PSJHJOBM QBSUJBMMZ PCTFSWBCMF QSPCMFN JOUP B DPOGPSNBOU QSPCMFN UIF TFDPOE GSPN UIF DPOGPSNBOU QSPCMFN JOUP B DMBTTJDBM POF ɩF TPMVUJPO UP UIF DMBTTJDBM QSPCMFN SFQSFTFOUT UIF mOJUF TUBUF DPOUSPMMFS UIBU JT TIPXO PO UIF SJHIU *OUFSFTUJOHMZ UIJT DPOUSPMMFS OPU POMZ TPMWFT UIF QSPCMFN TIPXO PO UIF MFGU CVU UIF SFBEFS DBO WFSJGZ UIBU JU BMTP TPMWFT BOZ NPEJmDBUJPO JO UIF QSPCMFN SFTVMUJOH GSPN DIBOHFT JO FJUIFS UIF EJNFOTJPOT PG UIF HSJE OVNCFS PS DPOmHVSBUJPO PG CMPDLT <#POFU FU BM > ɩJT JT RVJUF SFNBSLBCMF BOE JMMVTUSBUFT UIBU UIF DPNCJOFE VTF PG USBOTGPSNBUJPOT BOE DMBTTJDBM QMBOOFST DBO CF WFSZ QPXFSGVM JOEFFE
5&.103"--: &95&/%&% (0"-4
$MBTTJDBM QMBOOJOH JT BCPVU BDUJOH PO B TZTUFN UP ESJWF JU JOUP B mOBM TUBUF XIFSF B HPBM IPMET 4VDI UBTLT BSF TPNFUJNFT DBMMFE iSFBDIBCJMJUZw QSPCMFNT *O UIF MBTU GFX ZFBST UFNQPSBMMZ FYUFOEFE HPBMT FYQSFTTFE JO UFNQPSBM MPHJDT IBWF CFFO JODSFBTJOHMZ VTFE UP DBQUVSF B SJDIFS DMBTT PG QMBOT XIFSF SFTUSJDUJPOT PWFS UIF XIPMF TFRVFODF PG TUBUFT NVTU CF TBUJTmFE BT XFMM <#FSUPMJ FU BM EF (JBDPNP BOE 7BSEJ (FSFWJOJ FU BM > " UFNQPSBMMZ FYUFOEFE HPBM NBZ TUBUF GPS FYBNQMF UIBU BOZ CPSSPXFE UPPM TIPVME CF LFQU DMFBO VOUJM JU JT SFUVSOFE EFmOJOH B DPOTUSBJOU UIBU EPFT OPU BQQMZ UP B TJOHMF TUBUF CVU UP B XIPMF TUBUF TFRVFODF " QMBO BDIJFWFT B HPBM XIJMF TBUJTGZJOH B TUBUF USBKFDUPSZ DPOTUSBJOU XIFO UIF QMBO BDIJFWFT UIF HPBM JO UIF TUBOEBSE TFOTF BOE JO BEEJUJPO UIF TUBUF TFRVFODF UIBU JU HFOFSBUFT TBUJTmFT UIF DPOTUSBJOU " TUBOEBSE MBOHVBHF GPS FYQSFTTJOH USBKFDUPSZ DPOTUSBJOUT JT -JOFBS 5FNQPSBM -PHJD PS -5- B MPHJD PSJHJOBMMZ QSPQPTFE BT B TQFDJmDBUJPO MBOHVBHF GPS DPODVSSFOU QSPHSBNT <1OVFMJ > 'PSNVMBT PG -5- BSF CVJMU GSPN B TFU PG BUPNT BOE BSF DMPTFE VOEFS UIF CPPMFBO PQFSBUPST UIF VOBSZ UFNQPSBM PQFSBUPST BOE BOE UIF CJOBSZ UFNQPSBM PQFSBUPS *OUVJUJWFMZ TBZT UIBU IPMET BU UIF OFYU JOTUBOU TBZT UIBU XJMM FWFOUVBMMZ IPME BU TPNF GVUVSF JOTUBOU TBZT UIBU GSPN UIF DVSSFOU JOTUBOU PO XJMM BMXBZT IPME BOE TBZT UIBU BU TPNF GVUVSF JOTUBOU XJMM IPME BOE VOUJM UIBU
Figure 4.5: Left: Problem where a visual-marker (mark on the lower left cell) must be placed on top of a green block whose location is not known, by moving the mark one cell at a time, and by observing what’s in the marked cell. Right: Finite-state controller obtained with a classical planner from suitable translation. e controller solves the problem and any variation resulting from changes in either the number or configuration of blocks.
this cell is at the level of the table (T) or not (–). e finite-state controller shown on the right has been computed by running a classical planner over a translation obtained following the two steps above: one, from the original partially observable problem into a conformant problem; the second, from the conformant problem into a classical one. e solution to the classical problem represents the finitestate controller that is shown on the right. Interestingly, this controller not only solves the problem shown on the left, but the reader can verify that it also solves any modification in the problem resulting from changes in either the dimensions of the grid, number or configuration of blocks [Bonet et al., 2009]. is is quite remarkable and illustrates that the combined use of transformations and classical planners can be very powerful indeed.
4.5 TEMPORALLY EXTENDED GOALS
Classical planning is about acting on a system to drive it into a final state where a goal holds. Such tasks are sometimes called “reachability” problems. In the last few years, temporally extended goals expressed in temporal logics have been increasingly used to capture a richer class of plans where restrictions over the whole sequence of states must be satisfied as well [Bertoli et al., 2003, de Giacomo and Vardi, 1999, Gerevini et al., 2009]. A temporally extended goal may state, for example, that any borrowed tool should be kept clean until it is returned, defining a constraint that does not apply to a single state but to a whole state sequence. A plan achieves a goal while satisfying a state-trajectory constraint when the plan achieves the goal in the standard sense, and in addition, the state sequence that it generates satisfies the constraint. A standard language for expressing trajectory constraints is Linear Temporal Logic or LTL, a logic originally proposed as a specification language for concurrent programs [Pnueli, 1977]. Formulas of LTL are built from a set F of atoms and are closed under the boolean operators, the unary temporal operators ı, Þ, and , and the binary temporal operator U. Intuitively, ı' says that ' holds at the
next instant, Þ' says that ' will eventually hold at some future instant, ' says that from the current instant on ' will always hold, and 'U says that at some future instant will hold and until that


4.5. TEMPORALLY EXTENDED GOALS 63
point ' holds. As an example, the formula .p ıq/ says that if p is true at any time point, then q
must be true at the following time point. e semantics of LTL is given in terms of infinite state sequences D s0; s1; : : : ; si ; : : : where the indices i stand for time points, and the state si represents a truth valuation over F at time i. If we let .i/ stand for the state si in the sequence , the conditions under which a state sequence satisfies an arbitrary LTL formula ' at time i, written ; i ˆ ', can be given inductively as follows:
• ; i ˆ p, for p 2 F , iff p 2 .i/.
• ; i ˆ :' iff not ; i ˆ '.
• ; i ˆ ' ^ '0 iff ; i ˆ ' and ; i ˆ '0.
• ; i ˆ ı' iff ; iC1 ˆ '.
• ; i ˆ Þ' iff for some j i, we have that ; j ˆ '.
• ; i ˆ ' iff for all j i, we have that ; j ˆ '.
• ; i ˆ 'U'0 iff for some j i, we have that ; j ˆ '0 and for all k, i k < j , we have that ; k ˆ '.
A formula ' is true or satisfied in , written ˆ ', if ; 0 ˆ '. For determining whether a given plan D a0; : : : ; an 1 for a classical planning problem P D hF; I; O; Gi satisfies a temporally extended goal expressed as an LTL formula over F , it is normally assumed that the finite state sequence s0; : : : ; sn generated by the plan represents the infinite state sequence s0; : : : ; sn; sn; sn; : : : where the last state sn in the sequence is repeated forever [Bacchus and Kabanza, 2000]. is is an assumption that can be used for many LTL formulas but not for all, as some formulas may be satisfiable but not by sequences of this type. A formula like .Þat.p1/ ^ Þat.p2// expressing that from any time point on, the robot has to be eventually at position 1 and eventually at position 2, is one such example. ese formulas require the consideration of more general infinite state sequences where one finite sequence s0; : : : ; sn is followed by another finite state sequence sn; s01; : : : ; s0m; sn that forms a loop and is repeated infinitely often, and where the states si0 are different than sn. We’ll focus now on the fragment of temporally extended goals expressed in LTL where “completed” state sequences s0; : : : ; sn; sn; sn; : : : suffice. Following Bauer and Haslum [2010], we refer to this as the infinite-extension semantics for LTL, or simply the IE-semantics. We turn thus to the problem of computing a finite plan D a0; : : : ; an 1 for a classical planning problem P D hF; I; O; Gi such that the completed infinite state sequence s0; : : : ; sn; sn; sn; : : : that results from the plan satisfies an LTL formula '. It turns out that this problem can be solved by mapping the classical problem P and the formula ' into a new classical planning problem P' whose solutions represent plans for P that satisfy ' [Baier et al., 2009, Cresswell and Coddington, 2004, Edelkamp, 2006]. Rather than focusing on the syntactic details of the translation, we describe the main idea semantically. We know by now that the planning problem P D hF; I; O; Gi represents a state model S.P / D .S; s0; SG; A; f /, that can also be understood as a deterministic finite automaton AP D . ̇ P ; QP ; qP
0 ; ıP ; F P / where the input alphabet is  ̇P D O, the states are QP D S, the initial state
is qP
0 D s0, the transition function ıP is such that s0 2 ıP .a; s/ iff s0 D f .a; s/, and the accept
ing states are F P D SG. e LTL formula ' defines in turn a non-deterministic Büchi automaton


64 4. BEYOND CLASSICAL PLANNING: TRANSFORMATIONS
A' D . ̇ ' ; Q' ; q'
0 ; ı'; F '/ where the input alphabet is  ̇ ' D S, and the accepted inputs are the in
finite state sequences that satisfy ', defined as the inputs that generate state sequences over Q' that pass through accepting states in F ' infinitely often [Gerth et al., 1995, Vardi and Wolper, 1994]. Under the IE-semantics, however, it is enough to reach an accepting state once, and hence the automaton A' can be regarded as a standard non-deterministic finite automaton, which can be determinized using standard methods [Hopcroft and Ullman, 1979, Sipser, 2006]. erefore, under the IE-semantics, the valid plans that satisfy an LTL formula ' are the action sequences D a0; : : : ; an 1 that generate state sequences D s0; : : : ; sn such that is accepted by the first automaton AP and is accepted by the second automaton A'. us, the classical planning problem P' whose solutions encode the plans for P that satisfy the LTL formula ' can be expressed as the compact representation of the product of two deterministic automata: the deterministic automaton AP associated with the problem P , and the deterministic version of the automaton A' associated with the LTL goal '. e states over the problem P', which represent the truth-valuations over the atoms in P', stand for pairs .s; q/ where s captures the state on the first automaton and q captures the state on the second automaton. is construction requires the addition of atoms pq in P' for such states q, in addition to the atoms in P . e actions in P' are the actions in P but with effects on the atoms pq in correspondence with the second automaton. Likewise, the initial state of P' is the initial state of P extended with the atom pq0, and the goal in P' is the goal of P conjoined with the disjunction of atoms pq for accepting states q. Approaches and transformations for dealing with arbitrary LTL goals, that may require plans with loops, and “lasso” state sequences have also been developed [Albarghouthi et al., 2009, Kabanza and iébaux, 2005, Patrizi et al., 2011, 2013].


65
CHAPTER 5
Planning with Sensing: Logical Models
In this chapter we focus on models and methods for planning with uncertainty and sensing. is is usually called partial observable planning, planning with sensing, or contingent planning. In these models the true state of the environment is not assumed to be known or predictable, yet partial information about the state is assumed to be available from sensors. Uncertainty is represented by sets of states, referred to as beliefs. We will then consider probabilistic models where beliefs are not represented by sets of states but by probability distributions. Logical and probabilistic models however are closely related. A key difference is that, in the absence of probabilistic information, policies or plans are evaluated by their cost in the worst case rather than their expected cost. ere may indeed be policies with small expected cost to the goal but infinite cost in the worst case, as when the state trajectories that fail to reach the goal in a bounded number of steps have a vanishing small probability. Still, as we will see, the policies that ensure that the goal is achieved with certainty can be fully characterized in the logical setting without probabilities at all as the policies that are strongly cyclic [Daniele et al., 1999]. In this chapter we consider a general model for planning with sensing, a language for expressing these models in compact form, the notion of a solution, policy, or plan for such models, and offline and online algorithms for action selection. Since all these algorithms require keeping track of beliefs, we then present methods for tracking beliefs that exploit the structure of the problem and are exponential in a problem width parameter. We also review strong cyclic policies and methods for computing them.
5.1 MODEL AND LANGUAGE
e model for planning with sensing extends the model for (non-deterministic) conformant planning with a sensor model. More precisely, the state model for conformant planning is a tuple of the form S D hS; S0; SG; A; F i where
• S is a finite state space,
• S0 is a non-empty set of possible initial states, S0 S,
• SG is a non-empty set of goal states, SG S,
• A is a set of actions, with A.s/ denoting the sets of actions applicable at s 2 S,
• F is a non-deterministic state-transition function such that F .a; s/ denotes the non-empty set of possible successor states that follow action a in s, for a 2 A.s/, and
• c.a; s/ are positive action costs for s 2 S and a 2 A.s/.


66 5. PLANNING WITH SENSING: LOGICAL MODELS
e model extends the classical planning model by allowing uncertainty in the initial situation and in the transition function. A solution to a conformant model is an action sequence that maps each possible initial state into a goal state. More precisely, D ha0; : : : ; an 1i is a conformant plan if for each possible sequence of states s0; s1; : : : ; sn such that s0 2 S0 and siC1 2 F .ai ; si /, i D 0; : : : ; n 1, action ai is applicable in si and sn is a goal state. Conformant planning can be cast as a path-finding problem over a graph whose nodes are beliefs states: sets of states that the agent deems possible at one point. e initial node is the initial belief state b0 D S0 and the target nodes are the goal beliefs bG, non-empty sets of goal states s 2 SG. e actions a, whether deterministic or not, map a belief state b into the belief state ba:
ba D fs0 j there is a state s in b such that s0 2 F .a; s/g : (5.1)
For the resulting paths to encode conformant plans, an action a must be regarded as applicable in the belief state b, written a 2 A.b/, when a is applicable in each state s in b, or equivalently, when the preconditions of a are true (in all the states) in b. e model for planning with sensing S D hS; S0; SG; A; F; Oi is the model for conformant planning extended with a sensing model O: a function O.s; a/ that maps state-action pairs into nonempty sets of observation tokens. e expression o 2 O.s; a/ means that o is a possible observation token when s is the true state of the system and a is the last action done. at is, every time that the agent executes the action a resulting in the state s, the agent gets an observation token from O.s; a/. is observation o provides partial information about the true but possibly hidden state s, since it rules out states for which the observation token o is not possible, i.e., the states s0 for which o ... O.s0; a/. If two different observations belong to O.s; a/, then either one can be observed in s when a is the last action. We say that the sensing is deterministic or noiseless when O.s; a/ is a singleton for every pair .s; a/, else it is non-deterministic or noisy.
If the belief state for the agent is b and the observation o is obtained after applying the action a in b, the new belief state, denoted as bao, is given by the states in ba that are compatible with o:
bo
a D fs j s 2 ba and o 2 O.s; a/g : (5.2)
An observation o is possible in a belief state ba if o 2 O.s; a/ for some state s in ba. Alternatively, the observation o is possible in ba if and only if the resulting belief state bao is not empty.
LANGUAGE
Conformant models can be expressed in compact form through a set of state variables. For convenience, in the partially observable setting, we assume that these variables are not necessarily boolean. More precisely, a conformant planning problem is a tuple P D hV; I; A; Gi where V stands for the problem variables X, each one with a finite and discrete domain DX , I is a set of clauses over the V -literals defining the initial situation, A is set of actions, and G is a set of V -literals defining the goal. Every action a has a precondition P re.a/ given by a set of V -literals, and a set of conditional effects a W C ! E1j : : : jEn, where C and each Ei is a set (conjunction) of V -literals. e conditional effect is non-deterministic if n > 1. A non-deterministic action is an action with one or more non-deterministic effects.


5.2. SOLUTIONS AND SOLUTION FORMS 67
e conformant problem P D hV; I; A; Gi defines the conformant model S.P / D hS; S0; SG; A; F i, where S is the set of possible valuations over the variables in V , S0 and SG are the set of valuations that satisfy I and G respectively, A.s/ is the set of actions whose preconditions are true in s, and F .a; s/ is a non-deterministic state transition function where s0 2 F .a; s/ is a possible successor state of action a in state s for a 2 A.s/. e set F .a; s/ of such possible successors s0 is defined by the conditional effects a W C ! E1j : : : jEn, n 1, whose body C is true in s. Basically, any logically consistent choice of heads Ei , one for each conditional effect whose body is true in s must define a deterministic transition function f .a; s/. F .a; s/ is the non-deterministic transition function that results from collecting all the successor states s0 that are possible given any of these deterministic functions. A partially observable problem P is a tuple P D hV; I; A; G; V 0; W i that extends the description hV; I; A; Gi of a conformant model with a compact encoding of a sensor model. is sensor model is defined syntactically by means of a set V 0 of variables Y with a finite domain DY that are assumed to be observable, and a set W of formulas Wa.Y D y/ over the state variables V of the problem that determine the states s over which the atom Y D y may be observed. More precisely, the sensor model O.s; a/ defined by W is such that o 2 O.s; a/ iff o is a valuation over the observable variables Y 2 V 0 such that Y D y is true in o only if the formula Wa.Y D y/ is true in s for y 2 DY . In other words, an observation o represents a maximal consistent set of partial observations Y D y where Y is an observable variable and y is a possible value of Y . Such an observation o is possible in the state s after doing action a if the formulas Wa.Y D y/ are all true in s. Two last remarks. First, some of the state variables X may be observable and hence belong to both V and V 0. In such a case, the formula Wa.X D x/ for the different actions and possible values of X is given by X D x. Second, the formulas Wa.Y D y/ for the different values y in DY must be logically exhaustive, as every state-action pair must give rise to some observation over each observable variable Y . If in addition, the formulas Wa.Y D y/ for the different values y are logically exclusive, every state-action pair gives rise to a single observation Y D y and the sensing over Y is deterministic. As an example, if X encodes the location of an agent, and Y encodes the location of an object that can be seen by the agent when X D Y , we can have an observable variable Z 2 fYes; Nog encoding whether the object is seen by the agent or not, with observation model Wa.Z D Yes/ given by
W
l2D.X D l ^ Y D l/, where D is the set of possible locations and a is any action, and Wa.Z D No/ given by the negation of this formula. e resulting sensor is deterministic. A non-deterministic sensor could be used if, for example, the agent cannot detect with certainty the presence of the object at some other locations l 2 D0. For this, Wa.Z D Yes/ and Wa.Z D No/ can be set to the disjunction of their
previous expression and the formula W
l2D0 .X D l/. e result is that the two observations Z D Y es and Z D No will be possible in the states where the agent is at some location l 2 D0, whether the object is in the same location or not.
5.2 SOLUTIONS AND SOLUTION FORMS
An execution for a partially observable problem is an interleaved sequence of actions ak and observations ok, ha1; o1; a2; o2; : : :i. An execution may be finite or infinite. A finite execution ha1; o1; : : : ; ai ; oi i is also called a history. Associated with a finite execution or history hi D ha1; o1; : : : ; ai ; oi i, there is a belief bi . For the empty history h0, the belief is the initial belief b0, while for the history hiC1 D hi ; aiC1; oiC1, the resulting belief biC1 is obtained from the belief bi associated with the


68 5. PLANNING WITH SENSING: LOGICAL MODELS
history hi , the action aiC1, and the observation oiC1, using Eq. 5.2 for the belief bao, with b D bi , a D aiC1, and o D oiC1.
e executions that are possible are the ones where the actions are applicable and the observations are possible. More precisely, the possible executions or histories ha1; o1; a2; o2; : : :i are defined recursively: the empty history h0 is possible, and if history hi is possible, history hiC1 D hi ; aiC1; oiC1 is possible iff the action aiC1 is applicable in the belief bi that results from the history hi , and the observation oiC1 is possible in the belief ba for b D bi and a D aiC1. e solution of a planning problem with sensing is a choice of actions that ensures that all the resulting executions reach a goal belief in a finite number of steps. We make this precise below. We consider two different types of solution forms. In both cases, the choice of the action to do depends on the past actions and observations. In the first case, the choice is expressed as a function mapping histories into actions; in the second, as a function mapping belief states into actions. We will refer to these functions as control policies or simply as policies, and denote them with the symbol . Partially observable problems can also be solved by means of finite-state controllers, but we will not delve further on such an alternative solution form, which can be more compact but is harder to derive (yet see Section 4.4). One difference between history-based and belief-based policies is that the set of histories cannot be bounded a priori, while the set of beliefs is large but bounded: exponential in the number of states. On the other hand, actions define a graph over histories that is acyclic (histories can only grow in size), while the graph over beliefs can be cyclic. ese two features imply that different algorithms may be convenient for computing one type of policy or the other. We focus on policies where .hi / and .bi / express actions a that are applicable in the history hi or belief bi respectively. An action is applicable in a history h if it is applicable in the belief that results from this history. e functions , however, can be partial and don’t have to be defined for all possible inputs. We write .h/ D ? and .b/ D ? to express that policy is undefined for the history h or belief b respectively. e executions that are possible given a policy , whether history or belief-based, are defined as follows: the empty execution h0 is always possible given , and if hi is a possible execution given , the execution hi ; aiC1; oiC1 is possible given iff 1) aiC1 is the action dictated by the policy in hi , and 2) oiC1 is a possible observation after action aiC1 is applied in the belief bi that results from hi . A possible execution h given a policy is complete either if it is infinite or if .h/ D ? or .b/ D ?, where b is the belief that results from the history h. Finally, a policy solves a partially observable problem P iff all the complete executions that are possible given are finite, and terminate in a goal belief. Assuming that the cost of actions is uniform and equal to 1, the cost of a policy is defined as the number of actions in the longest execution that is possible given . e optimal policies that solve P are the ones that minimize the cost of achieving the goal in the worst case. A belief-based policy that solves a problem P induces a unique history-based policy 0 that solves P where 0.h/ D .b/ when b is the belief that results from history h. On the other hand, a history-based policy induces a unique belief-based policy 0 only when there are no two histories h and h0 with the same associated belief b such that .h/ ¤ .h0/. Still, if a problem has a solution in one form, it certainly has a solution in the other form.


5.3. OFFLINE SOLUTION METHODS 69
EXAMPLE
Let us consider a problem where there is a toy in one of two closed boxes, the goal is to have the toy, and the actions are to open a box, to inspect an open box, and to pick up the toy from a box if the box is open and contains the toy. e problem can be modeled by boolean state variables for encoding that a box is open (opened.box/), the toy is in a box (i n.toy; box/), the contents of a box are visible (visible.box/), and the toy is being held (hold.toy/). ese variables are all false in the initial situation, except for the variables i n.toy; box/, box 2 fbox1; box2g, which are not known initially. What is known instead is that either i n.toy; box1/ or i n.toy; box2/ is true. In addition, an observable variable Y with three possible values fyes; no; ‹g can be used to model whether the toy is seen in a box. For this, we can set the observable model formula Wa.Y D yes/ to i n.toy; box/ ^ visible.box/, Wa.Y D no/ to :i n.toy; box/ ^ visible.box/, for a D i nspect.box/ and box 2 fbox1; box2g, and both formulas to f alse when a ¤ i nspect.box/. Likewise, Wa.Y D ‹/ is :visible.box/ for a D i nspect.box/, and t rue otherwise. e action that makes the atom visible.box/ true is i nspect.box/ whose precondition is that the box is open. e actions of opening a box and picking up the toy from a box have preconditions :opened.box/ and i n.toy; box/ ^ opened.box/ respectively. A strategy for solving the problem is to open box 1, inspect its contents, pick up the toy if there, and else, open box 2 and pick up the toy from box 2. is strategy can be captured by the history-based policy where:
.h0/ D ope n.box1/ for the empty history h0 D hi,
.h1/ D i nspect .box1/ for h1 D hope n.box1/; Y D ‹i,
.h2/ D pi ckup.t oy; box1/ for h2 D hope n.box1/; Y D ‹; i nspect .box1/; Y D yesi,
.h02/ D ope n.box2/ for h02 D hope n.box1/; Y D ‹; i nspect .box1/; Y D noi,
.h03/ D pi ckup.t oy; box2/ for h03 D hh02; ope n.box2/; Y D ‹i.
No other executions are possible given this policy. Initially the belief state contains two states differing only in the truth of the atoms i n.toy; box1/ and i n.toy; box2/ After the action i nspect.box1/ and the resulting observation Y D yes or Y D no, the belief state reduces to a single state where i n.toy; box1/ and i n.toy; box2/ are true respectively. is is because for a D i nspect.box1/, the sensor model O.s; a/ is such that o D .Y D yes/ can be observed only when the state s satisfies the formula Wa.Y D yes/ D i n.t oy; box1/ ^ vi si ble.box1/, while o D .Y D no/ can be observed only when s satisfies the formula Wa.Y D no/ D :i n.toy; box1/ ^ visible.box/. us, after the history h2, i n.t oy; box1/ must be true, while after the history h02, i n.t oy; box1/ must be false. Since i n.toy; box1/ is false only in the state where i n.toy; box2/ is true, it follows that i n.toy; box2/ must be true after h02.
5.3 OFFLINE SOLUTION METHODS
As in classical planning, we consider two types of computational methods for partial observable planning: offline methods that produce policies that solve the problem, and online methods that select the action to do next without solving the whole problem first. Clearly, online methods are more practical and scale up better than offline methods but do not have the same guarantees. We consider two offline methods: an exhaustive method for computing belief-based policies, and a heuristic search method for computing history-based policies.


70 5. PLANNING WITH SENSING: LOGICAL MODELS
DYNAMIC PROGRAMMING
e cost of reaching the goal from a belief state b following a policy , denoted as V .b/, can be obtained as the solution of the Bellman equation
V .b/ D
0 if b is a goal belief,
c.a; b/ C maxo V .bao/ otherwise (5.3)
where a D .b/, o ranges over the observations that are possible in ba, and c.a; b/ is the cost of doing action a in the belief state b, which in the worst case is:
c.a; b/ D maxs2b c.a; s/ (5.4)
A policy is optimal if it minimizes the costs V .b/ over all beliefs b. e cost function V for an optimal policy D is the optimal cost function V , that is the solution of Bellman’s optimality equation
V .b/ D
0 if b is a goal belief,
mina2A.b/ Œc.a; b/ C maxo V .bao/ç otherwise. (5.5)
In the absence of dead-ends i.e., belief states b from which the goal cannot be reached, Equation 5.5 can be solved by a simple dynamic programming method called Value Iteration [Bellman, 1957], where the equation is used to update a value vector V over all beliefs b until a fixed point is reached. More precisely, in the version of Value Iteration (VI) known as Gauss-Seidel VI [Bertsekas, 1995], one starts with a value vector V .b/, initially set to 0 over all entries, and then iteratively updates each of the entries V .b/ over non-goal beliefs b as:
V .b/ WD mina2A.b/ Œc.a; b/ C maxo V .bao/ç : (5.6)
In this setting, Value Iteration converges in a finite number of steps to the single solution V D V . e optimal policy for solving the problem is then obtained from the greedy policy V
V .b/ D argmina2A.b/ Œc.a; b/ C maxo V .bao/ç (5.7)
using the value function V D V . We will see in Chapter 7 that the equations for solving POMDPs are similar but with the subexpression maxo V .bao/ representing cost in the worst case, replaced by
the expected costs P
o ba.o/V .bao/. Likewise, the beliefs b, ba, and bao will become then probability distributions, and ba.o/ will stand for the probability of observing o after doing the action a in b. A key difference when moving to POMDPs is that the set of possible beliefs b, representing probability distributions over the set of states, will no longer be finite.
HEURISTIC SEARCH: AO*
e problem with Value Iteration is that it is an exhaustive method that considers all belief states that are possible. While in the logical setting, this set is finite, it is still exponential in the number of states. We switch now to an alternative method that computes history-based policies incrementally. It


5.3. OFFLINE SOLUTION METHODS 71
is based on formulating these policies as solutions of an acyclic AND/OR graph that can be solved by the classical heuristic search algorithm AO* [Nilsson, 1980, Pearl, 1983]. An AND/OR graph is a rooted directed graph with three types of nodes: AND nodes, OR nodes, and terminal nodes. e terminal nodes can be either goal or failure nodes (dead-ends). An AND/OR graph is acyclic if there is no directed loop in the graph, i.e., no directed path that starts and ends in the same node. A solution to an acyclic AND/OR graph is a subgraph that includes the root node, one child of every OR node, all children of every AND node, and terminal nodes that are all goal nodes. In the absence of AND nodes, an AND/OR graph becomes a normal directed graph whose solution is a path from the root to a goal node. e childen of OR nodes represent choices, while the children of AND nodes represent contingencies, uncontrollable events, or adversarial moves that must all be handled in the solution of the problem. e cost of an AND/OR solution is the cost of its root node defined recursively as follows: the cost of terminal nodes is 0 and 1 for goal and failure nodes respectively, the cost of AND nodes is the cost of the child with MAX cost, and the cost of OR nodes is the cost of the child with MIN cost plus one (assuming action costs equal to 1). Other cost structures are possible over AND/OR graphs as when the max operation associated with AND nodes is replaced by a sum or a weighted sum. For capturing the history-based policies that solve a partially observable problem in terms of the solutions of an acyclic AND/OR graph, histories h are mapped to OR nodes n.h/, histories h extended with actions a applicable in h are mapped to AND nodes n.h; a/, and histories hh; a; oi, where o is a possible observation after action a in history h, are mapped to the children of the AND node n.h; a/. e root node of the graph corresponds to the empty history, and the terminal goal nodes to the histories whose associated beliefs are goal beliefs. Solutions to such an implicit AND/OR graph must include the empty history h, and for every included non-goal history h, an action a, and all possible histories hh; a; oi. e history-based policy encoded by such a solution is .h/ D a. e algorithm AO* is a heuristic search method for solving acyclic AND/OR graphs. AO* maintains a graph G, called the explicit graph, that incrementally explicates part of the implicit AND/OR graph, and a second graph G , called the best partial solution graph, that represents an optimal solution of G under the assumption that the tip nodes n of G are terminal nodes whose values are given by a heuristic h.n/. Initially, G contains the root node of the implicit graph only, and G is G. en, iteratively, a non-terminal tip node is selected from the best partial solution G , and the children of this node are explicated in G. e best partial solution G is then revised by a simple form of backward induction where the values of these new nodes are propagated up the (acyclic) graph. AO* finishes when the tip nodes of the best partial graph G are all terminal nodes. If the heuristic values are optimistic, the best partial solution G is then an optimal solution to the implicit AND/OR graph. If not, AO* produces a solution that is not necessarily optimal. Code for AO* is shown in Figure 5.1. Informative heuristics are crucial for the performance of AO*. Heuristics for partially observable problems estimate the cost from a belief b to any goal belief. If the actions have deterministic effects, one of the approaches that have been used to obtain such estimates replaces the belief b by states s in b, resulting in classical planning problems whose cost can be estimated and combined in a number of ways [Bonet and Geffner, 2000, Bryce et al., 2006]. If estimates are admissible, the maximum over all such states s, yields an admissible heuristic, while the sum yields a non-admissible heuristic. ese simplifications, however, remove the uncertainty in the belief b and hence ignore the need and value of sensing. Other heuristics for planning with sensing, called cardinality heuristics, focus exclusively on this uncertainty, defining the heuristic for b in terms of the number of states in b [Bertoli and Cimatti,


72 5. PLANNING WITH SENSING: LOGICAL MODELS
AO*
% G and G are explicit and best graphs, initially empty; V 0 is heuristic function.
Initialization
Insert node h0 in G where h0 is the empty history. Initialize V .h0/ WD V 0.b0/ where V 0 is admissible heuristic and b0 initial belief. Initialize best partial graph G to G.
Loop
Select non-terminal tip h from best partial graph G . If no such node, Exit. Expand h in G: for each a 2 A.b/ where b is belief associated with h, add node .h; a/ as child of h, and for each observation o possible in ba, add node .h; a; o/ as child of .h; a/. Initialize values V .h; a; o/ to the heuristic values V 0.bao/. Update h and its ancestor AND and OR nodes in G, bottom-up as:
V .h; a/ WD 1 C maxo V .h; a; o/,
V .h/ WD mina2A.b/ V .h; a/.
Mark best action in ancestor OR-nodes h to an action a with V .h/ D V .h; a/, maintaining marked action if still best. Recompute best partial graph G by following marked actions in G.
Figure 5.1: AO* for Computing History-based Policies for Partially Observable Problems.
2002]. More recent approaches have appealed to the translations developed for mapping conformant into classical planning problems (Section 4.2). Indeed, the delete-relaxation of a partially observable problem has a conformant solution once the preconditions of actions are pushed in as additional conditions of the actions’ conditional effects [Hoffmann and Brafman, 2005]. Such relaxations have the advantage that they do not need to assume that the information is complete [Albore et al., 2009, 2011].
We have considered an exhaustive dynamic programming method (Value Iteration) for finding policies over a potentially cyclic belief space, and a heuristic search method (AO*) for finding policies over the acyclic space of histories. e two types of methods are not incompatible however. More recent algorithms like LAO* [Hansen and Zilberstein, 2001] and RTDP [Barto et al., 1995] manage to get the best of both worlds, and variations of these algorithms can be used to compute optimal and non-optimal belief-based policies in an incremental fashion using heuristics. We will consider such algorithms in the next chapter.
5.4 ONLINE SOLUTION METHODS
Offline solution methods have an inherent limitation: the size of the policies required for solving a problem may have exponential size. A practical alternative is to avoid computing a whole solution


5.5. BELIEF TRACKING: WIDTH AND COMPLEXITY 73
before acting, and to decide on the action to do in the current situation, to do it, to observe the results, and to iterate this loop until the goal is reached. is is the idea of online planning. In Section 2.4, we have seen different methods for online planning in the classical setting, from selecting the action a to do in a state s, greedily, by minimizing the expression Q.a; s/ D c.a; s/ C h.s0/, where h is a heuristic function and s0 is the state that follows s after the action a is done, to various lookahead schemes where the choice of the action a in s is based on a deeper exploration of the local space around s, to learning schemes such as LRTA* where the heuristic function is updated as the online search progresses. e methods for selecting actions in the classical setting are all available in the partially observable setting but with two modifications: first, the local search is not over states s but over belief states b; second, the local space to search is not an OR graph but an AND/OR graph. us, the greedy action to do in a belief state b is the one that minimizes the expression Q.a; b/ D c.a; b/ C maxo V .bao/, where V is the heuristic over beliefs, and learning algorithms like LRTA* need to update the values V .b/ to mina2A.b/ c.a; b/ C maxo h.bao/. Similarly, anytime optimal algorithms to be used in the finite-horizon version of the problem should not be based on A* but on AO* [Bonet and Geffner, 2012a]. Yet, some of the best current online partially observable planners for deterministic problems, follow a different strategy, where the action to be done next in a belief state b is selected by solving a classical planning problem obtained from a simplification of the problem [Bonet and Geffner, 2011, Brafman and Shani, 2012b]. e classical plan that is obtained is executed as long as the observations that are gathered do not refute the assumptions made in the simplification. When they do, an alternative, more informed relaxation is constructed and solved, leading to a new classical plan, and so on. ese replanning approaches build on a formulation that extends the translation-based approach introduced for conformant planning [Palacios and Geffner, 2009] to the partially observable setting [Albore et al., 2009]. ey can be shown to be complete, hence reaching the goal when possible, provided that the translation is complete and that the problem features no dead-ends. In the presence of dead-ends, these schemes can benefit from the use of lookahead schemes. Still, the completeness of these replanners in the absence of dead-ends is no small feat given that online planners that plan over finite horizons may fall into a loop. ese replanners, on the other hand, exhibit an exploitationexploration property that precludes such loops, where in every replanning episode they either reach the goal or learn that one of the assumptions made in the simplification is wrong. ese replanners, however, are restricted to deterministic problems only.
5.5 BELIEF TRACKING: WIDTH AND COMPLEXITY
We have assumed so far that keeping track of beliefs is sufficiently simple. In the worst case, however, the computation of the belief state bao that follows the belief b after the action a and observation o, is exponential in the number of state variables. One way to deal with beliefs b containing many states is by representing them by logical formulas whose satisfying assignments are precisely the states in b [Bertoli et al., 2001, Bryce et al., 2006, To et al., 2011], or by logical formulas that capture the state trajectories that are possible given the past actions and observations, and which can be queried by state-of-the-art solvers for determining whether certain formulas are true in b [Brafman and Shani, 2012b, Hoffmann and Brafman, 2006]. Indeed, a planner just needs to perform two types of tests on beliefs b, namely, whether a goal X D x is true in b, and whether a precondition X D x of an action a is true in b. e first test determines whether a given history has achieved the goal or needs to be


74 5. PLANNING WITH SENSING: LOGICAL MODELS
extended, the second, which actions a can be used to extend it. e third required test, namely, whether an observation o is possible in ba, follows from the other two: o is impossible in ba if both X D x and its negation hold in bao, implying that bao is empty. We will thus focus on a method for belief tracking that is targeted at these two queries, and which is not necessarily complete for other queries. Such an incompleteness, however, does not compromise the completeness of the resulting planners. A key result is that it is possible to keep track of the beliefs necessary for determining the truth of action preconditions and goals in time and space that are exponential in a width parameter associated with the problem, which in many domains of interest is bounded and small. e result follows from the reductions of deterministic conformant problems into classical planning problems [Palacios and Geffner, 2009], and the extensions developed for handling partial observability [Albore et al., 2009] and non-deterministic actions [Bonet and Geffner, 2012b]. In this last formulation, the global beliefs b are factored into local beliefs bX , for each variable X appearing in a precondition or goal. Keeping track of these local beliefs bX is exponential in the number of state variables that are relevant to X. If this measure is called the width of X, w.X/, the width of the problem is the maximum w.X/ over the state variables X appearing in preconditions or goals. e notion of relevance underlying this complexity bound is related to the notion of relevance in Bayesian networks [Pearl, 1988], while exploiting the structure of goals and action preconditions, and the information that certain variables will not be observed. More precisely, a variable X is defined as an immediate cause of a variable Y in a problem P iff X ¤ Y , and either X occurs in the body C of a conditional effect C ! E1j jEn where Y occurs in a head Ei , 1 i n, or X occurs in a formula Wa.Y D y/ where Y is an observable variable and y 2 DY . en X is said to be causally relevant to Y if X D Y , X is an immediate cause of Y , or X is causally relevant to a variable Z that is causally relevant to Y . Finally, X is relevant to Y if X is causally relevant to Y , Y is causally relevant to X and X is an observable variable, or X is relevant to a variable Z that is relevant to Y . e set of state variables Y that are relevant to X is called the context of X, which defines the set of variables that must be tracked concurrently with X in order to know the possible values for X after an execution. Moreover, this context C tx.X/ defines a projected subproblem PX that is like P but with the state variables limited to those in C tx.X/. Belief tracking in PX is thus exponential in the width w.X/ of X, while sound and complete for determining the values of X that are possible after an execution. e factored belief tracking algorithm that tracks the beliefs bX over each of the projected problems PX , for each precondition and goal variable X in the problem P , is complete for planning and runs in time and space that are exponential in the problem width, which may be much smaller than the number of variables in the problem.
As an illustration, consider the DET-Ring domain [Cimatti et al., 2004] depicted in Figure 5.2, where an agent can move forward or backward along a ring with n rooms. Each room has a window that can be opened, closed, or locked when closed. Initially, the status of the windows is not known, the agent does not know his initial location, and the goal is to have all windows locked. A plan for this deterministic conformant problem is to repeat n times the actions .close; lock; f wd /, skipping the last f wd action (alternatively, f wd can be replaced by the action bwd throughout). e state variables for the problem encode the agent location Loc 2 f1; : : : ; ng, and the status of each window, W .i/ 2 fopen; closed; locked g, i D 1; : : : ; n. e location variable Loc is (causally) relevant to each window variable W .i/, but no window variable W .i/ is relevant to Loc or to W .k/ for k 6D i. e largest contexts are thus for the window variables which have size 2. As a result, the width of the domain is 2, which is independent of the number of variables for the problem that grows linearly with


5.5. BELIEF TRACKING: WIDTH AND COMPLEXITY 75
W7 W3
W1
W5
Wn W2
W6 W4
Figure 5.2: Ring problem with n windows that must be closed and locked. Initially, the agent does not know its location or the status of the windows. In NON-DET-Ring, each time the agent moves, the unlocked windows open or close non-deterministically. In another variation of the problem, the agent needs a key to lock the windows whose initial position is not known.
n. is means that belief tracking for this problem can be done in quadratic time since there are n contexts that need to be tracked, each of size O.n/ as the domain size for the window variables is constant. NON-DET-Ring is a variation of the domain where any movement of the agent, f wd or bwd , has a non-deterministic effect on the status of all windows that are not locked, capturing the possibility of external events that can open or close unlocked windows. is non-determinism has no effect on the relevance relation among the variables as Loc was already relevant to each variable W .i/. As a result, the change has no effect on the contexts or domain width that remains bounded and equal to 2. A further variation involves a key that is needed now to lock the windows, whose initial position is unknown. e agent may then perform a pick action that grabs the key when the key and the agent are in the same room. In all these variations, the problem width remains bounded and small. As a result, the belief tracking task for planning can be accomplished in low-order polynomial time even if the number and size of the beliefs is exponential in the number of rooms.
APPROXIMATIONS
Factored belief tracking is complete for planning and exponential in the problem width, yet this is still not good enough when problems have a large width. For such cases, however, it has been shown that it is possible to obtain meaningful approximations that are sound, polynomial, and powerful, even if not necessarily complete [Bonet and Geffner, 2013]. e idea is the consideration of a larger collection of projected subproblems PX , each one involving a smaller set of variables. e algorithm being sound means that when a literal X D x is reported as true or false after an execution, it is really true or false; while the algorithm being incomplete means that the literal X D x may fail to be reported as true or false after an execution when X D x is really true or false in the true belief. In the approximation, the variables X range not only over preconditions and goal variables, but also over observable variables, while the state variables that make it into the projected problem PX are only those that are causally relevant to X. e result of this alternative decomposition is that there are more projected problems PX but of smaller size whose local beliefs bX can be tracked more efficiently. In this scheme, however, a state variable Y may be involved in two subproblems PX and PX0, such that Y D y is known to be true in bX but not known to be true in bX0. e second step in the approximation is to enforce


76 5. PLANNING WITH SENSING: LOGICAL MODELS
a local form of consistency among the local beliefs, an operation that can be achieved in polynomial time. e resulting approximation algorithm has been used successfully, in combination with simple heuristics, for solving large instances of domains like Minesweeper, Battleship, and Wumpus, where belief tracking is key [Bonet and Geffner, 2013].
5.6 STRONG VS. STRONG CYCLIC SOLUTIONS
A policy that solves a partially observable problem P must be such that the only complete executions that are possible given are finite and end up in a goal belief. ere are problems, however, where these requirements are too strong and can’t be ensured by any policy. For example, if the action of hammering a nail into a wall has cost 1 but fails half of the time, then the expected cost of getting the nail into the wall is 2 but the cost in the worst case is not bounded. e result is that there is no policy that solves the problem in the logical setting, although there is a perfectly good policy in the probabilistic setting. is policy does not guarantee that the problem will be solved in one step, but can guarantee that the goal will be solved eventually with probability 1. Interestingly, there is a way to weaken the requirements on policies in the logical setting so that they can capture exactly the policies that achieve the goal with probability 1. is requires the assumption that the non-deterministic state transition function F .a; s/ captures exactly the set of states s0 that may follow action a in the state s with nonzero probability, even if the exact probabilities are not known. ese solutions are called proper policies in the probabilistic setting [Bertsekas, 1995], and strong cyclic policies in the logical setting [Cimatti et al., 2003, Daniele et al., 1999]. e solutions that we have considered so far are called strong policies, as they solve the problem in a bounded number of steps. We characterize the strong cyclic policies below and review methods for computing them. Since the distinction between strong and strong cyclic policies is independent of issues pertaining to partial observability, we will assume for simplicity that the environment is fully observable and that the initial state s0 is given. In the fully observable setting, an execution or history is a sequence of states and actions h D s0; a0; s1; a1; s2; : : :. An infinite execution is fair when for each state-action pair s; a that appears an infinite number of times in the execution, the triplet s; a; s0 also appears an infinite number of times for any s0 2 F .a; s/. In other words, the infinite execution is unfair if the action a is applied an infinite number of times in state s, and yet there is a possible successor state s0, s0 2 F .a; s/, that only occurs a finite number of times. Provided with this notion of fairness, we can define the strong cyclic policies as follows: is a strong cyclic policy for P iff all the complete executions that are possible given are either finite and terminate in a goal state, or are infinite and unfair. e difference with the strong policies for P is that infinite executions that do not reach a goal state are allowed as long as they are not fair. We will later discuss the equivalence between strong cyclic policies and proper policies for MDPs. We focus now on alternative methods for characterizing these policies and for computing them. Since the cost in the worst case V .s/ associated with a strong cyclic policy from a state s can be infinite, it is useful to introduce a second “optimistic” cost measure Vmin.s/ that results from the assumption that the successor state s0 of a non-deterministic action a D .s/ in the state s is the “best” possible outcome for the agent. at is, while the cost in the worst-case function V .s/ is the solution to the equation:
V .s/ D
0 if s is a goal state,
c.a; s/ C maxs02F .a;s/ V .s0/ otherwise for a D .s/, (5.8)


5.6. STRONG VS. STRONG CYCLIC SOLUTIONS 77
which is analogous to Eq. 5.3 under the assumption that the observations are over full states, the “optimistic” cost function Vmin.s/ is the solution to the equation:
Vmin.s/ D
0 if s is a goal state,
c.a; s/ C mins02F .a;s/ Vmin.s0/ otherwise for a D .s/. (5.9)
at is, Vmin.s/ measures the cost from s to the goal under the assumption that it is the agent rather than nature the one that chooses the successor state s0 2 F .a; s/ that follows an action a D .s/ in each state s. In particular, the cost Vmin.s/ is finite when the agent can get from s to the goal following if “lucky” enough, while Vmin.s/ is infinite when no amount of luck would help the agent as there are no state trajectories linking s to the goal while following the policy . It turns out that is a strong cyclic policy for a problem P with initial state s0 iff the policy is such that over all the states s that are reachable from s0 following , Vmin.s/ is finite. e set of states reachable from s0 and is the minimal set of states S0 that includes s0 and any state s0 such that s0 2 F .a; s/ for s 2 S0 and a D .s/. In other words, is strong cyclic when it drives the agent to states s all of which are separated from the goal by a finite trajectory s1; s2; : : : ; sn such that s1 D s, the state sn is a goal state, and siC1 2 F .a; si / for a D .si /, i D 1; : : : ; n 1. It is easy to show indeed that infinite executions that feature a state si in the trajectory an infinite number of times, but do not feature the successor state siC1 an infinite number of times, cannot be fair. e simplification of the problem that underlies the optimistic cost function Vmin has been used as a source of heuristics for non-deterministic and MDP problems where it is called the min-min relaxation [Bonet and Geffner, 2000, 2005]. It is also closely related to a different relaxation used in FFReplan for solving MDPs, called the deterministic relaxation [Yoon et al., 2007]. Ignoring probabilities for the moment and focusing on semantics rather than in syntax, the min-min relaxation replaces each non-deterministic action a by deterministic actions a1, ..., am, each one of which picks one of the possible outcomes of a, so that for any states s and s0, s0 2 F .a; s/ is true in the non-deterministic problem iff s0 D f .ak; s/ for some action ak in the deterministic problem. When this relaxation is done at the syntactic level, it produces a classical planning problem where the uncertainty about non-deterministic transitions is now controlled by the agent. Indeed, it is easy to see that the cost function Vmin.s/ is finite when such a classical planning problem has a solution from the state s. is all suggests two methods for computing strong cyclic policies for non-deterministic but fully observable problems P . A purely semantic and exhaustive method is to compute first, via Value Iteration, the optimal cost function Vmin.s/ for the min-min relaxation, where Vmin.s/ D min Vmin.s/. en the states s for which Vmin.s/ D 1 are removed from the problem, and the actions a that can possibly lead to such states from states s0 are removed from the sets A.s0/. is process of computing the value function Vmin and pruning the action sets is iterated until the set of states s and the sets A.s/ of applicable actions do not change further.1 If the initial state s0 is removed in the process, the problem has no strong cyclic solution, else, the policy that is greedy in the value function Vmin computed last is one such solution [Daniele et al., 1999]. Alternatively, one can compute strong cyclic plans using classical planners over the deterministic relaxation P 0 of the problem P . Let P 0.s/ be the classical problem obtained from P 0 by setting the initial situation to s. Define then a complete state-plan (SP) pair hS0;  ̇i as a set of states S0, including
1An optimization is to select the states to prune from those which are reachable from the initial state s0 with the policy that is greedy in Vmin. e iteration can be terminated when there are not such states.


78 5. PLANNING WITH SENSING: LOGICAL MODELS
the initial problem state s0, along with a set  ̇ of classical plans .s/ for the problem P 0.s/, one for each state s 2 S0. e SP pair hS0;  ̇i is consistent when the plans .s/ in  ̇ that pass through a state s0 2 S0 all apply the same action from P in s0. e expression  ̇.s0/ is used to denote this action. In particular, if the action is any of the determinizations ak of a in P ,  ̇.s/ is a. e consistent SP pair hS 0;  ̇ i is closed when a state s0 is in S 0 if there is state s in S0 such that s0 2 F .a; s/ for a D  ̇.s/. It can then be shown that the partial policy defined as .s/ D  ̇.s/ for complete SP pairs hS0;  ̇i that are consistent and closed, is a strong cyclic policy for P . is means that a strong cyclic policy for P can be computed using classical planners incrementally, starting with the initial incomplete SP pair hS0;  ̇i where S0 D fs0g and  ̇ D ;. For this, classical plans are added to  ̇ to make the pair complete, and states are added to S0 to make the pair closed. e state-plan pair is kept consistent by forcing the classical planner to respect the partial policy encoded by the pair. is can be achieved by adjusting the deterministic relaxations incrementally, or by modifying the classical planner used. An algorithm of this form will compute strong cyclic policies backtrack-free in problems with no dead-ends, but may have to backtrack otherwise. e first use of classical planners for computing strongly cyclic plans in this way is due to Kuter et al. [2008], and recent refinements to Fu et al. [2011] and Muise et al. [2012]. ese are all offline algorithms. e planner FF-Replan mentioned above and to be discussed again in the next chapter, can be regarded as an online version of these algorithms.


79
CHAPTER 6
MDP Planning: Stochastic Actions and Full Feedback
Markov Decision Processes (MDPs) generalize the model underlying classical planning by allowing actions with stochastic effects and fully observable states. In this chapter, we look at a variety of MDP models and the basic algorithms for solving them: from offline methods based on dynamic programming and heuristic search, to online methods where the action to do next is obtained by solving simplifications, like finite-horizon versions of the problem or deterministic relaxations.
6.1 GOAL, SHORTEST-PATH, AND DISCOUNTED MODELS
ere is a variety of MDP models, some more expressive than others [Bertsekas, 1995, Boutilier et al., 1999, Puterman, 1994]. We focus first on Goal MDPs that provide a direct generalization of the model underlying classical planning where the deterministic transition function f .a; s/ is replaced by transition probabilities Pa.s0js/. At the same time, while the next state cannot be predicted with certainty, it is assumed to be fully observed. A Goal MDP is thus given by:
• a finite and non-empty state space S,
• an initial state s0 2 S,
• a non-empty subset SG S of goal states,
• sets A.s/ of actions applicable at each state s 2 S,
• transition probabilities Pa.s0js/ for s0 being the next state after doing the action a 2 A.s/ in the state s 2 S, and
• positive action costs c.a; s/ for applying action a 2 A.s/ in the state s 2 S.
e planning task over Goal MDPs is to come up with an action strategy for reaching the goal with certainty given the uncertain effect of the actions and the observations gathered. e solution form for Goal MDPs cannot be thus a fixed action sequence as in classical planning; it must take observations into account. is is simple to do in MDPs, however, where observations are over full states and the dynamics and costs are Markovian, meaning that future states and costs depend on the current state but not on the previous history. e result is that the choice of the action to do next in MDPs just needs to take into account the last observation, and the solution form for MDPs is a function mapping (the observed) states into actions. ese functions are called closed-loop control policies or simply policies,


80 6. MDP PLANNING: STOCHASTIC ACTIONS AND FULL FEEDBACK
denoted by the symbol . We will assume for now that a policy maps every non-goal state s into an action a 2 A.s/. Policies of this type are said to be deterministic and stationary. A stochastic policy , on the other hand, is a function that maps states into probability distributions over actions, and a non-stationary policy is a function of both state and time. Stochastic and non-stationary policies can be used for controlling MDPs, but they are not strictly needed except in the setting of finite-horizon MDPs where optimal policies can be non-stationary. A (deterministic and stationary) MDP policy and state s define a probability for every state trajectory hs0; s1; : : : ; snC1i given by the product
P .s0js/ Pa0 .s1js0/ Pa1 .s2js1/ Pan .snC1jsn/ (6.1)
where ai D .si / is the action dictated by the policy in the state si , Pai .siC1jsi / is the state transition probability, and P .s0js/ is 1 if s0 D s and else is 0. e accumulated cost of a state trajectory hs0; s1; : : : ; snC1i given a policy , ai D .si /, is given in turn by the sum
c.a0; s0/ C c.a1; s1/ C C c.an; sn/ : (6.2)
e expected cost to reach the goal from state s using the policy , denoted as V .s/, stands for the sum of the accumulated costs of the different state trajectories that are possible given , weighted by their probabilities. e expected cost function V can also be characterized as the solution to a set of linear equations. For this, it is convenient to assume that goal states are absorbing and cost-free, meaning that some action a is applicable in each goal state s, and that such applicable actions a in a goal state s have zero costs and null effects; i.e., c.a; s/ D 0 and Pa.sjs/ D 1. Under the assumption that every policy selects one of these “dummy” actions in each goal state, the expected cost of policy from the state s, V .s/ can be defined by the expression
V .s/ D Es
P
i 0 c. .Xi /; Xi / (6.3)
where Xi is a random variable that represents the state at time i, and Es Œ ç is the expectation with respect to the probability distribution on state trajectories that start in the state s given by (6.1). Moving the first term of the sum out of the expectation, the following fixed point equation is obtained
V .s/ D c. .s/; s/ C
P
s02S P .s/.s0js/V .s0/ (6.4)
that defines the function V as the solution of a system of jSj linear equations with the border condition V .s/ D 0 for all goal states s. It is possible to show that a policy for a Goal MDP has a finite expected cost V .s/ if and only if starting in the state s, the application of the policy leads to a goal state with probability 1. A policy that leads to the goal with certainty for any possible initial state is called a proper policy. A necessary and sufficient condition for a policy to be proper is that for any state s, there is a finite state trajectory hs0; s1; : : : ; snC1i, starting in the state s0 D s and ending in a goal state snC1, such that all the state transitions in the trajectory are possible given ; i.e., P .si /.siC1jsi / > 0 for i D 0; : : : ; n. Notice that the exact value of these probabilities does not matter as long as they are different than zero. is explains the correspondence between the proper policies in the probabilistic setting, and


6.1. GOAL, SHORTEST-PATH, AND DISCOUNTED MODELS 81
the strong cyclic policies in the non-deterministic setting analyzed in Section 5.6 that do not involve probabilities at all. We will consider Goal MDPs where there are no dead-ends, i.e., states from which the goal cannot be reached. Formally, dead-ends are states s such that there is no state trajectory hs0; : : : ; snC1i with s0 D s, goal state snC1, and actions a0; : : : ; an such that the transition probabilities Pai .siC1jsi / are all positive for i D 0; : : : ; n. Clearly, if s is a dead-end, V .s/ is infinite for any policy , and alternatively, if there are no dead-ends, there must be a policy that is proper. We will relax the no dead-ends assumption for Goal MDPs when considering methods that compute partial policies. A policy is optimal for state s if V .s/ is minimum among all policies; i.e., V .s/ D min V .s/. While the optimal policies for Goal MDPs are the policies that are optimal for the given initial state s0, we will follow the standard notion that identifies the optimal policies as the policies that are optimal over all states. e cost function V for an optimal policy is the optimal cost function V , which can be characterized as the unique solution of Bellman’s optimality equation [Bellman, 1957]:
V .s/ D mina2A.s/ Œc.a; s/ C
P
s02S Pa.s0js/V .s0/ç (6.5)
for all non-goal states s, and V .s/ D 0 for goal states. A deterministic, stationary optimal policy can be obtained from the optimal cost function V , from the greedy policy V :
V .s/ D argmina2A.s/ Œc.a; s/ C
P
s02S Pa.s0js/V .s0/ç (6.6)
with the value function V set to V . e ties in (6.6) can be broken arbitrarily.
Figure 6.1(a) depicts a simple example of a Goal MDP in which there is an agent that has to navigate in a grid with obstacles from the cell marked A to the cell marked G. e agent can move one cell at a time in each of the four directions as long as there are no obstacles, and the intended moves succeed with high probability while leaving the agent in nearby cells with non-zero probability. e panel (b) in Figure 6.1 shows a proper policy for the problem as the action .s/ to do at each of the cells s, except at the cell G representing the goal state.
SHORTEST-PATH AND DISCOUNTED MDPS
Stochastic Shortest-Path MDPs (SSPs) generalize Goal MDPs by dropping the requirement that action costs c.a; s/ over non-goal states s are positive [Bertsekas, 1995]. Instead, such action costs c.a; s/ can be either negative or zero, as long as any policy that is not proper for a state s has an infinite expected cost V .s/. A policy is not proper for a state s when the probability of reaching the goal from s following the policy is less than 1. As for Goal MDPs, SSPs assume that there exists one policy that is proper for all the states. Discounted Cost-based MDPs do not require the presence of absorbing and cost-free goal states, or the assumption that action costs are positive. Instead, discounted models assume that future costs depreciate over time according to a fixed rate 0 < < 1, called the discount factor. In contrast to Eq. 6.2, the accumulated cost associated to a state trajectory hs0; : : : ; sni under a policy is
c.a0; s0/ C c.a1; s1/ C C n c.an; sn/ (6.7)


82 6. MDP PLANNING: STOCHASTIC ACTIONS AND FULL FEEDBACK
A
GG
(a) (b)
Figure 6.1: A Goal MDP in which an agent, initially at A, must reach the cell marked with G with certainty. e grey cells are obstacles that cannot be crossed. Each action moves the agent in the intended direction with non-zero probability but can also leave the agent in a nearby cell with non-zero probability as well. Panel (b) shows a proper policy for the problem depicted as the action to be done in each non-goal state.
where ai D .si /. Since the cost of any such trajectory is bounded from below and above by c=.1 / and c=.1 / respectively, where c and c are lower and upper bounds on the action costs c.a; s/, it follows that the expected costs V .s/ for all policies and states s are finite. e Bellman equation characterizing this cost function is:
V .s/ D c. .s/; s/ C
P
s02S P .s/.s0js/V .s0/ ; (6.8)
and similarly, the optimal cost function V for Discounted Cost-based MDPs is given by the unique solution of the optimality equation [Bertsekas, 1995, Puterman, 1994]:
V .s/ D mina2A.s/ Œc.a; s/ C
P
s02S Pa.s0js/V .s0/ç : (6.9)
Discounted Reward-based MDPs are like Discounted Cost-based MDPs but with costs c.a; s/ replaced by rewards r.a; s/, and minimization of expected costs replaced by maximization of expected rewards. An example of a reward-based MDP is one where an agent gets a positive reward of R every time it reaches a piece of food, that once consumed appears randomly at a different location. A discount factor of < 1 ensures that the maximum discounted reward accumulated never exceeds R=.1 /. e results and algorithms for Goal and Stochastic-Shortest Paths apply with small modification to Discounted MDPs. Moreover, Discounted MDPs can be easily compiled into equivalent Goal MDPs through a simple and efficient transformation [Bertsekas, 1995]. us, while certain problems, like the one above, can be more naturally expressed as Discounted MDPs, Discounted MDPs are not more expressive than Goal MDPs, and actually the opposite seems to be true as there is no known method for transforming general Goal MDPs into Discounted MDPs.1
1Some Goal MDPs can be transformed into equivalent Discounted MDPs, but the transformation is not general. For example, a Goal MDP with action costs c.a; s/ that are all uniform and equal to 1, can be transformed into an equivalent Discounted


6.1. GOAL, SHORTEST-PATH, AND DISCOUNTED MODELS 83
In order to make precise the notion of equivalence among different types of MDPs [Bonet and Geffner, 2009], let us say that two MDPs M and M 0, possibly of different types, are equivalent iff they have the same set of non-goal states and actions (and hence the same space of policies), and for any policy , the value functions VM and VM0 over M and M 0 are related by two constants  ̨ and ˇ through the linear equation
VM 0 .s/ D  ̨VM .s/ C ˇ (6.10)
over all non-goal states s. e equation ensures that policies have the same relative ranking in M and M 0; i.e., VM .s/ < V 0
M .s/ iff VM 0 .s/ < V 0
M0.s/. e constant  ̨ can’t be zero, and is negative only when M and M 0 have different signs: one being cost-based and the other reward-based. For showing that a Discounted Reward-based MDP M can be transformed into an equivalent Goal MDP M 0, one can show 1) that M is equivalent to a Discounted Reward-base MDP M1 that is like M but with a negative constant R added to all rewards to make them all negative, 2) that M1 is equivalent to a Discounted Cost-based MDP M2 where these negative rewards are transformed into positive costs, and 3) that M2 is equivalent to a Goal MDP M 0 that is like M2 but with a new (absorbing, cost-free) goal state t added, such that the transition probabilities P 0 in M 0 are expressed in terms of the transition probabilities P of M , M1, and M2 as:
P0
a.s0js/ D
Pa.s0js/ if s0 ¤ t
1 if s0 D t . (6.11)
In this expression s ranges over the states in the original discounted model M , and a is an action applicable in s. Notice that in the resulting Goal MDP, every policy is proper, as every applicable action a in each non-goal state s, maps s into the goal state t with a non-zero probability 1 . e equivalence between the Discounted Reward MDP M and the Goal MDP M 0 then follows from the relation between the value functions VM and VM0 that satisfies (6.10) for  ̨ D 1 and ˇ D R=.1 /.
FINITE-HORIZON MDPS
e MDPs above are said to be infinite horizon, as the costs and rewards accumulate over a horizon that is not bounded a priori. Finite-horizon MDPs, on the other hand, are concerned with the accumulation of costs or rewards over a fixed number H of stages, called the problem horizon. Finite-horizon MDPs can be converted into infinite-horizon MDPs by simply augmenting the problem states s with the horizon left d . us, if s0 is the initial state of the finite-horizon MDP M with horizon H , then the equivalent infinite-horizon MDP M 0 will have the pair hs0; H i as the initial state, the pairs hs; 0i as the goal states, and transition probabilities Pa0 .hs0; d 1ijhs; d i/ equal to the transition probabilities Pa.s0js/ in M . e same transformation is used for costs. e resulting Goal MDP M 0 has an important characteristic; namely, it is acyclic, meaning that the probability of any state trajectory starting and ending in the same state is zero. is is because time moves forward, and states hs; d i can only transition to states hs0; d 1i, and this only when d ¤ 0. Dynamic programming procedures like (Asynchronous) Value Iteration, to be considered next, can be used to solve finite-horizon MDPs,
Reward-based MDP with discount factor , 0 < < 1, and rewards r.a; s/ D 0 over non-goal states, and r.a; s/ D 1 over goal states. e goal states remain absorbing but not cost-free in this Discounted MDP. is transformation, however, does not ensure equivalence when the action costs c.a; s/ are not uniform.


84 6. MDP PLANNING: STOCHASTIC ACTIONS AND FULL FEEDBACK
and more generally acyclic MDPs, very efficiently, in a single pass over all the states, by considering the states in order: first the states hs; d i with d D 1, then the states with d D 2, and so on, until reaching the states with d D H . Still, even a single-pass over all the states may not be computationally feasible. We will thus also consider incremental, heuristic search algorithms for solving finite-horizon MDPs and their use in online planning over general infinite-horizon MDPs (Section 6.4).
6.2 DYNAMIC PROGRAMMING ALGORITHMS
We focus next on the two standard dynamic programming algorithms for solving MDPs. While we focus on Goal MDPs, the methods apply to Stochastic Shortest-Path MDPs, and with minor modification, to Discounted MDPs. We assume that there are no dead-ends and hence that the goal is reachable from all states. We will relax this assumption in the next section.
VALUE ITERATION
Value Iteration (VI) is a method for computing the optimal cost function V , which once plugged into the greedy policy V in place of V , yields the optimal policy . e optimal cost function V is the unique solution to the optimality equation
V .s/ D
0 if s is a goal state mina2A.s/ Œc.a; s/ C
P
s02S Pa.s0js/V .s0/ç otherwise. (6.12)
Value iteration solves this equation by setting V .s/ D 0 for goal states s and initializing the value of non-goal states arbitrarily, and then using Eq. 6.12 as an update
V .s/ WD mina2A.s/ Œc.a; s/ C
P
s02S Pa.s0js/V .s0/ç (6.13)
which is performed in parallel over all non-goal states. is operation, which is implemented by means of two value vectors V and V 0, is called a full or parallel DP update. Value iteration performs these parallel updates repeatedly. In the limit, the value vector V converges to the solution of Bellman’s optimality equation (6.12), and hence to the optimal cost function V . Since the convergence is asymptotic, Value Iteration is stopped when the value vector V is such that the maximum difference between the expressions in the left and right-hand sides of (6.12) is small enough. If this difference, called the residual and defined as
ResV
def
D mins2S
ˇˇV .s/ Œc.a; s/ C
P
s02S Pa.s0js/V .s0/çˇˇ ; (6.14)
is sufficiently small, the policy V greedy with respect to V is optimal. More generally, the value of the residual can be used to bound the loss V V .s0/ V .s0/ that results from following the greedy policy V from the initial state s0 rather than an optimal policy. For Discounted MDPs, this loss is no greater than 2 =.1 / if ResV < . While the loss can also be bounded in SSPs and Goal MDPs, the expression for the bound cannot be expressed in such a closed form [Bertsekas, 1995]. In a variation of VI, known as Asynchronous Value Iteration, the update in Eq. 6.13 is not performed over all states simultaneously but over some selected states. Provided that every state is updated infinitely often, Asynchronous VI also converges asymptotically to the optimal cost function


6.2. DYNAMIC PROGRAMMING ALGORITHMS 85
V I Starts with value function stored in vector V with V .s/ D 0 for goal states s repeat
flag := true for each non-goal state s do new-value WD mina2A.s/ Œc.a; s/ C
P
s02S Pa.s0js/V .s0/ç If jV .s/ new-valuej then V .s/ WD new-value flag := false end if end for
until flag = true
Figure 6.2: Simple Version of Asynchronous Value Iteration implemented using single vector that outputs a value function V with residual ResV smaller than the parameter , > 0.
V [Bertsekas and Tsitsiklis, 1989]. is implies, among other things, that the simple variant of Value Iteration, implemented using a single value vector that is updated one state at a time, is a form of Asynchronous VI that also converges to V . is version of VI is known as Gauss-Seidel VI. Code for a version of VI that delivers a value function with residual less than a given is shown in Figure 6.2. A suitable version of Asynchronous Value Iteration can be used to solve acyclic MDPs, including finite-horizon MDPs, in one pass. Recall that an MDP is acyclic when all state trajectories that start and end in the same state have zero probability, and that finite-horizon MDPs can be cast as infinitehorizon and acyclic MDPs where the state is extended with the information of the horizon-to-go. All that is needed for solving acyclic MDPs in a single pass is to order the updates so that a state s is updated before a state s0 when there is a state trajectory from s0 to s in the MDP with positive probability. In the case of finite-horizon MDPs, it suffices to update states hs; d 1i before states hs0; d i. It is simple to prove by induction that the values computed in one such pass are optimal. e procedure is also known as backward induction [Bertsekas, 1995].
POLICY ITERATION
e other standard dynamic programming algorithm for solving MDPs is Policy Iteration [Bertsekas, 1995, Howard, 1971, Puterman, 1994]. Whereas VI iterates over value functions in order to compute the optimal value function V , Policy Iteration (PI) iterates over policies, each one strictly better than the one before. Since the total number of (deterministic) policies is finite, PI converges to the optimal policy in a number of iterations that is bounded. Policy iteration applies two operations in sequence, starting with a proper policy D 0. First, it computes the value of the policy V .s/ over all states. It then finds a new policy 0 that is proper and improves if is not optimal. e first step, called Policy Evaluation, is done by solving the set of jSj linear equations given by (6.4) that characterize the value function V . e second step, called Policy Improvement, uses the value function V computed in the Policy Evaluation step, to see if there


86 6. MDP PLANNING: STOCHASTIC ACTIONS AND FULL FEEDBACK
are states s where the actions dictated by the policy are not best, under the assumption that after this first step, the policy will be followed. For this, Q-factors of the form
Q .a; s/ D c.a; s/ C
P
s02S Pa.s0js/V .s0/ (6.15)
are computed for each non-goal state s and action a 2 A.s/. e policy 0 that is like except in states s for which the following strict inequality holds
min
a2A.s/ Q .a; s/ < Q . .a/; s/ D V .s/ (6.16)
where 0.s/ D argmina2A.s/ Q .a; s/ is a policy that strictly improves , i.e., V 0 .s/ V .s/ with the inequality being strict over some states. If there are no such states, must be optimal, and Policy Iteration terminates. Policy iteration produces a sequence of policies 0; : : : ; n, starting with an initial proper policy 0 such that each policy iC1 is proper and strictly improves the previous one. e last policy n is a policy that cannot be improved further and is optimal. e length of the sequence is bounded by the total number of deterministic and stationary policies. A proper policy is needed initially in Policy Iteration, as the Policy Improvement step may not work when the expected costs are not bounded. For example, consider a Goal MDP with a single non-goal state s and two actions a and b such that a maps s into itself with probability 1, and b maps s into itself with probability 1=2, and into the goal with probability 1=2. If the initial policy is such that .s/ D a, then V .s/ is infinite, and therefore both Q .a; s/ and Q .b; s/ are infinite as well, so that the policy 0.s/ D b does not appear to improve even if 0 is optimal and is not. is can’t happen, however, when is a proper policy. Methods for computing proper or strong cyclic policies are discussed in Section 5.6. Yet, the stochastic policy that assigns to each state s an action in A.s/ with probability 1=jA.s/j is always proper in problems with no dead-ends. Code for a simple implementation of Policy Iteration is shown in Figure 6.3.
6.3 HEURISTIC SEARCH ALGORITHMS
Dynamic programming methods like VI and PI are exhaustive. ey consider all the states in the problem from the very beginning, and thus cannot be used to solve problems with a large number of states; e.g., jSj > 1010. Heuristic search methods, on the other hand, are incremental, and while they may end up considering many, if not all of the states in a problem, they use information about the initial state and lower bounds or admissible heuristic functions, to focus the search for solutions. In Sections 2.3 and 2.4, we reviewed heuristic search methods for solving directed (OR) graphs like A* and LRTA*, while in Section 5.3, we reviewed heuristic search methods for solving acyclic AND/OR graphs like AO*. In this section, we look at heuristic search algorithms for solving Goal MDPs, which can also be applied to Discounted MDPs after transforming them into Goal MDPs. One of the algorithms, RTDP, is a generalization of LRTA* to MDPs [Barto et al., 1995], another one, LAO*, is a generalization of AO* to cyclic AND/OR graphs. A crucial idea underlying these methods is that there is no need for computing complete policies that prescribe an action in every possible state when the initial state is given. A partial policy is a function that assigns an action .s/ to some states but not necessarily to all of them. For a partial policy to represent a solution to a Goal MDP, there is no need for to be