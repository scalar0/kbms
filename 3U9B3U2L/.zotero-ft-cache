Semantic AI in Knowledge
Graphs
Recent combinations of semantic technology and artificial intelligence (AI) present new techniques to build intelligent systems that identify more precise results. Semantic AI in Knowledge Graphs locates itself at the forefront of this novel development, uncovering the role of machine learning to extend the knowledge graphs by graph mapping or corpus-based ontology learning.
Securing efficient results via the combination of symbolic AI and statistical AI such as entity extraction based on machine learning, text mining methods, semantic knowledge graphs, and related reasoning power, this book is the first of its kind to explore semantic AI and knowledge graphs. A range of topics are covered, from neuro-symbolic AI, explainable AI and deep learning to knowledge discovery and mining, and knowledge representation and reasoning.
A trailblazing exploration of semantic AI in knowledge graphs, this book is a significant contribution to both researchers in the field of AI and data mining as well as beginner academicians.




Semantic AI in
Knowledge Graphs
Edited by Sanju Tiwari, Fernando Ortíz-Rodriguez, Sarra Ben Abbés, Patience Usoro Usip, and Rim Hantach


Designed cover image: © Shutterstock
First edition published 2024 by CRC Press 6000 Broken Sound Parkway NW, Suite 300, Boca Raton, FL 33487-2742
and by CRC Press 4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN
CRC Press is an imprint of Taylor & Francis Group, LLC
© 2024 selection and editorial matter, Sanju Tiwari, Fernando Ortíz-Rodriguez, Sarra Ben Abbés, Patience Usoro Usip and Rim Hantach; individual chapters, the contributors
Reasonable efforts have been made to publish reliable data and information, but the author and publisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmitted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information storage or retrieval system, without written permission from the publishers.
For permission to photocopy or use material electronically from this work, access www.copy right.com or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 978-750-8400. For works that are not available on CCC please contact mpkbooks permissions@tandf.co.uk
Trademark notice: Product or corporate names may be trademarks or registered trademarks and are used only for identification and explanation without intent to infringe.
Library of Congress Cataloging‐in‐Publication Data
Names: Tiwari, Sanju, 1979- editor. | Ortíz-Rodriguez, Fernando, 1974- editor. | Ben Abbès, Sarra, 1985- editor. | Usip, Patience Usoro, editor. | Hantach, Rim, editor. | KGSWC (Conference) (2021 : Online) Title: Semantic AI in knowledge graphs / edited by Sanju Tiwari, Fernando Ortíz-Rodriguez, Sarra Ben Abbés, Patience Usoro Usip and Rim Hantach. Description: Boca Raton : CRC Press, 2023. | “The book...comprises of extended papers from workshops collocated during the KGSWC 2021” --Preface. | Includes bibliographical references and index. | Identifiers: LCCN 2023002144 (print) | LCCN 2023002145 (ebook) | ISBN 9781032321851 (hardback) | ISBN 9781032321868 (paperback) | ISBN 9781003313267 (ebook) Subjects: LCSH: Information visualization--Congresses. | Semantic computing--Congresses. | Artificial intelligence--Congresses. Classification: LCC QA76.9.I52 S46 2023 (print) | LCC QA76.9.I52 (ebook) | DDC 001.4/226--dc23/eng/20230607 LC record available at https://lccn.loc.gov/2023002144 LC ebook record available at https://lccn.loc.gov/2023002145
ISBN: 978-1-032-32185-1 (hbk) ISBN: 978-1-032-32186-8 (pbk) ISBN: 978-1-003-31326-7 (ebk)
DOI: 10.1201/9781003313267
Typeset in Palatino by KnowledgeWorks Global Ltd.


v
Contents
List of Figures ....................................................................................................... vii List of Tables ...........................................................................................................xi Preface................................................................................................................... xiii Editors.....................................................................................................................xv Contributors........................................................................................................ xvii
1 Leveraging Semantic Knowledge Graphs in Educational Recommenders to Address the Cold-Start Problem .................................. 1 Sahan Bulathwela, María Pérez‐Ortiz, Emine Yilmaz, and John Shawe‐Taylor
2 Modeling Event-Centric Knowledge Graph for Crime Analysis
on Online News............................................................................................... 21 Federica Rollo and Laura Po
3 Semantic Natural Language Processing for Knowledge
Graphs Creation .............................................................................................. 45 Cameron De Sa, Edlira Vakaj, Hossein Ghomeshi, and Ryan McGranaghan
4 MSE**: Multi-Modal Semantic Embeddings for Datasets
with Several Positive Matchings ................................................................. 91 Jérémie Huteau, Adrian Basarab, and Florence Dupin de Saint‐Cyr
5 Text-Based Emergency Alert Framework for Under-Resourced
Languages in Southern Nigeria ................................................................. 111 Patience U. Usip, Funebi F. Ijebu, Ifiok J. Udo, and Ikechukwu K. Ollawa
6 Knowledge Graphs in Healthcare ............................................................. 127 Sanna Aizad and Dr. Bilal Arshad
7 Explainable Machine Learning-Based Knowledge Graph for Modeling Location-Based Recreational Services from Users Profile.......141 Daniel Ekpenyong Asuquo, Patience Usoro Usip, and Kingsley Friday Attai
8 Building Knowledge Graph from Relational Database........................ 163 Bilal Ben Mahria, Ilham Chaker, and Azeddine Zahi
Index ..................................................................................................................... 191




vii
List of Figures
Figure 1.1 Inferring the knowledge for the unseen topic (white circle) based on semantically related and seen ones (grey circles) by transferring knowledge (dotted lines). Topics are ML (machine learning), RL (reinforcement learning), Prob (probability), CV (computer vision), NLP (natural language processing), and W2V (Word2Vec). ...................................................................................... 7
Figure 1.2 (Left) Relationship between different behavioral characteristics of user-profiles and model recall performance presented using SROCC. The numbers and the intensity of each cell correspond to the Spearman r coefficient where a significant correlation is present (p < 0.01). Empty cells represent the lack of significant correlation. (Right) The average recall performance of the two models for the learner population at a different number of events. ............................. 13
Figure 2.1 Crime event representation in the Event-Centric Knowledge Graph. ........................................................................ 25
Figure 2.2 Interconnection between two crime events. ............................. 26
Figure 2.3 Representation of two events and their relationships (a) and generation of directed connections between event nodes in the Event-Centric Knowledge Graph (b)......... 27
Figure 2.4 Exemplar news article related to a theft with the corresponding Event-Centric Knowledge Graph..................... 32
Figure 2.5 Crime news nodes and their relationships. .............................. 33
Figure 2.6 Louvain Identified communities on the final directed graph with nodes scored by four different centrality algorithms: degree centrality (a), eigenvector centrality (b), page rank (c), and article rank (d). ....................................... 35
Figure 2.7 Normalized scores of CrimeNews nodes according to four different centrality algorithms. .......................................... 36
Figure 2.8 Communities detected by Louvain in the What graph weighted by the article rank. ...................................................... 37
Figure 2.9 Crimes reported to the authorities in the province of Modena from 2016 to 2020. .......................................................... 38


viii List of Figures
Figure 3.1 Overview of the IEP......................................................................54
Figure 3.2 Overview of ontology learning model methodology. ............. 59
Figure 3.3 Architecture of ontology learning model. ................................ 61
Figure 3.4 Dendrogram displaying number of clusters by Euclidean distance........................................................................ 78
Figure 4.1 (a) Toy dataset containing three tuples and two modalities. Each tuple contains one image and two legends. For a given element, index i defines the tuple, k its modality, and j its rank in the modality set. (b) Positive (green) and negative (red) legends for each image of this toy example............................................................ 95
Figure 4.2 Two images and their five captions from MS-COCO dataset........................................................................................... 106
Figure 5.1 Description of word concordance determination. ................. 118
Figure 5.2 Flow of the proposed hybridization......................................... 118
Figure 5.3 An SOA-based emergency alert system framework.............. 121
Figure 5.4 Ibibio–English Translator interface. ......................................... 123
Figure 5.5 Bidirectional translation sample............................................... 123
Figure 6.1 Example of FASTA file................................................................ 131
Figure 6.2 Graph schema of the reference genome. ................................. 132
Figure 6.3 Structure of the VCF file. (Reproduced from Aizad, S., & Anjum, A. (2019, August). Graph Data Modelling for Genomic Variants. In 2019 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ ATC/CBDCom/IOP/SCI) (pp. 1577–1584). IEEE. [12].) ........... 132
Figure 6.4 VCF records from VCF file......................................................... 133
Figure 6.5 Substitution mutations mapped to the reference genome. ........................................................................................ 134
Figure 6.6 Insertion mutations mapped to the reference genome. ........ 134
Figure 6.7 Deletion mutations mapped to the reference genome. ......... 135
Figure 6.8 Gene ontology on the knowledge graph. ................................ 136
Figure 6.9 Disease mapped to the knowledge graph............................... 136


List of Figures ix
Figure 6.10 Data model to integrate heterogenous sources as knowledge graphs....................................................................... 138
Figure 7.1 Toward eXplainable AI. ............................................................. 143
Figure 7.2 Examples of RDF statement....................................................... 147
Figure 7.3 Proposed ML-based KG framework for constructing context-aware applications. ....................................................... 152
Figure 7.4 Recreational facility KG. ............................................................ 155
Figure 7.5 KG filter showing suitable facilities for Tourist A with details. ................................................................................. 155
Figure 7.6 KG filter showing suitable facilities for Tourist B with details. ................................................................................. 156
Figure 8.1 The classification of ontology learning source of information. ................................................................................. 167
Figure 8.2 Methods to build ontology from RDBs. .................................. 170
Figure 8.3 Reverse engineering steps. ........................................................ 175




xi
List of Tables
Table 1.1 Predictive Performance of Adding SR to TrueLearn Novel Algorithm. The Different Configurations (SR Metric) of the Semantic TrueLearn Novel Algorithm (Our Proposal) Are Evaluated Using Precision (Prec.), Recall (Rec.), and F1 Score (F1) .................................................... 11
Table 1.2 The Performance of Semantic TrueLearn Model with W2V SR Metric Is Reported in Terms of Precision (Prec.), Recall (Rec.), and F1 Score (F1) ....................................... 12
Table 1.3 Predictive Performance of Semantic TrueLearn Model (Our Proposal) Using Precision (Prec.), Recall (Rec.), and F1 Score (F1) ........................................................................... 12
Table 1.4 Predictive Performance of Semantic Models (Our Proposals) Using Precision (Prec.), Recall (Rec.), and F1 Score (F1) ................................................................................... 13
Table 2.1 Previous Works on the Use of Knowledge Graph in the Context of Criminal Data ............................................................ 24
Table 2.2 Number of Nodes and Relationships of the Modena Crime Knowledge Graph............................................................. 32
Table 2.3 The Five Nodes with the Highest Centrality Scores................34
Table 2.4 Results of the Louvain Community Detection Based on the Relationship and Node Type (W) Included and the Centrality Algorithm Used...................................................34
Table 3.1 Performance Measure Metrics for NER Models ...................... 62
Table 3.2 Truth Table for NER Metrics .......................................................63
Table 3.3 Classes and Number of Instances ..............................................65
Table 3.4 Performance Metrics for Trained en_core_web_sm Model.............................................................................................. 76
Table 3.5 Performance Metrics for Untrained en_core_web_sm Model.............................................................................................. 76
Table 3.6 Top 12 Words by TFIDF Score.....................................................77
Table 3.7 Top Ten Words Most Similar to “Solar” ....................................77
Table 3.8 Top Ten Terms by Cluster ............................................................ 78


xii List of Tables
Table 3.9 Ontology Assessment Metrics .................................................... 79
Table 4.1 Validation Scores of VSE++ and Our Method (MSE**) on MS-COCO............................................................................... 104
Table 4.2 Validation Scores of VSE++ and Our Method (MSE**) on MS-COCO............................................................................... 105
Table 4.3 Similarities between Images a and b with Their Legends ........................................................................................ 105
Table 5.1 Tabulated Review Process ......................................................... 116
Table 6.1 Summary of Use of Knowledge Graphs in Healthcare......... 129
Table 7.1 The Size of Some Public Cross-Domain KGs.......................... 149
Table 7.2 Brief Detail of Some Recreational Facilities in Uyo Metropolis, AKS, Nigeria........................................................... 154
Table 8.1 Comparison of Conceptual Model Approach Methods........ 172
Table 8.2 Comparison of Logical and Physical Approach Methods........................................................................................ 174
Table 8.3 Mapping Languages Comparison............................................ 183


xiii
Preface
This book, Semantic Artificial Intelligence in Knowledge Graphs, comprises extended papers from workshops collocated during the Knowledge Graph and Semantic Web Conference (KGSWC) 2021. The workshops included the Third International Workshop on Semantic Web (IWSW 2021), First International Workshop on Multilingual Semantic Web (IWMSW 2021), and First International Workshop on Deep Learning for Question Answering (IWDLQ 2021). The papers explored major roles artificial intelligence (AI) with semantic technologies to present enhanced semantic AI architecture with knowledge graphs. In this book, the role of machine learning toward extending knowledge graphs by graph mapping or corpus-based ontology learning was discovered. Efficient results were obtained via the combination of symbolic AI and statistical AI such as entity extraction based on machine learning, text mining methods, semantic knowledge graphs, and related reasoning power. Topics covered in this book include Deep Semantics in Knowledge Graphs, Neuro-Symbolic AI and eXplainable AI, Deep Learning, Knowledge Discovery and Mining, Information Retrieval and Question Answering, Knowledge Representation and Reasoning, Natural Language Processing, Entity Linking, etc. this book is made of eight chapters: Leveraging Semantic Knowledge Graphs in Educational Recommenders to Address the Cold‐Start Problem; Modeling Event‐Centric Knowledge Graph for Crime Analysis on Online News; Semantic Natural Language Processing for Knowledge Graphs Creation; MSE**: Multi‐modal semantic embeddings for datasets with several positive matchings; Text‐Based Emergency Alert Framework for Under‐Resourced Languages in Southern Nigeria; Knowledge Graphs in Healthcare; Explainable Machine LearningBased Knowledge Graph for Modeling Location‐Based Recreational Services from Users Profile; and Building Knowledge Graph from Relational Database.
Firstly, our thanks go to all the organizers of the main conference and program committee members for ensuring a rigorous review process that led to the successful events. The efforts of the workshop chairs and co-chairs toward the success of the workshops were highly appreciated. We are also very thankful to the authors for their painstaking efforts to write up for the extension. Finally, we are thankful to the editorial board of Taylor & Francis for providing this book opportunity to publish all extended chapters.
Patience Usoro Usip, and Rim Hantach
Sanju Tiwari, Fernando Ortíz-Rodriguez, Sarra Ben Abbés,




xv
Editors
Dr. Sanju Tiwari (CEO and Founder of ShodhGuru Research Labs, India) is a Senior Researcher at Universidad Autonoma de Tamaulipas. She was DAAD Post-Doc-Net AI Fellow for 2021 and visited different German Universities under the DAAD fellowship. She is a Mentor of Google Summer of Code (GSoC 2022-23) at DBpedia and a member of InfAI, Leipzig University, Germany. She is also working as a curator of ORKG Grant Program, at TIB Hannover, Germany. Her current research interests include Semantic Web, Knowledge Graphs, Linked Data, Artificial Intelligence. She has to-date published >50 research papers and 3 Scopus indexed Books. She is working as a General Chair (KGSWC 2020-23, EGETC2022-23, AMLDA 2022-23, AI4S-2023), and Program Chair, Workshop Chair, Publicity Chair, Steering Committee and PC Member in different renowned International Conferences (The Web Conference 2023, SEMANTiCS 2019-23, ESWC2021-23, CIKM202022, AICCSA-2021, JOWO-2021, BiDEDE2022-23@ACM SIGMOD, VLIoT@ VLDB2022, SIMBIG2022, ICSC2023). She is working as a Guest Editor for SCI/ Scopus journals (SWJ IoS Press, TEL Emerald, IJWIS Emerald). She is the speaker of IEEE/IETE N2Women and Women’s Empowerment and NiWIIT (Nigerian Women In Information Technology).
Fernando Ortíz-Rodriguez is Full Professor, member of the National Research Council, Level C, and Director of the Research Institute UAT at Tamaulipas Autonomous University, Reynosa, Tamaulipas, Mexico. He is a member of the Information Technology research group and part of the knowledge graph and Semantic Web Community. His research interests include Semantic Web, Information Systems, e-Government and Artificial Intelligence. He has been the main Chair and Organizer of the KGSWC multiseries conference. He is a member of National Systems Researchers (SNI) of the National Council of Science and Technology (CONACYT), Mexico’s entity promoting scientific and technological activities and high-quality scientific research. He is also a member of the Association for Computing Machinery (ACM). He holds a PhD degree on computer science and Artificial Intelligence and Information Systems from the Technical University of Madrid, Spain.
Dr. Sarra Ben Abbès, PhD, is an R&D Expert on Artificial Intelligence. She has been working for more than 13 years in different domains of Artificial intelligence. She became an expert in these domains following her thesis project focused on the crossroads of knowledge engineering and the semantic web. She worked as a senior and lead scientist at Storyzy (2013), where she led several innovative projects combining Semantic Web technologies, NLP techniques, and Machine Learning algorithms for real-time news articles.


xvi Editors
Since 2018, she has joined Engie as R&D expert on Artificial Intelligence and she mainly worked to improve the use of semantics by bridging NLP, Deep Learning, and Machine Learning techniques. She is working as lead of work-packages and coordinator of several projects (internal and external) related to the energy sector such as the European H2020 project Platoon and Enershare. Her interests are in project management, artificial intelligence, explainable artificial intelligence, semantic interoperability, knowledge engineering, reasoning and inference, semantic information retrieval, decision-making support (machine learning & deep learning) and probabilistic semantic graphs and multi-Agent systems. She supervised the work of many Master trainees, Ph.D. students and consultants. She is an author of different scientific publications related to the artificial intelligence domain. She is also a workshop organizer in national and international conferences like ESWC, FOIS, KG, EGC, ICMLA, etc.
Patience Usoro Usip is a Senior Lecturer of Computer Science, University of Uyo, Uyo, Nigeria. She holds a PhD and MSc in Computer Science from University of Ibadan, Ibadan, Nigeria and BSc in Computer Science from University of Calabar, Calabar, Nigeria. She is a Post-doc fellow, Massachusetts Institute of Technology (MIT), USA and also an All Africa House Fellow, University of Cape Town, South Africa. Her research interests include knowledge representation and reasoning a sub-sub-field of Artificial Intelligence, Formal Representations, Computer Logic, Ontology Development, Knowledge Graphs, Multilingual Sematic Web, Intelligent Systems in several domains including health, etc. She has published locally and internationally in books, book chapters, journals, and conference proceedings. She has served as reviewer to several journals locally and internationally to include MTAP, ASTEJ, etc. She has served as External Examiner for MSc Dissertation, University of Cape Town, South Africa and for PhD Thesis, India, etc. She has served as Speaker, program committee member, and General Chair in winter schools, workshops, and conferences. She has several awards to her credit and is a member of many professional bodies.
Rim Hantach is an artificial intelligence expert and project manager at ENGIE France. She is working on deep learning, computer vision, natural language processing (NLP), knowledge graph, and machine learning techniques for text and image analysis.


xvii
List of Contributors
Sanna Aizad
University of Leicester Leicester, UK
Bilal Arshad
University of Derby Derby, UK
Daniel Ekpenyong Asuquo University of Uyo Uyo, Nigeria
Kingsley Friday Attai Ritman University Nigeria
Adrian Basarab
IRIT, Toulouse University France
Sahan Bulathwela
University College London UK
Ilham Chaker
Faculty of Science and Technology Fez, Morocco
Cameron De Sa
Birmingham City University UK
Hossein Ghomeshi
Birmingham City University UK
Jeremie Huteau
IRIT, Toulouse University France
Funebi F. Ijebu University of Uyo Uyo, Nigeria
Bilal Ben Mahria
Faculty of Science and Technology Fez, Morocco
Ryan McGranaghan
NASA Goddard Space Fight Center Greenbelt, MD, USA
Ikechukwu K. Ollawa University of Uyo Uyo, Nigeria
María Pérez-Ortiz
University College London UK
Laura Po
University of Modena and Reggio Emilia Italy
Federica Rollo
University of Modena and Reggio Emilia Italy
Florence Dupin de Saint-Cyr IRIT, Toulouse University France
John Shawe-Taylor
University College London UK


xviii List of Contributors
Ifiok J. Udo
University of Uyo Uyo, Nigeria
Patience Usoro Usip University of Uyo Uyo, Nigeria
Edlira Vakaj
Birmingham City University UK
Emine Yilmaz
University College London UK
Azeddine Zahi
Faculty of Science and Technology Fez, Morocco


DOI: 10.1201/9781003313267-1 1
1
Leveraging Semantic Knowledge Graphs in Educational Recommenders to Address the Cold-Start Problem
Sahan Bulathwela, María Pérez-Ortiz, Emine Yilmaz, and John Shawe-Taylor
Centre for Artificial Intelligence, University College London, UK
CONTENTS
1.1 Introduction.................................................................................................... 2 1.2 Related Work ..................................................................................................3 1.2.1 Wikipedia Concept-Based User Modeling..................................... 4 1.2.2 Representations from Graphs ..........................................................5 1.3 Methodology ..................................................................................................6 1.3.1 Problem Formulation ........................................................................6 1.3.2 Semantic TrueLearn...........................................................................7 1.3.2.1 Incorporating Semantic Relatedness between Concepts/KCs ...................................................................... 8 1.3.2.2 The Univariate Formulation .............................................. 8 1.3.2.3 The Multivariate Formulation...........................................8 1.4 Experiments and Results .............................................................................. 9 1.4.1 Semantic Relatedness Metric (SR Metric)....................................... 9 1.4.2 Models .................................................................................................9 1.4.3 Data .................................................................................................... 10 1.4.4 Experimental Design....................................................................... 10 1.4.4.1 Impact of Semantic Relatedness ..................................... 10 1.4.5 Evaluation ......................................................................................... 11 1.4.6 Results ............................................................................................... 12 1.4.7 Discussion......................................................................................... 13 1.4.8 Human-Intuitive Representations................................................. 15 1.4.9 Limitations........................................................................................ 16 1.5 Conclusions................................................................................................... 16 1.5.1 Future Work...................................................................................... 16 Acknowledgments ................................................................................................ 17 References............................................................................................................... 18


2 Semantic AI in Knowledge Graphs
1.1 Introduction
Developing artificial intelligence systems that, mildly at least, understand the structure of knowledge is foundational to building an effective recommendation system for education (Bauman & Tuzhilin, 2018; Jiang, Pardos, & Wei, 2019), as well as for many other applications (Lewis et al., 2020; Yano & Kang, 2016) related to knowledge management and tracing. Many intelligent educational recommenders at present use knowledge components (KCs)/topics to represent the skills that human learner masters over time. But many of these systems assume that these KCs are unrelated to each other when modeling learner knowledge (Yudelson, Koedinger, & Gordon, 2013). Otherwise, human experts are relied upon to annotate the concept relatedness. We identify knowledge bases as a rich source of information that can be utilized to automate harvesting semantic relatedness (SR) information needed for modeling. Our motivation in this work is to use Wikipedia, an open, multilingual, and dynamic encyclopedia to demonstrate that educational recommendation can be improved by leveraging automatically computed SR information, making the next generation of educational recommenders semantically aware. Through this work, we verify the utility of semantic knowledge graphs in improving educational recommender systems. Our proposal, Semantic TrueLearn, is a family of novel and transparent learner models that incorporate automatic entity linking and Wikipedia (a publicly available, humanly intuitive, domain-agnostic, and ever-evolving) knowledge graph, as a first step toward building an educational recommender that automatically labels materials and embeds the structure of universal knowledge using Wikipedia. Our proposal, Semantic TrueLearn, is a probabilistic graphical model (PGM) that maintains a symbolic representation of learners’ knowledge that allows explanations, rationalizations, and scrutiny. The proposed learner model is the perfect example of how probabilistic graphical modeling can harmonize with semantic knowledge graphs to build the accurate and subsymbolic learner models that are needed in many applications that require interaction and collaboration with the user. Toward verifying the utility of knowledge graphs to improve informational recommender systems, we:
i. Identify the ability to exploit the SR between entities in Wikipedia.
ii. Propose a novel sub-symbolic Bayesian learner model.
iii. Identify several research questions relating to validating the utility of the proposed learner model.
iv. Validate the research questions using a large dataset of learners engaging with educational resources.


Leveraging Semantic Knowledge Graphs 3
While our experiments focus on the educational domain, we hypothesize these findings may extend to any other informational recommenders. In this chapter, Section 1.1 introduces the context and outlines the motivation behind this chapter, while Section 1.2 describes the relevant literature on the topic and how they are related/different to our contribution. Section 1.3 formalizes the problem setting and proposes several approaches to model SR between concepts as a solution. Subsequently, Section 1.4 identifies the research questions relevant to the solutions proposed, while outlining the different experiments that are run in order to answer the defined research questions. The latter part of Section 1.4 presents the results observed from the experiments and goes further to discuss the results. Section 1.5 concludes this chapter.
1.2 Related Work
Knowledge tracing (KT) (Yudelson et al., 2013) is one of the most popular methods for user modeling in intelligent tutoring systems (ITS) and educational recommendation (EdRecSys) (Bulathwela, Pérez-Ortiz, Yilmaz, & Shawe-Taylor, 2020b) contexts. Incorporation of SR in KT systems is gaining more attention recently, where it is being utilized in prerequisite modeling (Carmona, Millán, Pérez-de-la Cruz, Trella, & Conejo, 2005; Chen, Lu, Zheng, & Pian, 2018), exercise similarity (Huang et al., 2019; Nakagawa, Iwasawa, & Matsuo, 2019; Pandey & Srivastava, 2020), and various other tasks (Bauman & Tuzhilin, 2018; Thaker, Zhang, He, & Brusilovsky, 2020). However, KT often relies on expert labeling of the KCs (Selent, Patikorn, & Heffernan, 2016) (sometimes also for knowledge hierarchies; Bauman & Tuzhilin, 2018), which is not scalable to large-scale lifelong learning applications in practice. In the majority of the approaches, the similarity between different items (exercises, educational materials) is modeled by using the overlapping KCs or the users’ co-consuming pairs of items. Both of these main approaches require either the experts or the learners to invest a substantial amount of human effort in the system before relatedness can be recovered. Another challenge in the above approaches is that the different proposals use different KC taxonomies making the different work hard to compare and inter-operate. The advancement of deep learning and graph neural networks has led to a new generation of neural models that are attempting to exploit the relatedness structures of educational materials using graph neural networks and attention mechanisms (Nakagawa et al., 2019; Song et al., 2021; Yang et al., 2020). However, these approaches require large quantities of data to train and lack interpretability, making them unsuitable for an educational recommendation system. Recent studies have also started questioning the superiority of these neural models over the classical approaches


4 Semantic AI in Knowledge Graphs
(Mandalapu, Gong, & Chen, 2021; Schmucker, Wang, Hu, & Mitchell, 2022). Furthermore, neural models do not focus on formulating humanly intuitive graphical models to model the data generation process positioning them beyond the scope of this work. Wikification, a form of entity linking (Brank, Leban, & Grobelnik, 2017), has shown substantial progress and great promise in automatically capturing the KCs covered in an educational resource addressing the scalable content annotation problem. Another advantage of using Wikification is that it grounds the KCs to a universal knowledge graph like Wikipedia, that is multi-lingual, cross-domain, and temporally dynamic (i.e., its knowledge evolves with time). Using Wikipedia as an ontology or knowledge graph to understand documents is not a new idea. While Wikipedia itself has been used as an ontology using page links and category links to describe “relates to” and “is a type of” relationships, respectively (Kawakami, Morita, & Yamaguchi, 2017; Syed, Finin, & Joshi, 2008), other works have pushed further and used the wealth of information in Wikipedia to build downstream knowledge bases and ontologies (Auer et al., 2007), as well as ontology-driven information retrieval systems (Grefenstette & Rafes, 2015). From the early days of Wikipedia, exploiting different aspects (such as text, link structure, etc.) to model SR that represents “relates to” links have been attempted. These SR metrics have evolved over time into recent proposals that are diverse and sophisticated metrics highly predictive of concept relatedness (Ponza, Ferragina, & Chakrabarti, 2020). However, the utility of these proposals with graphical models is underexplored and is investigated in this chapter.
1.2.1 Wikipedia Concept-Based User Modeling
State-based user modeling is a mature topic in personalization (Bulathwela et al., 2021a). As a reliable content-based feature, keywords/concepts/ entities/topics are widely used in user state modeling. These techniques in unison are identified as concept-based approaches (Zarrinkalam, Faralli, Piao, & Bagheri, 2020) where Wikipedia-based concept features are shown to be effective. Many concept-based approaches use the frequency of user interactions with the items related to a concept to build a concept profile for the user. Once the user profile is available, the similarity between the profile and the items can be used to rank them (Bulathwela, Pérez-Ortiz, Novak, Yilmaz, & Shawe-Taylor, 2021b; Piao, 2021). Recent works in educational recommendation have also used conceptbased user modeling to recommend Massively Online Open Courses (MOOCs) to learners (Piao & Breslin, 2016). In their approach, they consider the user session to be a document where the topics they visit over time are terms (words) in this document. They compute the Term Frequency (TF) for each user over time to build a user profile. The engagement is predicted by measuring the similarity between the user profile and the content using the


Leveraging Semantic Knowledge Graphs 5
cosine similarity. In recent work relating to EdRecSys, TrueLearn (Bulathwela, Pérez-Ortiz, Yilmaz, & Shawe-Taylor, 2020c), using Wikification, has demonstrated state-of-the-art performance in building PGMs on top of automatically extracting topics from a semantic knowledge graph, Wikipedia. In this work, Bulathwela, Pérez-Ortiz, et al. also introduce an online multi-skill KT model inspired by Bishop, Winn, and Diethe (2015),which is another PGM that relies on Wikipedia concepts. We identify these two models to be the most relevant prior work to the proposed Semantic TrueLearn model. TrueLearn Novel, the best performing model in Bulathwela et al. (2020c), builds a learner profile where the skill of each KC (Wikipedia topic) is updated based on the learner engagement with a fragment of an educational video. The model is a Bayesian factor graph that uses message passing to learn the KC parameters. This score can be used to rank the recommendations. While these models utilize a rich knowledge graph like Wikipedia, the exploitation of a rich source of semantic information in these cases can be considered a bare minimum as these methods merely use Wikipedia to automatically annotate and represent the KCs/concepts. All these models consider that Wikipedia concepts are independent and thus unrelated, introducing an obvious weakness to the model assumptions. This work breaks from these incorrect model assumptions to exploit the SR between the topics in Wikipedia, making the utility of Wikipedia in the EdRecSys domain rather as an ontology (that also models relates to relationships) than a simple taxonomy. More specifically, the contribution of this work is to improve the performance of the TrueLearn Novel model by incorporating the missing assumption of KC relatedness to address the cold-start problem. Our final experiments also demonstrate and validate if the incorporation of relatedness assumptions can apply to other Wikipedia concept-based models (e.g., Piao & Breslin, 2016).
1.2.2 Representations from Graphs
The core technical contribution of this work is proposing a method to infer the latent value of an unobserved skill parameter using observed ones via information sharing based on an SR graph. This requires developing a mechanism to exploit the connectedness structure of Wikipedia topics. Several works have proposed novel ways to use a relationship graph to recover a latent representation for an unknown node using a set of known nodes. Recently proposed Graph Convolutional Neural Networks (Kipf & Welling, 2017) infer hidden node embeddings ( +1
H ) by taking the weighted average of the embeddings of its neighbors. This is done using the adjacency matrix A and diagonal D of the relatedness graph as per + = − −
 1 21 21  
H D AD H W . Another popularizing idea in the representation learning research community is the attention mechanism (Bahdanau, Cho, & Bengio, 2015) that uses the concept of alignment. This mechanism learns to quantify the relatedness of the neighboring embeddings to compute the context embedding at


6 Semantic AI in Knowledge Graphs
a point in order to make a prediction. The alignment is used to compute a normalized weighted sum of the related embeddings, which becomes the context embedding used as part of the feature set when predicting. This method has revolutionized neural modeling significantly improving the state-of-the-art. Our work described in this chapter also uses these ideas, where we utilized the observed embeddings (KC variables) of the TrueLearn learner model in order to infer the value of unobserved variables. In summary, this work lays the foundations for applying SR in an educational recommender using:
i. A PGM
ii. The SR values extracted from Wikipedia
1.3 Methodology
Given that there is a gap in exploiting SR to improve concept-based recommendations, our work focuses on developing a method to do so. Specifically, we focus on the instance of cold-start in concept-based user modeling as a foundational step toward using semantic knowledge graphs. In the case of cold-start, the informational concept is novel, i.e., it has not been encountered by the user before in their interaction history. In the conventional user model outlined in Section 1.2, the system will not have any data to infer the learner’s interest/skill for such an unobserved concept. In the case of Bayesian probabilistic models such as TrueLearn Novel, the model will use a non-informative prior. To address this issue, we formalize the problem and propose a solution in this section.
1.3.1 Problem Formulation
Consider the case of a learner  interacting with a set of educational resources
{}
⊂...
 ,,
1
S r rQ over a period of T = (1,..., t) time steps, Q being the total of resources in the system. Each resource ri is characterized by the top KCs or topics covered { }
K ⊂ 1,..., N
ri (N is the total of KCs considered by the system) and the depth of coverage dri of those. We represent learner knowledge
at time t as a multivariate Gaussian distributionθθ μμ Σ
 
~ (, )
t tt
 , μμ ∈ 
t Q being the mean of knowledge and Σ
t the covariance matrix. TrueLearn assumed that Σ is a diagonal covariance matrix in all cases and thus knowledge top
builds toward considering a full covariance matrix, assuming that ρij (estimated SR) is a proxy for Σij for topics i and j when i ≠ j. The key idea behind TrueLearn Novel (Bulathwela et al., 2020c) is to model the probability of engagement { }
∈−
 1, 1
et ,r
i between learner  and resource ri
ics are completely independent from each other. The work in this chapter


Leveraging Semantic Knowledge Graphs 7
at time t as a function of the learner skills/knowledgeθθ 
t and resource representation dri for the top KCs covered Kri. When a new learner joins the recommender system, TrueLearn sets μ = 0
0 , Σii = β, where β is a hyperparameter of the system, and Σ = 0, i ≠ j
ij . Then, when the learner consumes an educational video fragment, TrueLearn updates the learner model/skills accordingly. Every skill that is not updated is set to the value from the last time step, meaning at time t there might be many unobserved skills, especially given the number of topics considered by the system (equal to the number of Wikipedia pages). Thus, TrueLearn assumes that the skill for topics in Kri can only be obtained through those topics and not semantically related ones. The same problem setting can be generalized to the other concept-based learner models outlined in prior works (Bulathwela et al., 2021b; Piao & Breslin, 2016). The key difference is that these models do not model uncertainty of the skill variables (assuming Σ = Σ = 0
ii ij as well).
1.3.2 Semantic TrueLearn
Extending the TrueLearn model (Bulathwela et al., 2020c), Semantic TrueLearn, is a learner model that infers the knowledge state of learners in an online fashion. Semantic TrueLearn exploits its current knowledge of observed concepts (through interactions from time steps 0...t − 1) and their SR to the novel concept encountered at time t to make a better prior skill estimate. This approach is graphically illustrated in Figure 1.1.
FIGURE 1.1
Inferring the knowledge for the unseen topic (white circle) based on semantically related and seen ones (grey circles) by transferring knowledge (dotted lines). Topics are ML (machine learning), RL (reinforcement learning), Prob (probability), CV (computer vision), NLP (natural language processing), and W2V (Word2Vec).


8 Semantic AI in Knowledge Graphs
1.3.2.1 Incorporating Semantic Relatedness between Concepts/KCs
The main assumption when incorporating SR is that knowledge can be shared across semantically related topics. In other words, we assume that the demonstration of having knowledge in certain KCs enables us to reason about their degree of knowledge of related, yet unobserved KCs. By taking inspiration from graph convolution (Kipf & Welling, 2017), we assume the relationship between concepts illustrated in Figure 1.1. The skill of the unobserved KC is calculated as the weighted average of the observed related skills as per Equation 1.1:
∑
θ = Ω γ ⋅θ θ μ σ
∈Ω




1 , where ~ ( , )
, ,
, ,2
,
ti
ij
ij t j t i
i
 (1.1)
where Ωi represents the set of topics used to infer the representation of topic i (e.g., most semantically related seen topics), where i ≠ j. The mixing factorsγij can be set to SR ρij or to a factor of the standard error of topic j (meaning most observed topics are used). In the TrueLearn (Bulathwela et al., 2020c) model, which we extend, θ is a Gaussian variable. Two mathematical formulations are tested to capture the relatedness among the topics, namely the (i) univariate and (ii) multivariate formulations.
1.3.2.2 The Univariate Formulation
This formulation assumes that relatedness exists exclusively between the unobserved topic and the set of observed related topics. The SR between the pairs of observed topics is ignored in this scenario. The motivation behind this is that the relatedness of the observed topics in the user profile doesn’t have a significant effect on the final estimation of the unobserved skill parameter. We use Equation 1.2 to calculate the unknown parameter θt,i in this formulation:
 
ˆ~ 1 , 1
,,
2
,
2
ti
ji
ij t j ji
ij t i
ii
∑ ∑
θ ρ μ ρ (σ )
Ω⋅ ⋅ Ω⋅
 

 ⋅






∈Ω ∈Ω
(1.2)
1.3.2.3 The Multivariate Formulation
This formulation, on the contrary to Equation 1.2, assumes that relatedness between all observed KCs also Equation 1.3 presents this formulation where σ in this case represents the covariance matrix:
   
ˆ~ 1 , 1 2
,,
2
ti 2 , , 2 , , , , ji
ij t j jz i
ij t j j iz z z
t iz ij t j z
i ii
 ∑ ∑∑
θ ρ μ ρσ ρ σ ρ ρσ
()
Ω⋅ ⋅ Ω
 

 + +






∈Ω ∈Ω ∈Ω
(1.3)


Leveraging Semantic Knowledge Graphs 9
1.4 Experiments and Results
We ask the following research questions:
• RQ1: Which SR Metric is the most suitable (ρ)?
• RQ2: How many observed related topics should be used (Ωi )?
• RQ3: Can Semantic TrueLearn outperform TrueLearn Novel?
• RQ4: Does semantic information contribute to the gains? How?
• RQ5: Can this approach generalize to other user models?
1.4.1 Semantic Relatedness Metric (SR Metric)
As mentioned in Section 1.2, different measures of SR for Wikipedia concepts exist (Ponza et al., 2020). We empirically evaluate if the predictive performance of an educational recommender can be improved by incorporating seven different SR Metrics to substitute ρij in Equations 1.2 and 1.3. We devise Milne and Witten (M&W), Entity Embeddings (W2V), Point-wise Mutual Information (PMI), Language Model (LM), Jaccard Similarity (Jaccard), Conditional Probability (CP), and Barabasi and Albert (BA) SR Metrics, where SR values are pre-computed and publicly available (Piccinno, 2017).
1.4.2 Models
The core objective of this research tested through RQ 1–4 is to validate if exploiting SR can improve the predictive capability of the TrueLearn Novel model. To test this we integrate the formulations outlined in Section 1.3 by using them whenever the model has not encountered that Wikipedia-based KC in the learner history before. The two models, Semantic TrueLearn Univariate (Semantic TLN Univ.) using Equation 1.2 and Semantic TrueLearn Multivariate (Semantic TLN Mult.) using Equation 1.3, are developed and tested against TrueLearn Novel (Bulathwela et al., 2020c) as the baseline. To test RQ5, we create the semantic counterparts of a set of relevant baselines that use Wikipedia concepts for user modeling. KT, a different PGM that models learner skills as Bernoulli variables and two user models, the Cosine model (Bulathwela et al., 2021b) and TF(Cosine) (Piao & Breslin, 2016), that are not PGMs are transformed into semantically aware models by using Equation 1.2. The variance calculation in the equation is omitted as the above models do not explicitly model the variance of the skill variables (σ = 0). This introduces Semantic Cosine and Semantic TF(Cosine) for empirically testing RQ5.


10 Semantic AI in Knowledge Graphs
1.4.3 Data
As the TrueLearn Novel model deals with video fragment recommendation, we test the new proposals using the same prediction task. We use the PEEK dataset (Bulathwela et al., 2021b), a dataset of more than 20,000 informal learners consuming video lectures in VideoLectures.Net1 platform. The dataset provides information about how different users consumed fragments of videos over time (Bulathwela, Kreitmayer, & Pérez-Ortiz, 2020a). The majority of videos in this repository are related to computer science. This dataset uses entity linking (Brank et al., 2017) to associate most related Wikipedia concepts to documents. We use the TagMe WAT API (Piccinno & Ferragina, 2014)2 to source the required SR annotations. As the KCs associated with a video fragment are highly related to each other, we exclusively use the most relevant KC from each video fragment to represent the topic covered by that video fragment ignoring the other KCs associated with that video fragment. Doing this helps us avoid any side effects that can dilute our objective of measuring if exploiting SR improves the predictive abilities of the model. It also exponentially decreases the number of SR annotations we need to run the experiments. To keep the computational complexity lower, a smaller dataset of the 20 most active users is used when validating RQ 1 and 2. Once the choice of SR metric and the number of related topics have been determined, the full dataset of 20,000 users is used to validate RQ 3–5, which are our primary research questions.
1.4.4 Experimental Design
We used a phased experimental methodology where the results from the early experiments determined the parameters for the subsequent experiments. We empirically evaluated models built with different SR metrics to answer RQ1. The best performant SR metric from the RQ1 experiment was then used to determine how many related topics should be used (RQ2). Then, we used both of these results in RQ3 to test Semantic TLN Univ. and Semantic TLN Mult. against the TrueLearn Novel baseline. Finally, the semantic counterparts of KT, Cosine and TF(Cosine) models were developed as per Equation 1.1 empirically tested for RQ5 with the same SR metric and number of topics that are predetermined in RQ 1 and 2 experiments, respectively. As the latter models used in the RQ5 experiment do not have a variance component, Equations 1.2 and 1.3 reduce to the same formulation as the mean μ is computed similarly.
1.4.4.1 Impact of Semantic Relatedness
We use the topics encountered in user sessions to build a topic-relatedness graph and extract a few attributes linked to graph connectedness for each user. Spearman’s Rank Order Correlation Coefficient (SROCC) statistic is then used to evaluate the correlation between the extracted features and the


Leveraging Semantic Knowledge Graphs 11
predictive performance. User’s number of events, number of unique topics, topic sparsity rate (Bulathwela et al., 2020c), positive label rate, Avg. Connectedness, i.e., average of the degree distribution of the topics, and Min. Cut Set Size, i.e., the minimum number of topics that need to be removed to break the graph into more sub-graphs, are analyzed. The correlation with the recall score is investigated as the improvement in recall attributes to the performance gains of the proposed model (see Table 1.1). To validate if SR is specifically influential in earlier parts of the user session, we plot the mean recall score of all users at event n, for different number of events (n).
1.4.5 Evaluation
In all the empirical evaluations (RQ 1, 2, 3, and 5), a sequential prediction design where engagement at time t is predicted using events 1 to t − 1 is utilized in this prediction task. A training set of 70% of learners is used for hyperparameter tuning and the remainder is used for testing and reporting. Being a binary classification task, precision, recall, and F1-measure are evaluated whereas F1-measure is used for overall model selection (Bulathwela et al., 2021b). The evaluation metrics are computed for each learner separately and the weighted average of the scores based on the number of learner’s events is reported. In cases where the entire dataset is used for evaluation (RQ3 experiment onward), we use a learner-wise one-tailed paired t-test to verify the statistical significance of the improvement. When measuring the correlation between different learner session-related attributes and the recall score in order to validate RQ4, we use SROCC to assess the degree of correlation between pairs of variables.
TABLE 1.1
Predictive Performance of Adding SR to TrueLearn Novel Algorithm. The Different Configurations (SR Metric) of the Semantic TrueLearn Novel Algorithm (Our Proposal) Are Evaluated Using Precision (Prec.), Recall (Rec.), and F1 Score (F1)
Model SR Metric Prec. Rec. F1
TrueLearn Novel – 0.7667 0.9476 0.8348
M&W 0.7701 0.9469 0.8364 W2V 0.7714 0.9467 0.8370 Semantic PMI 0.7682 0.9480 0.8355 TrueLearn LM 0.7605 0.9507 0.8322 Novel Jaccard 0.7605 0.9507 0.8322 CP 0.7621 0.9507 0.8330 BA 0.7704 0.9469 0.8364
Note: The most performant value and the next best value are highlighted in Bold and Italic faces, respectively. The Semantic TrueLearn algorithms that outperform the baseline model in terms of F1 score are Underlined.


12 Semantic AI in Knowledge Graphs
1.4.6 Results
We run experiments to answer the research questions outlined above. To identify the most suitable SR metric (RQ1), we evaluate the Semantic TrueLearn model using seven SR Metrics proposed in Section 1.3.2. The results are outlined in Table 1.1. To understand the effect of Ωi, the Number of Semantically Related Topics (RQ2), we use the identified SR Metric to experiment with different numbers of semantically related topics. The results of this experiment are reported in Table 1.2. Finally, we use the full PEEK dataset to validate if the use of SR data improves the baseline TrueLearn model (RQ3). The results obtained in this experiment are presented in Table 1.3. Figure 1.2 presents the results obtained in investigating the impact of SR (RQ4) where (left) the correlation investigation between topic connectivity of users and recall score and (right) the performance of the model based on a different number of events is reported. The predictive performance of the different user models (left) and their semantic counterparts (right) on the PEEK dataset are outlined in Table 1.4 (RQ5). To ensure fairness of comparisons, Cosine, TF(Cosine), and KT models are trained using the highest ranking topic for
TABLE 1.2
The Performance of Semantic TrueLearn Model with W2V SR Metric Is Reported in Terms of Precision (Prec.), Recall (Rec.), and F1 Score (F1)
Number of Topics (ΩΩ,i) Prec. Rec. F1
Most related topic 0.7717 0.9431 0.8359 Three most related Topics 0.7622 0.9486 0.8325 Five most related Topics 0.7659 0.9490 0.8345 Ten most related topics 0.7654 0.9490 0.8342 All related topics 0.7714 0.9467 0.8370
Note: The performance of the model is reported when different Ω,i top semantically related topics are utilized in Equation 1.1. The most performant value and the next best value are highlighted in Bold and Italic faces, respectively.
TABLE 1.3
Predictive Performance of Semantic TrueLearn Model (Our Proposal) Using Precision (Prec.), Recall (Rec.), and F1 Score (F1)
Model Prec. Rec. F1
Baseline TrueLearn Novel 0.5829 0.7924 0.6471
Semantic TLN
Univariate (Univ.) 0.5711 0.8563(*) 0.6512(*) Multivariate (Mult.) 0.5759 0.8251(*) 0.6480(*)
Note: The most performant value is highlighted in Bold face. The Semantic TrueLearn model that outperforms the baseline model (p < 0.01 in a onetailed paired t-test) is marked with·(*).


Leveraging Semantic Knowledge Graphs 13
each video fragment in the learner sessions. However, prior work shows that the cosine and the TF(Cosine) models perform best when using the five highest ranked topics. Therefore, we also report the performance of the model trained on five topics in Table 1.4 for a more informative comparison.
1.4.7 Discussion
It is evident from Table 1.1 that incorporating SR leads to improvements in overall F1 score in most of the SR metrics that beat the baseline TrueLearn algorithm. Four Semantic TrueLearn models (ones that use M&W, W2V, PMI, and BA) tend to outperform the baseline TrueLearn Novel model in terms of precision and F1. The remainder demonstrates superiority in the recall. When we consider the F1 score for model comparison, the model that uses
FIGURE 1.2
(Left) Relationship between different behavioral characteristics of user-profiles and model recall performance presented using SROCC. The numbers and the intensity of each cell correspond to the Spearman r coefficient where a significant correlation is present (p < 0.01). Empty cells represent the lack of significant correlation. (Right) The average recall performance of the two models for the learner population at a different number of events.
TABLE 1.4
Predictive Performance of Semantic Models (Our Proposals) Using Precision (Prec.), Recall (Rec.), and F1 Score (F1)
Model
Prior Work Semantic
Prec. Rec. F1 Prec. Rec. F1
Knowledge tracing 0.5325 0.2856 0.3451 0.5737(*) 0.5613(*) 0.5344(*) Cosine 0.4792 0.1599 0.2112 0.5652(*) 0.7377(*) 0.5978(*) • Five topics 0.5786 0.5845 0.5406 • • • TF(Cosine) 0.5231 0.3355 0.3670 0.5651(*) 0.6805(*) 0.5728(*) • Five topics 0.5675 0.6595 0.5711 • • •
Note: The most performant value and the next best value are highlighted in Bold and Italic faces, respectively. The semantic models that outperform the baseline model (p < 0.01 in a one-tailed paired t-test) are marked with·(*).


14 Semantic AI in Knowledge Graphs
the entity embedding-based SR metric (W2V) indicates the best performance among the different semantic models. This is expected, as neural-based SR measures often outperform their graph-based counterparts (Ponza et al., 2020). Therefore, we can observe that the most suitable SR metric for this task (RQ1) is the entity embedding-based metric. Our empirical results in experiments relating to RQ2, outlined in Table 1.2, show that using all semantically related topics to infer the skill of the unobserved KC gives the best prediction results in contrast to restricting the number of related topics used. The results in Table 1.3, which attempts to validate if the proposed semantic formulations can help TrueLearn Novel address the cold-start problem (RQ3), show the superiority of Semantic TrueLearn models in comparison to the baseline that does not exploit SR information from Wikipedia. This is a clear indication that a knowledge base such as Wikipedia can be critical to improving the assumptions used for a learner, modeled using a PGM in the education context. Both the Univ. and Mult. Semantic TrueLearn models outperform the baseline to a statistically significant degree in recall and F1. It is observed in Table 1.3 that this improvement of F1 score is attained by significantly increasing the recall of the model by sacrificing a smaller amount of precision. While this is a compromise of this model, the overall performance of the model is improved. It is also interesting to see that modeling the relatedness between the observed topics (Mult. formulation) leads to better precision than not doing it. This indicates that accounting for many different dynamics in the data generation process and capturing them leads to a more precise prediction. However, this precision doesn’t translate to overall model superiority in terms of the F1 score in comparison to the Univ. counterpart. It is also noteworthy that the computational complexity of the Mult. version is significantly higher as there is exponentially more SR connections that need to be used in the variance calculation. In an online, lifelong learning platform that needs to scale seamlessly, this can be a disadvantage. The results give promise that the information encoded in a knowledge base such as Wikipedia can be used in ways beyond representing contents in a universal taxonomy. Certain relationships in Wikipedia can be further utilized to improve the model assumptions. In this scenario, SR has shown truly valuable in the early stages of the user session when the interaction data about the user is limited, thus addressing the cold-start problem. The evaluation of correlations presented in Figure 1.2 (left) investigates the reasons behind the superior performance of the semantic models (RQ4). This sub-figure shows the lack of correlation between Positive Label Rate and recall score across both TrueLearn models. Although it has been demonstrated by the original authors that the TrueLearn algorithm capitalizes on recall, there is no information in the work regarding the positive label rate in the datasets. This observation confirms that the TrueLearn family of algorithms find true patterns in learner data rather than merely capitalizing on the positive labels to boost performance. Multiple observations in Figure 1.2 (left) give evidence of the superiority of Semantic TrueLearn exploiting the SR between topics to boost recall. The


Leveraging Semantic Knowledge Graphs 15
main two observations are the new model’s stronger Spearman’s rank correlation with learner Avg. Connectedness and Min. Cut Set Size. This is a strong indication that the Semantic TruLearn model is exploiting the topic correlations. The correlation between the number of events, number of unique topics, and topic connectedness causes the higher correlation between these features and the Semantic TrueLearn model. Figure 1.2 (right) clearly shows how the recall score of predictions is much larger in the Semantic TrueLearn algorithm regardless of the early or later stage of the learner session. Linking this to results in Table 1.1 shows that this impressive gain of recall score is achieved with a much smaller sacrifice of the precision score. Figure 1.2 (right) also shows the nature of this cold-start problem which can occur at any time during the learner’s session. Usually, the cold-start problem is associated with the early stages of a learner session, mainly because the scarcity of data is prominent in early stages of a user session in a recommender. However, in educational recommenders and other informational recommenders (e.g., news, podcasts, etc.) where the concept space can be very vast and the learner can journey in the entire knowledge space, the cold-start problem can occur at any given stage of the learner session. The analysis shows that the approach proposed helps combat this phenomenon successfully. The final question we want to answer is if exploiting SR goes beyond the TrueLearn Novel model (RQ5). The results in Table 1.4 provide solid evidence that this is the case. The table shows that adding the semantic extension proposed in this work to a variety of recently proposed Wikipedia concept-based user models leads to statistically significant improvements across precision, recall, and F1 score. When comparing with the five topic versions of the Cosine and TF(Cosine) models, the table gives evidence that the one topic version of semantic models still outperforms the five topic nonsemantic versions by a significant margin in recall and F1 score, the overall evaluation metric. This is further evidence that the utilization of SR coming from Wikipedia can have a strong positive impact on user modeling.
1.4.8 Human-Intuitive Representations
The Semantic TrueLearn model in unison with all the other models used in this work uses Wikipedia-based concepts to build the user representation. This makes the user models humanly intuitive and capable of diagnosis, interpretation, and scrutiny. As the concepts/KCs in the model are symbols familiar to human perception, user-friendly explanations and rationalizations can be produced using the model representation. Specifically, in the context of exploiting SR, the models proposed in this work use the mechanism presented in Figure 1.1. This is already a simple, user-friendly visualization of how the learner model is reasoning. Therefore, approaches such as this that rely on knowledge bases such as Wikipedia have the ability to connect the AI systems to the human users with richer explanations allowing the users to provide a higher degree of engagement and feedback, leading the systems to improve rapidly over time.


16 Semantic AI in Knowledge Graphs
1.4.9 Limitations
Amid the significant gains, we observe that most KCs encountered by the model in a session are highly correlated to each other (as the majority of video lectures on the source website are about computer science). This leads to overlapping information being propagated repeatedly when using Equation 1.1 which may lead to an overestimation of knowledge of unseen KCs. While restricting to exclusively using the top-ranked KC from each video fragment helps us reduce this effect, it doesn’t completely solve the problem. Methods to address this effect should be investigated further. As the proposed method primarily infers unobserved skills, its use diminishes over time when the user session matures (as new topics are encountered less often). While there is a chance to encounter new topics at any stage of the learner journey, the changes become slimmer over time and so is this approaches usefulness. Mechanisms to retain the use of semantic awareness to refine representations is a much-needed improvement to the proposed method.
1.5 Conclusions
Leveraging SR between Wikipedia topics has demonstrated promise to improve the predictive performance of informational recommenders such as TrueLearn, which are built on Wikipedia ontology and PGMs. In addition, we identify that restricting the number of related topics leads to degraded performance, suggesting the use of all available KCs extracted from Wikification. Our analysis also shows that topic connectedness within learner sessions is positively correlated with the performance gains of Semantic TrueLearn, giving clearer evidence of the positive impact of incorporating this aspect when modeling learners and their journey within an education setting. Further investigations also show that the proposed methods generalize to other learner modeling techniques that go beyond the TrueLearn family of models extending to both PGMs and classical concept-based user models.
1.5.1 Future Work
The proposed model is a stepping stone to accounting for SR. However, it still can do better in terms of capturing the correlation among the observed topics. We propose (i) using algorithms such as PageRank (Brin & Page, 1998) to derive uncorrelated skill parameters and (ii) incorporating richer ontologies (Auer et al., 2007) that contain more fine-grained relationships, entity definitions/categorizations, and constraints in the place of the raw Wikipedia graph to incorporate finer grain semantic awareness to the learner model. Mechanisms to continuously utilize SR information (even in the absence of


Leveraging Semantic Knowledge Graphs 17
new topics) should be identified and investigated in future work. Moreover, SR measures are not usually built and validated with educational datasets or topics, which is a limitation of the field. In the future, we should aim to validate the usefulness of proposed SR metrics with educational applications and thrive to improve them to align more with educational and information use cases. Also, more advanced model families (e.g., Bulathwela, Pérez-Ortiz, Yilmaz, & Shawe-Taylor, 2022) can benefit from the proposed techniques leading to a generation of semantically aware, integrative educational recommendation systems. As Semantic TrueLearn builds a sub-symbolic representation that is humanly intuitive, it is possible to create narratives and intelligent user interfaces (e.g., Bulathwela et al., 2020a; Pérez-Ortiz et al., 2021) that can be used to interpret and rationalize (Riedl & Bulitko, 2013) the learnings from the model leading toward more human-in-the-loop artificial intelligence. This will increase trust and enable verification and scrutinizing of the models (Balog, Radlinski, & Arakelyan, 2019). Going beyond recommendation, SR can be harnessed to improve information retrieval systems as well Ahmed and Bulathwela (2022).
Acknowledgments
This research is conducted as part of the X5GON project (www.x5gon.org) funded by the EU’s Horizon 2020 research and innovation program grant number 761758. We gratefully acknowledge support and funding from the US Army Research Laboratory and the US Army Research Office, and by the UK Ministry of Defence and the U.K. Engineering and Physical Sciences Research Council (EPSRC) under grant number EP/R013616/1. This work is also partially supported by the European Commission funded project “Humane AI: Toward AI Systems That Augment and Empower Humans by Understanding Us, our Society and the World Around Us” (grant 820437), EU Erasmus+ project “European Network for Catalysing Open Resources in Education” (project ref: 621586-EPP-1-2020-1-NO-EPPKA2-KA), and the EPSRC Fellowship titled “Task Based Information Retrieval” (grant EP/ P024289/1). The AT2030 program is funded by the UK Aid from the UK government and led by the Global Disability Innovation Hub.
Notes
1 www.videolectures.net 2 https://sobigdata.d4science.org/web/tagme/wat-api


18 Semantic AI in Knowledge Graphs
References
Ahmed, T., & Bulathwela, S. (2022). Towards proactive information retrieval in noisy text with Wikipedia concepts. In Proc. of the first workshop on proactive and agent-supported information retrieval at conference of information and knowledge management.
Auer, S., Bizer, C., Kobilarov, G., Lehmann, J., Cyganiak, R., & Ives, Z. (2007). DBpedia: A nucleus for a web of open data. In The Semantic Web: 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, ISWC 2007+ ASWC 2007, Busan, Korea, November 11–15, 2007. Proceedings (pp. 722–735). Berlin, Heidelberg: Springer. Bahdanau, D., Cho, K., & Bengio, Y. (2015). Neural machine translation by jointly learning to align and translate. In Y. Bengio & Y. LeCun (Eds.), Third international conference on learning representations, ICLR 2015, San Diego, CA, USA, May 7–9, 2015, conference track proceedings.
Balog, K., Radlinski, F., & Arakelyan, S. (2019). Transparent, scrutable and explainable user models for personalized recommendation. In Proc. of the 42nd international ACM SIGIR conference on research and development in information retrieval (SIGIR ’19).
Bauman, K., & Tuzhilin, A. (2018). Recommending remedial learning materials to students by filling their knowledge gaps. MIS Quarterly, 42(1):313–332. Bishop, C., Winn, J., & Diethe, T. (2015). Model-based machine learning. Early access version (http://www.mbmlbook.com/) (accessed 23-05-2019). Brank, J., Leban, G., & Grobelnik, M. (2017). Annotating documents with relevant wikipedia concepts. In Proc. of Slovenian KDD conference on data mining and data warehouses (SIKDD).
Brin, S., & Page, L. (1998). The anatomy of a large-scale hypertextual web search engine. In Proc. of international conference. on world wide web.
Bulathwela, S., Kreitmayer, S., & Pérez-Ortiz, M. (2020a). What’s in it for me? augmenting recommended learning resources with navigable annotations. In Proc. of international conference on intelligent user interfaces companion (pp. 114–115).
Bulathwela, S., Pérez-Ortiz, M., Mehrotra, R., Orlic, D., de la Higuera, C., ShaweTaylor, J., & Yilmaz, E. (2021a, February). Report on the WSDM 2020 workshop on state-based user modelling (sum’20). SIGIR Forum, 54(1), Article No.: 5: 1–11. Retrieved from https://doi.org/10.1145/3451964.3451969. Bulathwela, S., Pérez-Ortiz, M., Novak, E., Yilmaz, E., & Shawe-Taylor, J. (2021b). Peek: A large dataset of learner engagement with educational videos. Retrieved from https://arxiv.org/abs/2109.03154. Bulathwela, S., Pérez-Ortiz, M., Yilmaz, E., & Shawe-Taylor, J. (2020b). Towards an integrative educational recommender for lifelong learners. In AAAI conference on artificial intelligence. Retrieved from https://doi.org/10.1609/aaai.v34i10.7151. Bulathwela, S., Pérez-Ortiz, M., Yilmaz, E., & Shawe-Taylor, J. (2020c). Truelearn: A family of Bayesian algorithms to match lifelong learners to open educational resources. In AAAI conference on artificial intelligence. Retrieved from https://doi. org/10.1609/aaai.v34i01.5395. Bulathwela, S., Pérez-Ortiz, M., Yilmaz, E., & Shawe-Taylor, J. (2022). Power to the learner: Towards human-intuitive and integrative recommendations with open educational resources. Sustainability, 14(18). Retrieved from https://www.mdpi. com/2071-1050/14/18/11682.


Leveraging Semantic Knowledge Graphs 19
Carmona, C., Millán, E., Pérez-de-la Cruz, J. L., Trella, M., & Conejo, R. (2005). Introducing prerequisite relations in a multi-layered Bayesian student model. In Proc. of the international conference on user modeling (pp. 347–356).
Chen, P., Lu, Y., Zheng, V. W., & Pian, Y. (2018). Prerequisite-driven deep knowledge tracing. In 2018 IEEE international conference on data mining (ICDM) (pp. 39–48). Grefenstette, G., & Rafes, K. (2015). Transforming Wikipedia into an ontology-based information retrieval search engine for local experts using a third-party taxonomy. arXiv preprint arXiv:1511.01259. Retrieved from https://arxiv.org/abs/ 1511.01259. Huang, Z., Yin, Y., Chen, E., Xiong, H., Su, Y., Hu, G., et al. (2019). EKT: Exercise-aware knowledge tracing for student performance prediction. IEEE Transactions on Knowledge and Data Engineering, 13(1): 100–115.
Jiang, W., Pardos, Z. A., & Wei, Q. (2019). Goal-based course recommendation. In Proceedings of international conference on learning analytics & knowledge.
Kawakami, T., Morita, T., & Yamaguchi, T. (2017). Building Wikipedia ontology with more semi-structured information resources. In Semantic technology – 7th joint international conference, JIST 2017, proceedings. Springer.
Kipf, T. N., & Welling, M. (2017). Semi-supervised classification with graph convolutional networks. In International conference on learning representations (ICLR). Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., ... (2020). Retrieval-augmented generation for knowledge-intensive NLP tasks. arXiv preprint arXiv:2005.11401. Retrieved from https://arxiv.org/abs/2005.11401. Mandalapu, V., Gong, J., & Chen, L. (2021). Do we need to go deep? knowledge tracing with big data. arXiv preprint arXiv:2101.08349. Retrieved from https://arxiv.org/ abs/2101.08349. Nakagawa, H., Iwasawa, Y., & Matsuo, Y. (2019). Graph-based knowledge tracing: modeling student proficiency using graph neural network. In 2019 IEEE/WIC/ ACM international conference on web intelligence (WI) (pp. 156–163).
Pandey, S., & Srivastava, J. (2020). RKT: Relation-aware self-attention for knowledge tracing. arXiv preprint arXiv:2008.12736. Retrieved from https://arxiv.org/ abs/2008.12736. Pérez-Ortiz, M., Dormann, C., Rogers, Y., Bulathwela, S., Kreitmayer, S., Yilmaz, E., ... Shawe-Taylor, J. (2021). X5learn: A personalised learning companion at the intersection of AI and HCI. In 26th international conference on intelligent user interfaces (pp. 70–74). Piao, G. (2021). Recommending knowledge concepts on mooc platforms with metapath-based representation learning. In Proc. of international conference on educational data mining.
Piao, G., & Breslin, J. G. (2016). Analyzing MOOC entries of professionals on linkedin for user modeling and personalized MOOC recommendations. In Proceedings of the 2016 conference on user modeling adaptation and personalization. Piccinno, F. (2017). Algorithms and data structures for big labeled graphs (Unpublished doctoral dissertation). Universitad́ e Pisa. Piccinno, F., & Ferragina, P. (2014). From TagMe to wat: A new entity annotator. In Proc. of the first international workshop on entity recognition & disambiguation (ERD’14). Retrieved from https://doi.org/10.1145/2633211.2634350. Ponza, M., Ferragina, P., & Chakrabarti, S. (2020). On computing entity relatedness in wikipedia, with applications. Knowledge-Based Systems, 188. Riedl, M. O., & Bulitko, V. (2013). Interactive narrative: An intelligent systems approach. Ai Magazine, 34(1):67–67.


20 Semantic AI in Knowledge Graphs
Schmucker, R., Wang, J., Hu, S., & Mitchell, T. (2022, June). Assessing the performance of online students - new data, new approaches, improved accuracy. Journal of Educational Data Mining, 14(1):1–45. Retrieved from https://jedm.educational datamining.org/index.php/JEDM/article/view/541. Selent, D., Patikorn, T., & Heffernan, N. (2016). ASSISTments dataset from multiple randomized controlled experiments. In Proc. of the conference on learning @ scale (L@S ‘16). Retrieved from https://doi.org/10.1145/2876034.2893409. Song, X., Li, J., Tang, Y., Zhao, T., Chen, Y., & Guan, Z. (2021). JKT: A joint graph convolutional network based deep knowledge tracing. Information Sciences, 580:510–523. Syed, Z., Finin, T., & Joshi, A. (2008, March). Wikipedia as an ontology for describing documents. In Proc. of international conference on weblogs and social media. AAAI Press. Thaker, K., Zhang, L., He, D., & Brusilovsky, P. (2020). Recommending remedial readings using student knowledge state. In Proc. of international conference on EDM. Yano, T., & Kang, M. (2016). Taking advantage of Wikipedia in natural language processing (Tech. Rep.). Carnegie Mellon University Language Technologies Institute. Yang, Y., Shen, J., Qu, Y., Liu, Y., Wang, K., Zhu, Y., Yu, Y.... (2020). GIKT: a graph-based interaction model for knowledge tracing. In Joint European conference on machine learning and knowledge discovery in databases (pp. 299–315).
Yudelson, M. V., Koedinger, K. R., & Gordon, G. J. (2013). Individualized Bayesian knowledge tracing models. In H. C. Lane, K. Yacef, J. Mostow, & P. Pavlik (Eds.), Proc. of artificial intelligence in education.
Zarrinkalam, F., Faralli, S., Piao, G., & Bagheri, E. (2020). Extracting, mining and predicting users’ interests from social media. Foundations and Trends® in Information, 14(5): 445–617. Boston, MA: Now publishers. Retrieved from http://dx.doi. org/10.1561/1500000078.


DOI: 10.1201/9781003313267-2 21
2
Modeling Event-Centric Knowledge Graph for Crime Analysis on Online News
Federica Rollo and Laura Po
Department of Engineering, University of Modena and Reggio Emilia, Modena, Italy
CONTENTS
2.1 Introduction.................................................................................................. 21 2.2 Related Work ................................................................................................22 2.3 Event-Centric Knowledge Graph............................................................... 24 2.3.1 Node Centrality................................................................................ 28 2.3.2 Community Detection.....................................................................30 2.4 Application.................................................................................................... 31 2.4.1 Dataset ............................................................................................... 31 2.4.2 Modena Crime Knowledge Graph ................................................ 32 2.4.3 Modena Crime Analysis ................................................................. 36 2.5 Conclusion and Future Work ..................................................................... 38 Disclosure Statement ............................................................................................ 39 References............................................................................................................... 40
2.1 Introduction
Crime analysis is the set of quantitative and qualitative techniques to analyze crime data, including not only the analysis of actual crimes, criminals, and victims but also the understanding of problems related to the quality of life in a community, the socio-demographic aspects and other factors that can influence the frequency of crime in that community. Also, crime analysis aims at identifying crime patterns and trend correlations that can help law enforcement agencies (LEAs) in crime reduction, prevention, and evaluation. Police reports can be helpful for these scopes since they provide a complete description of crimes; however, these documents are usually private and authorization is required for access. In this context, newspapers are valuable sources of information. The extraction of structured information on events from online sources for the purpose of crime intelligence gathering has been acknowledged to be of paramount importance by various organizations worldwide. Newspapers


22 Semantic AI in Knowledge Graphs
provide reliable, localized, and timely data (the time delay between the occurrence of the event and the publication of the related news article does not exceed 24/48 hours). The main drawback is that newspapers do not collect and publish all the facts related to crimes, but only the ones that arouse the readers’ interest. Therefore, a percentage of police reports will not be turned into news articles and is lost. Natural language processing (NLP) techniques can be exploited for understanding the content of the news articles and extracting semantically enriched data. Moreover, information about an event is usually spread across multiple news articles. Over time, more details are provided about the dynamics of the event. Identifying the news articles related to the same event is of key importance to merge duplicates and make crime analysis more reliable. On the other hand, clustering similar news articles allows to perform statistical analysis on crime events, e.g., finding the number of car thefts occurred in a specific month in a certain neighborhood of the city or the rate of armed robberies w.r.t. the total number of robberies in the city. For this reason, a representation of the events and their relations is needed. The Event-Centric Knowledge Graphs are specific knowledge graphs in which information is centered on the event instead of the entities, as defined by Rospocher et al. (2016). These knowledge graphs are able to provide an accurate description of the events and allow for the interconnection between them. Each event is represented by a central node that is connected to other nodes which express the characteristics of the event. These nodes can be connected to more central nodes, which means the corresponding events have something in common. Community detection algorithms can be applied to the graph to distinguish groups of similar event nodes. In this chapter, we propose a methodology to build in Neo4j an EventCentric Knowledge Graph related to crime events as they are described in news articles. Centrality algorithms are used to find the importance of the central nodes, and community detection allows to find similar events to perform crime analysis. The methodology is applied to the Italian Crime News dataset1 demonstrating the advantages of using graph-based analysis in crime monitoring. The remainder of the contribution is organized as follows: Section 2.2 introduces some previous related work, then Section 2.3 explains the workflow to build our Event-Centric Knowledge Graph, while the experiments on an Italian dataset are presented in Section 2.4 as well as some possible analysis on crime events. We conclude with a discussion and future work in Section 2.5.
2.2 Related Work
In recent years, researchers have taken an increasing interest in the automatic construction of knowledge graphs. Indeed, knowledge graphs have been found to be very helpful representations applied in a multiple number


Modeling Event-Centric Knowledge Graph for Crime Analysis on Online News 23
of contexts. In particular, they can be employed for representing events and discovering their relations. In literature, there are few works focusing on the construction of event-based knowledge graphs for the representation of information contained in unstructured data, e.g., freeform text (Guo, Jiang, and Zhang, 2020; Lakshika and Caldera, 2021). A graph analytical approach was proposed by Po, Rollo, and Lado (2016) to identify the main topics published on social media. The graph is based on the co-occurrence of words across the news articles; however, semantic relationships are not included. An extension was provided by Rollo (2017) to consider also entities for the generation of the graph. In our recent work (Rollo and Po, 2022), we described a method to build knowledge graphs from textual data using Entity Linking and Automatic Keyphrase Extraction. Moreover, a consistent number of previous works refer to methodologies for the extraction and analysis of named events, e.g., historical events of global importance, from existing knowledge graph (Kuculo, 2022). In most cases, knowledge graphs focus on Entity-Centric knowledge; for example, this is the case of large-scale knowledge graphs such as Wikidata (Vrandecic, 2012), DBpedia (Auer et al., 2007), and YAGO (Mahdisoltani, Biega, and Suchanek, 2015). The concept of Event-Centric Knowledge Graph was defined by Rospocher et al. (2016). In the knowledge graph centered on the event all the data are stored w.r.t. the event, this feature allows to capture the dynamic of the event. Several analyses can be performed on the graph to derive new knowledge about the events, make prediction, generate a storyline of the events, understand their causality (Li et al., 2023; Yan and Tang, 2022). Some works focus on expressing the temporal relation between events (Gottschalk and Demidova, 2018; Knez, 2022; Park et al., 2022). The use of knowledge graph in the context of criminal data allows to develop technique for investigating, fighting, and preventing crime (Abdul Jalil et al., 2017; Jedrzejek and Bak, 2012; Onnoom et al., 2014; Venkata Srimukh and Shridevi, 2020). Robinson and Scogings (2018) proposed the GraphExtract algorithm to build a weighted graph for proactively identifying criminal events and the actors responsible. Elezaj et al. (2019) extended the SMONT ontology developed by Kalemi et al. (2017) and defined a knowledge graph-based framework to identify the murderer by inferring the person who has the motive, opportunity, and method starting from social networks. Peppes et al. (2020) proposed a visualization tool based on the use of an ontology for performing advanced crime analysis. Szekely et al. (2015) suggested a method to crawl sexual ads from the web and provide LEAs a knowledge graph tool to fight human trafficking and support victims. Data from the web are organized in a predefined ontology, then, the authors address the problem of duplicates through text similarity and entity resolution. Table 2.1 summarizes the main aspects of previous works on the use of knowledge graph in the context of crimes. With respect to the cited related works, in this chapter, we combine techniques based on graph structure with semantic-based extraction and relationship generation for the analysis of crime events. Adding semantics makes the data not only interconnected but also smarter, allowing for inference,


24 Semantic AI in Knowledge Graphs
2.3 Event-Centric Knowledge Graph
Our Event-Centric Knwoledge Graph aims at providing a comprehensive representation of the events and highlighting the relationships between different events.
TABLE 2.1
Previous Works on the Use of Knowledge Graph in the Context of Criminal Data
Reference Goal(G), Use case(U), Limitation(L)
Abdul Jalil et al. (2017)
G: development of a model to match similar crimes and help investigation officer in targeting suspects within the shortest time. U: motorcycle thefts L: thefts are connected each other just considering the exact match of some data (modus operandi, motorcycle type, crime scene, and time), no semantic approach is used to link semantically similar values Venkata Srimukh and Shridevi (2020)
G: description of an ontology to represent crimes reported to LEAs
L: the model does not allow to express key information of crimes, e.g., what was used to commit the crime, the stolen objects in a theft. Besides, the ontology is not available for integration and reuse Onnoom et al. (2014) G: ontology development to recommend words to fill in reports of crime scene investigation U: real crime cases from the Forensic Science Police Center 4 of Thailand L: the proposed ontology is limited to the specific use case, generalization requires training on new documents Jedrzejek and Bak (2012)
G: extension of a model for representing economic crimes committed by employees L: the complexity of the ontology assumes a deep preprocessing of the data to represent, this could be a limit for usability Elezaj et al. (2019) G: definition of a knowledge graph-based framework for crime analysis starting from social networks L: the framework is not implemented; therefore, it is not possible to evaluate its efficiency Szekely et al. (2015) G: development of an ontology-based knowledge graph from online sexual ads U: fighting human trafficking and supporting victims L: the text similarity used to find duplicates does not consider synonyms and/or semantic relationships, e.g., connecting the entities to a known vocabulary
analytics, and learning. Moreover, we propose a novel representation of the events that is specific of events as they are described in the news articles. To the best of our knowledge, this is the first work that proposes the construction of such a knowledge graph starting from news articles for crime analysis purposes.


Modeling Event-Centric Knowledge Graph for Crime Analysis on Online News 25
In journalism, the 5W + 1H are the questions the reporter needs to answer in reporting an event:
• What (what is happening?)
• Who (who is involved?)
• Where (where did it happen?)
• When (when did it happen?)
• Why (why did it happen?)
• How (how did it happen?)
FIGURE 2.1
Crime event representation in the Event-Centric Knowledge Graph.
Each news article is complete if it contains the answers to all the questions, on the other hand, extracting these answers allows at giving a complete description of the event. In some contexts, the answer to some of these questions may not be present, for instance, this is often the case of question Why. In news articles reporting crime events, the people involved (Who) can be detailed based on the role in the event: the author(s) of the crime, the victim(s), and other participants, e.g., the police. Considering this subdivision, the total number of questions is eight. Since in most cases the scope of the news articles is to provide information related to single events, a singlecrime-event-per-document assumption is made. Thus, event and news article can be considered as synonyms in this chapter. Figure 2.1 shows an exemplar representation of a crime event through the Event-Centric Knowledge Graph. The central node E representing the event is connected to eight nodes representing the eight questions. The same answer to a certain question can be extracted from different news articles and shared by multiple events.


26 Semantic AI in Knowledge Graphs
Also, the semantically similar answers can be connected each other. Different approaches can be exploited to calculate the semantic similarity of two answers. We propose to use the contextualized word embeddings of BERT (Bidirectional Encoder Representations from Transformers). BERT was introduced in 2019 and is a bidirectional transformer-based language model (Devlin et al., 2019). After training a BERT model on a consistent corpus and extracting the word embeddings, the model can be fine-tuned to perform different tasks. Word embeddings are dense vector representations of words in a lower dimensional space. Despite the static word embeddings of traditional models like Word2Vec, the contextualized word embeddings of BERT are able to capture the meaning of a word based on the context where it is used. In this way, the same word used in two different sentences can have two different vector representations. Similarity links are generated in the graph based on the value of the vectors similarity, i.e., the semantic similarity of the two answers. The resulting graph appears like the one in Figure 2.3a. Then, the objective is to create connections between the event nodes themselves to express how “connected” two events are. We generated the initial knowledge graph in Neo4j, then several operations are made on the graph using the Cypher query language to create Event-Event connections. As illustrated in Figure 2.3b, two directed connections are generated for each of the 5W + 1H questions if at least one answer is shared. The weight of the relationships is obtained summing the number of shared nodes for that question and the similarity values of the nodes with similarity higher than a certain threshold. The Cypher queries used are reported in Listing 2.1, these queries are executed for each question type.
FIGURE 2.2
Interconnection between two crime events.
Representing events in such a way allows to understand the interconnection between the news articles and, consequently, the events. Just as an example, Figure 2.2 illustrates two events (E1 and E2) sharing one node What, one Who author, and one Who victim.


Modeling Event-Centric Knowledge Graph for Crime Analysis on Online News 27
Listing 2.1: Cypher queries to generate relationships between the event nodes based on the shared answers to the 5W + 1H questions.
MATCH (c1:CrimeNews)–[w1:WHAT]->(w:What)<-[w2:WHAT](c2:CrimeNews) WHERE c1.id>c2.id WITH c1, c2, COUNT(*) AS num_count CREATE (c1)−[r:CONNECTED_SAME_WHAT]−>(c2) SET r.weight=apoc.convert.toFloat(num_count) MATCH (c1:CrimeNews)−[r1:WHAT]−>(w1:What)− [r:SIM_WHAT]−(w2:What)<−[r2:WHAT]−(c2:CrimeNews) WHERE c1.id>c2.id WITH c1, c2, r CREATE (c1)−[r_new:CONNECTED_SIM_WHAT]−>(c2) SET r_new.weight=apoc.convert.toFloat(r.weight)
FIGURE 2.3
Representation of two events and their relationships (a) and generation of directed connections between event nodes in the Event-Centric Knowledge Graph (b).


28 Semantic AI in Knowledge Graphs
MATCH (c1:CrimeNews), (c2:CrimeNews) OPTIONAL MATCH (c1)−[r1:CONNECTED_SAME_WHAT]−(c2) OPTIONAL MATCH (c1)−[r2:CONNECTED_SIM_WHAT]−(c2) WITH c1, c2, sum(r1.weight) AS sumW1, sum(r2.weight) AS sumW2 WHERE c1.id>c2.id AND apoc.convert.toFloat(sumW1+sumW2) > 0.0 CREATE (c1)−[:CONNECTED_WHAT {weight:apoc.convert.toFloat (sumW1+sumW2)}]−>(c2) CREATE (c2)−[:CONNECTED_WHAT {weight:apoc.convert.toFloat (sumW1+sumW2)}]−>(c1)
Before the identification of communities, the importance of the nodes in the obtained weighted graph is calculated by the centrality algorithm.
2.3.1 Node Centrality
In graph theory and network analysis, centrality is a metric of key importance since it helps to better understand the network and navigate through chaos while extracting information from a network. There are a lot of iterative algorithms to calculate the centrality of nodes. Each algorithm has a different perspective and assigns scores based on different factors. If considering a directed graph, each node can have incoming relationships, i.e., links incident on the node, and outgoing relationships, i.e., nodes directed at other nodes. The degree centrality (Freeman, 1978) exploits incoming and/or outgoing relationships to calculate the degree of each node. Given a graph G: = (V, E), where V is the set of vertices (i.e., nodes) and E the set of edges (i.e., relationships), the adjacency matrix ,
Av t is defined. Each element av,t of that matrix is 1 if vertex v is connected to vertex t, 0 otherwise. The score of vertex v is computed by the formula:
∑
=− =
1
11
,
xn A
v
t
n
vt
where n is the number of vertices in the graph. Therefore, in the degree centrality, the importance of a vertex depends only on the number of its neighbors. The eigenvector centrality, as described in Ruhnau (2000), aims at measuring the influence of a node in the graph. It is more suitable for undirected graph. In our graph, eigenvector centrality can measure how much an event influences another event. The algorithm is based on the idea that relationships with high-scoring nodes contribute more to the score of a node w.r.t connections with low-scoring nodes. In other words, a node connected to a few number of nodes with high scores is more important than a node connected to a higher number of low-scoring nodes. The score of node v is given by the formula:
∑∑
λλ
==
∈∈
11
()
,
x x ax
v
t Mv
t
tV
vt t


Modeling Event-Centric Knowledge Graph for Crime Analysis on Online News 29
where M(v) is the set of neighbors of v and λ is a constant. The algorithm can be applied also to weighted graph. In this case, the score of a node sent to its neighbors is multiplied by the normalized weight of the relationship. Therefore, the score depends on the weight of the relationship. Two extensions of the eigenvector centrality are the page rank and the article rank. The page rank, introduced by Brin and Page (1998), assigns a score to the nodes of the graph considering both directed and undirected edges and optional edge weights. Assuming a node v is connected to nodes { }
,...,
T1 Tn , its page rank (PR) is calculated as:
∑
=−+
=
() 1 ( )
()
1
PR v d d PR T
CT
t
n t
t
where d ∈ [0, 1) is a damping factor and PR(Tt) and C(Tt) are the page rank and the number of outgoing links of the neighbor Tt, respectively. The damping factor controls the convergence speed of page rank algorithm. A low damping factor is used to determine the score of a node based on the score received from external nodes and allows the iterations to quickly converge. In contrast with the eigenvector centrality, the idea beyond page rank is that relationships originating from low-degree nodes have a higher influence than relationships from high-degree nodes. With respect to page rank, the article rank (Li and Willett, 2009) lowers the influence of low-degree nodes. This is a more recent iterative algorithm used to measure the transitive influence of nodes in a graph. The score of node v at iteration i is given by:
∑
=−+ +
∈
−
() 1 ( )
()
()
1
AR v d d AR w
NwN
i
wN v
i
out out
in
where Nin(v) and N (w)
out denote incoming and outgoing neighbors of node v and w, respectively, d ∈ [0, 1) is a damping factor, and Nout is the average out-degree. The implementation of all the described algorithms is available in the Neo4j Graph Data Science library2 and allows the application to directed and weighted graphs, matching our use case. The algorithms take in input the name of the nodes to consider, the relationships, and their weights. Also, some configuration parameters are allowed. In the degree centrality, it is possible to specify the direction of the relationship to consider for the calculation of the degree. Since in our graph each relationship is generated in both directions, we can consider just one direction in the algorithm. In the eigenvector centrality, the node scores are normalized using the Euclidean norm. The damping factor can be specified in the configuration of the page rank and the article rank algorithms, 0.85 is the default value in Neo4j since it is the one suggested by the authors of the original paper.


30 Semantic AI in Knowledge Graphs
2.3.2 Community Detection
Community detection is usually the first step in extracting information from graphs. A community is a dense subgraph within a larger graph that corresponds to a specific function (Aviyente and Karaaslanli, 2022). We are interested in identifying groups of most densely connected nodes, i.e., communities, because if nodes are densely connected each other, the events they represent are similar. Similar events mean that the events have some characteristics in common. Community detection allows identifying these events in short time, then further crime analysis can be performed on the events in the same community. The Neo4j Graph Data Science library mentioned before offers some already implemented community detection algorithms. The label propagation algorithm (Rezaei, Far, and Soleymani, 2015) detects communities exploiting only the graph structure. It assigns to a node the label occurring with the highest frequency among its neighbors. This operation is repeated more times, iteratively. A label can quickly become dominant in a group of closely connected nodes, but will reach with difficulty sparsely connected region. At the end of the iterations, densely connected nodes have the same label that means they are part of the same community. Louvain is an iterative heuristic algorithm introduced in 2008 by Blondel et al. (2008). It tries to identify communities in a graph by optimizing the modularity score. Modularity is a numerical value between –0.5 (non-modular clustering) and 1 (fully modular clustering) that quantifies the quality of an assignment of nodes to communities and evaluate how densely connected the nodes in the same community are w.r.t. relationships outside communities. The modularity of a community c is given by the formula:
∑∑
=−
2 (2 )
c2
Qm m
in tot
where m is the sum of all of the relationship weights in the graph, ∑in is the sum of relationship weights between nodes within the community c considering each relationship twice, and ∑tot is the sum of all relationship weights of nodes within the community including relationships which link to nodes of other communities. The iterative procedure of Louvain groups nodes into communities based on how closely connected nodes are and calculates the modularity. The nodes are assigned to a different community if this change leads to increased modularity. The weakly connected components algorithm identifies groups of nodes where each node is connected to all the other nodes (Monge and Elkan, 1997). The result does not depend on the direction of the relationships. These algorithms have different perspectives; we expect the communities identified by the weakly connected components algorithm to be different from the ones of the previous two algorithms since the scope is slightly


Modeling Event-Centric Knowledge Graph for Crime Analysis on Online News 31
different. However, it will be interesting to investigate their results and make some comparisons.
2.4 Application
In this section, we show the application of our methodology to an opensource dataset of Italian news articles reporting crime events occurred in the city of Modena.
2.4.1 Dataset
The dataset contains 10,395 news articles from the Gazzetta di Modena newspaper.3 The news articles are related to some crime events occurred in the province of Modena from 2011 to 2021 and cover 13 types/categories of crimes (theft, robbery, murder, sexual violence, mistreatment, aggression, illegal sale, drug dealing, scam, fraud, money laundering, evasion, and kidnapping). The dataset has been obtained by the application of web crawler method along with several semantic approaches for information retrieval: crime categorization, named entity extraction, 5W + 1H identification, linked data mapping, geo-localization, time expression normalization, entity linking, and duplicate detection. The framework that allowed to generate the dataset has been described in Rollo and Po (2020) and Rollo, Po, and Bonisoli (2022) while details on the text categorization task developed to understand the type of crime reported in the news articles are provided in Bonisoli, Rollo, and Po (2021) and Rollo, Bonisoli, and Po (2021). The dataset is openly available1 and is the first one of its kind for the Italian language. The dataset is unbalanced on the crime category: the most news articles are related to thefts (70%), while sexual violence, money laundering, evasion, and fraud are less than 1% of the dataset.
repository.4 The answers to Why and How are rarely reported, so they are excluded from the experiments. The question for What is used to identify the stolen object(s), i.e., bike, jewels, money, car, phone, and other objects. Some answers to When indicate the date of the crime event or the moment of the day, i.e., morning, evening, and so on. The answers to Who author can be generic, such as il ladro (the thief), sometimes more specific information are indicated, e.g., the nationality or the age of the thief, or the number of thieves if the responsible for the theft is a gang. The same consideration can be done for Who victim, generic answers are il titolare del negozio (the shop owner)
The experiments described in this chapter are related to 285 news articles about thefts occurred in 2020. The answers to the 5W + 1H questions have been manually extracted from the text of the news articles by a group of bachelor students. The annotated dataset is available online in a GitHub


32 Semantic AI in Knowledge Graphs
or il proprietario dell’auto (the car owner). An example of news article and its knowledge graph is provided in Figure 2.4.
2.4.2 Modena Crime Knowledge Graph
Each news article is represented in the Modena Crime Knowledge Graph as reported in Figure 2.1. Table 2.2 reports the number of nodes and relationships, i.e., the relationships from the crime event node to the answers (incoming relationships) and the ones between the similar answers (similarity relationships). The BERT model used to calculate the similarity is the Italian cased model (Schweter, 2020). Semantic similarity was not calculated for Where nodes since usually the answer to Dove è avvenuto il crimine? (Where did the crime event happen?) is the proper name of a place, the name of a city or a specific address. Therefore, semantic similarity in this case is meaningless. In the other cases, the similarity threshold was set to 0.85. The Cypher queries have been executed to create six types of relationships, named “connected_what,” “connected_where,” “connected_when,” “connected_who_author,” “connected_who_victim,” and “connected_who_other,”
FIGURE 2.4
Exemplar news article related to a theft with the corresponding Event-Centric Knowledge
03/12/news/castelvetro-assalto-al-bar-del-parco-spariti-soldi-e-bibite-1.41294303}).
TABLE 2.2
Number of Nodes and Relationships of the Modena Crime Knowledge Graph
Node Instances Incoming rel. Similarity rel.
CrimeNews 285 – What 318 382 101 Where 270 342 When 219 255 320 WhoAuthor 268 398 257 WhoVictim 285 375 75 WhoOther 199 289 437
Graph. (News ext racted from \url{htt ps://w w w.gazzettadimodena.it/modena/cronaca/2022/


Modeling Event-Centric Knowledge Graph for Crime Analysis on Online News 33
between the CrimeNews nodes. Thus, we obtain the final weighted directed graph that consists of 285 CrimeNews nodes and 11,047 relationships. Part of the graph is shown in Figure 2.5. As can be noticed in the figure, there are some isolated groups of nodes that do not have external connections with other nodes. All the centrality algorithms described in Section 2.3.1 have been applied to the graph. The damping factor in the page rank and article rank algorithms was set to 0.5 because in our graph the number of outgoing relationships for each node is equal to the number of its incoming relationships. Analyzing the scores assigned by each algorithm, we notice that the events with the highest scores are approximately the same for all the algorithms. Table 2.3 reports the top five events and the corresponding centrality scores. These nodes are also among the ones with the highest number of relationships with other CrimeNews nodes. Seven projections of the final graph have been generated in Neo4j: one projection contains all the CrimeNews nodes and the relationships of the final graph, with the relationship weights and the four centrality scores of the nodes, while the other six projections contain only one type of relationship between the CrimeNews nodes, i.e., “connected_what,” “connected_where,” and so on. The community detection algorithms described in Section 2.3.2 have been applied to all the seven generated graphs. Comparing the communities identified on the same graph by different algorithms, we noticed that
FIGURE 2.5
Crime news nodes and their relationships.


34 Semantic AI in Knowledge Graphs
there is a very substantial overlap. This means that the result is almost the same regardless the algorithm used. Table 2.4 shows the number of communities identified by Louvain and the corresponding modularity value based on the centrality score and the type of relationships and nodes (W) included in the graph (in the table, “all”
TABLE 2.3
The Five Nodes with the Highest Centrality Scores
Crime News
ID English Title Degree Eigenvector
Article Rank
Page Rank
1917556 Hunting the blue car of Modena and Castelnuovo: three thefts in a few hours
140.812 227.173 0.515 2.888
246 Thieves in the apartment “They destroyed everything”
191.455 218.145 0.486 3.640
1773111 Thieves discovered, flight into the night and theft foiled
129.042 217.915 0.479 2.565
1743523 Tevere Street in the crosshairs: robbed and damaged two businesses
131.802 205.752 0.467 2.665
187 New alarms at the deli robbed by a gang
110.226 205.303 0.456 2.204
Note: The English title was derived from the translation of the original Italian title.
TABLE 2.4
Results of the Louvain Community Detection Based on the Relationship and Node Type (W) Included and the Centrality Algorithm Used
W
Centrality Algorithm #Community Modularity
Centrality Algorithm #Community Modularity
What Degree 156 0.900 Eigenvector 156 0.900
Where 213 0.652 213 0.652
When 142 0.689 142 0.689
Who author 106 0.255 106 0.255
Who victim 169 0.490 169 0.490
Who other 116 0.630 116 0.630
All 10 0.353 15 0.360
What Page rank 156 0.900 Article rank 156 0.900
Where 213 0.652 213 0.652
When 142 0.689 142 0.689
Who author 106 0.255 106 0.255
Who victim 169 0.490 169 0.490
Who other 116 0.630 116 0.630
All 17 0.360 15 0.350


Modeling Event-Centric Knowledge Graph for Crime Analysis on Online News 35
means that all the relationships – connected_what, connected_where, etc. are included). The highest value of modularity was reached in the What graph, this is probably due to the fact that in the other graphs the nodes are more densely connected and it is more difficult to identify the communities. Moreover, the centrality score does not seem to affect the results since the same values of modularity is reported regardless the centrality algorithm. Only when all the relationship types are included, the number of identified communities changes based on the centrality algorithm as well as the modularity values. This means that the community detection algorithm exploits the centrality scores associated to the event nodes to generate the communities. Figure 2.6 shows four graphs, which contain all the relationship types; in each graph a different centrality algorithm was used. The communities identified by Louvain are highlighted with a different color; the size of the nodes depends on the centrality score. In all the graphs, the small groups of nodes connected each other are detected as one single community in most cases. However, sometimes even if the number of nodes connected each other and with no relationship with other nodes is very low, these nodes are
FIGURE 2.6
Louvain Identified communities on the final directed graph with nodes scored by four different centrality algorithms: degree centrality (a), eigenvector centrality (b), page rank (c), and article rank (d).


36 Semantic AI in Knowledge Graphs
assigned to different communities while we expect to be part of the same community. The page rank is the algorithm that identifies the highest number of communities (17). To increase the influence of node centrality, a new weight has been associated to each relationship r(i, j), following the formula:
= ++
_2
(,) (,)
new weight weight C C
rij rij
ij
where ( , )
weightr i j is the weight already associated to the relationship that connects the node i to the node j, and C(i) and C( j) are the centrality scores of the nodes i and j, respectively. Four new weights have been calculated for each relationship, each using a different centrality algorithm. Community detection has been applied to the graph projections including the new weights, one at a time. Using the weakly connected components algorithm or the label propagation, the result is not affected by the centrality score used in the new weight. Probably, this is due to the fact that the scores of the centrality algorithm are very similar each other. Figure 2.7 shows the centrality scores of some nodes in the graph normalized by the min-max scaler. There is a clear overlap of the four lines representing the four algorithms. There are few differences in the obtained communities when Louvain is used. In this case, the highest modularity value was reached when using the weights derived from article rank and considering only the CrimeNews relationships generated by the answers to What. Indeed, as can be seen in Figure 2.8, the communities clearly identify the nodes that are densely connected each other.
2.4.3 Modena Crime Analysis
The data of the Italian Department of Public Security of the Minister of the Interior (published by Sole24Ore5) classifies the city of Modena at the 12th position among the other Italian cities based on the number of crimes reported to the police. The total number of police reports in Modena in 2021 was 26,328 (3,722 reports per 100,000 inhabitants). The first city in the national
FIGURE 2.7
Normalized scores of CrimeNews nodes according to four different centrality algorithms.


Modeling Event-Centric Knowledge Graph for Crime Analysis on Online News 37
ranking is Milan with 159,613 reports (4,866 reports per 100,000 inhabitants). According to the latest report of ISTAT,6 the most frequent crimes in Modena from 2016 to 2020 are thefts, damages, scams and computer fraud, threats, and willful injury. Figure 2.9 reports the number of the mentioned crimes reported to the authorities. As can be seen, the number of thefts decreases from 2019 to 2020. This is probably due to the lockdown caused by the COVID-19 emergency. In fact, because of the pandemic, the government had imposed severe restrictive measures, allowing only essential displacements. As a result, the population was often at home and worked from home, if allowed. On the other hand, the number of computer fraud increased (from 1,985 in 2019 to 2,773 in 2020). This kind of reports allows to give an overview of the crime situation of the city, however, there is no detailed information on the victims and the authors, the dynamics, the place where the crime occurred with specification of the address or neighborhood, and so on. Thanks to the extraction of the 5W + 1H answers from the text of the news articles and the use of graph analysis techniques, it is possible to generate these data. Also, it is possible to geolocate the crime events and generate heatmaps as we discussed in a previous work (Po and Rollo, 2018). For example, looking at the graph generated by the 285 news articles related to thefts, it is possible to know which is the most stolen items in Modena. They are the What node with the highest number of incoming relationships. In our graph, these nodes are: handbag, bicycle, safety deposit box, car, phone, wallet, and cash register. The same can
FIGURE 2.8
Communities detected by Louvain in the What graph weighted by the article rank.


38 Semantic AI in Knowledge Graphs
be done to discover the cities and the area of the city where the most crimes occur. These data can be combined with information on the activities/shops located in the same address to understand the cause of the crime events and alerts the police to monitor the situation in that area.
2.5 Conclusion and Future Work
FIGURE 2.9
Crimes reported to the authorities in the province of Modena from 2016 to 2020. (Source: ISTAT, data of the Italian Ministry of the Interior.)
The chapter presented a methodology for the construction of a knowledge graph for the representation of crime events as they are described in news articles. The Event-Centric Knowledge Graph has been generated based on the extraction of the answers to the 5W + 1H journalistic questions from the text of the news. Then, we added direct relationships in the graph among the nodes representing the events to indicate that they share some characteristics. Centrality algorithms were used to determine the importance of the individual nodes, while community detection algorithms allowed to distinguishing groups of similar nodes within the overall graph. The tool used for the creation and the analysis of the knowledge graph is Neo4j. Some experiments have been conducted on a manually annotated Italian dataset containing news articles related to thefts in the province of Modena. We focus on thefts as they


Modeling Event-Centric Knowledge Graph for Crime Analysis on Online News 39
Since the manual extraction of the 5W + 1H answers is very time consuming, as future work, we will work on automatic approaches through the question answering using BERT. This will allow to increase the amount of annotated news articles and the size of the generated knowledge graph as community detection algorithms work better in large graphs. The increasing dimension of the graph will probably improve also the influence of the centrality scores onthe final results. Moreover, further analysis of the graph could be performed to measure the similarity of the text of the news articles in the same community and identify the news articles related to the same event. This analysis should allow to create a storyline of the event. Finally, it could be interesting to explore the possibility of connecting the answers to the 5W + 1H questions to external ontologies, taxonomies, or vocabularies such as WordNet, BabelNet to better understand their similarity. This should allow to increase the number of similarity relationships. Another future work will focus on building a knowledge graph mapping information published in newspapers with respect to the new Crime Event Model, developed by Rollo, Po, and Castellucci (2023).
Disclosure Statement
The authors declare no conflict of interest.
Funding
This work is partially supported by the project “Deep Learning for Urban Event Extraction from News and Social media streams” founded by the Engineering Department “Enzo Ferrari” of the University of Modena and Reggio Emilia.
Notes
1 Italian Crime News dataset: https://paperswithcode.com/dataset/italiancrime-news 2 https://neo4j.com/docs/graph-data-science/
are the most frequent crime in Modena. However, the methodology developed depends neither on the language of the news articles nor on the type of event described since all the events can be represented by the answers to the 5W + 1H questions. The results of the experiments are promising and demonstrate how it is possible to develop crime analysis techniques by using knowledge graph.


40 Semantic AI in Knowledge Graphs
3 https://gazzettadimodena.gelocal.it 4 https://github.com/federicarollo/W-1H-extraction-in-news-articles-for-eventdetection 5 https://lab24.ilsole24ore.com/indice-della-criminalita/?Modena 6 http://dati.istat.it/Index.aspx?DataSetCode=dccv_delittips
References
Abdul Jalil, Masita, Chia Pui Ling, Noor Maizura Mohamad Noor, and Fatihah Mohd. 2017. “Knowledge Representation Model for Crime Analysis.” Procedia Computer Science 116: 484–491. Discovery and innovation of computer science technology in artificial intelligence era: The 2nd International Conference on Computer Science and Computational Intelligence (ICCSCI 2017), https:// www.sciencedirect.com/science/article/pii/S1877050917321178 Auer, Sören, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary G. Ives. 2007. “DBpedia: A Nucleus for a Web of Open Data.” In The Semantic Web, 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, ISWC 2007 + ASWC 2007, Busan, Korea, November 11–15, 2007, edited by Karl Aberer, Key-Sun Choi, Natasha Fridman Noy, Dean Allemang, Kyung-Il Lee, Lyndon J. B. Nixon, Jennifer Golbeck, Peter Mika, Diana Maynard, Riichiro Mizoguchi, Guus Schreiber, and Philippe Cudré-Mauroux, Vol. 4825 of Lecture Notes in Computer Science, 722–735. Springer. https://doi. org/10.1007/978-3-540-76298-0_52 Aviyente, Selin, and Abdullah Karaaslanli. 2022. “Explainability in Graph Data Science: Interpretability, Replicability, and Reproducibility of Community Detection.” IEEE Signal Processing Magazine 39 (4): 25–39. https://doi.org/10.1109/ MSP.2022.3149471. Blondel, Vincent D, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. 2008. “Fast Unfolding of Communities in Large Networks.” Journal of Statistical Mechanics: Theory and Experiment 2008 (10): P10008. https://doi. org/10.1088/1742-5468/2008/10/p10008 Bonisoli, Giovanni, Federica Rollo, and Laura Po. 2021. “Using Word Embeddings for Italian Crime News Categorization.” In Proceedings of the 16th Conference on Computer Science and Intelligence Systems, Online, September 2–5, 2021, edited by Maria Ganzha, Leszek A. Maciaszek, Marcin Paprzycki, and Dominik Slezak, Vol. 25 of Annals of Computer Science and Information Systems, 461–470. https:// doi.org/10.15439/2021F118 Brin, Sergey, and Lawrence Page. 1998. “The Anatomy of a Large-Scale Hypertextual Web Search Engine.” Computer Networks 30 (1–7): 107–117. https://doi.org/ 10.1016/S0169-7552(98)00110-X Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019,


Modeling Event-Centric Knowledge Graph for Crime Analysis on Online News 41
Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), edited by Jill Burstein, Christy Doran, and Thamar Solorio, 4171–4186. Association for Computational Linguistics. https://doi.org/10.18653/v1/n19-1423 Elezaj, Ogerta, Sule Yildirim Yayilgan, Edlira Kalemi, Linda Wendelberg, Mohamed Abomhara, and Javed Ahmed. 2019. “Towards Designing a Knowledge GraphBased Framework for Investigating and Preventing Crime on Online Social Networks.” In E-Democracy - Safeguarding Democracy and Human Rights in the Digital Age – 8th International Conference, e-Democracy 2019, Athens, Greece, December 12–13, 2019, Proceedings, edited by Sokratis K. Katsikas and Vasilios Zorkadis, Vol. 1111 of Communications in Computer and Information Science, 181–195. Springer. https://doi.org/10.1007/978-3-030-37545-4_12 Freeman, Linton C. 1978. “Centrality in social networks conceptual clarification.” Social Networks 1 (3): 215–239. https://www.sciencedirect.com/science/article/ pii/0378873378900217 Gottschalk, Simon, and Elena Demidova. 2018. “EventKG: A Multilingual EventCentric Temporal Knowledge Graph.” In The Semantic Web – 15th International Conference, ESWC 2018, Heraklion, Crete, Greece, June 3–7, 2018, Proceedings, edited by Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler, Raphaël Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam, Vol. 10843 of Lecture Notes in Computer Science, 272–287. Springer. https://doi. org/10.1007/978-3-319-93417-4_18 Guo, Kaihao, Tianpei Jiang, and Haipeng Zhang. 2020. “Knowledge Graph Enhanced Event Extraction in Financial Documents.” In 2020 IEEE International Conference on Big Data (IEEE BigData 2020), Atlanta, GA, USA, December 10–13, 2020, edited by Xintao Wu, Chris Jermaine, Li Xiong, Xiaohua Hu, Olivera Kotevska, Siyuan Lu, Weija Xu, Srinivas Aluru, Chengxiang Zhai, Eyhab Al-Masri, Zhiyuan Chen, and Jeff Saltz, 1322–1329. IEEE. https://doi.org/10.1109/BigData50022.2020.9378471 Jedrzejek, Czeslaw, and Jaroslaw Bak. 2012. “Application of an Ontology-Based Model to a Wide-Class Fraudulent Disbursement Economic Crimes.” In Multimedia and Internet Systems: Theory and Practice – Proceedings of the 8th International Conference MISSI 2012, Wrocław, Poland, 2012, edited by Aleksander Zgrzywa, Kazimierz Choros, and Andrzej Sieminski, Vol. 183 of Advances in Intelligent Systems and Computing, 109–118. Springer. https://doi.org/10.1007/978-3-642-32335-5_11 Kalemi, Edlira, Sule Yildirim Yayilgan, Elton Domnori, and Ogerta Elezaj. 2017. “SMONT: an ontology for crime solving through social media.” International Journal of Metadata, Semantics and Ontologies 12 (2/3): 71–81. https://doi.org/ 10.1504/IJMSO.2017.10011827 Knez, Timotej. 2022. “Multi-task Learning for Automatic Event-Centric Temporal Knowledge Graph Construction.” In Research Challenges in Information Science, edited by Renata Guizzardi, Jolita Ralyté, and Xavier Franch, 811–818. Springer International Publishing, Berlin/Heidelberg, Germany. Kuculo, Tin. 2022. “Comprehensive Event Representations Using Event Knowledge Graphs and Natural Language Processing.” In Companion Proceedings of the Web Conference 2022, WWW ‘22, New York, NY, USA, 359–363. Association for Computing Machinery. https://doi.org/10.1145/3487553.3524199 Lakshika, M. V. P. T., and H. A. Caldera. 2021. “Knowledge Graphs Representation for Event-Related E-News Articles.” Machine Learning Knowledge Extraction 3 (4): 802–818. https://doi.org/10.3390/make3040040


42 Semantic AI in Knowledge Graphs
Li, Jiang, and Peter Willett. 2009. “ArticleRank: A PageRank-Based Alternative to Numbers of Citations for Analysing Citation Networks.” Aslib Proceedings 61 (6): 605–618. https://doi.org/10.1108/00012530911005544 Li, Zhipeng, Shanshan Feng, Jun Shi, Yang Zhou, Yong Liao, Yangzhao Yang, Yangyang Li, Nenghai Yu, and Xun Shao. 2023. “Future Event Prediction Based on Temporal Knowledge Graph Embedding.” Computer Systems Science and Engineering 44 (3): 2411–2423. https://doi.org/10.32604/csse.2023.026823. Mahdisoltani, Farzaneh, Joanna Biega, and Fabian M. Suchanek. 2015. “YAGO3: A Knowledge Base from Multilingual Wikipedias.” In Seventh Biennial Conference on Innovative Data Systems Research, CIDR 2015, Asilomar, CA, USA, January 4–7, 2015, Online Proceedings, www.cidrdb.org. http://cidrdb.org/cidr2015/Papers/ CIDR15_Paper1.pdf Monge, Alvaro E., and Charles Elkan. 1997. “An Efficient Domain-Independent Algorithm for Detecting Approximately Duplicate Database Records.” In Workshop on Research Issues on Data Mining and Knowledge Discovery, DMKD 1997 in cooperation with ACM SIGMOD ‘97, Tucson, AZ, USA, May 11, 1997.
Onnoom, Boonyarin, Sirapat Chiewchanwattana, Khamron Sunat, and Nutcharee Wichiennit. 2014. “An Ontology Framework for Recommendation about a Crime Scene Investigation.” In 2014 14th International Symposium on Communications and Information Technologies (ISCIT), 176–180.
Park, N., F. Liu, P. Mehta, D. Cristofor, C. Faloutsos, and Y. Dong. 2022. “EvoKG: Jointly Modeling Event Time and Network Structure for Reasoning over Remporal Knowledge Graphs.” In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM ‘22). Association for Computing Machinery, New York, NY, USA, 794–803. https://doi.org/10.1145/3488560.3498451. Peppes, N., T. Alexakis, E. Adamopoulou, K. Remoundou, and K. Demestichas. 2020. “A Semantic Engine and an Ontology Visualization Tool for Advanced Crime Analysis.” Procedia Computer Science 176: 1829–1838. Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020, https://www.sciencedirect.com/science/article/pii/S1877050920321244 Po, Laura, and Federica Rollo. 2018. “Building an Urban Theft Map by Analyzing Newspaper Crime Reports.” In 2018 13th International Workshop on Semantic and Social Media Adaptation and Personalization (SMAP), Zaragoza, Spain, 2018, pp. 13–18, doi: 10.1109/SMAP.2018.8501866. Po, Laura, Federica Rollo, and Raquel Trillo Lado. 2016. “Topic Detection in Multichannel Italian Newspapers.” In Semantic Keyword-Based Search on Structured Data Sources COST Action IC1302 Second International KEYSTONE Conference, IKC 2016, ClujNapoca, Romania, September 8–9, 2016, Revised Selected Papers, edited by Andrea Calì, Dorian Gorgan, and Martín Ugarte, Vol. 10151 of Lecture Notes in Computer Science, 62–75. https://doi.org/10.1007/978-3-319-53640-8_6 Rezaei, Aria, Saeed Mahlouji Far, and Mahdieh Soleymani. 2015. “Near LinearTime Community Detection in Networks with Hardly Detectable Community Structure.” In Proceedings of the 2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining 2015, ASONAM ‘15, New York, NY, USA, 65–72. Association for Computing Machinery. https://doi. org/10.1145/2808797.2808903 Robinson, David, and Chris Scogings. 2018. “The Detection of Criminal Groups in Real-world Fused Data: Using the Graph-mining Algorithm “GraphExtract”.” Security Informatics 7 (1): 2. https://doi.org/10.1186/s13388-018-0031-9


Modeling Event-Centric Knowledge Graph for Crime Analysis on Online News 43
Rollo, Federica. 2017. “A key-entity graph for clustering multichannel news: student research abstract.” In Proceedings of the Symposium on Applied Computing, SAC 2017, Marrakech, Morocco, April 3–7, 2017, edited by Ahmed Seffah, Birgit Penzenstadler, Carina Alves, and Xin Peng, 699–700. ACM. https://doi. org/10.1145/3019612.3019930 Rollo, Federica, Giovanni Bonisoli, and Laura Po. 2021. “Supervised and Unsupervised Categorization of an Imbalanced Italian Crime News Dataset.” In Information Technology for Management: Business and Social Issues – 16th Conference, ISM 2021, and FedCSIS-AIST 2021 Track, Held as Part of FedCSIS 2021, Virtual Event, September 2–5, 2021, Extended and Revised Selected Papers, edited by Ewa Ziemba and Witold Chmielarz, Vol. 442 of Lecture Notes in Business Information Processing, 117–139. Springer. https://doi.org/10.1007/978-3-030-98997-2_6 Rollo, Federica, and Laura Po. 2020. “Crime Event Localization and Deduplication.” In The Semantic Web – ISWC 2020 – 19th International Semantic Web Conference, Athens, Greece, November 2–6, 2020, Proceedings, Part II, edited by Jeff Z. Pan, Valentina A. M. Tamma, Claudia d’Amato, Krzysztof Janowicz, Bo Fu, Axel Polleres, Oshani Seneviratne, and Lalana Kagal, Vol. 12507 of Lecture Notes in Computer Science, 361–377. Springer. Rollo, F., Po, L. (2022). Knowledge Graphs for Community Detection in Textual
Martín-Moncunill, D. (eds) Knowledge Graphs and Semantic Web. KGSWC 2022. Communications in Computer and Information Science, vol 1686. Springer, Cham. https://doi.org/10.1007/978-3-031-21422-6_15 Rollo, Federica, Laura Po, and Giovanni Bonisoli. 2022. “Online News Event Extraction for Crime Analysis.” In Proceedings of the 30th Italian Symposium on Advanced Database Systems, SEBD 2022, Tirrenia (PI), Italy, June 19–22, 2022, edited by Giuseppe Amato, Valentina Bartalesi, Devis Bianchini, Claudio Gennaro, and Riccardo Torlone, Vol. 3194 of CEUR Workshop Proceedings, 223–230. CEUR-WS.org. http://ceur-ws.org/Vol-3194/paper28.pdf Rollo, Federica, Laura Po, and Alessandro Castellucci. 2023. “CEM: An Ontology for Crime Events in Newspaper Articles.” In Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing, SAC 2023, Tallinn, Estonia, March 27–31, 2023, ACM. https://doi.org/10.1145/3555776.3577862 Rospocher, Marco, Marieke van Erp, Piek Vossen, Antske Fokkens, Itziar Aldabe, German Rigau, Aitor Soroa, Thomas Ploeger, and Tessel Bogaard. 2016. “Building Event-Centric Knowledge Graphs from News.” Journal of Web Semantics 37–38: 132–151. https://www.sciencedirect.com/science/article/pii/ S1570826815001456 Ruhnau, Britta. 2000. “Eigenvector-Centrality — A Node-centrality?” Social Networks 22 (4): 357–365. https://www.sciencedirect.com/science/article/pii/ S0378873300000319 Schweter, Stefan. 2020. “Italian BERT and ELECTRA Models.” https://doi.org/10.5281/ zenodo.4263142 Szekely, Pedro A., Craig A. Knoblock, Jason Slepicka, Andrew Philpot, Amandeep Singh, Chengye Yin, Dipsy Kapoor, et al. 2015. “Building and Using a Knowledge Graph to Combat Human Trafficking.” In The Semantic Web – ISWC 2015 – 14th International Semantic Web Conference, Bethlehem, PA, USA, October 11–15, 2015, Proceedings, Part II, edited by Marcelo Arenas, Óscar Corcho, Elena Simperl, Markus Strohmaier, Mathieu d’Aquin, Kavitha Srinivas, Paul Groth,
Data. In: Villazón-Terrazas, B., Ortíz-Rodriguez, F., Tiwari, S., Sicilia, MA.,


44 Semantic AI in Knowledge Graphs
Michel Dumontier, Jeff Heflin, Krishnaprasad Thirunarayan, and Steffen Staab, Vol. 9367 of Lecture Notes in Computer Science, 205–221. Springer. https://doi. org/10.1007/978-3-319-25010-6_12 Venkata Srimukh, P., and S. Shridevi. 2020. “Ontology-Based Crime Investigation Process.” In Advances in Smart Grid Technology, edited by Pierluigi Siano and K. Jamuna, Singapore, 497–509. Springer Singapore. Vrandecic, Denny. 2012. “Wikidata: A New Platform for Collaborative Data Collection.” In Proceedings of the 21st World Wide Web Conference, WWW 2012, Lyon, France, April 16–20, 2012 (Companion Volume), edited by Alain Mille, Fabien Gandon, Jacques Misselis, Michael Rabinovich, and Steffen Staab, 1063–1064. ACM. https://doi.org/10.1145/2187980.2188242. Yan, Zhihua, and Xijin Tang. 2022. “Hierarchical Storyline Generation Based on Eventcentric Temporal Knowledge Graph.” In Knowledge and Systems Sciences, edited by Jian Chen, Takashi Hashimoto, Xijin Tang, and Jiangning Wu, Singapore, 149–159. Springer Nature Singapore.


DOI: 10.1201/9781003313267-3 45
3
Semantic Natural Language Processing for Knowledge Graphs Creation
Cameron De Saa, Edlira Vakaja, Hossein Ghomeshia, and Ryan McGranaghanb,c
aBirmingham City University, Natural Language Processing Lab, UK
bOrion Space Solutions. Louisville, CO, USA
cNASA Jet Propulsion Laboratory, Greenbelt, MD, USA
CONTENTS
3.1 Introduction.................................................................................................. 46 3.1.1 Background....................................................................................... 48 3.1.2 Research Questions ......................................................................... 49 3.2 Literature Review......................................................................................... 50 3.2.1 Ontology Learning .......................................................................... 50 3.2.2 Scientific Efforts and Ontologies ................................................... 52 3.2.3 Named Entity Recognition............................................................. 53 3.2.4 NLP Models for Ontology Learning............................................. 55 3.3 From Text to Ontology ................................................................................ 57 3.3.1 NLP in Ontology Learning ............................................................ 59 3.3.2 Prototype Evaluation....................................................................... 62 3.4 Experimental Study.....................................................................................63 3.4.1 Overview of the Proposed System ................................................64 3.4.1.1 Data Sourcing ....................................................................64 3.4.1.2 Data Preprocessing ...........................................................64 3.4.1.3 Model Construction..........................................................64 3.4.2 Data Sourcing ...................................................................................64 3.4.2.1 Heliophysics Text Corpus ................................................64 3.4.2.2 CfHA Meeting Notes........................................................ 66 3.4.2.3 Discussion of Approach ................................................... 67 3.4.2.4 Ethical Considerations...................................................... 68 3.4.3 Outline of Model Objectives .......................................................... 68 3.4.3.1 Objective 1: NER Optimization....................................... 68 3.4.3.2 Objective 2: Entity Relationship Modeling Optimization ..................................................................... 69 3.4.4 Model Implementation.................................................................... 69 3.4.4.1 Text Corpus Overview ..................................................... 69


46 Semantic AI in Knowledge Graphs
3.1 Introduction
Throughout history, humans have striven to assign categorical meaning to concepts in the world around them. However, it was not until 1613 that these practices became a systematic field of study [78], when the term “ontology” (or “ontologia”) was separately coined by the philosophers [24] in his Lexicon philosophicum and [45] in his Theatrum philosophicum. Ontologics in the philosophical sense refers to a field of study that focuses on the nature of being and the conception of entities and things [40]. However, this is not the only definition. A computational ontology concerns itself with constructing a conceptual model of what it means for a domain to exist, and assign meaning and relationships to the entities in that domain [57]. As a result, a computational ontology provides an effective method for modeling a particular conceptual domain and the entities and relationships therein. Computational ontologies have wide-ranging applications. Some research domains that have been modeled by ontologies include biomedicine [38, 73], historical research [65], and space data [71, 72, 76]. Although these topics are disparate, they demonstrate that computational ontologies are effective tools for modeling complex, interrelated, heterogeneous data. They can also be used to support a variety of tasks, such as classification, data exploration, discovering new topics, and detecting research communities [75]. These factors make ontologies suited for representing information about scientific fields. Overall, being able to capture a domain in an ontology presents particular advantages, such as making complex information accessible, discovering new connections, and facilitating the ability to explore data.
3.4.4.2 Preprocessing for Text Mining (TM).............................. 70 3.4.4.3 Named Entity Recognition (NER) Using spaCy........... 71 3.4.4.4 Entity and Relationship Extraction Using POS Tagging ............................................................................... 72 3.5 Results and Analysis ................................................................................... 75 3.5.1 Exploratory Analysis....................................................................... 75 3.5.1.1 NER Performance Metrics ............................................... 75 3.5.1.2 NER Word Embeddings and Clustering ....................... 76 3.5.1.3 Ontology Analysis ............................................................ 79 3.6 Discussion.....................................................................................................80 3.6.1 Text Mining Implementation .........................................................80 3.6.2 Discussion of Spacy NER Model ...................................................80 3.6.3 Automated vs. Semi-automated vs. Manual Ontology Creation ............................................................................................. 82 3.7 Conclusion .................................................................................................... 82 References............................................................................................................... 84


Semantic Natural Language Processing for Knowledge Graphs Creation 47
However, there are specific challenges related to the development of computational ontologies. The complexity of a domain may mean that scalability is difficult, particularly when manually adding data. Additionally, data properties such as multiple data types, continuous evolution, expansive content, the semantic nature of data, and varying levels of relationships may make information difficult to capture [82]. Tools such as Protégé contain a suite of tools to enable ontology development, including automatic reasoners that create inferences between data [66, 73], although manual intervention is still needed to ensure accuracy. Additionally, evaluating the efficacy and validity of an ontology is an essential step for ensuring its quality. Nevertheless, this aspect of development has been frequently underreported in prior research [42]. Ontology evaluation is also complex, often requiring the manual knowledge of domain experts. Overall, there are many issues related to ontology development, particularly over the matter of how to efficiently capture the complexity of a topic. Natural language processing (NLP) is a field of Machine Learning (ML) that contains a theoretically motivated range of computational techniques for the analysis and representation of naturally occurring texts at one or more levels of linguistic analysis in order to achieve a human-like level of understanding for a range of tasks and applications [41]. Because NLP techniques can be applied to a text or data corpus, they can be used for automatic or semi-automatic ontology generation. The intersection of NLP ML techniques and ontology creation is known as ontology learning [50]. Many ontologies have utilized NLP techniques, including ones for risk management [51], biomedicine [3, 44], and clinical texts [35]. Owing to the complex nature of many domains, NLP techniques can be helpful for parsing relevant text corpuses, and ontology learning can be used to establish relationships and entities. Then, this may allow for additional resources to be directed toward other aspects of a scientific effort. However, NLP techniques often rely on a rigid corpus of rules in order to create entities and relationships, which tend to be limited to the domain being modeled, and are not easily modified for other domains. In short, NLP techniques have shown promise for the semi-automatic or automatic population of ontologies. Additional methods have been proposed to address the problem of domainrestricted NLP models. One such technique was utilized by Ayadi et al. [3], who utilized a deep learning-based ontology population system to enhance a biomedical network ontology. An additional technique utilized by Elnagar et al. [15] employed Complex Embeddings (ComplEx) to ensure completeness and reference ontologies to refine the model. However, there is room for additional research into training an NLP model on a text corpus to automatically populate an ontology for a complex scientific domain. Information extraction (IE) refers to the process of extracting structured information from semi-structured or unstructured text [64]. IE pipelines (IEPs) have been formulated for several IE efforts, ranging from scientific literature [89] to hotel information [82]. Examining these efforts facilitates the creation of an IEP, so that relevant entities and relationships can be generated from a relevant text corpus. This IEP begins with the preprocessing stage,


48 Semantic AI in Knowledge Graphs
where the text data is cleaned of extraneous information. It then goes through a named entity recognition (NER) stage, where significant entities in the text are extracted based on training information [1]. Finally, a knowledge graph (KG) showing the data nodes and relationships between them can be generated, and analysis can be done on the results of the model. Consequently, the aim of IE is to utilize technology in order to provide meaning to text. This research utilizes NASA Centre for Helio-Analytics (CfHA) data and NLP techniques to create an ontology learning model centering on the domain of heliophysics. This is done with the goal of examining the techniques used to develop the model in order to determine their efficacy and how they may be applied to future models. As a result, this model provides a case study for examining NLP methods for ontology learning. Furthermore, prior models have been examined in Section 3.2 in order to establish a foundation for this research. This chapter is organized as follows:
1. Section 3.1 discusses the purpose of the research, the driving questions, and the objectives that organize and guide this effort.
2. Section 3.2 provides a review of existing literature on ontologies and NER models that have been developed for scientific domains.
3. Section 3.3 discusses the journey from text to ontology; from collecting data to developing the NER and ontology learning parts of the model, and evaluating the results.
4. Section 3.4 lays out the results of the model, including a review of the reasoning behind the choices made when developing each part.
5. Section 3.5 provides an exploratory analysis of data generated from each major part of the model.
6. Section 3.6 presents the results along with the research questions, placing the results in context with prior literature. It also brings up the limitations of this project.
7. Section 3.7 summarizes the results of the experiments and provides an overview of areas for further research.
3.1.1 Background
The National Aeronautics and Space Administration (NASA) is an organization that employs individuals who work in a broad range of specializations across space-related disciplines. Consequently, employees collaborate on research projects with diverse aims that reflect their different domains of knowledge. As collaboration facilitates scientific discovery, there is the need to explore methods to enable knowledge-sharing. For example, the NASA Center for Helio-Analytics (CfHA) is a cross-disciplinary community that develops methodologies centered on applications for emerging technologies and techniques to hasten the development of space physics research. McGranaghan et al. [56] developed a KG as a response to the need


Semantic Natural Language Processing for Knowledge Graphs Creation 49
for facilitating cross-discipline knowledge-sharing. This was proposed to address difficulties faced by members of the CfHA community when attempting to discover information about other members and projects. As a result, this led to the creation of the CfHA ontology, which was manually developed. By creating a centralized means of accessing community information, gaps in skills and knowledge can be identified, and cross-group collaboration facilitated. In contrast, alternative methods of knowledge representation may face shortcomings when used for these ends. For example, spreadsheets are less structured when compared to many programming languages, which may lead users to cause redundancy, loss of data, and corruption [11]. Furthermore, spreadsheets cannot be searched or queried in the same way as a KG. Meanwhile, databases contain organizational structures that can more accurately model real-world domains, but are often limited by issues of scalability and multi-tenancy when attempting to provide information to many users [32]. Furthermore, querying information from databases frequently necessitates specialist knowledge, unless there is an interface to simplify the process. Therefore, a KG provides a promising avenue for representing a complex domain through its representation of entity nodes and the relationships between them. Owing to the multidisciplinary emphasis of the CfHA and the projects that its members are involved with, it presents the opportunity for a case study on the benefits and applications of ontology learning in scientific domains. Indeed, there are several potential drawbacks to a purely manual approach that may be addressed by an NLP-based model. For one, although the ontology can be manually updated, it may be difficult to accurately capture current developments in the CfHA in a timely fashion. In addition, introducing automation to ontology population efforts can free up human resources that can go toward other areas of a project. Furthermore, although CfHA data is used to train the ontology learning model, the implications of this research may inform broader approaches to information representation and KG population. In summary, there is room to study the uses for NLP techniques, and how they can be harnessed to construct a model based on CfHA-related data to automatically populate a CfHA ontology in a way that facilitates knowledgesharing between the group. In the process, methods for increasing the accuracy and reliability of the model can be analyzed.
3.1.2 Research Questions
In this chapter, the following Research Questions (RQ) around ontology learning and how to utilize state-of-the-art NLP techniques for populating ontologies are addressed.
• RQ1: What are the current state-of-the-art approaches to ontology learning?
• RQ2: Which NLP techniques for NER are the most relevant to the automatic/semi-automatic population of ontologies?


50 Semantic AI in Knowledge Graphs
• RQ3: How can an automated approach to ontology population facilitate information sharing between members of the CfHA?
• RQ4: How can an automated approach toward ontology population benefit a broader research effort?
3.2 Literature Review
In order to formulate an effective model, research into prior efforts must be undertaken. Critical evaluation of what sources are available is important for understanding both what methodologies have already been formulated, and where there are gaps that can be addressed.
3.2.1 Ontology Learning
The history of ontology learning as a field is inextricable from developments in ML and the Semantic Web. At the beginning of the 21st century, the web was inefficient due to a lack of standardization and quality control measures [2]. As a result, it was difficult to piece together meaningful information. The semantic web was popularized by Berners-Lee et al. [8], who published an article in Scientific American that provided an overview of concepts that were vital to the scientific web. Notably, this article detailed the role of ontologies in providing a formally structured method of representing data in a particular domain. Another article on the relevance of ontologies for facilitating cooperative information discovery was published in AI Magazine, where Maedche and Staab [48] used knowledge portals, many of which were domain- or market-specific, as case studies for how ontologies could facilitate better information access. Ontologies were proposed as a method for organizing the semantic web, but they had use in broader scientific efforts. Maedche and Staab [49] elaborated further on the potential utility of ontologies by proposing the concept of ontology learning. The vision that they put forth built upon structured, semi-structured, or unstructured data to support a semi-automatic, cooperative ontology engineering process. NLP models for ontology learning have been developed since the early 2000s [50, 55]. A two-stage methodology was proposed by Valarakos et al. [85] for automatically populating an allergens ontology. Their model utilized a NER and classification (NERC) model, which was trained by using Hidden Markov Models (HMMs). For the allergens ontology, semi-automation meant that a domain expert would only need to be consulted before the second processing stage, when extraction rules for populating the ontology needed to be created. This methodology was an early adopter of NLP techniques for ontology learning and used Precision and Recall metrics to determine the accuracy of their work.


Semantic Natural Language Processing for Knowledge Graphs Creation 51
Nevertheless, the authors were not able to robustly test the entirety of their performance approach and may have benefited from a narrower methodology scope. Overall, the early history of ontology learning was sparser compared to more recent efforts, but the usefulness of NLP techniques in ontology learning was reflected in research at the time. Ontology learning research efforts became more commonplace in the 2010s. As the field has developed, increasingly sophisticated NLP techniques such as deep learning have been employed. For example, Ayadi et al. [3] utilized a variety of biological documents to populate an already-existing biomolecular network ontology. Their methodology involved using tokenization to preprocess the biological documents, followed by normalization to convert words into a unified format. Word2vec, an algorithm that employs shallow neural networks to learn word embeddings, was employed for the representation of words as vectors. Word2vec was specifically utilized to group semantically close words together, whilst moving unrelated words away from each other. This is useful when attempting to create entities, as entity recognition algorithms may face problems with syntactic disambiguation. When assessing the model, Ayadi et al. [3] used Precision, Recall, and the F-measure to determine the accuracy of their model, whereas in comparison, Valarakos et al. [85] used only Precision and Recall to assess their model. The model developed by Ayadi et al. [3] ultimately performed well in Precision for all measures, whereas Recall fell behind Precision, and the F-measure fell inbetween. What this demonstrates is that an approach using shallow neural networks may benefit from additional training in order to ensure that labels are accurately classified. Furthermore, more cutting-edge NLP techniques may not perform better in all measures than traditional NLP approaches. One additional ontology learning effort was made by Youn et al. [90]. Similarly to [3], they utilized Word2Vec for word embeddings with the aim of populating a food ontology. Unlike [3], they used the GloVe and fastText algorithms to test the efficacy of pre-trained word embeddings. Youn et al. [90] used Precision as the metric for evaluating the algorithms, which demonstrated that those that employed embedding performed better. However, only Precision was used as an evaluation metric. Ayadi et al. [3] used Precision, Recall, and the F-measure to evaluate their algorithm, which demonstrated that a high Precision score may not correlate to a high Recall score, and vice versa. Although the algorithms that Youn et al. [90] tested demonstrated that embedded algorithms scored higher than non-embedded algorithms in Precision, this is not the sole metric that can be used to determine the efficacy of an algorithm. Recall, for instance, would have revealed which proportion of true positives is accurately classified, and the F-score would demonstrate the trade-off between Recall and Precision. Nevertheless, the robustness in comparing different algorithms is important in establishing which models outperform others and analyzing why that may be the case. An initial analysis of existing research demonstrates that ontology learning efforts have significantly evolved in terms of methodology and NLP


52 Semantic AI in Knowledge Graphs
techniques used since they were first introduced. Furthermore, new innovations in NLP have opened up a range of new possibilities for facilitating the IEP through techniques such as shallow neural networks. Nevertheless, there is room to explore how the accuracy of ontology learning models can be improved, particularly when assessing how models extract entities.
3.2.2 Scientific Efforts and Ontologies
The original use for an ontology is rooted in philosophy. Lorhard [45] in Theatrum philosophicum and Gōckel [24] in Lexicon philosophicum both independently used the term, as “ontologia.” The term was used in reference to metaphysics, which is a discipline that studies the philosophical nature of existence. Subsequent philosophical ontologists sought to provide a definitive and exhaustive classification of all entities in all spheres of being [79]. The definition of an ontology in computer science is similar, as ontologies were originally conceived to construct meaning for the semantic web. Ontology-based formalisms were used to add structure where none had previously existed, and the W3C Web Ontology Language (OWL) was developed to create a language for constructing ontologies. Similarly, the Resource Description Framework Schema (RDF/S) was developed as a data model for storing metadata about an ontology [19]. This transition from philosophy to computer science is united by an aim of representing the nature of a particular domain. Although computer science ontologies were initially developed to assign meaning to the semantic web, they came to be used as a method of modeling data in particular domains. Munir and Sheraz Anjum [61] detail the use of ontologies as an alternative to databases for managing information. The authors specify that a major advantage of using a domain ontology is its ability to define a semantic model of the data combined with the associated domain knowledge. Zemmouchi-Ghomari et al. [91] detail several primary differences between ontologies and databases. Where databases are intended for the closed-world storage of data, ontologies are an open-world representation of a domain. Ontology schemata tend to be more complex than databases, and ontologies tend to be independent of a specific application or problem. These features make them appropriate for modeling scientific domains. Managing space-related data is a particularly pertinent area of application for ontologies. Rovetto [71] mentions that ontologies are useful for the knowledge management of space-related disciplines due to their knowledgerich nature. The author then provides an overview of existing ontologies for orbital space, the NASA taxonomy, and planetary data. Many of the projects described are currently ongoing, which demonstrates the open-world, evolving nature of ontologies and provides support for the necessity of efficient ontology population methods. Describing space systems through ontologies can help overcome challenges associated with non-ontology methods such as semantics being ignored for ease of implementation, missing discipline


Semantic Natural Language Processing for Knowledge Graphs Creation 53
context information, and there being no existing knowledge capture and mechanism for applying knowledge [28]. A project that has benefitted from the inclusion of an ontology include the Ontology-driven Interactive Search Environment for Earth Sciences (ODISEES). Rutherford et al. [74] describe the purpose of this ontology as aiding researchers aiming to find usable data among a proliferation of closely related data. This effort was aimed at making data easily available to the public, serving both scientists and researchers as well as laymen. An automated approach may be beneficial, considering the scale of the data that the ODISEES handles. Another effort is the Orbital Debris Ontology (ODO), which Rovetto et al. [72] describe as an ontology for monitoring the amount of orbital detritus, particularly the threat it poses to assets in orbit. As a result, timeliness in populating the ontology is vital to ensure that accurate, up-to-date data is reflected. The authors also mention that ontologies are relatively easy to modify, as they do not require code maintenance, and that changing domain knowledge can be reflected in an ontology. Nevertheless, ontologies that reflect domains with time-sensitive information may be hindered if some automation is not employed, especially in larger ontologies. However, the high-risk nature of tracking orbital debris means that some human intervention is still needed.
3.2.3 Named Entity Recognition
One of the most crucial steps of the IEP is the process of extracting meaningful entities from a text corpus. This procedure is known as NER. These NE are nouns – people, places, or things. For example, NER can involve the extraction of Persons, Locations, or Organizations from a selected text [53]. This process is one stage in the information extraction pipeline (IEP), which refers to the whole procedure for taking a text corpus and converting it into data that is meaningful for a particular objective [82]. The Pipeline can be visualized in Figure 3.1. Out of all steps in the IEP, NER is perhaps the most important for ontology creation, as it is this step that deals with identifying and classifying texts into pre-defined ontological classes. Named entities (NEs) often bear important information and must be recognized and translated appropriately, and they are important for the construction of a domain grammar [92]. Consequently, it is worth examining existing research and the efficacy of methodologies for NER.
In conclusion, ontologies present a promising addition to scientific efforts, particularly in domains that are like the CfHA ontology that this chapter uses as a case study. Ontologies also present certain advantages over databases for modeling complex domains. Furthermore, the case studies described by Rutherford et al. [74], Rovetto [71], and Rovetto et al. [72] demonstrate how ontologies are useful for a scientific effort by allowing for large quantities of data to be quickly sorted through and analyzed. These case studies also demonstrate the potential use of automation in ontology population to reduce the need for human intervention.


54 Semantic AI in Knowledge Graphs
Batbaatar and Ryu [7] present a recurrent neural network approach to ontology-based NER based on Twitter messages. Their work utilized the Pytorch library to implement the BiLSTM-CRF model for NER, which was applied to a corpus of health data extracted from Twitter. The BiLSTM-CRF model has four layers. The embedding layer examines embedding features, character features, and additional word features. The BiLSTM layer learns contextual information. The CRF layer calculates tagging scores for word input. Finally, the Viterbi layer is used to find a tag sequence to maximize the tagging scores. The algorithm evaluation was based on Precision, Recall, and F-score measures alongside a comprehensive comparison against variant models such as LSTM-CRF (word, char, part-of-speech [POS], and combinations) and BiLSTM-CRF (word, char, POS, and combinations). This demonstrated that BiLSTM-CRF scored high on Precision, with a maximum of 94.53% for the Disease or Symptom, 90.83% for Sign or Symptom, and 94.93% for the Pharmacologic Substance predictive performance. Recall scored lower, with a maximum of 73.31% for the word + char + POS metric for Disease or Syndrome, 81.98% for the Sign or Symptom, and 73.47% for the Pharmacologic Substance predictive performance. Overall, the authors presented a thorough approach to model testing for NER. Another study by Wang et al. [88] explores NERO, a biomedical NER Ontology. This ontology was designed with minimizing arbitrary annotative semantic text labels in mind and aimed to represent textual entities recognized by text mining tools. The Conditional Random Fields (CRF) algorithm was used for the NE recognizer, and the CRF is often used for NER, POS tagging, and gene prediction. The authors measured NER performance using Precision, Recall, and F1-score metrics. The overall performance was measured at 54.9% Precision, 37.3% Recall, and a 43.4% F1-score. This indicates that the CRF algorithm was not as robust as the algorithm presented by Batbaatar and Ryu [7]. The main limitation of the study was that, although the authors aimed for the NERO ontology to cover all entities in the biomedical research literature, not all levels of granularity were covered in classifying entities. Furthermore, many concept types were not well represented due to the heavy-tail distribution in the frequencies of ontology classes. This indicates that ontology learning methods might be ideal for addressing completeness problems in ontologies where there is a lack of sufficient data, especially in complex domains.
FIGURE 3.1
Overview of the IEP.


Semantic Natural Language Processing for Knowledge Graphs Creation 55
A third effort detailed in [31] is the SatelliteNER, a tool for automatically
specifically examines NER in the context of a space-related domain, which is relevant to the focus of this research effort. Although the purpose of the SatelliteNER is not ontology population, NER is used specifically for recognizing satellite entities. The spaCy module was selected to build SatelliteNER, and the Pseudo-rehearsal strategy was chosen for the algorithm to run through existing training data and remember assigned weights. However, as there can be common entities, the resultant model is not fast. The authors evaluated several different models to compare them to the SatelliteNER model, including StanfordNer, Stanza, GoogleNER, and MicrosoftNER. The evaluation criteria were based on Precision, Recall, the F1-score, and Processing Time, and the models were tested on three datasets. SatelliteNER had the best Precision, as it was built to only detect models that the authors deemed relevant. SatelliteNER had a high Recall in testing dataset 1, as this was the training dataset, as well as a Recall of over 50% for datasets 2 and 3. The neural network-based models also had higher Recall scores compared to other models. The F1-score of SatelliteNER was also the highest, as the F1-score is the weighted average of Precision and Recall. Finally, SatelliteNER performed the quickest in Processing Time. Ultimately, this demonstrates that custom built NER algorithms for a specific domain tend to outperform generic alternatives and supports the use of a NER model tailored to the CfHA ontology and trained on a quality, relevant data corpus. In conclusion, several case studies of NER models were examined. Not every case study detailed the use of NER in ontology creation [31, 82], although many did [7, 54, 92]. These studies highlighted the importance of developing an efficient and accurate NER model for extracting labels from a text corpus, as [31] demonstrated by comparing the Precision, Recall, F1-score, and Runtime of a custom-built algorithm that was trained on satellite detection data against other, generic NER algorithms. This demonstrates the benefit of creating an algorithm that is trained on existing CfHA ontology data, because the more accurate the entity extraction is, the more accurate the data for the ontology population will be.
3.2.4 NLP Models for Ontology Learning
Examining existing models that have been developed for the ontology population is important for determining how to represent a particular domain. Because ontology domains are often heterogeneous and feature technical concepts, ontology population models often benefit from an automated or semi-automated mechanism for extracting information and populating ontologies [43]. Some type of ontology-based information extraction (OBIE) is typically used for the population of an ontology. Maynard et al. [55] detail that this involves determining the key terms in a specific text and relating them to existing terms in the ontology. IE usually consists of linguistic
detecting satellite entities from different sources of textual data. This chapter


56 Semantic AI in Knowledge Graphs
preprocessing followed by a NER technique, before an ontology population algorithm is used. However, the authors do not elaborate on specific ontology population methodologies. Thus, examining case studies can provide further insight into existing ontology population algorithms. di Buono et al. [14] present a case study on applying computational linguistics to the cultural heritage domain. The authors posit that NLP techniques can be used for bridging the information gap and improving access to cultural resources. Lexicon-Grammar (LG) is the NLP theoretical and practical framework used. The authors describe that it is based on the Operator Argument Grammar developed by Harris [27], where human languages are self-organizing systems where word syntactic and semantic properties can be calculated based on their relationships with co-occurring words inside nuclear or simple sentence contexts. Furthermore, electronic dictionaries are used to describe syntax. Finite-State Automata (FSA) variables were used for identifying ontological classes and properties for subjects, objects, and predicates within RDF graphs. The authors developed an FSA with variables that apply to specific POS. Finally, linguistic data was matched to RDF triples and translated into SPARQL and SERQL path expressions. Although the authors describe their methodology for ontology population, they do not provide an evaluation section, so the efficacy of their process is not elaborated on. Nevertheless, it provides insight into how a grammar can be constructed for populating an ontology. Another case study conducted by Peña et al. [67] focused on Aragon Open Data, a project to open data by the Government of Aragon. Due to the volume of data released, the authors proposed a methodology for allowing unstructured institutional information to become structured data that can be analyzed and browsed. Consequently, an ontology was designed to standardize public administration information. The authors implemented a set of subprocesses through an AI software framework called Moriarty. This is based on two concepts: workitem, a class that implements an atomic function and can be used in multiple contexts, and workflow, which is composed of workitems or other workflows that receive some inputs and perform transformations on them generating and returning outputs. Additionally, a neural network known as the MultiLayer Perceptron (MLP) was used for NER. Extracted knowledge was stored in OpenLink Virtuoso, which uses subprocesses to insert data extracted using the MLP. The algorithm was able to crawl through 667 websites, process 3,963 URLs, and populate the ontology with 95,978 new instances. Although the authors did not provide metrics for specific aspects of their method, the overall results demonstrate how automation can aid in collecting large amounts of information. One additional study conducted by Makki et al. [51] focuses on automatic ontology population for risk management. It is important to have up-to-date information in order to establish accurate risk assessments. Adopting a fully automated method is dangerous, as risk assessment requires human control and validation, but using NLP techniques can be used to enrich an ontology.


Semantic Natural Language Processing for Knowledge Graphs Creation 57
The authors detailed a semi-automated ontology population method on a generic risk management ontology. First, the corpus of risk-related texts was processed using TreeTagger, a POS tagger that annotates texts. Associations between verbs were built through synonyms generated through the lexical resource WordNet, as well as frequent verbs extracted from the annotated corpus and human interference. Finally, triplets were identified and extracted from the list of verbs, which were then validated by a domain expert. The authors formulated a sample experiment to validate their work by using the PRIMA risk management ontology. First, POS tagging was applied to a corpus by the Environmental Protection Agency, and a list of related verbs was built. Then, triplets were extracted and proposed to the syntactic structure recognition procedure, which generated 150 triplets. Eighty-five percent of these were evaluated as acceptable triplets. Although a fully automated method is not ideal for a domain such as risk management, the 85% acceptability rate for triplets indicates that adopting a semiautomated methodology is beneficial. There is still room to test additional parts of the process using evaluation metrics in order to optimize the methodology further. The reviewed methodologies focused on different domains, ranging from risk management [52] to government data [67] to cultural heritage [14]. There were additional differences in their methodologies. di Buono et al. [14] used LG and FSA variables in order to process relevant keywords, before they were matched to RDF triples. Meanwhile, Peña et al. [67] used OpenLink Virtuoso subprocesses in order to extract data and populate the ontology. Finally, Makki et al. [52] used TreeTagger and WordNet to process and create triplets. Out of these surveys, Makki et al. [51] described an 85% acceptability rate for the ontology triplets. However, the performance of the model was not elaborated on, which indicates that there is room for more detailed survey methods in assessing the acceptability of automatically generated ontology entities.
3.3 From Text to Ontology
This section discusses the steps toward development of the ontology learning model based on the text corpus. It specifies, in detail, the algorithms that have been employed in each stage of the IEP. Although prior research efforts
are few that contain a model that spans every stage. Nevertheless, examining existing models that correlate to stages in the IEP is useful when developing a methodology. For example, Witte et al. [20] presented a model that utilized NLP for NER, which is the process of assigning meaningful labels to entities extracted from a text corpus. The researchers created a model that went
have made use of one or more stages of the IEP employed in this chapter, there


58 Semantic AI in Knowledge Graphs
through standard textual preprocessing, including tokenization and POS tagging. NEs were then detected through a two-step process:
1. Step 1: An OntoGazetter labeled each word token in the text with all possible ontology classes it could belong to.
2. Step 2: Ontology grammar rules written in the JAPE language were used to find NEs.
One additional approach utilized by Jafari et al. [31] used the Spacy library alongside the Pseudo-rehearsal strategy in order to train their model on existing data in order to discover entities. These recognized entities were then added to the training data, to ensure that the model does not forget learned weights. The issue of an NLP model forgetting earlier items after learning a new item is referred to as the “catastrophic forgetting” problem. Therefore, introducing a “pseudorehearsal” strategy is a simple way to solve this problem, in which random inputs are temporarily stored along with their outputs [18]. Jafari et al. [31] also created a from-scratch NER model entitled SatelliteNER, although they did not elaborate on how they developed or trained this model within the paper. Other ontology learning research, including the NERO biomedical ontology [88], also used spaCy for the NER process. Therefore, spaCy has been used for the NER portion of the CfHA model. The methodology for developing the NLP model built upon the procedures described in this section alongside the literature that has been evaluated in Chapter 3. spaCy has been utilized for IE purposes to label instances with class labels from the CfHA ontology. For the sake of limiting the scope of this project, seven class labels have been chosen: PERSON and ORG, which already exist in the spaCy NER package, and the custom labels ASTROPHYSICS, HELIOPHYSICS, MISSION, PROJECT, and PAPER. All of these labels correspond to class types in the CfHA ontology and have been trained on data pulled from a heliophysics text corpus. The ontology learning methodology stages are as follows:
• Step 1: The text corpus will be tokenized, preprocessed, lemmatized, and stemmed.
• Step 2: The spaCy NER library is imported.
• Step 3: The dependency tree for each sentence is parsed according to a rule intended to extract subjects, objects, modifiers, and compound words.
• Step 4: Relations are extracted as well by using dependency parsing based on the root or verb of the sentence. This process is broken down by step in Figure 3.2.
• Step 5: The entities and relationships are converted into RDF triples, and a KG is generated. These triples are then placed into a Protégé ontology.


Semantic Natural Language Processing for Knowledge Graphs Creation 59
3.3.1 NLP in Ontology Learning
To construct a foundation for the ontology learning model, the concepts underpinning NLP must be explored. NLP is a field that sits at the intersection between Artificial Intelligence (AI) and linguistics and uses data science techniques to bridge the gap between human forms of communication and machine communication [63]. NLP techniques have been proposed for their use in enhancing ontology development. As an example, Lame [39] implemented an ontology by using Syntex text parsing in order to extract keywords and relationships from legal text corpora and performing statistical analysis in order to assign importance to certain concepts. Statistical approaches are commonly used in NLP, such as dependency analysis, lexicosyntactic analysis, term subsumption, formal concept analysis (FCA), hierarchical clustering, and association rule mining (ARM) [2]. These concepts provide a foundation for the NLP elements utilized in the model. ML principles underlie NLP. ML can be defined as automated computing procedures that aim to mimic human reasoning and generate classifying expressions that are simple enough to be understood by humans [4]. ML techniques can be divided into supervised learning, where input data and an output target variable are known and a model is trained against this variable, and unsupervised learning, where only input data is available. Supervised learning includes methods such as classification and regression, while unsupervised learning includes methods such as clustering [12]. Hybrid approaches are also possible. These techniques are appropriate for the development of the CfHA ontology learning model, where the aim is to infer classes and relationships similarly to how a human can. There are additional fields that are relevant to the process of IE. Data mining (DM) involves the extraction of information from structured databases, which is part of the broader field of Knowledge Discovery in Databases
FIGURE 3.2
Overview of ontology learning model methodology.


60 Semantic AI in Knowledge Graphs
(KDD) [70]. KDD is defined as the non-trivial process of identifying valid, novel, potentially useful, and ultimately understandable patterns in data [84]. Meanwhile, text mining (TM) deals with the extraction of information from unstructured, textual forms. TM holds more relevancy to the aims of this research, as unstructured text is used for developing the model. TM allows for the systematic extraction of meaningful content from a particular corpus of text, which is relevant to the ontology population from CfHA meeting notes. However, TM applications apply constraints on NLP tools, as they usually rely on large volumes of textual data and do not allow for exponential algorithms to be used. Furthermore, semantic models for a given domain are typically not available, and this limits the sophistication of the semantic and pragmatic levels of a model [70]. This is the reason why models for ontology learning are often rule-based and specific to a particular domain. The process of creating a KG is divided into two parts. The first half focuses on TM and preprocessing in order to extract information from the text, while the second half focuses on the creation of the ontology from extracted data. TM techniques are used to generate meaning from an unstructured text corpus, based on a trained spaCy model. According to [80], TM is divided into several stages.
1. The first stage involves the collection of unstructured data from different sources that are available in different file formats.
2. The second stage involves preprocessing and cleansing operations, which aim to eliminate abnormalities and capture the essence of the text. Cleansing also aims to remove stop words, as well as stemming and indexing the data.
3. The third stage applies processing and controlling operations in order to audit and clean the data set by automatic processing.
4. The fourth stage involves pattern analysis implemented by the Management Information System (MIS).
5. The fifth stage is to synthesize the information extracted from the text in order to inform decisions and further utilize the processed data.
Extracting meaning from an unstructured text is often done manually. Inniss et al. [30] describe that the typical process for generating a biomedical ontology involves interviewing experts, transcribing the text, and manually mining the text for feature-attribute pairs. Automated TM instead uses speech and language processing concepts in order to construct a structured data object. Where manually parsing a text corpus relies on a human understanding of grammar and disambiguation, NLP for TM must go through several main steps in order to interpret a corpus. These steps include “Lexical Analysis,” “Syntactic Analysis,” “Semantic Analysis,” “Pragmatic Analysis,” and “Discourse Analysis,” each comprising a different subfield of NLP [62].


Semantic Natural Language Processing for Knowledge Graphs Creation 61
For the CfHA ontology learning model, these various sources of information have been synthesized into the theoretical concepts that underlie each step. The overall system architecture is depicted in Figure 3.3.
1. Step 1: Here, TM concepts are used to clean and preprocess a text corpus. The corpus is tokenized, stripped of unnecessary symbols and stopwords, and lemmatized.
2. Step 2: NER techniques must be employed in order to extract entities from the cleaned text. The model must also be trained to recognize labels that do not already exist in the default model.
3. Step 3: Based on the NER labels extracted earlier, concepts and relations must be extracted from the text.
4. Step 4: A KG is built from the concepts and relations.
5. Step 5: The KG values are processed into RDF triples and inserted into a Protégé ontology.
FIGURE 3.3
Architecture of ontology learning model.


62 Semantic AI in Knowledge Graphs
3.3.2 Prototype Evaluation
To complete the methodology, there needs to be a system for evaluating the efficacy and accuracy of the model. Furthermore, a thorough evaluation of the methodology assesses the quality of the research backing the model. Systematic reviews are preferential, as no type of study should be evaluated in isolation. An assessment should be balanced and draw on a variety of sources [21]. Qualitative data analysis methods provide insight into the suitability of both the text used in TM as well as the ML models themselves. Sonntag [80] defined four categories matching text quality dimensions: contextual, which deals with the amount of data, completeness, relevancy, and timeliness, representational, which deals with consistent, concise representation, ease of understanding, and interpretability, intrinsic, which deals with accuracy, objectivity and reputation, and accessibility. These metrics are listed in Table 3.1. For the rest of the model, additional metrics suited to assessing NLP models must be used. Siebert et al. [77] provide various metrics for assessing ML models based on quality attributes, such as Accuracy, Precision, Recall, and the F-score, for development and runtime correctness. Additional quality attributes such as robustness can be evaluated using Equalized Loss of Accuracy (ELA), and the level of interpretability can be assessed by using complexity metrics. Gunawardana and Shani [23] survey accuracy evaluation metrics to establish which ones apply to a particular domain. Accuracy metrics that measure truth values such as the Root of the Mean Square Error (RMSE) method, the ROC area curve, and confusion matrices are relevant to classification problems [87]. The performance metrics that are used on the ML model are summarized in Table 3.1. However, additional focus will be given to Precision, Recall, and the F-Score, as these are commonly employed for NER problems [3, 85, 88]. For these metrics, each object is associated with a binary label L, which corresponds to the correctness of an object. Additionally, there is an assignment A that corresponds to the relevance of an object [22]. This experimental outcome may be summarized in a truth table (Table 3.2). Here, a true positive would correspond to an entity that was assigned a label, and that the NER model correctly identified as an entity. A false negative would
TABLE 3.1
Performance Measure Metrics for NER Models
Performance Metrics
Evaluation metric Description
Accuracy Determines the overall proportion of true results among the total results Precision Determines how many predicted positives match up with the true number of positives Recall Determines what proportion of true positives is accurately classified F1-score Evaluates the accuracy of a test, calculated from the harmonic mean of the precision and recall values


Semantic Natural Language Processing for Knowledge Graphs Creation 63
correspond to an entity that was assigned a label and that the NER model did not identify as an entity. A false positive would correspond to a token that was incorrectly identified as an entity by the NER model, and a true negative would mean a token that was correctly identified as not being an entity. From this truth table, the equations for determining Precision and Recall can be calculated. The equation for Precision is displayed in Equation 3.1:
precision = (TruePositive)/((TruePositive)+ (FalsePositive)) (3.1)
This can be interpreted as the total number of actual entities over the total number of identified entities, whether those be correctly identified or not. The equation for Recall is displayed in Equation 3.2:
recall = (TruePositive)/((TruePositive)+ (FalseNegative)) (3.2)
This can be interpreted as the total number of actual entities over the total number of actual entities as well as the total number of entities that were not identified. From here, the F1-score can be calculated. It is the harmonic mean of Precision and Recall. This is displayed in Equation 3.3:
F1 = (TruePositive)/(TruePositive + 1/2(FalsePositive + FalseNegative)) (3.3)
This can be taken as a measure of the accuracy of a test. As a result, it provides a means of demonstrating an average between Precision and Recall. In cases where Precision or Recall are different values, it can demonstrate the overall efficacy of a model. Consequently, Precision, Recall, and the F1-score are employed to evaluate this model.
3.4 Experimental Study
This section details the development of the NLP model, from selecting the data to the creation of the final ontology. The reasoning behind choices made during development is described in the following sections. The code
TABLE 3.2
Truth Table for NER Metrics
Evaluation Metrics
Assignment
Positive Negative
Binary label L Positive True positive False negative Negative False positive True negative


64 Semantic AI in Knowledge Graphs
associated with the development of the model is publicly available online.2 The developed repository contains an overview of the model prototype, starting from importing text and finishing with the final ontology with all the necessary steps explained in the document.
3.4.1 Overview of the Proposed System
3.4.1.1 Data Sourcing
The data for this research was sourced from both private documents and publicly available research. The CfHA Meeting Notes are running notes that document developments in the CfHA ontology and formed part of the training data for the NER model. Permission was obtained to use these meeting notes for the project. The second portion of the data used for this project came from a JSON file of heliophysics-related paper titles, bibcodes, and abstracts. Permission was also obtained to use this information.
3.4.1.2 Data Preprocessing
Select text snippets were sourced from both the CfHA Meeting Notes and the heliophysics text corpus. These snippets were chosen for the variety in the entities that were contained within them, in order to train the NER model with relevant data. Concepts that were selected for were already reflected in the CfHA ontology. The text was cleaned and preprocessed in order to remove unnecessary information such as stopwords, the process of which is elaborated on in Section 3.4.4. This process was performed on the heliophysics text corpus, which was used for this research due to its robustness and the amount of available data.
3.4.1.3 Model Construction
After preprocessing the text corpus, the NER and KG portions of the model were constructed (see Section 3.4.4.1) in order to identify important entities in the heliophysics text corpus and extract relations between the entities. Additional data analysis was performed on the IEP in order to provide insights into the model (see Section 5.1).
3.4.2 Data Sourcing
3.4.2.1 Heliophysics Text Corpus
Due to the advent of social media and digital information sharing, large swaths of data can be generated and disseminated. Having large amounts of data for a NER model is important to ensure the accuracy of a NER model and should be indicative of the data that the model is used to predict. Consequently, sourcing quality data is an important stage in the model development process. Jafari et al. [31] constructed the SatelliteNER model by using Wikipedia data on governmental and private space agency names, as well as a list of orbital launch


Semantic Natural Language Processing for Knowledge Graphs Creation 65
systems for each country and for each agency. Wikipedia presents an easily accessible and robust source of information to train a NER model. However, Wikipedia is open-access, and many citations reference news articles, YouTube videos, or other non-peer-reviewed sources [17]. This model focuses on a scientific domain and contains five custom entity labels: Astrophysics, HELIOPHYSICS, PAPER, PROJECT, and MISSION, alongside two default spaCy labels PERSON and ORG. As a result, peerreviewed research data is particularly valuable for NER training. This is because peer-reviewed research is likely to be written by experts and feature correct usage of entities and relationships in context. As a result, a corpus consisting of heliophysics paper titles, abstracts, and bibliographic codes was sourced to train and test the model. This heliophysics corpus was provided by Ryan McGranaghan. It consists of a JSON file that contains academic publications, including the title, bibcode, and abstract for all heliophysics-related articles from 2020. All articles in the NASA Astrophysics Data Service were sub-selected using the criteria of only journals that are relevant to the domain of heliophysics. The data was manually sorted through to identify entities that matched up to instances that were already present in the CfHA ontology. Text snippets containing these entities were selected and used as training and test validation data. The different classes were split up a relatively even amount of instances, and the proportion of classes and instances is visualized in Table 3.3. Subclasses are grouped with superclasses.
TABLE 3.3
Classes and Number of Instances
Word Number of Occurrences
Action 0 Activity (incl. Affiliate, Match, Meeting, Mission, Project)
8
Event (incl. Presentation, Workshop) 0 Funds 0 Object 0 Organization (incl. Group) 95 Other 0 Output 0 Person 74 Position 0 Program 0 Publication (incl. Data, Paper, Software) 0 Role 0 Skill 0 Team 0 Topic (Astrophysics, Heliophysics, etc.) 159


66 Semantic AI in Knowledge Graphs
Table 3.3 demonstrates where there is a need for additional data in the CfHA ontology, with several classes such as Action, Event, and Publication not having any instances. This reveals a gap in the information that is represented in the CfHA ontology. The training and test data has aimed to encompass both instances that are well represented in the ontology and instances that are not. However, the classes without instances cannot be matched to instances in training data. Furthermore, the majority of the entities in the heliophysics text corpus related to heliophysics and astrophysics. Consequently, the data that was extracted from the heliophysics text corpus contained an unbalanced amount of entities. According to [34], training a spaCy model with custom annotations involved around 100 occurrences of each entity. As a result, it is important to ensure that the training data contains a sufficient representative sample for each entity.
3.4.2.2 CfHA Meeting Notes
There was an additional problem to be confronted when it comes to data sources. Because the entities that were manually extracted from the heliophysics text corpus were unbalanced, additional entities representing people and organizations had to be included. Consequently, an appropriate additional data source had to be procured to train the model. The Person entity was the label that primarily lacked sample data, and a text corpus was sought out that contained entities relevant to the CfHA ontology. While developing the CfHA ontology, a paper that documented the outcomes and important points of each meeting was created. This is referred to as the CfHA Running Notes and contains information about each person involved in the ontology development as well as actions related to the ontology [56]. As a result, it presented a relevant source of training data for the spaCy NER model. The process of selecting and extracting entities from the CfHA Running Notes was similar to the process outlined in Section 3.4.2.2 for extracting entities from the heliophysics text corpus. Entities were identified through dynamic embedding, which is based on the context of the word as it appears in the sentence [34]. Dynamic embedding is important to the performance of the NER model, as entities may appear in different contexts within a text corpus. For example, NASA may appear as a standalone ORG, or it may appear as part of the title of a PROJECT. Due to the need for a sufficient amount of examples for each entity, the CfHA Meeting Notes was utilized as an auxiliary text corpus alongside the heliophysics text corpus. It was particularly important to provide data for the custom entities, as those did not have any existing pre-training in the spaCy default English model. However, providing additional data for the PERSON and ORG entity labels is beneficial for further improving the accuracy of the spaCy model.


Semantic Natural Language Processing for Knowledge Graphs Creation 67
3.4.2.3 Discussion of Approach
The next stage of the model development is to critically appraise the data collection approach utilized. This includes a discussion of the benefits and limitations. To begin with, choosing an appropriate data source was crucial to facilitate the custom entity label training. Having a robust heliophysics text corpus was vital for this end, especially one that reflected the domain expertise displayed by peer-reviewed research papers. However, the specialized nature of this text corpus caused limitations. There were almost no entities that could be used to train the PERSON class, and less entities for the ORG, PROJECT, and MISSION classes compared to heliophysics and astrophysics-related topics. This meant that the training and test data had plenty of HELIOPHYSICS and ASTROPHYSICS examples, with comparatively less PERSON, ORG, PROJECT, and MISSION instances. As the amount of instances used to train the data was not balanced, this fails to meet the standards outlined by the “Corpus-based” evaluation method for an ontology, as a large amount of the domain cannot be covered [69]. Another problem related to the source of the data was that the language used was at an academic level. Academic descriptions show entities in context, however complex language exacerbates ambiguity problems with disambiguation, or determining the right context that a word appears in. Overcoming this problem is known as Word Sense Disambiguation (WSD), but solutions often necessitate large linguistic databases such as WordNet to have a proper sample size to draw from [25]. This is further exacerbated by the use of custom topics, as the default SpaCy model has no training data to draw from. Heliophysics and astrophysics-related topics have appropriate samples to draw from, but for entities with less examples, this demonstrates the importance of having an appropriately large sample size. If the sample size is small, it may lead to issues with predicting Actual Positive classes and how many of the predicted entities are correctly identified by producing false negatives, or entities that are NEs, but were not labeled as being NEs. The issues described above presented the opportunity to additionally use the CfHA Running Notes text corpus. As it is a body of work that is relevant to the CfHA Ontology, further examples can be sourced from it. However, there are limitations associated with the CfHA Running Notes text corpus. For example, unstructured phrases in contrast to structured and edited sentences may include abbreviated words, irregular grammar and spelling, and mixed languages which negatively impact entity detection [36]. Furthermore, time restrictions meant that additional data sources could not be utilized to provide more examples for entities. Nevertheless, there are advantages to this approach. Both bodies of text are relevant to the research and, together, provide sufficient examples for the purposes of this research. The data was provided specifically for this model, so data collection is done with consent and with respect to privacy. The data is at a high academic standard, and there is a wealth of high-quality


68 Semantic AI in Knowledge Graphs
examples for some labels. Furthermore, the data was easily available and can be further utilized to enhance the model beyond the scope of this research.
3.4.2.4 Ethical Considerations
This research centers on detecting NEs and relationships between NEs relating to the domain of the CfHA and heliophysics research. Informed consent in the use of data is paramount, as the data that the model is trained on involves current research and the work of other individuals. All data was freely provided with the knowledge that it would be used for this research. Another ethical consideration when it comes to training the model is in being mindful of algorithmic bias. Demographic bias in NER is a recognized phenomenon [13]. For example, the spaCy model surveyed in [59] had the highest accuracy score for recognizing Names that were labeled as White Male. Consequently, this must be accounted for when developing the NER model. This research was approved by Birmingham City University.
3.4.3 Outline of Model Objectives
3.4.3.1 Objective 1: NER Optimization
In order to guide the performance of the model, suitable objectives must be outlined. The first of these deals with the optimization of the NER portion of the model. There are several factors that may improve the accuracy of a NER model. For one, providing NE information to a dependency parser can improve the accuracy of the parsing [10]. Furthermore, FernándezPedauye et al. [16] mention the importance of having a robust training data set with a variety of contexts that entities appear in. It was this factor that contributed the most to the NER optimization. Fernández-Pedauye et al. [16] also detail how selecting appropriate preprocessing techniques can also improve the performance of a NER model. These factors are echoed by Jafari et al. [31]. After evaluating prior methods of optimizing NER models, an important goal of this research is to source an adequate amount of data. As has been discussed in Section 3.4.2, the two text corpora used in this research both provide a large amount of examples for classification. Furthermore, the techniques in textual preprocessing are carefully chosen in order to ensure that the text corpus that goes into the NER model does not face issues with noise or extraneous tokens. Finally, there are metrics that can be employed to evaluate the performance of a NER model. Siebert et al. [77] suggest various metrics for assessing ML models based on quality attributes, such as Accuracy, Precision, Recall, and the F-score, for development and runtime correctness. Additional quality attributes such as robustness can be evaluated using ELA, and the level of


Semantic Natural Language Processing for Knowledge Graphs Creation 69
interpretability can be assessed by using complexity metrics. These metrics are further elaborated on in Section 3.2.3.
3.4.3.2 Objective 2: Entity Relationship Modeling Optimization
A suitable objective must also guide the development of the KG portion of the model. Evaluating an ontology can be more difficult than evaluating a NER model, as metrics for evaluation do not rely on numerical values and thus can be subjective. Success metrics include whether the ontology is capable of accomplishing tasks in the target domain, if hierarchical and taxonomical concepts are well represented, and whether the ontology is meeting the technical specifications [29]. A well-functioning ontology that is easy to use is particularly important in a multidisciplinary team such as that of the CfHA. Ma et al. [46] explore a framework that examines ontology usability based on System Usability Scale (SUS), a ten-item Likert scale. From this, the authors came up with a pool of statements separated into three primary categories: syntax, semantics, and pragmatics. Another survey by Raad and Cruz [69] examines several evaluation methods: gold standard-based, which compares the learned ontology with a previously created “gold standard” reference ontology, Corpus-based, which evaluates how far an ontology covers a given domain, Task-based, which measures how far an ontology goes toward improving the results of a certain task, and Criteria-based, which is divided into Structure-based, which compute various structure properties, and Complex/Expert-based, which involves expert evaluation. Hooi et al. [29] also describe level-based evaluation: Syntax, which assess whether the syntax of the formal language is correct, Structure, which assesses if the concepts and hierarchy are sound, Lexical, which assesses the terms used to represent knowledge, Semantic, which refers to the ontology coping with different terms that relate to the same concept, and Context, which examines how the ontology affects the functionality and usability of ontology-driven applications. Furthermore, Brank et al. [9] add other semantic relations as a level of evaluation. Because different authors use different frameworks for ontology evaluation, it is useful to single out which measures are the most useful for the target domain. In summary, there are multiple measures that can be used to evaluate an ontology. However, studies focused on evaluating an ontology that has been populated through semi-automated or automated means remain elusive, so selecting measures for evaluation must be carefully done.
3.4.4 Model Implementation
3.4.4.1 Text Corpus Overview
The model utilized the heliophysics text corpus outlined in Section 3.4.2.1 As this corpus was originally in JSON format, it had to be converted into a text file in order to employ preprocessing techniques. The JSON file contained


70 Semantic AI in Knowledge Graphs
objects with the properties “bibcode,” “title,” and “abstract.” The total size of the file is 13.7 MB, and there are 8990 entries, 1,955,540 words, and it is 13,683,768 characters long. Due to the large size of the file, there were processing problems when attempting to run the model. Running environment and processing restrictions limited the total maximum file size. Furthermore, using every entry in the document would create a KG that is laden with so much information that it is difficult to read, even when displaying parts of the graph by relationship. Ma et al. [47] found that training a NER model on word embeddings learned from unlabeled data is effective even when data is sparse. Furthermore, Baeza-Yates and Liaghat [5] detail other considerations in regard to the size of a data training corpus, such as training data size, learning time, and quality obtained. They found that, generally, with increased data size came increased quality. However, the performance versus data size curve peaked with a data size of 5 MB. After considering these restrictions, a sample was procured from the total heliophysics dataset. The size of the file is 354 KB, and there are 249 entries, 50,549 words, and 353,773 characters. This text corpus sample was chosen when considering training data size balanced against the time the model takes to run, as well as the quality of the finished model.
3.4.4.2 Preprocessing for Text Mining (TM)
Employing well-selected textual preprocessing techniques is an important factor in the overall success of a model [16]. Indeed, preprocessing can take up to 80% of the total efforts in knowledge discovery [60]. The primary aims of text preprocessing are to extract key features from a corpus, to improve the relevancy between words and documents, and between words and classes, as well as to convert a text corpus into raw data [33]. Consequently, preprocessing is an important stage to consider. First, an overview of textual preprocessing methods must be provided. Common techniques for preprocessing a corpus for any TM task include tokenization, which converts raw texts into segmented textual units, stop word removal, which involves the removal of commonly repeated features such as conjunctions and pronouns, and stemming, which involves the removal of affixes (prefixes and suffixes) from a document [33]. More specifically, Asim et al. [2] describe POS tagging, sentence parsing, and lemmatization as the linguistic-based preprocessing techniques that are used in almost every ontology learning methodology. POS tagging involves labeling corpus words with their corresponding POS tags. Parsing is a type of syntactic analysis that discovers the dependencies between words in a sentence and represents them in a parsing tree data structure. Lemmatization is used to bring terms into a normal form by removing word stems. For example, “processing” and “processed” become “process.” This model also utilizes the preprocessing techniques outlined by Asim et al. [2]. First, the heliophysics corpus sample (see Section 3.4.4.1) was saved


Semantic Natural Language Processing for Knowledge Graphs Creation 71
in a text file and read by the model, before being tokenized using the Natural Language ToolKit (NLTK). Regular expressions used for pattern-matching were employed to remove unnecessary punctuation, whitespace, and characters, before the NLTK toolkit was used to remove stopwords from the text. The cleaned text was then written to a new file for further preprocessing. After the text went through initial preprocessing, stemming and lemmatization modules from NLTK were imported. This allowed for the text to be converted into a standard format and was done after tokenization and stop word removal so that extraneous characters did not impact the rest of the preprocessing. Vectorization involves the process of converting documents into a numerical vector form, which makes it possible to analyze them and create instances in which the model works [37]. After vectorizing the text, it is stemmed to remove all word stems, and lemmatized to group together different inflections of a word together. This concludes the general preprocessing phase.
3.4.4.3 Named Entity Recognition (NER) Using spaCy
NER is a particularly important step in the IEP for ontology creation, as it involves the identification of entities from texts and categorization of them into predefined ontological classes. NEs are vital for constructing a domain grammar [92]. Therefore, ensuring that the NER portion of the model is accurately constructed is important to ensure that the domain is correctly represented. The spaCy NER library was used as a basis for developing the NER portion of this model. As spaCy is a common choice for NER in multiple scientific domains [31, 74, 88], it was selected as an appropriate basis for the domain of heliophysics. spaCy NER contains the option for one of several pretrained models to be imported, as well as for creating a model from scratch. The en_ core_web_sm pretrained model was selected as a basis for the heliophysics NER model. This decision was made because training a blank NER model would have taken a larger amount of data and resources, and the time scale for this research is limited. Thus, the heliophysics model was already partially trained. This utilizes the principle of transfer learning – where the performance of a target learner on a target domain is improved by transferring the knowledge that is already contained in a different, related source domain [93]. As the NER model must be tailored for the CfHA ontology, the default labels provided by spaCy are insufficient. Thus, additional labels were appended to the heliophysics model. Alongside the default labels PERSON and ORG, which correspond to the Person and Organisation classes, five additional labels were added: ASTROPHYSICS, HELIOPHYSICS, MISSION, PROJECT, and PAPER. Due to time limitations, the seven labels were selected on the basis of representing a cross section of the classes already in the ontology. This was done on the basis of labels representing concepts that are the most


72 Semantic AI in Knowledge Graphs
important to the ontology, as it is a person-focused ontology that involves research into heliophysics through missions and projects and produces output on this research such as papers. The next stage of developing the NER model is training it. This step is necessary if there are custom entities, as spaCy relies on in-context examples for entity identification [26]. Jafari et al. [31] detail the process of training the SatelliteNER model to detect custom entities, specifying that a model trained on a particular corpus is suited to detecting entities in a similar corpus. Part of the effort involved updating a pre trained spaCy model, which involved the use of Wikipedia text corpora and an automatic entity tagger to train the model to detect the custom entities orgName, rocketName, and satelliteName – therefore, the trained model was suitable for detecting entities in articles. Similarly, the heliophysics NER model training data was primarily sourced from scientific article abstracts, as the CfHA ontology models scientific data. Therefore, sentences containing instances of the selected entity categories were extracted from the text corpus. After providing training data, it was appended to the NER model using a “pseudo rehearsal” strategy, in which random inputs are temporarily stored along with their outputs. The NER model also had to be tested for suitability, so an additional corpus of data with labeled entities was utilized to test the NER model. The results are elaborated on in Chapter 6. This testing involved the use of separate data from the training corpus in order to verify the performance of the model. The next stage involved using the model to extract entities. spaCy contains functionality for visualizing entities, labels, and label descriptions in order to assess the performance of the NER model, so these were utilized in order to assess which entities the model discovered. Further assessment metrics, including TFIDF, word similarity, clustering, and visualization diagrams were applied to the model to determine its quality, which are detailed in Chapter 6.
3.4.4.4 Entity and Relationship Extraction Using POS Tagging
The other important stage in the ontology learning pipeline involves the extraction of entities and relationships from a text corpus in order to create a KG Maynard et al. [55] describe that OBIE is usually used for ontology population, which involves determining key terms in a text and relating them to existing terms in the ontology. Therefore, the entity extraction process involves checking for entity categories that relate to already-existing instances in the CfHA ontology. One additional factor involves determining what constitutes an entity. POS tagging can identify entities that are single words, as they would be nouns and proper nouns – however, the dependency tree of a sentence must be parsed if entities are multiple words long. di Buono et al. [14] describe


Semantic Natural Language Processing for Knowledge Graphs Creation 73
a system that is based on the Operator-Argument Grammar developed by Harris [27], where FSA variables apply to specific parts of speech, and word syntactic and semantic properties can be calculated based on relationships with co-occurring words inside nuclear or simple sentence contexts. Consequently, dependency tree parsing must be utilized to check the various parts of a sentence and construct entities from there. Entities are not the only elements that need to be extracted to create a KG. Relations between nodes is the other part, which necessitates discovering the root, or verb of a sentence. Any predicate verb can be taken to indicate a relationship between entities and is taken as the relationship type [58]. This will serve to connect the subject and object entities together. From here, two distinct algorithms can be developed, one for entity extraction, and one for relationship extraction. The entity extraction algorithm is detailed in Algorithm 1, and the relationship extraction algorithm is detailed in Algorithm 2. Algorithm 1 is based on a dependency tree parsing approach. Extraneous characters and stopwords are ignored, and tokens are checked for whether they are part of a compound word. The subject and object tokens are checked to ensure that they are NEs that are recognized by the NER portion of the model. Only entities that are identified as NEs are stored as nodes in the KG. After the subject and object are captured, the previous token and dependency tag are updated. Algorithm 2 utilizes a pattern matching approach. Essentially, a Matcher object can be used to determine relationships within sentences that match a particular pattern. In this case, patterns with the root, or verb of the sentence, are classified as relationships. Once the root is identified, the pattern Matcher checks if it is followed by a preposition. If this is the case, the preposition is appended to the root. Similarly to the approach elaborated on in [14], POS tagging was employed to match up the roots, or verbs, of a sentence. Furthermore, the subject and object of a sentence were determined to be the entities and checked as to whether they were already recognized as NEs. After cleaning the resulting entity and relation lists to ensure there are no blank nodes, a Pandas graph is employed to visualize the KG. These results are elaborated on further in Section 5 and visualized in Section 5.1. The final stage of the model involves the importation of the KG into an ontology visualization software. Protégé was chosen for this purpose, as the original CfHA ontology was developed in Protégé. The rdflib library, which represents information as RDF triples, was selected to facilitate the importation. A graph was employed to map the KG triples to, and the nodes were separated out into instances, classes, and object properties based on their status as a source, edge, or target. Classes corresponding to entities were then appended to the graph. Finally, the RDF triples were saved to an alreadycreated tester ontology file to visualize the results. Images of the final Protégé ontology are visualized in Section 5.1.


74 Semantic AI in Knowledge Graphs
Algorithm 1 Entity Extraction
function get entities (sentence)
Ensure: contents ← heliophysics ner text Initialise Part of Speech variables Populate Subject and Object lists for token in sentence do
if token ≠ punctuation then if token == compound then prefix ← tokentext
if previous token dependency == compound then Add previous token to current token if previous token in stopwords then prefix ←” end if end if end if
if token dependency ends with a modifier then modifier ← token text if previous token dependency == compound then Add previous token to current token if previous token in stopwords then modifier ←” end if end if
if token dependency in subjects or token dependency in objects then if token dependency subject exists and token in contents then
tokeninfo ← token
if token dependency subject exists and token in contents then
tokeninfo ←” end if
Addmodifier, prefix, and tokeninfo Reset prefix, modifier, and previous token dependency if token dependency object exists then tokeninfo ← token Addmodifier, prefix, and tokeninfo end if end if end if
Set previous token dependency to current token dependency Set previous token text to token end if end if end for
return entity 1 and entity 2 end function


Semantic Natural Language Processing for Knowledge Graphs Creation 75
Algorithm 2 Relationship Extraction
function get_relationship (sentence)
Ensure: contents ← heliophysics ner text matcher ← Matcher
Assign dependency patterns to extract sentence roots Add patterns to Matcher matches ← Apply Matcher to contents k ← len (matches) − 1
span ← the result of pattern matching each sentence in the text corpus return span end function
3.5 Results and Analysis
The results of the model analysis described in Section 3.4 are elaborated on within this section. This covers all major stages of the model, including NER evaluation, word embeddings and clustering, and an analysis of the ontology.
3.5.1 Exploratory Analysis
In order to create a foundation for assessing the model, the performance metrics must be contextualized and examined according to stage. Examining the performance results also provides insight into how the NLP techniques used in the model construction handled the text corpora.
3.5.1.1 NER Performance Metrics
The metrics chosen to evaluate the NER portion of the model were Precision, Recall, and the F1-score. As is described in Table 3.1, precision is a measure of whether a classifier successfully does not label a positive sample as negative. Recall is a measure of whether a classifier successfully finds all positive samples. The F1-score is the harmonic mean of Precision and Recall. It is used to provide an average of the performances of Precision and Recall [77]. It must be noted that the training corpus has been run twice, as spaCy flags an error when NEs span multiple words. Table 3.4 shows the results of running the performance metrics for the en_core_web_sm model that has been trained and tested on heliophysics data. Meanwhile, Table 3.5 shows the same metrics generated from an untrained en_core_web_sm model that has not had the custom labels appended. If the metrics were calculated from an untrained en_core_web_sm model with the custom labels, the scores would be 0, as spaCy would have no frame of reference to extract entities and spaCy


76 Semantic AI in Knowledge Graphs
needs in-context training information [26]. This demonstrates the utility of transfer learning, where less training was necessitated due to the knowledge already contained in spaCy en_core_web_sm.
3.5.1.2 NER Word Embeddings and Clustering
After running performance metrics on the NER model, there are additional metrics that can be applied to the text corpus in order to provide further insight. Term Frequency – Inverse Document Frequency (TF-IDF) is a model that can be used for text to numeric conversion, in order to identify the most important words in a corpus [68]. It assigns higher value to certain words over others, so that even important words that occur infrequently are assigned high weights. A dictionary of words was created from the vectorized heliophysics text corpus and was sorted using TF-IDF in order to produce words with the highest weights and, therefore, importance. Table 3.6 displays the top ten words and their TFIDF scores. Another tool is word embeddings, which is a language modeling method that is used to map words to vectors that consist of real numbers. Words that occur in similar contexts should hypothetically be closer to each other in vector space. Therefore, related words in the ontology can be extracted by using word embeddings. Word2vec is an algorithm that uses shallow neural networks for word embeddings, and it can be used to represent words as vectors [90]. A Word2Vec model for the heliophysics corpus was constructed and trained. Dimensionality reduction algorithms transform data
TABLE 3.4
Performance Metrics for Trained en_core_web_sm Model
Evaluation Metrics
Precision 100% Accuracy 100% Recall 66.6% F1-score 80%
TABLE 3.5
Performance Metrics for Untrained en_core_web_sm Model
Evaluation Metrics
Precision 0% Accuracy 100% Recall 0% F1-score 0%


Semantic Natural Language Processing for Knowledge Graphs Creation 77
with a high number of dimensions, such as images, into a lower amount of dimensions [86]. This allows for the interpretation of relationships between vectors extracted from the heliophysics text corpus. One such example is determining the top-K most similar words to a particular term and was applied to the word “solar.” The results are displayed in text and graph format in Table 3.7. A final tool that provides insight into the text is clustering. In ML, clustering is an unsupervised technique that groups entities based on similar features. Clustering can be used to discover hitherto unknown patterns in data and can use several different measures for calculating distance [6]. Hierarchical clustering, which divides data into clusters without manually specifying a number of clusters, was initially applied to the TF-IDF features
TABLE 3.6
Top 12 Words by TFIDF Score
Word TFIDF Score (Rounded)
Sub 0.29333 Abstract 0.25186 Bibcode 0.25186 Title 0.25186 sup 0.176000 Observations 0.14767 Model 0.14667 Models 0.13655 Climate 0.12846 Data 0.12845 Solar 0.11936 Surface 0.11733
TABLE 3.7
Top Ten Words Most Similar to “Solar”
Evaluation Metrics
Word Similarity Score (Rounded)
The 0.98165 SUB 0.98064 We 0.98041 Global 0.97834 Also 0.97728 Using 0.97676 Abstract 0.97646 Models 0.97641 Data 0.97607


78 Semantic AI in Knowledge Graphs
to determine how the algorithm would divide up the data. The output of the algorithm is displayed in Figure 3.4 and demonstrates that the text corpus can be separated into three clusters. The hierarchical clustering algorithm demonstrated that the text corpus separates into three clusters. Choosing an appropriate number of clusters is particularly important for K-means clustering. The utility of applying K-means clustering to the data is that it can determine what values are assigned to which clusters. It aims to separate n data values into K clusters, where K = 3 as is established in Figure 3.4. The clusters generated by K-means are visualized in Table 3.8.
FIGURE 3.4
Dendrogram displaying number of clusters by Euclidean distance.
TABLE 3.8
Top Ten Terms by Cluster
Cluster 0 Cluster 1 Cluster 2
sub Title Magnetic sup Bibcode Measurements Model Abstract Analysis Models 2020georl Field Observations Mars Waves Surface Climate Induced High 2020ssrv Fields Data 216 Solar Results Global Pressure Time Dust Signals


Semantic Natural Language Processing for Knowledge Graphs Creation 79
3.5.1.3 Ontology Analysis
The final part of the exploratory analysis involves examining the resulting KG and resultant ontology. Assessing an ontology is more challenging than assessing a NER model, as the success of an ontology depends on several measures, many of which are more subjective than NER evaluation metrics such as Precision, Accuracy, and Recall. These measures include whether the ontology is capable of solving tasks in the target domain, whether hierarchical and taxonomic concepts are represented, and ensuring that the ontology meets technical specifications [29]. Additionally, there are systematic frameworks for evaluating ontology performance, some of which were discussed in Section 3.4.3. For the purposes of this research, a level-based evaluation is used. This was chosen for several reasons. The primary aim of this research is populating an already-existing ontology rather than creating one from scratch, so the primary objective that must be assessed is whether the entities and relationships created from the text corpus are similar to entities and relationships in the CfHA Ontology. To this end, the levels that are used to assess the ontology are Syntax, Structure, Lexical, Semantic, and Context, which are based on the metrics described by Hooi et al. [29]. These evaluation metrics and a summary of their performance are summarized in Table 3.9.
TABLE 3.9
Ontology Assessment Metrics
Level Description Assessment
Syntax Assesses whether the syntax of the formal language is correct
Instances are mostly grammatically sound. There are a handful of exceptions with out-of-context numbers or tokens. Entity relationships also make logical grammatical sense Structure Assesses if the concepts and hierarchy are sound
Concepts are in-line with those in the CfHA ontology. When examining the assignment of instances to classes, there are some that do not match Lexical Assesses the terms used to represent knowledge
Classes and instances reflect terms and concepts used in the CfHA ontology and source heliophysics corpus. Terms reflect many domain concepts Semantic Refers to how the ontology copes with different terms that relate to the same concept
Due to text preprocessing, particularly stemming and lemmatization, having multiple similar terms was not a significant issue Context Examines how the ontology affects the functionality and usability of ontology-driven applications
The automated results can be integrated with the existing CfHA ontology after some manual annotation, which is the purpose of the tool


80 Semantic AI in Knowledge Graphs
3.6 Discussion
This section discusses in detail the model described in Section 3.4 and the results described in Section 3.5, linking back to the literature review done in Section 3.2. It weighs the successes and limitations of the model as well.
3.6.1 Text Mining Implementation
Overall, the TM portion of the model involved the use of well-established TM techniques [2, 33] in order to process the text into a suitable format for constructing the ontology learning model. Removing stop words and special characters was a particularly important step, as many of the tokens in the heliophysics text corpus included special characters. This is particularly important to consider for an ontology creation model, as the process of formatting RDF triples involves the formulation of IRIs, where the inclusion of certain special characters or spaces renders them syntactically invalid. Stemming and lemmatization were also crucial stages in the model development, which is echoed by Asim et al. [2]. Although these two steps are important when being applied to other text corpora, the heliophysics corpus particularly benefited from this stage. Expert intervention is frequently required for the development of scientific ontologies [30], as scientific domains are particularly open-world and information-rich [91]. Concepts in scientific domains are systematic and frequently interrelated – for example, “sunrise” is related to “presunrise” and “sunset.” Therefore, stemming and lemmatization allow for the root concept to be extracted, which results in the most important concepts to be highlighted and terms to be simplified for the NER stage.
3.6.2 Discussion of Spacy NER Model
The NER stage was crucial for determining which entities are relevant to the aims of developing the ontology. Without a robust NER model, constructing an accurate domain grammar would be difficult [92]. Answering RQ1, about already-existing approaches to ontology learning, revealed several existing NER models such as StanfordNER, Stanza, GoogleNER, and MicrosoftNER.
Training a NER model on custom labels necessitates a large amount of data [34]. However, this research had to be completed in a limited amount of time, and part of compromising for the sake of time involved selecting a pre-trained NER model as a basis and adding additional data for the custom
However, in the research for this chapter, there were no NER models that covered precisely the same domain as the one detailed in this chapter. This was particularly important when a primary aim of this research is to harness ontology learning to populate an already-existing ontology. Therefore, the NER model had to allow for custom labeling and training.


Semantic Natural Language Processing for Knowledge Graphs Creation 81
entities. spaCy in particular was chosen after conducting the literature review, as it was frequently used as a NER model for scientific domains [31, 88] and has robust documentation. spaCy did not perform as well as the custom-built SatelliteNER model built by Jafari et al. [31], although the benefits of convenience and the comparatively small difference in performance between a custom-built model and one trained on top of the spaCy en_core_web_sm model justified using a pre-trained spaCy model for this effort. The NER model leveraged transfer learning, where the pre-trained model trained with additional information scored highly on evaluation metrics which worked well within the limited constraints of this effort. A similar methodology was used for time reduction purposes by Kamat Tarcar et al. [34], who harnessed transfer learning by using pre-trained spaCy models on biomedical data. One additional factor was the performance of the NER model. This research utilized Precision, Recall, and the F1-score for evaluation, which was done to provide a holistic view of how the model performed when identifying entities. Some prior studies only used one or two of these metrics [85, 90]. However, Precision was included to determine whether the NER model was incorrectly classifying non-entities as being entities, whereas Recall was included to determine whether the NER model was overlooking entities. The F1-score was included because it provides a look at the average performance of the model, in order to determine the trade-off between Precision and Recall and whether one needs to be improved at the potential expense of the other. Comparing the performance of the custom-build spaCy heliophysics model to prior models revealed that it performed similarly and that the patterns resemble those seen in prior research. The custom heliophysics spaCy NER model scored 100% on Precision, 66.6% on Recall, and had an F1-score of 80%. The BiLSTM-CRF model developed by Batbaatar and Ryu (2019) scored higher on Precision (with a maximum of 94.53% for the Disease or Symptom, 90.83% for Sign or Symptom, and 94.93% for the Pharmacologic Substance) and lower on Recall (with a maximum of 73.31% for the word + char + POS metric for Disease or Syndrome, 81.98% for the Sign or Symptom, and 73.47% for the Pharmacologic Substance). This pattern is also demonstrated by the NERO NER model developed by Wang et al. [88], where the overall performance was measured at 54.9% Precision and 37.3% Recall, alongside a 43.4% F1-score. This demonstrates that the heliophysics NER model performed better, which is attributable to the fact that many concept types were not well represented in the NERO ontology due to the heavy-tail distribution of ontology classes. Additionally, the SatelliteNER model had Precision as the best metric, a high Recall that was still lower than Precision, and an F1-score that sat in-between Precision and Recall. What this reveals is that NER models in general uniformly appear to have significantly lower Recall than Precision, which is attributable to the specialist terms that are used in many scientific