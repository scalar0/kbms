Markov decision processes: dynamic programming and
applications
ENSTA Course SOD312 & M2 optimization (Paris-Saclay
University and Institut Polytechnique de Paris)
2024 Lecture notes
Marianne Akian
INRIA Saclay - Iˆle-de-France and
CMAP E ́cole polytechnique CNRS IP Paris,
marianne.akian@inria.fr
http://www.cmap.polytechnique.fr/~akian/cours-ensta/




Abstract
The aim of this course is to introduce different stochastic control models and to present dynamic programming as a tool for solving them. Illustrations selected among stock management, portfolio selection, Yield management, transportation or Web PageRank optimisation will be presented.
We shall consider essentially stochastic dynamical systems with discrete time and finite state space, or finite Markov chains. This framework already contains the essential difficulties (for instance for long term problems), and allows one to give at the same time an insight of algorithms, mathematical techniques and qualitative properties. We may however consider some examples with infinite state space or continuous time.
We shall present the different types of stochastic control problems: complete and incomplete observation problems, criteria with finite horizon, discounted infinite horizon, stopping time, ergodic criteria, risk-sensitive criteria, constrainted problems, armed bandit problems. For some of these criteria, we shall state the corresponding dynamic programming equations, study their qualitative properties, and the algorithms for solving them (value iterations, policy iterations, linear programming), and deduce in some cases the structure of optimal strategies.
Key Words: Markov Decision processes, Stochastic control, Ergodic control, Risk-sensitive control, Dynamic programming, Max-plus algebra, Value iteration, Policy iteration.
i


ii


Contents
Motivations and Introduction 1 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Aim of the course . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1 Dynamic programming principle for deterministic optimal control 3 1.1 Dynamical systems (some recalls) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2 Deterministic optimal control problems with additive payoff and finite horizon . . . 4 1.3 Dynamic programming equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.4 Properties of Dynamic programming . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 1.4.1 Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 1.4.2 Operator properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 1.5 Infinite horizon problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 1.6 Max-plus or Tropical algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 1.7 Solutions of Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
2 Markov chains and Kolmogorov equations 21 2.1 Introduction and Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 2.2 Markov property . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 2.3 Elementary Properties and representations . . . . . . . . . . . . . . . . . . . . . . . . 24 2.4 The digraph of a stationary Markov chain . . . . . . . . . . . . . . . . . . . . . . . . 26 2.5 Kolmogorov equation for finite horizon criteria . . . . . . . . . . . . . . . . . . . . . 28 2.6 Kolmogorov Equations for infinite horizon criteria . . . . . . . . . . . . . . . . . . . 34 2.7 Kolmogorov Equations for stopping time criteria . . . . . . . . . . . . . . . . . . . . 35 2.8 Further examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 2.9 Solutions of Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
3 Markov decision processes with finite horizon criteria 41 3.1 Markov decision processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 3.2 Markov decision problems with additive finite horizon criteria . . . . . . . . . . . . . 45 3.3 Properties of Bellman operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 3.4 Proof of Theorem 3.13 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 3.5 Problems with multiplicative or discounted finite horizon payoff . . . . . . . . . . . . 50 3.6 Problems with exit time in finite horizon . . . . . . . . . . . . . . . . . . . . . . . . . 54 3.7 The example of optimal stopping time problems with finite horizon . . . . . . . . . . 56
iii


3.8 Examples and Exercices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 3.9 Problem: Airline Revenue Management . . . . . . . . . . . . . . . . . . . . . . . . . 60
4 Markov decision problems with infinite horizon 63 4.1 Discounted infinite horizon problems . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 4.1.1 The stationary dynamic programming equation . . . . . . . . . . . . . . . . . 65 4.1.2 The Bellman operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 4.1.3 Proof of the Stationary Dynamic programming equation . . . . . . . . . . . . 66 4.2 Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 4.2.1 Value iteration algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 4.2.2 Policy iteration algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 4.2.3 Additional properties of Policy iterations for discounted problems . . . . . . . 71 4.3 Optimal stopping time problems with infinite horizon . . . . . . . . . . . . . . . . . 73 4.4 Problems with variably discounted infinite horizon payoff . . . . . . . . . . . . . . . 76 4.5 Problems with exit time in infinite horizon . . . . . . . . . . . . . . . . . . . . . . . . 77 4.6 Problem: Divorce of Birds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
5 Long run average payoff problems 81 5.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 5.2 Long term behavior of Markov chains . . . . . . . . . . . . . . . . . . . . . . . . . . 83 5.2.1 Ergodicity of Markov chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 5.2.2 Graph properties of a Markov matrix . . . . . . . . . . . . . . . . . . . . . . 84 5.2.3 Perron-Frobenius theorem for irreducible matrices . . . . . . . . . . . . . . . 86 5.2.4 Linear algebra techniques and the multichain case . . . . . . . . . . . . . . . 88 5.2.5 The ergodic Kolmogorov equation . . . . . . . . . . . . . . . . . . . . . . . . 91 5.3 The controlled case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 5.3.1 The ergodic dynamic programming equation . . . . . . . . . . . . . . . . . . 93 5.3.2 Application to Pagerank optimization . . . . . . . . . . . . . . . . . . . . . . 96 5.3.3 Vanishing discount approach . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 5.3.4 An existence result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 5.3.5 Policy iteration algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 5.4 Risk sensitive control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112 5.4.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112 5.4.2 Risk sensitive control in finite horizon . . . . . . . . . . . . . . . . . . . . . . 113 5.4.3 Risk sensitive control in infinite horizon . . . . . . . . . . . . . . . . . . . . . 116 5.5 Problem: Machine replacement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119 5.6 Problem: Portfolio selection with transaction cost . . . . . . . . . . . . . . . . . . . . 121
6 Markov decision problems with partial observation 123 6.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123 6.2 Partially observable Markov decision processes . . . . . . . . . . . . . . . . . . . . . 125 6.3 POMDP with additive criteria and finite horizon . . . . . . . . . . . . . . . . . . . . 128 6.4 A sufficient statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130 6.5 The dynamics of the belief process . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132 6.6 Infinite horizon problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135 6.7 Problem: Machine replacement with partial observation . . . . . . . . . . . . . . . . 136
iv


6.7.1 The corresponding POMDP . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136 6.7.2 Solving the DP equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
7 Constrained Markov decision processes and Linear programming formulation of Dynamic programming 139 7.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139 7.2 Constrained MDP with finite horizon . . . . . . . . . . . . . . . . . . . . . . . . . . . 141 7.3 Constrained MDP with infinite horizon . . . . . . . . . . . . . . . . . . . . . . . . . 147 7.4 Constrained MDP with long run time average payoff . . . . . . . . . . . . . . . . . . 149 7.5 Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
v


vi


Motivations and Introduction
A Markov decision problem, or a deterministic or stochastic control problem consists in the maximization or minimization of some functional involving a (possibly random) dynamical system and constructed dynamically. This means that we consider an optimization problem in which the variables are:
• A dynamical system (Xt)t≥0 over a state space E;
• A control process (Ut)t≥0 taking its values in a control or action space C, on which the states depend;
• both may depend on a random process (Wt)t≥0.
In particular the simplest stochastic control problem satisfies
Xk+1 = fk(Xk, Uk, Wk), k ∈ N, (0.1)
where (Wk)k≥0 is a sequence of independent random variables. The criteria J to be optimized has a dynamical structure, which “separate” past and future of the state, and thus allows to apply Dynamic Programming method (introduced by (Bellman, 53)). For instance, it may be additive:
J(X; U ) :=
T
∑
k=0
gk(Xk, Uk) (for a discrete time problem).
Applications
Several real life problems can be modelized as Markov decision processes (MDP) or Stochastic Control Problems. Here are some examples:
• Airline Revenue management;
• Portfolio selection;
• Dam management;
• Stock management;
• Transportation or Web PageRank optimisation;
• Divorce of birds;
1


Aim of the course
• Modelize real life problems.
• Apply dynamic programming approach.
• Solve dynamic programming equations:
– with analytical tools (convexity,monotonicity,...)
– with numerical algorithms (value and policy iterations, linear programming)
One shall consider essentially deterministic or stochastic dynamical systems with discrete time and finite state space, and go from simple to more sophisticated models/problems:
• from deterministic to stochastic problems;
• from uncontrolled to controlled problems;
• from complete to incomplete observation problems;
• additive criteria with finite horizon, discounted infinite horizon, stopping time, ergodic criteria, risk-sensitive criteria;
• from unconstrained to constrainted problems.
References
References [2, 3, 5, 7] contain material similar to the contains of this course. Background material can be found in [4, 1, 6]. Additional references will be given during the lectures.
[1] Robert B. Ash. Basic probability theory. John Wiley & Sons, Inc., New York-London-Sydney, 1970.
[2] D. P. Bertsekas. Dynamic programming. Prentice Hall Inc., Englewood Cliffs, NJ, 1987.
[3] E.B. Dynkin and A.A. Yushkevich. Controlled Markov processes. Springer-Verlag, New York, 1979.
[4] William Feller. An introduction to probability theory and its applications. Vol. I. Third edition. John Wiley & Sons, Inc., New York-London-Sydney, 1968.
[5] M. L. Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons Inc., New York, 1994.
[6] R. T. Rockafellar. Convex analysis. Princeton University Press Princeton, N.J., 1970.
[7] P. Whittle. Optimization over time. Vol. I and II. John Wiley & Sons Ltd., Chichester, 1982, 1983.
2


Chapter 1
Dynamic programming principle for
deterministic optimal control
1.1 Dynamical systems (some recalls)
Let us recall what is a general dynamical system.
Definition 1.1. A (deterministic) dynamical system consists in a function (or sequence) from a set T of times to a set E of states, denoted (Xt)t∈T , such that, for each time t ∈ T , the state Xt of the system at time t is a (deterministic) function ft of the history of the states until time t, that is
(Xτ )τ<t.
There, the set of times T may be:
• Z or N and t = n: we speak about discrete time dynamical system.
• R or R+: we speak about continuous time dynamical system.
The state space E may be:
• a finite set: finite state space.
• a finite or countable set: discrete state space.
• Rn: (finite dimensional) continuous state space /system.
• a space of functions : infinite dimensional continuous state space /system.
The system may satisfy:
• Xn = fn(Xn−1), n ≥ 0.
• X ̇ = gt(X), t ≥ 0, with gt Lipschitz continuous: mechanical or physical systems.
• ∂X
∂t = −∆X. Heat equation.
• the discretization of a ODE or PDE.
In particular,
3


Definition 1.2. A (deterministic) dynamical system with discrete time and state space E is a sequence (Xk)k∈Z or (Xk)k∈N with values in the set E, such that, for each k ∈ N, the state Xk at time (or stage) k is a (deterministic) function fk of the states at previous times, that is of Xk−1, Xk−2, . . ..
The sequence (Xk)k≥0 is called the trajectory of the system starting from X0.
Examples of discrete time dynamical systems are as follows:
1. Xn+1 = fn(Xn), n ≥ 0, where fn : E → E.
2. Xn+1 = fn(Xn, Xn−1), n ≥ 1, with fn : E × E → E.
3. Xn+1 = fn(Xn, Xn−1, . . . , X0), n ≥ 0, with fn : En+1 → E.
4. Xn+1 = fn(Xn−τ(n)), n ≥ 0, where τ : N → N is a variable delay.
Fact 1.3. Any discrete time dynamical system can be reduced to a system of type 1.
Proof. If the time set is Z, then a dynamical system is such that Xn+1 = fn(Xn, Xn−1, . . .), with fn : EN → E, for all n ∈ Z. Consider the new state X′
k = (Xk, Xk−1, . . .) belonging to the larger state space E′ = EN, then
X′
k has the dynamics X′
k+1 = f ′
k (X ′
k) with
f′
k((x0, x1, . . .)) = (fk(x0, x1, . . .), x0, x1, . . .) .
If the time set is N, a dynamical system is such that Xn+1 = fn(Xn, Xn−1, . . . , X0), with fn : En+1 → E, for all n ≥ 0. We can reduce the new state space to E′ = ∪k≥0Ek+1, which is countable if E is a finite set. Indeed, the new state X′
k = (Xk, Xk−1, . . . , X0) belongs to E′ and has the dynamics X′
k = f′
k (X ′
k−1),
where f ′
k is only defined on Ek by
f′
k((x0, . . . , xk−1)) = (fk(x0, . . . , xk−1), x0, x1, . . . , xk−1) ∈ Ek+1 .
A drawback of the above construction is that even if E were a finite set, the new state space E′ may be infinite noncountable, in particular, when the time set is Z. Also to initialize the sequence, one need an initial state of the form X′0 = (X0, X−1, . . .) to be given. However, when the time set
is N, and E is a finite set, then the state space E′ is countable. The above construction is similar to the one used to transform a second order differential equation to a first order one for instance.
1.2 Deterministic optimal control problems with additive payoff
and finite horizon
The simplest deterministic control problem is the following. Consider a discrete time dynamical system (Xn)n≥0 with finite (or discrete) state space E and a dynamics of type 1:
Xn+1 = fn(Xn), n ≥ 1 .
4


Assume now that we (or somebody) are able to change the behavior of this system, that is its dynamics fn. That is, the dynamics is supposed to depend not only on the state but also on a parameter, called the action or the control:
Xn+1 = fn(Xn, Un), n ≥ 1 .
An optimal control problem is the problem of choosing the actions U0, . . . , Uk, . . . in such a way that they minimize (resp. maximize) a certain functional, called the total cost (resp. the total payoff) of the sequences X = (Xk)k≥0 and U = (Uk)k≥0. We assume in this part that all the states are observable, which means in particular that the initial state X0 is known. We speak about complete observation. Let us consider or denote:
• a finite or discrete state space E;
• an action space C
• for all k ∈ N and x ∈ E, the subset Ck(x) ⊂ C of all possible actions at time k, when the state is equal to x;
• for all k ∈ N, the set Ak := {(x, u) | x ∈ E, u ∈ Ck(x)} of all possibles couples (state, action) at time k;
• for all k ≥ 0, the dynamics at time k, which is a map fk : Ak → E;
• for all k ∈ N, the instantaneous/running reward/payoff at time k, which is a map rk : Ak → R;
• a final reward, which is a map φ : E → R;
• an initial state x0 ∈ E.
• for all sequences X = (Xk)k≥0 and U = (Uk)k≥0 in E and C respectively, the total additive payoff with finite horizon T ≥ 1:
J(X; U ) :=
(T −1
∑
k=0
rk(Xk, Uk)
)
+ φ(XT ) . (1.1)
Moreover, we shall replace the words reward or payoff by cost, and the notation rk by ck, when the criterion J is to be minimized, instead of maximized.
Definition 1.4. A deterministic control problem with discrete time, complete observation, and the above data and additive criteria consists in the following optimization problem:
max
X,U J (X; U )
where the optimization holds over all sequences X = (Xk)k≥0 and U = (Uk)k≥0 in E and C respectively, such that
Xk+1 = fk(Xk, Uk), X0 = x0, Uk ∈ Ck(Xk), k ∈ N .
The optimum of above criteria is called the value of the problem. A sequence U = (Uk)k≥0 of an optimal solution (X, U ) is called an optimal control (process). Moreover, maximization can be replaced by minimization.
5


Definition 1.5. For all x0 ∈ E, let v(x0) be the value of the problem of Definition 1.4 when the initial state is x0. The map v : E → R, x0 7→ v(x0) is called the value function.
Some small generalizations of the above problem can be done.
• One can also consider functions rk and φ taking their values in R∪{−∞} (for a maximization problem).
• This allows one to restrict the state space at each time. Indeed, for all k ≥ 0, let Ek ⊂ E, and take rk(x, u) = −∞ when x 6∈ Sk and φ(x) = −∞ when x 6∈ ET , then the maximum of J is attained only for a sequence X such that Xk ∈ Ek for all k ≥ 0.
• In particular, if φ(x) = 0 when x = xT and φ(x) = −∞ otherwise, then the final state is necessarily equal to xT .
• Conversely, one can replace the constraint X0 = x0 by the addition to J of an initial reward ψ : E → R ∪ {−∞}, as in
max
X,U ψ(X0) + J (X; U )
where the optimization holds over all sequences X = (Xk)k≥0 and U = (Uk)k≥0 in E and C respectively, such that
Xk+1 = fk(Xk, Uk), Uk ∈ Ck(Xk), k ∈ N .
• If ψ(x) = 0 when x = x0 and φ(x) = −∞ otherwise, we recover the previous problem.
• If vψ denotes the value of the problem, then
vψ = max
x0∈E(ψ(x0) + v(x0)) .
Example 1.6 (Shortest path problem). Consider a directed graph G = (N , A), where N is the set of nodes, and A ⊂ N 2 is the set of arcs. Let ` be a weight function representing the “lengths” of arcs: ` : A → R+. One can think for instance to a network of towns: the nodes are the towns, the arcs the direct roads between some of them (that is the ones containing no other town), and ` can be either the true length (in km) of the road or the gas consumption necessary to travel on it. The shortest path problem starting from node x0 and ending in xf consists in solving:
min
{N −1
∑
k=0
`(xk, xk+1) | N ∈ N, (x0, . . . , xN ) is a path of G, xN = xf
}
.
3
5
3
1
24
1
1
1
6
4
6


When we restrict the minimization to paths with length (that is number of arcs) less or equal to N , we get:
min
{n−1
∑
k=0
`(xk, xk+1) | n ≤ N, (x0, . . . , xn) is a path of G, xn = xf
}
.
This problem is an optimal control problem with finite horizon N with
• E = C = N.
• Ck(x) is the set of nodes y such that (x, y) ∈ A, when x 6= xf and Ck(xf ) = {xf }.
• The dynamics is fk(x, u) = u.
• The instantaneous cost is : rk(x, u) = `(x, u) if x 6= xf and rk(xf , xf ) = 0.
• The final cost is φ(x) = 0 if x = xf and φ(x) = +∞ otherwise.
• The initial state is x0.
Example 1.7 (Ressource allocation problem). An investor can invest M ∈ N units (of money, capacity, requets,...) in N ∈ N ressources (stocks, flats, planes, parallel processors,...).
• We assume that the reward obtained when he invests x units in the ith ressource is equal to Ri(x).
• We also assume that the units that are not invested yield a zero reward.
• So the investor wants to maximize his total reward, that is equivalent to find
max
{N ∑
i=1
Ri(ui) | ui ∈ N, i = 1, . . . , N,
N
∑
i=1
ui ≤ M
}
.
Consider the deterministic optimal control problem in which:
• The ressource number is considered as a stage/time;
• The state at each stage is equal to the number of units remaining to be invested.
• The state space is E = {0, . . . , M };
• The action spaces are C = E and Ck(x) = {0, . . . , x}.
• The dynamics at each time k is : fk(x, u) = x − u;
• for all k ∈ N, the instantaneous reward is : rk(x, u) = Rk+1(u).
• The final reward is φ(x) = 0.
• The initial state is x0 = M .
7


• The total additive payoff with finite horizon T = N is then:
J(X; U ) =
T −1
∑
k=0
rk(Xk, Uk) =
N −1
∑
k=0
Rk+1(Uk) .
Then the value of this problem coincides with the one of the ressource allocation problem : take uk = Uk−1 and Xk equal to the number of units remaining to be invested, when the number of units u1, . . . , uk, invested in ressources 1 to k have already been chosen (Xk = X0 − u1 − · · · − uk).
Example 1.8 (Knapsack problem). Given N items, each with a weight wi and a value mi, i = 1, . . . , N , one need to determine the number ui of each item i to include in a knapsack so that the total weight is less than or equal to W and the total value is as large as possible. This consists in the optimization problem:
max
{N ∑
i=1
miui | ui ∈ N, i = 1, . . . , N,
N
∑
i=1
wiui ≤ W
}
.
Additionnaly, the numbers ui can be restricted to be in {0, 1} or to be in {0, . . . , c}. If wi ∈ N∗, for all i = 1, . . . , N , considering the ressource allocation problem in which the rewards are linear with Ri(x) = mi
wi x, and adding constraints induced by non-divisibility of ressources, or
restricting the sets Ci(x) of the optimal control problem, we recover all types of knapsack problems. For instance, for 0-1 knapsack problem, one can consider Ci−1(x) = {0, . . . , x} ∩ {0, wi}. For the unbounded knapsack problem, one can consider Ci−1(x) = {0, . . . , x} ∩ wiN.
1.3 Dynamic programming equation
In the optimal control problem of Definition 1.4:
max
X,U J (X; U ) (1.2a)
Xk+1 = fk(Xk, Uk), X0 = x0, Uk ∈ Ck(Xk), k ∈ N (1.2b)
the optimization is done over all sequences U = (Uk)k≥0 satisfying the above constraints. One may wish to obtain an optimal control which yields a dynamical system, that is a system which is causal in the sense that the state Xk+1 at time k + 1 only depends on the past states X0, . . . , Xk. Moreover, one may wish to take a decision at time k using only the informations we have in hand, that is the history of the trajectories of X and U before the decision. A strategy is precisely a rule which tells how to take this decision.
Definition 1.9. The set Hk = A0 × · · · × Ak−1 × E is called the set of histories at time k. A strategy for the (perfect information) optimal control problem of Definition 1.4 is a sequence σ = (σ0, . . . , σT −1) such that, for all k = 0, . . . , T − 1, σk is a map from Hk to C satisfying
σk(x0, u0, . . . , xk−1, uk−1, xk) ∈ Ck(xk), for all (x0, u0, . . . , xk−1, uk−1, xk) ∈ Hk .
We denote by Σ(T ) the set of all strategies. A strategy gives rise to a dynamical system (Xk, Uk)k≥0 satisfying the dynamics: Xk+1 = fk(Xk, Uk) and Uk = σk(X0, U0, . . . , Xk−1, Uk−1, Xk). Such a sequence (Xk, Uk)k≥0 is also called an admissible sequence of states and controls. A strategy is optimal if the sequence (X, U ) is optimal for the optimization problem (1.2) restricted to admissible sequences of states and controls (that is to strategies).
8


Definition 1.10. A strategy σ is a feedback policy if each map σk depends only on the information on the state at the current time, that is
σk(x0, u0, . . . , xk−1, uk−1, xk) = πk(xk) ,
where πk : E → C is such that πk(xk) ∈ Ck(xk). We then denote by π = (π0, . . . , πT −1) such a policy, and by Π(T ) the set of all feedback policies.
Definition 1.11. An open-loop control is a strategy such that each map σk depends only on the state x0 at the initial time, that is
σk(x0, u0, . . . , xk−1, uk−1, xk) = ωk(x0) .
We denote by O(T ) the set of all open-loop controls.
The following result shows that the optimization of the total reward J over each of the above types of strategies gives the same value. However, one can show that feedback policies are more robust with respect to disturbances on the model, that is disturbances on the maps fk and rk.
Theorem 1.12 ((Bellman) Dynamic programming method for deterministic optimal control problems). Assume that the maps φ, rk, k ≥ 0 are bounded from above. Define the functions vt : E → R, t = 0, . . . , T , by the backward recursion:
vT (x) = φ(x) ∀x ∈ E , (1.3a)
vk(x) = sup{rk(x, u) + vk+1(fk(x, u)) | u ∈ Ck(x)} ∀x ∈ E, k ≤ T − 1. (1.3b)
Then the value function v of the optimal control problem of Definition 1.4 coincides with v0. Moreover, the value v coincides with the optimum of (1.2) (that is of J) restricted to admissible controls (or strategies), or to feedback policies, or to open-loop controls. Assume in addition that the maximum of (1.3b), is attained for an action u ∈ Ck(x) and let us denote by πk(x) this action, then the feedback policy π = (πk)0≤k≤T −1 is an optimal strategy of the problem, and the dynamics Xk+1 = fk(Xk, πk(Xk)) with Uk = πk(Xk) furnishes an optimal solution (X, U ) of the optimal control problem.
Proof. The dynamic programming equation by moving suprema. The value v(x0) of the problem of Definition 1.4 is by definition the optimum of
J(X; U ) :=
(T −1
∑
k=0
rk(Xk, Uk)
)
+ φ(XT )
over all sequences X = (Xk)k≥0 and U = (Uk)k≥0 of E and C satisfying the constraints (1.2b):
v(x0) = sup
{(T −1
∑
k=0
rk(Xk, Uk)
)
+ φ(XT ) | (X, U ) satisfies (1.2b)
}
.
Note that these sequences are not necessarily admissible.
9


This can be rewritten as:
v(x0) = sup
U0∈C0(x0), X1=f0(x0,U0)
(
· · · sup
UT −1∈CT −1(XT −1), XT =fT −1(XT −1,UT −1)
((T −2
∑
k=0
rk(Xk, Uk)
)
+ rT −1(XT −1, UT −1) + φ(XT )
)
···
)
,
Consider the maps vk, k ≥ 0, as in (1.3). Since r0(X0, U0), . . . , rT −2(XT −2, UT −2) do not depend on UT −1 nor XT , but only on the first states and actions X0, . . . , XT −2 and U0, . . . , UT −2, we deduce that for X0 = x0, we have
v(x0) = sup
U0∈C0(X0), X1=f0(X0,U0)
(
· · · sup
UT −2∈CT −2(XT −2), XT −1=fT −2(XT −2,UT −2)
( (T −2
∑
k=0
rk(Xk, Uk)
)
+
sup
UT −1∈CT −1(XT −1)
(rT −1(XT −1, UT −1) + vT (fT −1(XT −1, UT −1)))
)
···
)
,
= sup
U0∈C0(X0), X1=f0(X0,U0)
(
· · · sup
UT −2∈CT −2(XT −2), XT −1=fT −2(XT −2,UT −2)
( (T −2
∑
k=0
rk(Xk, Uk)
)
+ vT −1(XT −1)
)
···
)
,
= · · · = v0(X0) .
which shows that the value function v of the problem of Definition 1.4 coincides with the function v0 defined recursively by (1.3).
Optimality and an alternative proof of dynamic programming equation Applying (1.3b) recursively, it is easy to show that, for all sequences (X, U ) of states and actions satisfying the dynamics Xk+1 = fk(Xk, Uk), we have
v0(x0) ≥ r0(x0, U0) + v1(X1) ≥ · · · ≥ J (X; U ) .
Taking the supremum over all sequences, we deduce that v0(x0) ≥ v(x0). Now, if πk(x) is optimal in the criteria (1.3b), then taking the sequence (X, U ) such that Uk = πk(Xk) and Xk+1 = fk(Xk, Uk), we get v0(x0) = r0(X0, U0) + v1(X1) = · · · = J (X; U ) ≤ v(x0). Hence, since v0(x0) ≥ v(x0), we deduce the equality and that (X, U ) is optimal. Moreover, this action is a function of k and Xk, hence it comes from a (feedback) strategy, which is in particular a strategy. This shows that the maximum v(x0) over all sequences is equal to the maximum over all feedback strategies, or over all strategies. Since the dynamics is deterministic, the constraints Uk = πk(Xk) together with (1.2b) allow to write Uk as a function of X0 only, that is as an open-loop control, hence we also get that the maximum v(x0) coincides with the optimum of J restricted to all open-loop controls.
10


When the action sets are infinite, but the suprema are finite, which is the case when the maps φ, rk, k ≥ 0, are bounded from above, one can prove the same result by considering actions πk(x) that are -optimal for (1.3b), that is satisfying for all x ∈ E, and k ≤ T − 1,
vk(x) ≤ + rk(x, πk(x)) + vk+1(fk(x, πk(x))) .
Indeed, this gives a control sequence U = (πk(Xk))k≥0 which is (T )-optimal for the criteria J :
v0(x0) ≤ + r0(x0, U0) + v1(X1) ≤ · · · ≤ T + J (X; U ) ≤ T + v(x0) .
Since this holds for all > 0, we obtain that v0(x0) ≤ v(x0) and so the equality as in the above case of finite action sets. Moreover, we obtain that v(x) ≤ v0(x0) ≤ T + J(X; U ) and since the sequence (X, U ) comes from a feedback strategy, this shows that the supremum of J(X; U ) over all feedback strategies is equal to v(x0). The other assertions are shown as for the case of finite action sets.
Remark 1.13. Let us consider the partial criteria:
Jn(X; U ) :=
(T −1
∑
k=n
rk(Xk, Uk)
)
+ φ(XT )
Then, the iterations (vn)0≤n≤T defined in the dynamic programming equation have the following interpretation: vn(xn) = max
X,U Jn(X; U ) (1.4a)
where the optimization is done over all sequences X = (Xk)k≥n and U = (Uk)k≥n satisfying the following constraints
Xk+1 = fk(Xk, Uk), Xn = xn, Uk ∈ Ck(Xk), n ≤ k ≤ T − 1 (1.4b)
Example 1.14 (Shortest path problem (continued)). Let us consider the shortest path problem described in Example 1.6. Using the optimal control interpretation of the value of the shortest path from x0 to xf with paths with length (that is number of arcs) less or equal to N :
v(N)(x0) := min
{n−1
∑
k=0
`(xk, xk+1) | n ≤ N, (x0, . . . , xn) is a path of G, xn = xf
}
,
we obtain the dynamic programming equation:
v(N )
N (x) = +∞ ∀x ∈ N \ {xf } ,
v(N )
N (xf ) = 0 ,
v(N )
k (x) = min{`(x, y) + v(N)
k+1(y) | y, (x, y) ∈ A} ∀x ∈ N \ {xf }, k ≤ N − 1 ,
v(N )
k (xf ) = v(N)
k+1(xf ) .
Morover, since ` does not depend on time k, we can rewrite theses equation as a forward
recurrence for wk = v(k)
0 , such that v(N)
k = wN−k:
w0(x) = +∞ ∀x ∈ N \ {xf } ,
w0(xf ) = 0 ,
wk(x) = min{`(x, y) + wk−1(y) | y, (x, y) ∈ A} ∀x ∈ N \ {xf }, k ≤ N − 1 ,
wk(xf ) = wk−1(xf ) .
11


Example 1.15 (Ressource allocation problem (continued)). Let us consider the ressource allocation problem described in Example 1.7. Using the deterministic optimal control problem interpretation, the value of the ressource allocation problem coincides with v0(M ), where vt, t = 0, . . . , N satisfy the dynamic programming equation:
vN (x) = 0 ∀x ∈ {0, . . . , M } ,
vk(x) = sup{Rk+1(u) + vk+1(x − u)) | 0 ≤ u ≤ x} ∀0 ≤ x ≤ M, k ≤ N − 1.
Example 1.16 (Knapsack problem (continued)). Let us consider Knapsack problem described in Example 1.8:
max
{N ∑
i=1
miui | ui ∈ {0, 1}, i = 1, . . . , N,
N
∑
i=1
wiui ≤ W
}
.
As for the ressource allocation problem, the value of the problem is equal to v0(W ) where vt, t = 0, . . . , N satisfy the dynamic programming equation:
vN (x) = 0 ∀x ∈ {0, . . . , W } ,
vk(x) = sup{mk+1u + vk+1(x − wk+1u)) | u ∈ {0, 1}, wk+1u ≤ x} ∀0 ≤ x ≤ W, k ≤ N − 1.
So the recurrence equation reduces to:
vk(x) = max(vk+1(x), mk+1 + vk+1(x − wk+1)) (1.5)
if x ≥ wk+1, and vk(x) = vk+1(x) otherwise. Moreover, one can use (1.5) for all x ≥ 0, by extending the functions vk by vk(x) = −∞ for all negative integers x.
Exercise 1.3.1. Solve the Knapsack problem
max {4u1 + 3u2 + 2u3 | ui ∈ {0, 1}, i = 1, . . . , 3, 5u1 + 4u2 + 3u3 ≤ 10} ,
using dynamic programming equation.
1.4 Properties of Dynamic programming
1.4.1 Complexity
Corollary 1.17 (of Dynamic programming). Under the assumptions of Theorem 1.12, denote, for x, y ∈ E, and k ≥ 0:
Gk(x, y) = sup{rk(x, u) | u ∈ Ck(x), fk(x, u) = y} ∈ R ∪ {−∞} .
Then, the dynamic programming equation of Theorem 1.12 can be rewritten as
vk(x) = sup{Gk(x, y) + vk+1(y) | y ∈ E} ∀x ∈ E, k ≤ T − 1.
Note also that an optimal feedback policy πk(x) can be obtained as the composition: πk(x) = π′
k(x, π′′
k (x)) where
π′
k(x, y) ∈ Argmax{rk(x, u) | u ∈ Ck(x), fk(x, u) = y}
and
π′′
k (x) ∈ Argmax{Gk(x, y) + vk+1(y) | y ∈ E}
12


If the maps Gk do not depend on k, one can construct a weighted directed graph with set of nodes E, an arc (x, y) if G(x, y) 6= −∞, with the weight G(x, y). For instance for E = {1, 2, 3} and the following table of values of G
G=


−1 2 −∞ 10 −4 3 −∞ −∞ 0

,
we obtain the (di)graph:
13
2
10
-4
0
2
-1
3
Then, the weight of a path is the sum of the weights of its arcs, and an optimal sequence of states (Xk)k≥0 is a path in this graph which has a maximal weight among the paths with same initial and final nodes. When the weights are nonpositive, this is a “shortest path” (for the lengths which are the opposite of G). In general, denote
• n = card(E), the number of states;
• mk = card({(x, y) ∈ E × E | Gk(x, y) 6= −∞}), the number of arcs in the graph.
Fact 1.18. Once an oracle is available to compute Gk, the computational complexity of Dynamic programming equation is O(∑
k mk) = O(T n2). This has to be compared with O(nT ), if we solve the optimization directly, that is we solve the combinatorial optimization problem consisting in optimizing the criterion over all trajectories (x0, . . . , xT ) in ET +1. The storage complexity is O(∑
k mk) if Gk depends on k and is O(m + T n) otherwise.
1.4.2 Operator properties
Definition 1.19. For k ≤ T − 1, let Bk : RE → RE be the map such that for all v ∈ RE , and x ∈ E, we have
[Bk(v)](x) = sup{rk(x, u) + v(fk(x, u)) | u ∈ Ck(x)}
= sup{Gk(x, y) + v(y) | y ∈ E} .
The map Bk is called the Bellman operator at time k of the optimal control problem.
The dynamic programming equation can then be rewritten in functional form:
vT = φ, vk = Bk(vk+1), for T − 1, . . . , 0.
Definition 1.20. Denotes by ≤ the partial order on RE : v ≤ w if v(x) ≤ w(x) for all x ∈ E. We say that an operator B : RE → RE is monotone or order preserving if it preserves the partial order of RE , that is if, for all v, w ∈ RE , we have
v ≤ w ⇒ B(v) ≤ B(w) .
13


Definition 1.21. Denote by 1 the element of RE which is the constant function (or vector) equal to 1: 1(x) = 1 for all x ∈ E. We say that an operator B : RE → RE is additively homogeneous if it commutes with the addition of a constant, that is, for all v ∈ RE and λ ∈ R, we have:
B(v + λ1) = B(v) + λ1 .
Proposition 1.22. The above Bellman operators are monotone and additively homogeneous.
Proposition 1.23. Any monotone additively homogeneous operator B : RE → RS is nonexpansif, that is Liptschitz continuous with Lipschitz constant 1, for the sup-norm ( ‖v‖∞ = sup{|v(x)| | x ∈ E }): ‖B(v) − B(w)‖∞ ≤ ‖v − w‖∞ ∀v, w ∈ RE .
Proof. For all v, w ∈ RE , we have v(x) − w(x) ≤ ‖v − w‖∞, for all x ∈ E, hence v − w ≤ ‖v − w‖∞1. So v ≤ ‖v − w‖∞1 + w. Using monotonicity of B, and then additive homogeneity, we get
B(v) ≤ B(‖v − w‖∞1 + w) ≤ ‖v − w‖∞1 + B(w) .
Hence B(v) − B(w) ≤ ‖v − w‖∞1, that is [B(v) − B(w)](x) ≤ ‖v − w‖∞, for all x ∈ E. By symmetry, we also get [B(w) − B(v)](x) ≤ ‖w − v‖∞, and deduce ‖B(v) − B(w)‖∞ ≤ ‖v − w‖∞.
Remark 1.24. Another proof of the nonexpansivity of the operator B of a deterministic control problem is as follows. Such an operator acts on RE and satisfies
[B(v)](x) = sup{r(x, u) + v(f (x, u)) | u ∈ C(x)}
= sup{G(x, y) + v(y) | y ∈ E} ,
for all v ∈ RE and x ∈ E, for some control sets C(x) and maps r, f and G. Given v, w ∈ RE and x ∈ E, choose y ∈ E which is optimal in the expression of [B(v)](x), that is such that [B(v)](x) = G(x, y) + v(y). Since [B(w)](x) ≥ G(x, y) + w(y), we obtain
[B(v)](x) − [B(w)](x) ≤ v(y) − w(y) ≤ ‖v − w‖∞ .
By symmetry, we also get [B(w)](x) − [B(v)](x) ≤ ‖v − w‖∞, and taking the maximum over x ∈ E, we deduce ‖B(v) − B(w)‖∞ ≤ ‖v − w‖∞.
1.5 Infinite horizon problems
Assume now that the horizon T is infinite, that is the functional J is replaced by:
J(X; U ) :=
∞
∑
k=0
rk(Xk, Uk) (1.6)
and that the infinite sum is well defined in R ∪ {−∞} for all sequences Xk and Uk satisfying (1.2b). Then, one may try to compute again the maximum (or supremum) v of J, as in finite horizon problems. This is the case when
14


(A1) Ck and fk do not depend on k (then we omit k in the notations), rk(x, u) = αkr(x, u) for all k ≥ 0, for some function r bounded from above and some constant α > 0;
and one of the following assumptions hold:
(A2) α < 1;
(A3) α = 1 and r(x, u) ≤ 0 for all x ∈ E and u ∈ C, and for all x0 ∈ E, there exists a sequence (Xk, Uk) satisfying (1.2b), such that r(Xk, Uk) = 0 for k large enough.
(A4) α = 1, if G is constructed as in Section 1.4.1, then any circuit of the graph of G has a total weight ≤ 0, and for all x0 ∈ E, there exists an infinite path x0, x1, . . . in the graph of G, such that G(xk, xk+1) = 0 for k large enough.
Definition 1.25. The parameter α is called the discount factor. We say that the infinite horizon optimal control problem is discounted if α < 1, and that it is undiscounted when α = 1.
Definition 1.26. We define strategies, feedback policies and open-loop controls of an infinite horizon problem as for finite horizon problems, but with T = ∞. We say that a feedback policy π∗ = (πk)k≥0 of an infinite horizon control problem is stationary if πk = π0 for all k ≥ 0. Moreover we will sometimes use the same notation for π∗ and π0.
Theorem 1.27 (Stationnary deterministic dynamic programming). Assume that E is a finite set, and that (A1) holds together with one of the assumptions (A2), (A3) or (A4). Then the value function v of the problem
v(x0) = sup{J(X; U ) | (X, U ) satisfies (1.2b) } (1.7)
with J as in (1.6), satisfies the equation:
v(x) = sup{r(x, u) + αv(f (x, u)) | u ∈ C(x)} ∀x ∈ E . (1.8)
For all x0 ∈ E, the value v(x0) coincides with the optimum of J over all strategies or over all feedback policies, or over all open-loop controls. When α < 1, the solution of (1.8) is unique. Moreover, assume that the maximum of (1.8) is attained for an action u ∈ C(x) and let us denote by π(x) this action, then the stationary feedback policy π∗ = (πk)k≥0, with πk = π for all k ≥ 0, is an optimal strategy of the problem, and the dynamics Xk+1 = f (Xk, π(Xk)) with Uk = π(Xk) furnishes an optimal solution (X, U ) of the infinite horizon control problem.
To show this result, we shall use the Bellman operator.
Definition 1.28. Let Bα : RE → RE be the map such that for all v ∈ RE , and x ∈ E, we have
[Bα(v)](x) = sup{r(x, u) + αv(f (x, u)) | u ∈ C(x)}
= sup{G(x, y) + αv(y) | y ∈ E} ,
where
G(x, y) = sup{r(x, u) | u ∈ C(x), f (x, u) = y} ∈ R ∪ {−∞} .
The map Bα is called the Bellman operator of the discounted infinite horizon optimal control problem.
15


Note that since r is bounded from above, G(x, y) exists in R ∪ {−∞}. Then, if E a finite set, we get that the values of G(x, y) which are finite (that is 6= −∞) are bounded from below. Then, one may have assumed from the begining that r is bounded from below and above. The dynamic programming equation can be rewritten in functional form as the fixed point equation of the Bellman operator Bα: v = Bα(v) .
Fact 1.29. The undiscounted Bellman operator B1 is monotone and additively homogenous.
Corollary 1.30. The discounted Bellman operator Bα is Lipschitz continuous for the sup-norm with Lipschitz constant α, thus it is α-contracting when α < 1.
Proof. From Proposition 1.23, B1 is nonexpansive for the sup-norm. We have Bα(v) = B1(αv), so ‖Bα(v) − Bα(w)‖∞ = ‖B1(αv) − B1(αw)‖∞ ≤ ‖αv − αw‖∞ = α‖v − w‖∞.
Corollary 1.31. When E is finite and α < 1, the operator Bα admits a unique fixed point v∗. Moreover, for any initial point v0 ∈ RE , the sequence vn+1 = Bα(vn) converges towards v∗:
‖vn − v∗‖∞ ≤ αn‖v0 − v∗‖∞ .
Proof. This follows from the fixed point theorem since RE is a Banach space and Bα is contracting.
Proof of Theorem 1.27 when α < 1. Assume that E is a finite set and that α < 1. Let v∗ be the unique solution of the Bellman equation v = Bα(v), by Corollary 1.31. Let v(N) be the value function of the finite horizon problem:
v(N)(x) = max
X,U {J (N)(X; U ) | (Xk, Yk) satisfying (1.2b)}
with
J (N)(X; U ) :=
(N −1
∑
k=0
αkr(Xk, Uk)
)
+0 .
From Theorem 1.12, v(N) = v(N)
0 with v(N)
k solution of the dynamic programming equation:
v(N )
N (x) = 0 ∀x ∈ E ,
v(N )
k (x) = sup{αkr(x, u) + v(N)
k+1(f (x, u)) | u ∈ C(x)} ∀x ∈ E, k ≤ N − 1.
This can be rewritten as v(N)
k = αkBα(v(N)
k+1/αk+1). Hence, v(N) = BαN (0) := Bα ◦ · · · ◦ Bα(0) (where
the composition is done N times). Therefore, limN→∞ v(N) = v∗ where the limit is uniform in E (limit for the sup-norm of RE ). Let C be a bound of the finite values of |G(x, y)|, with G as in Definition 1.28. Then, for any infinite sequences X and U , we have
|J (N)(X; U ) − J (X; U )| ≤
∞
∑
k=N
αkC = αN C
1−α .
16


Using the definition of the value function v of the infinite horizon problem and that of v(N), as the supremum of J(X; U ) and J(N)(X; U ) respectively, we deduce
‖v − v(N)‖∞ ≤ αN C
1−α ,
so limN→∞ v(N) = v, which implies that v = v∗. The proof of optimality is similar to the finite horizon case.
Proof of Theorem 1.27 when α = 1. Under Assumption (A3), we get that v ≤ 0 and v(x) ∈ R for all x ∈ E. Moreover, v(x) ≤ v(N)(x) ≤ v(N−1)(x) for all x ∈ E and N ≥ 1. This implies that v(N) has a limit v∗ which satisfies v ≤ v∗. Since v(N) = B1(v(N−1)) for all N ≥ 1, and B1 is continuous, we get that v∗ = B1(v∗). It remains to prove that v = v∗. We shall use the finiteness of E. Let δ > 0 be a lower bound of −G(x, y) over all (x, y) such that G(x, y) < 0 and let n be the cardinality of E. Let ε > 0 be such that ε < δ, and N such that v(N)(x) ≤ v∗(x) + ε/3. We get that v(N)(x) ≤ v(N+n)(x) + ε/3 and if (X, U ) is ε/3-optimal for v(N+n), we deduce that ∑N−1
k=0 G(Xk, Xk+1) ≤ v(N)(x) ≤ v(N+n)(x)+ε/3 ≤
∑N +n−1
k=0 G(Xk, Xk+1) + 2 × ε/3 so, for all k = N, . . . N + n − 1, we have −G(Xk, Xk+1) ≤ 2 × ε/3. This implies that all these (Xk, Xk+1) are such that G(Xk, Xk+1) = 0, and since the cardinality of E is equal to n, two elements of the sequence (XN , . . . , XN+n) are equal, which means that there is a cycle (X`, . . . , X`′ = X`). Consider the infinite sequence obtained by concatening (X0, . . . , X`)
with an infinite number of the cycle (X`, . . . X`′). We obtain that J(X; U ) = ∑`−1
k=0 G(Xk, Xk+1) =
∑N +n−1
k=0 G(Xk, Xk+1) ≥ v(N+n)(x) − ε/3 ≥ v∗ − ε/3. Since v(x) ≥ J (X; U ) we deduce that
v(x) ≥ v∗(x) − ε/3. Since this holds for all ε > 0 (with ε < δ), we get that v(x) ≥ v∗(x), and so the equality.
Definition 1.32. The algorithm constructing the sequence vn+1 = Bα(vn) is called value iterations.
In practice one uses rather a variant similar to Gauss-Seidel algorithm (wrt to Jacobi) for the solution of linear systems, in order to avoid useless storage. The resulting algorithm is called Ford-Bellman algorithm. It depends on some ordering on E. When E = {1, . . . , N }, it is as follows:
vk,0 = vk,
for j = 1, . . . , N, vk,j(j) = [Bα(vk,j−1)](j), vk,j(`) = vk,j−1(`) ∀` 6= j,
vk+1 = vk,N .
When α = 1, under suitable conditions, the value iterations converge in finite time ≤ N = card(E) to the solution. So with a computational time in O(mN ), where m = card({(x, y) ∈ E × E | G(x, y) 6= −∞}).
When in addition (A3) holds (in particular r ≤ 0), the problem is equivalent to a shortest path problem with weight G and no constraints in the length of paths. The fixed point equation can be solved using Dijkstra algorithm which is equivalent to one full step of Ford-Bellman algorithm with an appropriate ordering of states. The computational time is then in O(m + N log N ) (with the implementation of Fredman and Tarjan).
Exercise 1.5.1 (Hierarchical shortest path problem). Alice and Bob are in holidays in Venezia with the little Charlie (1 year). Venizia is composed of islets connected with bridges with several stairs, which are thus difficult to cross with the heavy stroller of Charlie.
17


Assume that the map of Venezia is approximated by a graph in which nodes correspond to landmarks and arcs correspond to streets and bridges, and that we know the travel time and number of stairs to cross between nodes. Alice et Bob want to find the paths between two points xi and xf of the graph which minimize first the number of stairs and among all paths minimizing the number of stairs, they want to choose the ones which minimize also the travel time. Modelize this problem as a deterministic control problem.
1.6 Max-plus or Tropical algebra
The equations in (1.3b) can be seen as linear over the following semifield.
Consider the set R ∪ {−∞} of real numbers extended by −∞, endowed with the maximization as an addition and the usual addition as a multiplication: a ⊕ b = max(a, b) and a ⊗ b = a + b for all a, b ∈ R ∪ {−∞}.
The addition is commutative, associative and has the zero element −∞, which is absorbing for the multiplication, the multiplication is commutative, associative, has the unit (neutral) element 0, and it distributes over the addition ⊕.
The addition is idempotent, meaning that a ⊕ a = a for all a. Therefore opposites to non zero elements do not exist.
Inverses (for the multiplication) exist for all non zero elements. So we obtain a semifield called the max-plus algebra or the tropical algebra, that is often denoted Rmax.
Then the dynamic programming equation of deterministic control with finite horizon (1.3b) can be rewritten as
vk(x) =
⊕
u∈Ck (x)
rk(x, u) ⊗ vk+1(fk(x, u)) ∀x ∈ E, and k = T − 1, . . . , 0 , (1.9)
which is a linear equation over Rmax. In particular, denoting
M (k)
xy = sup{rk(x, u) | u ∈ Ck(x), fk(x, u) = y}
for all x, y ∈ E (this was Gk above), then (1.9) can be rewritten as
vk(x) =
⊕
y∈E
M (k)
xy ⊗ vk+1(y) ∀x ∈ E, and k = T − 1, . . . , 0 ,
that is the vector vk is the product of the tropical matrix M (k) with entries M (k)
xy , x, y ∈ E by the vector vk+1. Hence, v0 is obtained by applying the product of the T matrices M (0), . . . , M (T −1) to the vector vT = φ.
In this way, the Bellman equation can be seen as a Kolmogorov equation associated to a Markov chain, as in Chapter 2.
In some situations, the analogy with usual numerical linear algebra may suggest some algorithms. These algorithms are often called tropical numerical methods.
18


1.7 Solutions of Exercises
Exercise 1.3.1. The example can be solved by applying (1.5) with N = 3, m1 = 4, m2 = 3, m3 = 2, w1 = 5, w2 = 4, w3 = 3. These equations reduce to:
v3(x) = 0 for x ∈ {0, . . . , 10}
v2(x) = max(v3(x), 2 + v3(x − 3))
v1(x) = max(v2(x), 3 + v2(x − 4))
v0(x) = max(v1(x), 4 + v1(x − 5))
with the extension of vk to −∞ for x < 0. This gives the following table of values of the problem:
x 0 1 2 3 4 5 6 7 8 9 10 v3 0 0 0 0 0 0 0 0 0 0 0 v2 0 0 0 2 2 2 2 2 2 2 2 v1 0 0 0 2 3 3 3 5 5 5 5 v0 0 0 0 2 3 4 4 5 6 7 7
This table determines the optimal policy at each step k = 0, 1, 2: πk(x) = 0 if vk(x) = vk+1(x) and πk(x) = 1 otherwise. The value of the knapsack problem is equal to v0(10) = 7. It is obtained by starting with X0 = 10 and taking the controls uk = Uk−1 = πk−1(Xk−1), and Xk = Xk−1 − wk−1Uk−1. In view of the above table, we have v0(10) 6= v1(10), so u1 = U0 = 1 and X1 = 10 − 5 = 5. For X1 = 5, we have v1(5) = 3 6= v2(5), so u2 = U1 = 1 and X2 = 5−4 = 1. For X2 = 1, we have v2(1) = 0 = v3(1), so u3 = U2 = 0 and X3 = 1.
Exercise 1.5.1.
19


20


Chapter 2
Markov chains and Kolmogorov
equations
2.1 Introduction and Notations
We shall consider here Markov chains over a finite (or countable) state space E. These are the random version of the dynamical system Xn+1 = fn(Xn). We shall also consider functionals similar to the ones optimized in the optimal control problems of Chapter 1. and prove a “linear version” of Bellman dynamic programming equation: the Kolmogorov equation. Bellow are some general notations used in all the course.
• The state space E is assumed to be finite or possibly countable, that is discrete. Let N = card(E) ∈ N ∪ {∞}.
• The elements of E are (linearly) ordered, one can identify E with {1, . . . , N } (or N if N = ∞), and any element of RE , that is any function E → R, to a column vector in RN .
• We shall use this identification, without fixing any order on E.
• More generally, we shall speak about matrices over E: an element M of RE×E is a matrix over E, and its entries are denoted (Mxy)x,y∈E .
• E beeing at most countable, we shall endow E with the σ-algebra P(E) of all subsets of E.
• A probability law p over E will be identified to a row vector, although we will also write p ∈ RE , more precisely this is an element of the simplex:
∆E = {p ∈ RE | px ≥ 0 ∀x ∈ E, p1 =
∑
x∈E
px = 1} .
• The Dirac measure over E in state x ∈ E will be denoted δx: its entries are 1 in x and 0 elsewhere.
• A matrix M ∈ RE×E is a Markov (or a stochastic) matrix if its entries are all nonnegative (Mxy ≥ 0 for all x, y ∈ E) and M 1 = 1 (∑
y∈E Mxy = 1 for all x ∈ E).
21


• Given a probability space (Ω, A, P ), a random variable taking its values in E is by definition a measurable function from (Ω, A) to (E, P(E)).
• Given a filtration (Fn)n∈N on (Ω, A), that is a nondecreasing sequence of σ-algebras Fn ⊂ A, we say that a sequence (Xn)n≥0 of random variables (also called a random process) taking its values in E is adapted to the filtration if for all n ≥ 0, Xn is measurable from (Ω, Fn) to (E, P(E)).
2.2 Markov property
Definition 2.1. A sequence (Xn)n∈N of random variables over (Ω, A, P ), taking its values in E, is a Markov chain (or a discrete time Markov process) if it satisfies:
P (Xn+1 = xn+1 | X0 = x0, . . . , Xn = xn) = P (Xn+1 = xn+1 | Xn = xn) (2.1)
for all n ≥ 1, x0, . . . , xn+1 ∈ E. The probability measure p(0) and the Markov matrices M (n) over E defined by:
p(0)
x = P (X0 = x)
M (n)
xy = P (Xn+1 = y | Xn = x)
for all x, y ∈ E are respectively called the initial law and the transition matrix at time n of the Markov chain (Xn)n∈N. The Markov chain is stationary if the transition matrices M (n) do not depend on n. If p(0) = δx0, we say that x0 is the initial state of the Markov chain.
Example 2.2 (Random walk). A particule or a drunk man is walking on a line: at each unit of time, he is going forward with probability p ∈ [0, 1], and backward with probability 1 − p. If he stops at boundaries of E = {0, . . . , N }, then the position Xn at time n defines a Markov chain Xn over E such that
P (Xn+1 = x + 1 | Xn = x) = 1 − P (Xn+1 = x − 1 | Xn = x) = p,
when x 6= 0, N . The transition matrix is given by:
M=

        
1 0 0 0 ··· 0 1−p 0 p 0 ··· 0
0 . . . . . . . . . ...
... . . . . . . . . . 0
0 ··· 0 1−p 0 p 0 ··· 0 0 0 1

        
Proposition 2.3. Let (Xn)n∈N be a sequence of random variables over (Ω, A, P ), taking its values in E, with initial law p(0). Then the following are equivalent:
1. (Xn)n∈N is a Markov chain with transition matrices M (n) at time n ∈ N;
22


2. P (X0 = x0, . . . , Xn = xn) = p(0)
x0 M (0)
x0x1 · · · M (n−1)
xn−1xn , for all n ≥ 1, x0, . . . , xn ∈ E;
3. P (Xn+1 = xn+1 | X0 = x0, . . . , Xn = xn) = M (n)
xnxn+1 , for all n ≥ 0, x0, . . . , xn+1 ∈ E ;
Proof. Let (Xn)n∈N be a sequence of random variables over (Ω, A, P ), taking its values in E, with initial law p(0). 1. ⇒ 2. Assume (Xn)n∈N is a Markov chain with transition matrices M (n) at time n ∈ N, then it satisfies:
P (X0 = x0, . . . , Xn = xn)
= P (X0 = x0, . . . , Xn−1 = xn−1)P (Xn = xn | X0 = x0, . . . , Xn−1 = xn−1)
= P (X0 = x0, . . . , Xn−1 = xn−1)M (n−1)
xn−1xn ,
from which one deduce 2. by induction. 2. ⇒ 3. Assume that (Xn) satisfies 2. Then,
P (Xn+1 = xn+1 | X0 = x0, . . . , Xn = xn)
= P (X0 = x0, . . . , Xn+1 = xn+1)
P (X0 = x0, . . . , Xn = xn)
= p(0)
x0 M (0)
x0x1 · · · M (n)
xn xn+1 p(0)
x0 M (0)
x0x1 · · · M (n−1)
xn−1 xn = M (n)
xnxn+1 ,
that is 3. 3. ⇒ 1. Assume now that (Xn) satisfies 3., or equivalently assume that P (Xn+1 = xn+1 | X0 = x0, . . . , Xn = xn) does not depend on x0, . . . , xn−1. Let us deduce that it is also equal to P (Xn+1 = xn+1 | Xn = xn), that is the Markov property. Indeed,
P (Xn = xn, Xn+1 = xn+1)
=
∑
x0,...,xn−1∈E
P (X0 = x0, . . . , Xn = xn, Xn+1 = xn+1)
=
∑
x0,...,xn−1∈E
(P (X0 = x0, . . . , Xn = xn)P (Xn+1 = xn+1 | X0 = x0, . . . , Xn = xn))
=


∑
x0,...,xn−1∈E
P (X0 = x0, . . . , Xn = xn)

 M (n)
xn xn+1
= P (Xn = xn)P (Xn+1 = xn+1 | X0 = x0, . . . , Xn = xn) .
Corollary 2.4. Let (Xn)n∈N be a sequence of random variables over (Ω, A, P ), taking its values in E. If, for all n ≥ 0, x0, . . . , xn+1 ∈ E, P (Xn+1 = xn+1 | X0 = x0, . . . , Xn = xn) does not depend on x0, . . . , xn−1, that is is a function of n, xn, xn+1 only, then (Xn)n∈N is a Markov chain.
23


Theorem 2.5. Let p(0) be a probabilty over E and M (n) be Markov matrices over E, for all n ∈ N. Then, there exists a probability space (Ω, A, P ) and a Markov chain (Xn)n∈N on (Ω, A, P ) taking its values in E, with initial law p(0) and transition matrices M (n).
Sketch of proof. We know that necessarily, the law of Xn is given by the formula in 2 of Proposition 2.3. This allows to compute the probability of all the cylinders A0 × · · · × An × EN of EN. Therefore, it is sufficient to consider the canonical probability space Ω = EN, with A the σ-algebra generated by finite cylinders, and P the probability on (Ω, A) already given on cylinders. Such a probability exists and is unique by Kolmogorov extension theorem.
2.3 Elementary Properties and representations
Proposition 2.6 (Fokker-Plank equation). Let (Xn)n∈N be a Markov chain over (Ω, A, P ), taking its values in E, with initial law p(0) and Markov transition matrices M (n) at time n ∈ N. Then, the law p(n) of the random variable Xn satisfies the Fokker-Plank recurrence equation:
p(n+1) = p(n)M (n) .
Proof. Using the definition of p(n), M (n) and of conditional probabilities, we get, for all xn+1 ∈ E,
p(n+1)
xn+1 = P (Xn+1 = xn+1)
=
∑
xn∈E
P (Xn = xn, Xn+1 = xn+1)
=
∑
xn∈E
P (Xn+1 = xn+1 | Xn = xn)P (Xn = xn)
=
∑
xn∈E
M (n)
xnxn+1 p(n)
xn = (p(n)M (n))xn+1 .
Remark 2.7. The proof of Fokker-Plank equation does not use the Markov property, but only the value of P (Xn+1 = xn+1 | Xn = xn).
Proposition 2.8. Let (Xn)n∈N be a Markov chain over (Ω, A, P ), taking its values in E, with initial law p(0) and Markov transition matrices M (n) at time n ∈ N. Then, the sequence (Xn+k)n∈N is a Markov chain with initial law p(k) (given by Fokker-Plank equation) and Markov transition matrices M (n+k) at time n ∈ N.
Proof. By Proposition 2.3, the sequence (Xn)n∈N satisfies (2). Therefore, for k, n ∈ N, we have
P (X0 = x0, . . . , Xn+k = xn+k) = p(0)
x0 M (0)
x0x1 · · · M (k−1)
xk−1xk · · · M (n+k)
xn+k−1xn+k .
Taking the sum for all x0, . . . , xk−1 ∈ E, we get
P (Xk = xk, . . . , Xn+k = xn+k) =


∑
x0,...,xk−1∈E
p(0)
x0 M (0)
x0x1 · · · M (k−1)
xk−1 xk

 M (k)
xkxk+1 · · · M (n+k)
xn+k−1xn+k .
(2.2)
24


For n = 0 this equation writes
P (Xk = xk) =
∑
x0,...,xk−1∈E
p(0)
x0 M (0)
x0x1 · · · M (k−1)
xk−1xk .
Together with (2.2) for any n ∈ N, this gives
P (Xk = xk, . . . , Xn+k = xn+k) = P (Xk = xk)M (k)
xkxk+1 · · · M (n+k)
xn+k−1 xn+k = p(k)
xk M (k)
xkxk+1 · · · M (n+k)
xn+k−1xn+k ,
where p(k) is the law of Xk. Therefore, using Proposition 2.3, we get that (Xn+k)n∈N is a Markov chain with initial law p(k) and transition matrix M (n+k) at time n ∈ N. Moreover, from Proposition 2.6, p(k) satisfies Fokker-Plank equation.
Proposition 2.9. Let (Xn)n∈N be a Markov chain over (Ω, A, P ), taking its values in E, with initial law p(0) and Markov transition matrices M (n) at time n ∈ N. Then, for all x0, . . . , xT ∈ E, 0 ≤ k ≤ T , we have
P (Xk+1 = xk+1, . . . , XT = xT | X0 = x0, . . . , Xk = xk) = M (k)
xkxk+1 · · · M (T −1)
xT −1xT .
Proposition 2.10. Let (Xn)n∈N be a Markov chain over (Ω, A, P ), taking its values in E, with initial law p(0) and Markov transition matrices M (n) at time n ∈ N. Then, for all k ≥ 1, the sequence (Xnk)n∈N is a Markov chain with initial law p(0) and Markov transition matrices M (nk) · · · M ((n+1)k−1) at time n ∈ N.
Proof. From Proposition 2.3, the sequence (Xn)n∈N satisfies (2). Then, for all k ≥ 1 and n ∈ N, we have
P (X0 = x0, . . . , Xnk = xnk) = p(0)
x0 M (0)
x0x1 · · · M (nk)
xnk−1xnk .
Taking the sum over all x1, . . . , xk−1, xk+1, . . . , x2k−1, . . . x(n−1)k+1, . . . , xnk−1 ∈ E, we get:
P (X0 = x0, . . . , Xmk = xmk, . . . , Xnk = xnk)
=
∑
x1 ,...,xk−1 ,xk+1 ,...,x2k−1 ,...x(n−1)k+1 ,...,xnk−1 ∈E
p(0)
x0 M (0)
x0x1 · · · M (nk)
xnk−1xnk .
Denoting M (k,n) := M nk · · · M ((n+1)k−1), we obtain
P (X0 = x0, . . . , Xmk = xmk, . . . , Xnk = xnk) = p(0)
x0 M (k,0)
x0xk · · · M (k,n)
x(n−1)kxnk ,
which with Proposition 2.3 shows the result.
Practical examples are often constructed in the following way.
Fact 2.11. Let (Ω, A, P ) be a probability space. Let X0 be a random variable taking its values in E, (Wn)n≥0 be a sequence of independent random variables taking its values in a finite set W, and independent of X0, and for all n ∈ N, let fn : E × W → E be a map. Then, the sequence Xn defined recursively by:
Xn+1 = fn(Xn, Wn), n ∈ N ,
is a Markov chain taking its values in E.
25


Proof. Indeed Xn is a deterministic function of X0, W0, . . . , Wn−1, so is independent of Wn. Hence, P (Xn+1 = xn+1 | X0 = x0, . . . , Xn = xn) = P (fn(Xn, Wn) = xn+1 | X0 = x0, . . . , Xn = xn) = P (fn(xn, Wn) = xn+1 | X0 = x0, . . . , Xn = xn) = P (fn(xn, Wn) = xn+1) only de
pends on xn and xn+1. From Proposition 2.3, Xn is a Markov chain with transition matrix
M (n)
xnxn+1 = P (fn(xn, Wn) = xn+1)
Proposition 2.12. Conversely, let p0) and M (n) be the initial law and transition Markov matrices of a Markov chain taking its values in E. There exists a probability space (Ω, A, P ), a set W, a sequence (fn)n≥0 of maps fn : E × W → E, and, over (Ω, A, P ), a random variable X0 taking its values in E with law p(0), and a sequence (Wn)n≥0 of independent random variables, taking their values in W, and independent of X0, such that the sequence (Xn)n≥0 defined recursively by Xn+1 = fn(Xn, Wn) is a Markov chain with transition Markov matrices M (n).
Proof. Assume to simplify that E is a finite set. Let W be the set of maps from E to itself. For each n ≥ 0, denote by qn the probability law on W defined, for all w ∈ W (w : E → E, x 7→ w(x)),
by qn(w) = ∏
x∈E M (n)
xw(x). Let X0, W0, . . . , Wn, . . . be independent random variables with values in
E, W ,. . . , W, . . . , respectively with laws p(0), q0, . . . , qn, . . . , respectively. Such a sequence can be constructed on Ω = E × WN, with A the set of cylinders. Then, taking fn(x, w) = w(x), we get that Xn+1 = fn(Xn, Wn) defines a Markov chain with transition Markov matrices M (n).
Exercise 2.3.1. Let (Ω, A, P ) be a probability space. Let X0 be a random variable taking its values in a finite set E, let (Wn)n≥0 be a Markov chain taking its values in a finite set W, with initial law q(0), transition matrix M (n) at time n ≥ 0, and independent of X0, and for all n ∈ N, let fn : E × W → E be a map. Consider the sequence Xn of random variables taking its values in E, and defined recursively by:
Xn+1 = fn(Xn, Wn), n ∈ N .
Show that ((Xn, Wn))n≥0 is a Markov chain and compute its transition matrix.
2.4 The digraph of a stationary Markov chain
Definition 2.13. Let M ∈ RE×E be a matrix with nonnegative entries, in particular a Markov matrix. We associate to M a digraph, denoted G(M ), with set of nodes E and set of arcs A such that (x, y) ∈ E × E is in A if and only if Mxy > 0. We associate also the weight map w : A → R, (x, y) 7→ Mxy. Then, the weight of a path p = (x0, . . . , xn) of G(M ) (i.e. such that x0, . . . , xn ∈ E and (x0, x1), . . . , (xn+1, xn) are arcs in G(M )), is defined as w(p) = w((x0, x1)) × · · · × w((xn−1, xn)) =
Mx0x1 · · · Mxn−1xn .
If Xn is a stationary Markov chain with transition matrix M , we have
P (X1 = x1, . . . , Xn = xn | X0 = x0) =
{
w(x0, . . . , xn) if (x0, . . . , xn) is a path of G(M )
0 otherwise.
Moreover,
P (Xn+k = y | Xk = x) = (M n)xy =
∑
p path of length n in G(M ), from x to y
w(p),
26


where the length of a path is the number of its arcs. Indeed,
Mn
xy =
∑
(x1 ,...,xn−1 )∈E n−1
Mxx1 · · · Mxn−2xn−1 Mxn−1y
=
∑
(x1 ,...,xn−1 )∈E n−1
P (Xk+1 = x1, . . . Xk+n−1 = xn−1, Xk+n = y | Xk = x)
=
∑
(x1 ,...,xn−1 )∈E n−1 Mxx1 >0,...,Mxn−1y>0
w(x, x1, . . . , xn−1, y) .
Example 2.14. A Markov matrix and its associated digraph G(M ) (with weights written on arcs):
M=


0.5 0.5 0 0.7 0.1 0.2 001


13
2
0.7
0.1
1
0.5
0.5
0.2
Example 2.15 (Random walks). The digraph associated to the random walk of Example 2.2 is as follows:
Z
p pp
1−p 1−p 1−p
n−2 n−1 n n+1
The digraph associated to a random walk on Z2 with probability p, q, r, s > 0 to go respectively to right, up, left and down (with p + q + r + s = 1) is shown in
27


p pp
Z2
sq
r
sq
2.5 Kolmogorov equation for finite horizon criteria
The following equation is the dual of Fokker-Plank equation (seen in Proposition 2.6):
Proposition 2.16 (Kolmogorov equation without instantaneous reward). Let (Xn)n∈N be a Markov chain over (Ω, A, P ), taking its values in E, with Markov transition matrices M (n) at time n ∈ N. Let φ ∈ RE and T ∈ N, and denote:
vk(x) = E [φ(XT ) | Xk = x] .
Then, the value vk satisfies the following backward recurrence equation, called Kolmogorov equation :
vk = M (k)vk+1, 0 ≤ k ≤ T − 1 ,
with final condition:
vT = φ .
28


Proof. One way is to use the formula already proved for the probabilities of the tuple (X0, . . . , Xn):
vk(x) =
∑
xk+1,...,xT ∈E
(P (Xk+1 = xk+1, . . . , XT = xT | Xk = x)φ(xT ))
=
∑
xk+1,...,xT ∈E
( P (Xk = x, Xk+1 = xk+1, . . . , XT = xT )
P (Xk = x) φ(xT )
)
=
∑
xk+1,...,xT ∈E
(
M (k)
xxk+1 · · · M (T −1)
xT −1xT φ(xT )
)
= (M (k) · · · M (T −1)φ)x .
Then vk = M (k) · · · M (T −1)φ, which implies vk = M (k)vk+1 and vT = φ. Another way is to obtain Kolmogorov equation directely from conditional expectations:
vk(x) = E [φ(XT ) | Xk = x] = E [E [φ(XT ) | Xk+1] | Xk = x] .
E [φ(XT ) | Xk+1] is the projection of φ(XT ) on the σ-algebra genererated by the random variable Xk+1. Since E is finite, we also get E [φ(XT ) | Xk+1] = h(Xk+1), where h is the deterministic function defined by h : E → R, x 7→ E [φ(XT ) | Xk+1 = x], that is h = vk+1. We deduce:
vk(x) = E [vk+1(Xk+1) | Xk = x]
=
∑
y∈E
P (Xk+1 = y | Xk = x)vk+1(y)
=
∑
y∈E
M (k)
xy vk+1(y) = (M (k)vk+1)x ,
so vk = M (k)vk+1.
Remark 2.17. Another way to prove Proposition 2.16 is to show that the sequence
Mn = vn(Xn), n ≥ 0,
is a Martingale for the filtration (Fn)n≥0 associated to the Markov chain (Xn)n≥0, that is
E [Mn+1 | Fn] = Mn .
Indeed, using the definition of vn, we get, for n ≥ 0,
Mn = E [φ(XT ) | Xn] .
Moreover, since Xn is a Markov chain, the above conditional expectation is equivalent to the one with respect to Fn. Indeed, E [φ(XT ) | FT −1] is necessarily of the form ψT (X0, . . . XT −1), where ψT is measurable and given by: ψT (x0, . . . , xT −1) = E [φ(XT ) | X0 = x0, . . . , XT −1 = xT −1] and since Xn is a Markov chain, we get that ψT (x0, . . . , xT −1) = E [φ(XT ) | XT −1 = xT −1] depends only on xT −1, so E [φ(XT ) | FT −1] = ψT (XT −1). By induction, we get also that E [φ(XT ) | Fn] is a measurable function of Xn. Since the σ-algebra generated by Xn is smaller than Fn (which is generated by X0, . . . , Xn), we deduce that E [φ(XT ) | Fn] = E [φ(XT ) | Xn].
29


Therefore, Mn = E [φ(XT ) | Fn]
which is clearly a Martingale, by the property of compositions of conditional expectations:
E [Mn+1 | Fn] = E [E [φ(XT ) | Fn+1] | Fn] = E [φ(XT ) | Fn] = Mn .
This implies that
vk(Xk) = Mk = E [Mk+1 | Fk] = E [vk+1(Xk+1) | Xk] ,
which gives the recurrence equation of Proposition 2.16.
The following more general result will be used in what follows in order to establish the Bellman dynamic programming equation for Markov decision problems, which will be a nonlinear extension of Kolmogorov equation.
Theorem 2.18 (Kolmogorov Equation for an additive functional). Let (Xn)n∈N be a Markov chain over (Ω, A, P ), taking its values in E, with Markov transition matrices M (n) at time n ∈ N. Let φ ∈ RE , T ∈ N, and rk ∈ RE for 0 ≤ k ≤ T − 1, and denote:
vk(x) = E
[(T −1
∑
`=k
r`(X`)
)
+ φ(XT ) | Xk = x
]
.
Then, vk satisfies the following backward recurrence equation, called Kolmogorov equation :
vk = rk + M (k)vk+1, 0 ≤ k ≤ T − 1 , (2.3a)
with final condition:
vT = φ . (2.3b)
Proof. Use the second way of proof of previous Kolmogorov equation:
vk(x) = E
[(T −1
∑
`=k
r`(X`)
)
+ φ(XT ) | Xk = x
]
= E [rk(Xk) | Xk = x] + E
[( T −1
∑
`=k+1
r`(X`)
)
+ φ(XT ) | Xk = x
]
= rk(x) + E
[
E
[( T −1
∑
`=k+1
r`(X`)
)
+ φ(XT ) | Xk+1
]
| Xk = x
]
= rk(x) + E [vk+1(Xk+1) | Xk = x]
= rk(x) +
∑
y∈E
P (Xk+1 = y | Xk = x)vk+1(y)
= rk(x) +
∑
y∈E
M (k)
xy vk+1(y)
= (rk + M (k)vk+1)x ,
hence vk = rk + M (k)vk+1.
30


Remark 2.19. As in Remark 2.17, another way to prove Theorem 2.18 is to show that for all k ≥ 0, the sequence
Mn =
(n−1
∑
`=k
r`(X`)
)
+ vn(Xn), n ≥ k,
is a Martingale for the filtration (Fn)n≥0 associated to the Markov chain (Xn)n≥k. Indeed, using the definition of vn, we get, for n ≥ k,
Mn =
(n−1
∑
`=k
r`(X`)
)
+E
[(T −1
∑
`=n
r`(X`)
)
+ φ(XT ) | Xn
]
and since Xk, . . . , Xn−1 are measurable with respect to Fn and Xn is a Markov chain, so the above conditional expectation with respect to Xn is equivalent to the one with respect to Fn, we get
Mn = E
[(n−1
∑
`=k
r`(X`)
)
+
(T −1
∑
`=n
r`(X`)
)
+ φ(XT ) | Fn
]
=E
[(T −1
∑
`=k
r`(X`)
)
+ φ(XT ) | Fn
]
which is clearly a Martingale. This implies that
vk(Xk) = Mk = E [Mk+1 | Fk] = rk(Xk) + E [vk+1(Xk+1) | Fk] = rk(Xk) + E [vk+1(Xk+1) | Xk] ,
which gives the recurrence equation of Theorem 2.18.
Remark 2.20. When rk = r and M (k) = M do not depend on k, (hence the Markov chain (Xn)n∈N is stationary), the Kolmogorov equation writes vk = r + M vk+1, so that one can consider the value function as a function of the remaining time until the end:
v(t)(x) = E
[( t−1
∑
`=0
r`(X`)
)
+ φ(Xt) | X0 = x
]
,
which satisfies a forward Kolmogorov equation:
v(t+1) = r + M v(t) .
Remark 2.21. Moreover, Kolmogorov equation can be rewritten as
vk − vk−1 + (M − I)vk + r = 0 .
which is analogue to the Kolmogorov equation of a Markov process (with continuous time):
dv
dt + (M − I)v + r = 0 .
Then, M − I is called the infinitesimal generator of the Markov chain. (A Markov process is obtained when the holding times in each state are random independent with exponential law). A similar Kolmogorov equation is obtained when (Xt)t≥0 is a Wiener process:
dv
dt + 1
2 ∆v + r = 0 .
31


Exercise 2.5.1. Compute
v := E
[(T −1
∑
`=0
r`(X`)
)
+ φ(XT )
]
,
for any initial law of the Markov chain (Xn)n≥0.
Exercise 2.5.2. Let Xn be a Markov chain with values in (a finite subset of) N, and consider the sequence Yn = X0 + · · · + Xn. Compute
v(x) = E [φ(YT ) | X0 = x] .
Do the same for Yn = max(X0, . . . , Xn).
Exercise 2.5.3. Consider a Markov chain (Xn)n≥0 with values in E and transition matrix M ∈ RE×E (independent of time). Let f be a function from E to R, compute
vT (x) = P (∃n ∈ {0, . . . , T }, f (Xn) ≥ 1 | X0 = x) .
Taking the exponential of an additive functional does not change optimization problems, but it does change expectation.
Theorem 2.22 (Kolmogorov equation for a multiplicative functional). Let (Xn)n∈N be a Markov chain over (Ω, A, P ), taking its values in E, with Markov transition matrices M (n) at time n ∈ N. Let φ ∈ RE , T ∈ N, and αk ∈ RE+, for 0 ≤ k ≤ T − 1, and denote:
vk(x) = E
[(T −1
∏
`=k
α`(X`)
)
φ(XT ) | Xk = x
]
.
Let A(k) ∈ RE×E be the matrix with nonnegative entries A(k)
xy = αk(x)M (k)
xy , for x, y ∈ E. Then, vk satisfies the following backward recurrence equation:
vk = A(k)vk+1, 0 ≤ k ≤ T − 1 , (2.4a)
with final condition:
vT = φ . (2.4b)
Proof. We use again the same arguments as for the previous Kolmogorov equations:
vk(x) = E
[(T −1
∏
`=k
α`(X`)
)
φ(XT ) | Xk = x
]
= αk(x)E
[
E
[( T −1
∏
`=k+1
α`(X`)
)
φ(XT ) | Xk+1
]
| Xk = x
]
= αk(x)E [vk+1(Xk+1) | Xk = x]
= αk(x)


∑
y∈E
P (Xk+1 = y | Xk = x)vk+1(y)


= αk(x)


∑
y∈E
M (k)
xy vk+1(y)


=
∑
y∈E
A(k)
xy vk+1(y) ,
32


which gives vk = A(k)vk+1.
Again, with the same arguments, one prove:
Theorem 2.23 (Kolmogorov equation for a mixed functional). Let (Xn)n∈N be a Markov chain over (Ω, A, P ), taking its values in E, with Markov transition matrices M (n) at time n ∈ N. Let φ ∈ RE , T ∈ N, rk ∈ RE , and αk ∈ RE+, for 0 ≤ k ≤ T − 1, and denote:
vk(x) = E
[(T −1
∑
`=k
( `−1
∏
m=k
αm(Xm)
)
r`(X`)
)
+
( T −1
∏
m=k
αm(Xm)
)
φ(XT ) | Xk = x
]
.
Let A(k) ∈ RE×E be the matrix with nonnegative entries A(k)
xy = αk(x)M (k)
xy , for x, y ∈ E. Then, vk satisfies the following backward recurrence equation:
vk = rk + A(k)vk+1, 0 ≤ k ≤ T − 1 , (2.5a)
with final condition:
vT = φ . (2.5b)
When αk(x) ≡ α < 1, α is the discount factor, as for determinitic control problems with infinite horizon.
More generally, when αk(x) ≤ 1 depends on x, we call it a variable discount factor.
In that case, the matrix A(k) satisfies A(k)1 ≤ 1. A matrix A with nonnegative entries and such that A1 ≤ 1 is called a submarkovian matrix.
When the matrices A(k) are submarkovian, one can reduce the previous problem/functional to a problem with additive criteria, by adding to the state space E a cemetery point. Indeed, let c denote this cemetery point, assume that c 6∈ E, and consider E′ = E ∪ {c}. Let M ′(k) ∈ RE′×E′ be the matrix such that
M ′(k)
xy = A(k)
xy , when x, y ∈ E
M ′(k)
cc = 1,
M ′(k)
cx = 0, ∀x ∈ E
M ′(k)
xc = 1 − αk(x), ∀x ∈ E ,
and extend rk and φ to E′ in r′
k and φ′ respectively by mapping c to 0.
Proposition 2.24. The value function vk of Theorem 2.23 is the restriction to E of the value function v′
k obtained in Theorem 2.18 for the matrices M ′(k) and functions r′
k et φ′.
Note that v′
k satisfies necessarily v′
k(c) = v′
k+1(c) = · · · = φ′(c) = 0, so the boundary equation:
v′
k(c) = 0 .
33


2.6 Kolmogorov Equations for infinite horizon criteria
Theorem 2.25 (Kolmogorov Equations for a discounted infinite horizon functional). Let (Xn)n∈N be a stationary Markov chain over (Ω, A, P ), taking its values in E, with Markov transition matrix M . Let r, α ∈ RE , satisfying 0 ≤ α(x) ≤ α ̄ for all x ∈ E, for some constant α ̄ < 1. Assume that r is bounded in sup-norm (or E is finite). Denote:
.v(x) = E
[( ∞
∑
`=0
( `−1
∏
m=0
α(Xm)
)
r(X`)
)
| X0 = x
]
. (2.6)
Let A ∈ RE×E be the matrix with nonnegative entries Axy = α(x)Mxy, for x, y ∈ E. Then, v is the unique solution of the fixed point linear equation:
v = r + Av , (2.7)
Before giving the proof let us state some properties of Markov matrices. Recall that for any matrix M ∈ RE×E , the matrix norm ‖M ‖∞ associated to the sup-norm of vectors, defined by:
‖M ‖∞ := sup
u∈RE \{0}
‖M u‖∞
‖u‖∞
, ‖u‖∞ =: sup
x∈E
|ux| ,
satisfies
‖M ‖∞ = sup
x∈E


∑
y∈E
|Mxy |

,
and that spectral radius of M , denoted ρ(M ), which is the maximum of the modulus of its eigenvalues satisfies necessarily ρ(M ) ≤ ‖M ‖ for any matrix norm.
Lemma 2.26. For any Markov matrix M ∈ RE×E , we have
ρ(M ) = ‖M ‖∞ = 1 .
Proof. Since a Markov matrix M has nonnegative entries, and the sum of each row is equal to 1, we get ‖M ‖∞ = 1. Since 1 is an eigenvalue of M associated to the eigenvector 1 (M 1 = 1), we get ρ(M ) ≥ 1. Since 1 ≤ ρ(M ) ≤ ‖M ‖∞ = 1, we deduce the result.
Proof of Theorem 2.25. Assume that E is finite. Since α(Xm) ≤ α ̄ < 1, for all m ≥ 0, and r is bounded, the series inside the expectation in (2.6) is normally converging (for sup-norm), and its sum is a.s. bounded by C/(1 − α ̄), where C is a bound of r. Hence, the expectation v(x) exists, for all x ∈ E, which allows one to define the value function v : x 7→ v(x). By the same arguments as in previous proofs, one shows that v satisfies (2.7). So it remains to prove that (2.7) has a unique solution. This holds if I −A is invertible, that is 1 is not an eigenvalue of A. Using the formula of the sup-norm of a matrix, we get that ‖A‖∞ ≤ α ̄‖M ‖∞ = α ̄ < 1, which implies that ρ(A) ≤ α ̄ < 1, and so I − A is invertible. One can also show that the Kolmogorov operator: K : v 7→ r + Av is monotone and contracting for the sup-norm with factor α ̄ < 1. Indeed
‖K(v) − K(w)‖∞ = ‖A(v − w)‖∞ ≤ ‖A‖∞‖v − w‖∞ ≤ α ̄‖v − w‖∞ .
34


So K has a unique fixed point. This property holds also when E is a countable set (discrete infinite), by considering K as an operator on the Banach space L∞(E) of bounded functions from E to R, endowed with the sup-norm.
In sustainable development problems, on may wish to consider a functional which put more weight on future rewards. This would mean that some of the discount factors α(x) are greater than 1, and so A is no more contracting for the sup-norm. However, the previous result remains if the spectral radius of A remains lower than 1 as in the following result.
Theorem 2.27. Let (Xn)n∈N be a stationary Markov chain over (Ω, A, P ), taking its values in a finite set E, with Markov transition matrix M . Let r, α ∈ RE , satisfying 0 ≤ α(x) for all x ∈ E. Let A ∈ RE×E be the matrix with nonnegative entries Axy = α(x)Mxy, for x, y ∈ E. Assume that the spectral radius α ̄ := ρ(A) is < 1. Then, for all x ∈ S, the value v(x) of (2.6) is well defined and the function v : E → R, x ∈ E 7→ v(x) is the unique solution of the fixed point linear equation (2.7).
Proof. Since S is a finite set, r is bounded by some constant C. Hence
∞
∑
`=0
∣ ∣ ∣ ∣ ∣
( `−1
∏
m=0
α(Xm)
)
r(X`)
∣ ∣ ∣ ∣ ∣
≤ CY
where Y is the nonnegative random variable defined by
Y :=
∞
∑
`=0
( `−1
∏
m=0
α(Xm)
)
.
Since Y is the sum of series with nonnegative coeffcients, its expectation exists and is equal to
E [Y | X0 = x] =
∞
∑
`=0
wk(x) with wk(x) := E
[ k−1
∏
m=0
α(Xm) | X0 = x
]
.
Using Kolmogorov equation for multiplicative functionals, we obtain that wk satisfies wk = Awk−1
and w0 = 1, hence wk = Ak1. Therefore, E [Y | X0 = x] = ∑∞
`=0(Ak1)x. Since ρ(A) < 1, the
previous series converges and is equal to ((I − A)−11)x. Then, Y has a finite expectation, which
implies (by dominated convergence theorem) that the random variable ∑∞
`=0
(
∏`−1
m=0 α(Xm)
)
r(X`)
is well defined (exists almost surely) and has a finite expectation. Hence, the value v(x) in (2.6) is well defined. The rest of the result can be proved using the same arguments as for previous theorem.
2.7 Kolmogorov Equations for stopping time criteria
Definition 2.28. Given a filtration (Fn)n∈N on (Ω, A), a random variable τ with values in N∪{+∞} is a stopping time with respect to (Fn)n∈N if {τ = n} ∈ Fn, for all n ∈ N. We then denote
Fτ := {A ∈ A | {τ ≤ t} ∩ A ∈ Ft, ∀t ≥ 0} .
Fact 2.29. τ is a stopping time if and only if {τ ≤ n} ∈ Fn, for all n ∈ N.
35


Proof. If τ is a stopping time, then {τ ≤ n} = ∪k=0,...,n{τ = k} ∈ Fn, since Fk ⊂ Fn, for all k ≤ n. Conversely, if {τ ≤ n} ∈ Fn, for all n ∈ N, then {τ = n} = {τ ≤ n} \ {τ ≤ n − 1} ∈ Fn, since
Fn−1 ⊂ Fn.
Definition 2.30. Given a Markov chain (Xn)n∈N over (Ω, A, P ), we associate the filtration (Fn)n∈N:
Fn := σa(X0, . . . , Xn), ∀n ∈ N .
Then, a random variable τ with values in N ∪ {+∞} is a stopping time with respect to the Markov chain (Xn)n∈N if it is a stopping time with respect to (Fn)n∈N.
Note that the filtration associated to a Markov chain (Xn)n≥0 is the minimal filtration such that (Xn)n≥0 is adapted to it.
Example 2.31. Let (Xn)n∈N be a Markov chain over (Ω, A, P ), and let B be a subset of E. Then, for all k ∈ N,
τk
B := inf{n ≥ k | Xn 6∈ B}
is a stopping time with respect to the Markov chain (Xn)n≥k. Indeed, for t ≥ 0, {τ k
B > t} =
⋂
k≤`≤t{X` ∈ B} belongs to σa(Xk, . . . , Xt), so does its complementary {τ k
B ≤ t}. The stopping
time τ k
B is called the exit time from B of the Markov chain starting at time k.
Theorem 2.32 (Strong Markov property). Let (Xn)n∈N be a Markov chain over (Ω, A, P ), taking its values in E. Let τ be a stopping time with respect to (Xn)n≥0. We have,
P (Xτ+1 = y1, . . . , Xτ+k = yk | X0 = x0, . . . , Xτ−1 = xτ−1, Xτ = y0 and τ < +∞)
= P (Xτ+1 = y1, . . . , Xτ+k = yk | Xτ = y0 and τ < +∞) .
for all k, ` ∈ N, sequences (x`)`≥0, and y0, . . . , yk ∈ E. Moreover if the chain is stationary then
P (Xτ+1 = y1, . . . , Xτ+k = yk | Xτ = y0 and τ < +∞) = P (X1 = y1, . . . , Xk = yk | X0 = y0) .
Theorem 2.33 (Kolmogorov Equation for a finite horizon functional with stopping time). Let (Xn)n∈N be a Markov chain over (Ω, A, P ), taking its values in E, with Markov transition matrices M (n) at time n ∈ N. Let φ ∈ RE , B ⊂ E be nonempty, T ∈ N, and rk ∈ RB for 0 ≤ k ≤ T − 1, and denote, for all x ∈ E:
vk(x) = E




T ∧τ k
B −1
∑
`=k
r`(X`)

 + φ(XT ∧τk
B ) | Xk = x

.
Then, vk satisfies the following backward recurrence equation, called Kolmogorov equation :
vk(x) = rk(x) + (M (k)vk+1)(x), x ∈ B, 0 ≤ k ≤ T − 1 , (2.8a)
with boundary condition
vk(x) = φ(x), x 6∈ B , (2.8b)
and final condition
vT (x) = φ(x), x ∈ B . (2.8c)
36


Note that here, we used the same name φ for the function involved in the boundary condition and in the final condition. In general, and in particular when the state space is continuous, one consider two maps φ ∈ RB and ψ ∈ RE\B, and define vk as the functional
vk(x) = E




T ∧τ k
B −1
∑
`=k
r`(X`)

 + φ(XT )1T <τk
B + ψ(Xτ k
B )1T ≥τ k
B | Xk = x

.
Proof of Theorem 2.33. Consider the sequence Yn = Xn∧τk
B , for n ≥ k. With the same arguments
as for the strong Markov property, we can show that this is a Markov chain starting at time n = k. Indeed,
P (Yn = yn, · · · , Yk = yk) =
n−1
∑
p=k
P (Yn = yn, · · · , Yk = yk, τ k
B = p)
+ P (Yn = yn, · · · , Yk = yk, τ k
B ≥ n)
=
n−1
∑
p=k
P (Xn∧p = yn, · · · , Xk = yk, τ k
B = p)
+ P (Xn = yn, · · · , Xk = yk, τ k
B ≥ n)
=
n−1
∑
p=k
P (Xp = yp, · · · , Xk = yk)1yk∈B · · · 1yp−1∈B 1yp6∈B 1yp=yp+1=···=yn
+ P (Xn = yn, · · · , Xk = yk)1yk∈B · · · 1yn−1∈B
=
n=1
∑
p=k
M (p−1)
yp−1yp · · · M (k)
ykyk+1 1yk∈B · · · 1yp−1∈B 1yp6∈B 1yp=yp+1=···=yn
+ M (n−1)
yn−1yn · · · M (k)
ykyk+1 1yk∈B · · · 1yn−1∈B
=M (B,n−1)
yn−1yn · · · M (B,k)
yk yk+1
where, for n ≥ k, M (B,n) is given by:
M (B,n)
xy =

 
 
M (n)
xy for x ∈ B, y ∈ E
1 for x = y 6∈ B
0 otherwise.
Hence, by Proposition 2.3, (Yn)n≥k is a Markov chain with transition matrix M (B,n) at time n ≥ k. For all n ≥ 0, extend rn by 0 on E \ B. Then, for all x ∈ E, vk coincides with:
vk(x) = E
[(T −1
∑
`=k
r`(Y`)
)
+ φ(YT ) | Yk = x
]
.
From Theorem 2.18, we get the recurrence equation
vk = rk + M (B,k)vk+1, 0 ≤ k ≤ T − 1 ,
with final condition: vT = φ. This final condition implies (2.8c). When x ∈ B, the recurrence equation gives (2.8a). When x 6∈ B, the recurrence equation gives vk(x) = vk+1(x), which with the final condition implies vk(x) = φ(x), hence the boundary equation (2.8b).
37


Another way to prove Theorem 2.33 is to use Theorem 2.23 and the following result, which is easy to check.
Fact 2.34. The value function vk of Theorem 2.33 can rewritten as the value function of the mixed functional :
vk(x) = E
[(T −1
∑
`=k
( `−1
∏
m=k
αm(Xm)
)
r′
`(X`)
)
+
( T −1
∏
m=k
αm(Xm)
)
φ(XT ) | Xk = x
]
,
for the same Markov chain (Xn)n≥0, with the same final reward φ, and the instantaneous rewards r′
k and variable discount factors αk given by:
r′
k(x) = rk(x), for x ∈ B
r′
k(x) = φ(x), for x 6∈ B
αk(x) = 1, for x ∈ B
αk(x) = 0, for x 6∈ B .
We can derive similarly the solution of the discounted infinite horizon problem with stopping time.
Theorem 2.35 (Kolmogorov Equations for discounted infinite horizon with stopping time). Let (Xn)n∈N be a stationary Markov chain over (Ω, A, P ), taking its values in E, with Markov transition matrix M . Let B ⊂ E be nonempty, r ∈ RB, and α ∈ (0, 1), Assume that r is bounded in sup-norm (or E is finite). Denote, for all x ∈ E,
v(x) = E
[(τB −1
∑
`=0
α`r(X`)
)
+ ατB φ(XτB ) | X0 = x
]
. (2.9)
Then, v is the unique solution of the fixed point linear equation:
{ v(x) = r(x) + α(M v)(x) x ∈ B , v(x) = φ(x) x 6∈ B .
We can in some cases avoid the discount factor.
Theorem 2.36 (Kolmogorov Equations for undiscounted infinite horizon with stopping time). Let (Xn)n∈N be a stationary Markov chain over (Ω, A, P ), taking its values in E, with Markov transition matrix M . Let B ⊂ E be nonempty, r ∈ RB. Assume that r is bounded in sup-norm (or E is finite), and that the matrix MBB ∈ RB×B, which is the restriction of M to rows and columns in B ((MBB)xy = Mxy for all x, y ∈ B) has a spectral radius ρ < 1. Denote, for all x ∈ E,
v(x) = E
[(τB −1
∑
`=0
r(X`)
)
+ φ(XτB ) | X0 = x
]
. (2.10)
Then, v is well defined and it is the unique solution of the fixed point linear equation:
{ v(x) = r(x) + (M v)(x) x ∈ B , v(x) = φ(x) x 6∈ B .
38


Proof. Same arguments as for the proof of Theorem 2.27.
Example 2.37. Consider a Markov chain with transition matrix
M=


1/2 1/2 0 1/2 1/2 0 0 1/2 1/2

.
If one consider B = {1, 2} ⊂ E, then τB = +∞ almost surely since B is a recurrence class. Then,
E
[
(
∑τB −1
k=0 r(Xk)) + φ(XτB ) | X0 = x
]
= ∞ if for instance r = (1 1 0)T .
Now if B = {2, 3}, then τB < +∞ almost surely, and
MBB =
[1/2 0 1/2 1/2
]
satisfies ρ(MBB) = 1/2. Then, v(x) = E
[
(
∑τB −1
k=0 r(Xk)) + φ(XτB ) | X0 = x
]
exists and is solution
of
v(1) = φ(1)
v(2) = r(2) + 1
2 v(1) + 1
2 v(2)
v(3) = r(3) + 1
2 v(2) + 1
2 v(3) .
This gives
v(1) = φ(1)
v(2) = 2r(2) + φ(1)
v(3) = 2r(3) + 2r(2) + φ(1) .
2.8 Further examples
Example 2.38 (A rolling dice game). Consider a game in N steps. At each step, the player is rolling a dice. If the dice falls on 6, the player is loosing all his previous gains positive or negative, otherwise he receives the value of the dice minus 3 as an additional gain. Denoting by Wn the value of the dice at the nth stage of the game, we get that (Wn)n≥0 is an i.i.d. sequence of random variables with laws: P (Wn = i) = 1/6 for i ∈ {1, . . . , 6}. If the dice neither fall on 6, the total gain/payoff of the player at stage n would be equal to W1 − 3 + · · · + Wn − 3. Since the player may loose everything, one need to consider a new sequence Xn of states consisting in the (possible) total reward at time n. We starts with X0 = 0 and get that
Xn+1 =
{
Xn + Wn+1 − 3 if Wn+1 6= 6
0 otherwise,
for all n = 0, . . . , N − 1. Such a dynamics can be written Xn+1 = f (Xn, Wn+1), which implies that the sequence (Xn)n≥0 is a Markov chain with values in Z. The expected total reward at the end of the game is then equal to E [XT ], which has the form of the criterion considered in Proposition 2.16.
39


Then it can be computed by using the Kolmogorov equation. Using the dynamics f , instead of transition probabilities, Kolmogorov equation can be written as follows
vk(x) = E [vk+1(f (x, Wk+1))] , vT (x) = x, x ∈ Z .
Then, E [XT ] = v0(0) (since X0 = 0). Using the above informations on f and the law of (Wn)n≥0, we can rewrite the Kolmogorov equation as:
vk(x) = 1
6 {vk+1(0) +
5
∑
i=1
vk+1(x + i − 3)} x ∈ Z .
Then, using that vT (x) = x, we can prove by backward induction on k that vk is linear in x: vk(x) = zkx with zk = zk+1 5
6 . This implies that the game has no expected gain.
2.9 Solutions of Exercises
Exercise 2.3.1.
Exercise 2.5.1. Let
v := E
[(T −1
∑
`=0
r`(X`)
)
+ φ(XT )
]
.
Using the property that v = E
[
E
[(
∑T −1
`=0 r`(X`)
)
+ φ(XT ) | X0
]]
, we get that v = p(0)v0, where
v0 is the solution of Kolmogorov equation.
Exercise 2.5.2. Let Xn be a Markov chain with values in (a finite subset E of) N, and consider the sequence Yn = X0 + · · · + Xn. Then, Yn satisfies the recurrence Yn+1 = Yn + Xn+1. Since Xn is a Markov chain, then (Xn, Yn) is a Markov chain on E × N. Let M ̃ (n) be its transition probability
matrix. We have
M ̃ (n)
(x,y)(x′,y′) = M (n)
xx′ 1(y+x′)y′ .
Moreover v(x) = E [φ(YT ) | X0 = x] = E [φ ̃(XT , YT ) | (X0, Y0) = (x, 0)] ,
with φ ̃(x, y) = φ(y). So v(x) = w0(x, 0) where wk satisfies Kolmogorov equation wk = M ̃ (k)wk+1, that is
wk(x, y) =
∑
x′∈E
M (k)
xx′ wk+1(x′, y + x′) ,
with wT = φ ̃.
Exercise 2.5.3. Consider a Markov chain (Xn)n≥0 with values in E and transition matrix M ∈ RE×E (independent of time). Let f be a function from E to R, compute
vT (x) = P (∃n ∈ {0, . . . , T }, f (Xn) ≥ 1 | X0 = x) .
40


Chapter 3
Markov decision processes with finite
horizon criteria
We now consider a discrete time dynamical system (Xn)n≥0 with finite (or discrete) state space E and a dynamics of the following type:
Xn+1 = fn(Xn), n ≥ 1 ,
which can both be changed (by a company manager, an investor, a provider, a driver,...), by applying an action or control Un at each time or stage n ≥ 0, and be subject to randomness. For instance:
Xn+1 = fn(Xn, Un, Wn), n ≥ 0 .
The aim is still to choose the sequence of actions U0, . . . , Uk, . . . in such a way that they minimize (resp. maximize) a certain functional, called the total cost (resp. the total payoff). However, we assume that information on the sequences Xn and Wn arrive sequentially, so that at time n, the “manager” only knows X0, . . . , Xn and W0, . . . , Wn. Then, the decision to choose Un is taken at time n using this information. We are still in the context of complete observation since we know all the past states, however we do not know the future states. This is the main difference with deterministic control problems, in which, given the model, and the state at some time n, we can infer all the state trajectory (Xk)k≥n. Since we cannot observe the future realizations of the Markov chain (when the sequence of actions is fixed for instance), we shall optimze a functional which is the expectation of the total payoff given the initial state, which is a criteria already considered in the chapter on Markov chains, Chapter 2. Another difficulty is that the choice of the actions (Un)n≥0 changes the random process (Xn)n≥0, so we cannot start with a Markov chain on a given probability space as in previous chapter. We can only define a model with all the parameters of the system, like the dynamics or the transition probabilities, and the initial law. This is what is called a Markov decision process or a controlled Markov chain.
3.1 Markov decision processes
Definition 3.1. A Markov Decision Process (MDP) or a controlled Markov chain consists in giving the following parameters:
41


• a finite or discrete state space E;
• an action space C
• for all k ∈ N and x ∈ E, the subset Ck(x) ⊂ C of all possible actions at time k, when the state is equal to x;
• for all k ∈ N, the set Ak := {(x, u) | x ∈ E, u ∈ Ck(x)} of all possibles couples (state, action) at time k;
• an initial probability p(0) ∈ ∆E on E, or an initial state x0 ∈ E, which is equivalent to the case where p(0) is the Dirac measure at x0;
• for all k ∈ N, x ∈ E and u ∈ Ck(x), a probability row vector M (k,u)
x over E, the entries of
which will be denoted
(
M (k,u)
xy
)
y∈E
.
The MDP is stationary if Ck(x) and M (k,u)
x do not depend on time k. In this case, the index or argument k is omitted. It is uncontrolled if the sets Ck(x) are singletons. In this case, the argument u is omitted.
In the uncontrolled case, the above parameters allow one to construct a Markov chain (and a probability space), by considering the transition probability matrices
M (k)
xy = P (Xk+1 = y | Xk = x) .
In the general case, one wish to construct (a probability space and) two discrete time processes
(Xk)k≥0 and (Uk)k≥0 taking their values in E and C respectively, with transition probabilities M (k,u)
xy :
M (k,u)
xy = P (Xk+1 = y | Xk = x, Uk = u) , (3.1a)
and such that (Xk) satisfies the following Markov property:
P (Xk+1 = xk+1 | Xk = xk, Uk = uk, Xk−1 = xk−1, Uk−1 = uk−1, . . . , X0 = x0, U0 = u0)
= P (Xk+1 = xk+1 | Xk =k, Uk = uk) , ∀xi ∈ E, ui ∈ Ci(xi), for i ≥ 0 . (3.1b)
To define the underlying probability, one need to assume that the control process (Uk)k≥0 is admissible, meaning that Uk depends only on the past of states and actions, or more precisely that (Uk)k≥0 is obtained from a strategy, where we extend the notion of strategy as follows.
Definition 3.2. Given a MDP as above, the set Hk = A0 × · · · × Ak−1 × E is called the set of histories at time k. A pure strategy is a sequence σ = (σk)k≥0 such that, for all k ≥ 0, σk, called the strategy at time k, is a map from Hk to C satisfying
σk(x0, u0, . . . , xk−1, uk−1, xk) ∈ Ck(xk), for all (x0, u0, . . . , xk−1, uk−1, xk) ∈ Hk .
We denote by Σ the set of all pure strategies. A pure strategy gives rise to the stochastic process (Xk, Uk)k≥0 with transition probabilities as in (3.1), satisfying in addition
Uk = σk(X0, U0, . . . , Xk−1, Uk−1, Xk) ,
that is there exists a probability space (Ω, A, P ) and a stochastic process (Xk, Uk)k≥0 over this space satisfying all the above properties. Such a sequence (Xk, Uk)k≥0 is also called an admissible sequence of states and controls.
42


Definition 3.3. A random (or relaxed) strategy is a sequence σ = (σk)k≥0 such that, for all k ≥ 0, σk is a map from Hk to the space of probabilities, denoted here CR, over a given probability space (C, AC, P ) such that the support of σk(x0, u0, . . . , xk−1, uk−1, xk) is included in Ck(xk), for all (x0, u0, . . . , xk−1, uk−1, xk) ∈ Hk. Such a strategy gives rise to a stochastic process (Xk, Uk)k≥0 satisfying, for all B ∈ AC,
P (Uk ∈ B | X0, U0, . . . , Xk−1, Uk−1, Xk) = [σk(X0, U0, . . . , Xk−1, Uk−1, Xk)](B) .
We denote by ΣR the set of all relaxed strategies.
Definition 3.4. A pure or random strategy is said Markovian if each map σk depends only on the information on the state at the current time, that is
σk(x0, u0, . . . , xk−1, uk−1, xk) = πk(xk) ,
for some map πk from S to C or CR. A pure Markovian strategy is also called a feedback strategy or feedback policy. We denote by π = (πk)k≥0 such a policy and call πk the policy at time k. We denote by Π and ΠR the sets of all feedback and Markov strategies respectively, and by Πk and ΠR
k the sets of k-coordinates of elements of Π and ΠR respectively.
When Ck and M (k,u)
x do not depend on k (for all x ∈ E and u ∈ C(x)), we say that a (pure or relaxed) Markovian strategy is stationary if πk does not depend on k, in which case π also denotes each of the πk.
Definition 3.5. A pure strategy is an open-loop strategy if σk depends only on the initial state, that is
σk(x0, u0, . . . , xk−1, uk−1, xk) = ωk(x0) for all (x0, u0, . . . , xk−1, uk−1, xk) ∈ Hk .
We denote by OL the set of all open-loop strategies.
Definition 3.6. Given a MDP as in Definition 3.1, and a feedback policy π = (πk)k≥0 ∈ Π, we associate the Markov transition matrices M (k,πk) at time k, where for all k ∈ N, π ∈ Πk, the matrix
M (k,π) is defined by
M (k,π)
xy := M (k,π(x))
xy , ∀x, y ∈ E .
Fact 3.7. Given a MDP as in Definition 3.1, and a feedback policy π = (πk)k≥0 ∈ Π, the associated stochastic process (Xk, Uk)k≥0 as in Definition 3.2 is such that (Xk)k≥0 is a Markov chain with initial law p(0) and transition probability matrices M (k,πk) at time k. Moreover, Uk = πk(Xk), and so (Xk, Uk)k≥0 is also a Markov chain taking its values in E × C.
Another definition of a MDP is sometimes given, which can be shown to be equivalent to the previous one at least in the case of finite state and action spaces, up to the choice of the probability space.
Definition 3.8. A Markov Decision Process (MDP) or a controlled Markov chain consists in giving the following parameters:
• a finite or discrete state space E;
43


• an action space C
• for all k ∈ N and x ∈ E, the subset Ck(x) ⊂ C of all possible actions at time k, when the state is equal to x;
• for all k ∈ N, the set Ak := {(x, u) | x ∈ E, u ∈ Ck(x)} of all possibles couples (state, action) at time k;
• an initial probability p(0) ∈ ∆E on E, or an initial state x0 ∈ E, which is equivalent to the case where p(0) is the Dirac measure at x0;
• a probability space (Ω, A, P ), a random variable X0 with values in E and law p(0), and a sequence of independent random variables (Wn)n≥0 with values in some discrete space W, independent from X0;
• for all k ≥ 0, the dynamics at time k, which is a map fk : Ak × W → E.
The MDP is stationary if Ck(x) and fk do not depend on time k, and if the Wk are identically distributed. In this case, the index or argument k is omitted. It is uncontrolled if the sets Ck(x) are singletons. In this case, the argument u is omitted.
Given a MDP in the sense of Definition 3.8, and a strategy of one of the above form, one can construct on a probability space (extending (Ω, A, P )), two discrete time processes (Xk)k≥0 and (Uk)k≥0 taking their values in E and C respectively, satisfying:
Xn+1 = fn(Xn, Un, Wn), n ≥ 0 . (3.2)
Fact 3.9. Given a MDP in the sense of Definition 3.8, we can construct the following transition probabilities which define a MDP in the sense of Definition 3.1, with same behavior as the initial MDP:
M (k,u)
xy = P (fk(x, u, Wk) = y) .
Proof. Indeed, given the probability space as in Definition 3.8, and the random variables X0 and Wn, for all pure strategies σ, one can associate the random process (Xk, Uk)k≥0 satisfying both (3.2) and
Uk = σk(X0, U0, . . . , Xk−1, Uk−1, Xk) .
In that case,
M (k,u)
xy = P (Xk+1 = y | Xk = x, Uk = u)
= P (fk(x, u, Wk) = y) .
Moreover, if W is finite or discrete, then
P (fk(x, u, Wk) = y) =
∑
w∈W, s.t. fk(x,u,w)=y
P (Wk = w) .
If one consider a relaxed strategy however, one need to increase the probability space in order to handle all the possible probability laws σk(X0, U0, . . . , Xk−1, Uk−1, Xk).
44


Associated to a Markov decision process, we can consider a Markov decision problem or a discrete time stochastic control problem which consists in maximizing (or minimizing) a criteria equal to the expected value of a functional of the random processes (Xk)k≥0 and (Uk)k≥0 induced by the above model among all (relaxed) strategies or among a restricted set of strategies. As for deterministic control problems, the criteria can be of several types:
• Finite horizon (time) additive or multiplicative or mixed criteria.
• Infinite horizon discounted (additive) criteria.
• Additive criteria with stopping time, which may be fixed or to be optimized.
• Long run time average criteria.
3.2 Markov decision problems with additive finite horizon criteria
Let be given a Markov decision process as in Definition 3.1 or Definition 3.8, and consider or denote:
• for all k ∈ N, the instantaneous/running reward/payoff at time k, which is a map rk : Ak → R;
• a final reward, which is a map φ : E → R;
• for all strategies σ = (σk)k≥0 in Σ or ΣR (or Π and ΠR), the total additive payoff with finite horizon T ≥ 1:
J (T,σ) := J T (X; U ) := E
[(T −1
∑
k=0
rk(Xk, Uk)
)
+ φ(XT )
]
, (3.3)
where (X, U ) := (Xk, Uk)k≥0 is the process induced by σ as in Definition 3.2 or Definition 3.3, on a probability space (Ω, A, P ).
• and for strategies associated to the MDP starting at time t, the additive payoff starting at time t:
J (T,σ)
t (x) := J T
t,x(X; U ) := E
[(T −1
∑
k=t
rk(Xk, Uk)
)
+ φ(XT ) | Xt = x
]
. (3.4)
• for all k ≥ 0 and π ∈ Πk (feedback policy at time k), the associated reward vector at time k
r(k,π) with r(k,π)
x := rk(x, π(x)), for x ∈ E.
Definition 3.10. A Markov decision problem with complete observation, and the above data consists in the following optimization problem:
mσax J (T,σ)
where the optimization holds over either all relaxed strategies σ ∈ ΣR, or all pure strategies, or all Markov strategies, or all feedback policies. The optimum of above criteria is called the value of the problem. An optimal solution σ is called an optimal strategy, and the corresponding process Uk or (Xk, Uk) an optimal control process. Moreover, maximization can be replaced by minimization.
45


Definition 3.11. For all x ∈ E, and t ≤ T , let vt(x) be the value of the Markov decision problem with a criteria starting at time t:
mσax J (T,σ)
t (x) .
The map v : {0, . . . , T } × E → R, (t, x 7→ vt(x) is called the value function of the MDP.
Using Theorem 2.18 and Fact 3.7, we deduce the following result.
Lemma 3.12. When σ = π = (πk)k≥0 is a feedback policy, then vt := J (T,π)
t satisfies the Kolmogorov equation:
vk = r(k,πk) + M (k,πk)vk+1, 0 ≤ k ≤ T − 1 ,
with final condition:
vT = φ .
In the controlled case, we obtain the nonlinear Bellman equation.
Theorem 3.13 (Dynamic programming equation for Markov decision problems with finite horizon). Assume that the maps φ, rk, k ≥ 0 are bounded from above. Let vk be the value function of the Markov decision problem:
vk(x) := mσax J (T,σ)
k (x) ,
where the maximum is taken over all relaxed strategies starting at time k. Then, v satisfies the following backward recurrence, called the Bellman dynamic programming equation:
vk(x) = sup
u∈Ck (x)

rk(x, u) +
∑
y∈E
M (k,u)
xy vk+1(y)

 ∀x ∈ E, 0 ≤ k ≤ T − 1 . (3.5)
with final condition
vT = φ .
Moreover, the values v obtained by optimizing over the restricted sets of pure strategies, Markov strategies, or feedback policies, coincide. Assume in addition that the maximum of (3.5) is attained for an action u ∈ Ck(x) and let us denote by πk(x) this action, then the feedback policy π = (πk)0≤k≤T −1 is an optimal strategy of the problem.
Note that we also have v = p(0)v0 for the value of the Markov decision problem with total additive payoff.
3.3 Properties of Bellman operators
We shall use similar operator notations as for deterministic control problems. For all k ∈ N and π ∈ Πk, B(k,π) and B(k) denote the maps from RE to itself such that (assuming that the rk are
bounded from above)
B(k,π)(v) = r(k,π) + M (k,π)v
[B(k)(v)](x) = sup
u∈Ck (x)

rk(x, u) +
∑
y∈E
M (k,u)
xy v(y)

 . (3.6)
46


The dynamic programming equation (3.5) can be rewritten as vk(x) = [B(k)(vk+1)](x) for all x ∈ E, or simply vk = B(k)(vk+1). (3.7)
Moreover, the supremum in (3.6) is finite as soon as E is finite, v ∈ RE , and the functions rk are bounded from above. Then, the operator B(k) is well defined from RE to itself.
Remark 3.14. As for deterministic or uncontrolled problems, when the MDP is stationary, that is
Ck(x) and M (k,u)
x do not depend on k, and the reward rk is independent of time k too, then B does not depend on k, and one can consider the value function as a function of the remaining time until the end: v(t)(x) = mσax J (t,σ)
0 (x) .
Then, the backward Bellman equation for the value function is equivalent to the following forward Bellman equation:
v(k+1)(x) = sup
u∈C(x)

r(x, u) +
∑
y∈E
M (u)
xy v(k)(y)

 ∀x ∈ E, 0 ≤ k ≤ T − 1 ,
or equivalently to
v(k+1) = B(v(k)) ,
with initial condition v(0) = φ.
Definition 3.15. For all k ≤ T − 1, the map B(k) is called the Bellman operator at time k of the Markov decision problem. The maps B(k,π) are the Kolmogorov or Bellman operators at time k of the Markov decision problem, when the policy is freezed.
Since [B(k,π)(v)](x) = rk(x, π(x)) +
∑
y∈E
M (k,π(x))
xy v(y)
only depends on π(x), and not on all the other values of the map π, we get:
[B(k)(v)](x) = sup
π∈Πk
[B(k,π)(v)](x) ∀v ∈ RE , ∀x ∈ E.
which implies that
B(k)(v) = sup
π∈Πk
B(k,π)(v) ∀v ∈ RE , (3.8)
where the suppremum is taken for the partial order of RE . Moreover, the existence of an optimum u in the dynamic programming equation (3.5) for all x ∈ E, is equivalent to the existence of π ∈ Πk such that B(k)(vk+1) = B(k,π)(vk+1), that is to the property that the supremum in (3.8) is a maximum when v = vk+1. The operators B(k,π) : RE → RE are affine operators that are the Kolmogorov operators associated to the Markov chain of Fact 3.7. We already know that the operators B(k,π) are monotone additively homogeneous (since a Markov matrix M has nonnegative entries and satisfies M 1 = 1). The Bellman operators as suprema of such operators satisfy the same property:
47


Lemma 3.16. The Bellman operators B(k) are monotone and additively homogeneous.
However, the Bellman operators B(k) are in general nonlinear both in usual algebra and tropical algebra. They are however convex in the following sense, as suprema of affine maps.
Definition 3.17. An operator B : RE → RE is convex if for all x ∈ E, the function v ∈ RE 7→ [B(v)](x) ∈ R is convex, or equivalently, for all v, w ∈ RE and t ∈ [0, 1], B((1 − t)v + tw) ≤ (1 − t)B(v) + tB(w) for the partial order of RS.
Fact 3.18. The Bellman operators B(k) are convex.
3.4 Proof of Theorem 3.13
We shall show the results by using the previous notations and properties, although one may do everything without using these properties explicitely. Denote by (wk)0≤k≤T the sequence defined (uniquely) by the final condition wT = φ, and the backward recurrence equations (3.5) or equivalently (3.7): wk = B(k)(wk+1). We denote also w := p(0)w0. We need to show that wk = vk for all k ∈ {0, . . . , T } and that w = v.
1. Proof of wk ≥ vk and w ≥ v (without any assumption). Since for vk, we use strategies starting at time k, the inequality wk ≥ vk is of same type as w0 ≥ v0. So we show w0 ≥ v0 only. Let σ ∈ ΣR be fixed, let (Xt, Ut)t≥0 be the process associated to the strategy σ, and let Hk = (X0, U0, X1, U1, . . . , Xk), for all k ≥ 0, be the history process. For all 0 ≤ k ≤ T , and all histories hk = (x0, u0, . . . , xk−1, uk−1, xk) ∈ Hk, denote
z(σ)
k (hk) = E
[(T −1
∑
t=k
rt(Xt, Ut)
)
+ φ(XT ) | Hk = hk
]
.
Then, z(σ)
0 (x0) = J (T,σ)
0 (x0), and so J (T,σ) = E
[
z(σ)
0 (X0)
]
= p(0)z(σ)
0.
The process Hk is a Markov chain taking its values in the variable state space Hk. Therefore,
z(σ)
k satisfies the Kolmogorov equation for an additive functional (see Theorem 2.18)
z(σ)
n (hn) = E
[
rn(xn, Un) + z(σ)
n+1(hn, Un, Xn+1) | Hn = hn
]
(3.9)
if hn = (x0, u0, . . . , xn) ∈ Hn, with the final condition z(σ)
T (hT ) = φ(xT ). wk satisfies (3.5), which can be rewritten as
wn(x) = sup
u∈Cn(x)
(rn(x, u) + E [wn+1(Xn+1) | Xn = x, Un = u]) ∀x ∈ E . (3.10)
Let us show that wk(xk) ≥ z(σ)
k (hk) by backward induction on k, when hk = (x0, u0, . . . , xk) ∈ Hk.
This is true for k = T since z(σ)
T (hT ) = φ(xT ) = wT (xT ). If the inequality is true for k + 1, then from the above equations (3.9) and (3.10), we deduce
z(σ)
k (hk) ≤ E [rk(xk, Uk) + wk+1(Xk+1) | Hk = hk] =
E [rk(xk, Uk) + E [wk+1(Xk+1) | Xk = xk, Uk] | Hk = hk] =
∑
uk ∈Ck (xk )
σk(hk)(uk) {rk(xk, uk) + E [wk+1(Xk+1) | Xk = xk, Uk = uk]} ,
48


where the last equality holds when σ(hk) is a probability with a countable support (in Ck(xk)).
Using (3.10), we deduce z(σ)
k (hk) ≤ E [wk(xk) | Hk = hk] = wk(xk), which proves the induction.
In particular z(σ)
0 (x0) ≤ w0(x0) for all x0 ∈ E. Since z(σ)
0 (x0) = J (T,σ)
0 (x0), taking the maximum
over all strategies, we deduce that v0(x0) ≤ w0(x0). Since J (T,σ) = p(0)z(σ)
0 ≤ p(0)w0, taking again the maximum over all strategies, we deduce v ≤ w.
2. Proof of wk ≤ vk and w ≤ v when the supremum in (3.5) is attained. Assume that the maximum in
wk = sup
π∈Πk
B(k,π)(wk+1) (3.11)
is attained for some policy π ∈ Πk (for instance if the sets Ck(x) are finite). Denote π = (πk)k≥0. Then,
wk = B(k,πk)(wk+1), k = 0, . . . , T − 1,
which means that (wk)n≥0 satisfies the same Kolmogorov equation as J (T,π)
k , given in Lemma 3.12,
with same final condition wT = φ = J (T,π)
T . Hence, wk = J (T,π)
k , for all k ≥ 0. So wk ≤ vk, where the value vk is obtained as the maximum over any set of strategies containing at least feedback policies. Similarly, w = p(0)w0 = J(T,π) ≤ v. Moreover, π is an optimal strategy which is a feedback strategy.
3. Proof of wk ≤ vk and w ≤ v in general. Assume now that the maximum in (3.11) is not attained. We only assume that the maps rk are bounded from above, for all k ≤ T − 1. This condition ensures in particular that the operators B(k) are well defined as operators from RE to itself. The supremum in (3.6) is finite, therefore for all ε > 0, k ≤ T − 1 and v ∈ RE , there exists π ∈ Πk such that [B(k,π)(v)](x) ≥ [B(k)(v)](x) − ε ∀x ∈ E ,
which can be rewritten as B(k,π)(v) ≥ B(k)(v) − ε1.
Let πk ∈ Πk such that
B(k,πk)(wk+1) ≥ B(k)(wk+1) − ε1.
We have
B(k,πk)(wk+1) ≥ wk − ε1.
Denote zk = wk + (k − T )ε1. We have zT = wT = φ and
B(k,πk)(zk+1) = B(k,πk)(wk+1) + (k + 1 − T )ε1 ≥ wk + (k − T )ε1 = zk (3.12)
since B(k,πk) is additively homogeneous. Then, the functions zk are sub-solutions of the Kolmogorov equation of Lemma 3.12. We shall
show the inequality zk ≤ J (T,π)
k by backward induction on k. Indeed the inequality is true for
k = T , since zT = φ = J (T,π)
T . If it holds for k + 1, then
zk ≤ B(k,πk)(zk+1) ≤ B(k,πk)(J (T,π)
k+1 ) = J (T,π)
k ≤ vk ,
49


where the first inequality follows from (3.12), the second one from the induction assumption and the monotonicity of B(k,πk), the third one from the Kolmogorov equation of Lemma 3.12 which is
satisfied by J (T,π)
k , and the last one by definition of vk. This shows z0 ≤ J (T,π)
0 ≤ v0, hence we get
p(0)z0 ≤ p(0)J (T,π)
0 = E [J (T,π)] ≤ v.
Therefore,
wk = zk + (T − k)ε1 ≤ vk + (T − k)ε1
for all k ∈ {0, . . . , T }. Since we have shown this inequality for all ε > 0, we deduce that wk ≤ vk for all k ∈ {0, . . . , T }. Similarly w = p(0)w0 = p(0)z0 + T ε ≤ v + T ε, and since this holds for all ε > 0, we get w ≤ v.
Remark 3.19. In the present case of an additive functional, another way to prove that the values v obtained by optimizing over either all relaxed strategies or all Markov strategies coincide is
to show that for all π ∈ ΣR, there exists π′ ∈ ΠR such that E [J(π)] = E
[
J (π′)]
. Indeed, let
(π′
k(x))(B) = P (Uk ∈ B, Xk = x)/P (Xk = x), where P is the probability on the process (Xk, Uk)
induced by π as in Definition 3.3. Then, if (X′
k, U′
k) is the process induced by π′, we get that for
all k ≥ 0, the laws of the random variables (Xk, Uk) and X′
k, U′
k) coincide (not the ones of the
processes). Therefore E [J(π)] = E
[
J (π′)]
, since they are both sums of expectations of functions of
the random variables (Xk, Uk) only and not of all the process (Xk, Uk)k≥0.
3.5 Problems with multiplicative or discounted finite horizon pay
off
Let be given a Markov decision process as in Definition 3.1 or Definition 3.8, and consider or denote:
• for all k ∈ N, the instantaneous/running reward/payoff at time k, which is a map rk : Ak → R;
• for all k ∈ N, a variable discount factor at time k, which is a map αk : Ak → R+;
• a final reward, which is a map φ : E → R;
• for all strategies σ = (σk)k≥0 in Σ or ΣR, the mixed payoff with finite horizon T ≥ 1:
J (T,σ) := J T (X; U ) :=E
[(T −1
∑
`=0
( `−1
∏
m=0
αm(Xm, Um)
)
r`(X`, U`)
)
+
(T −1
∏
m=0
αm(Xm, Um)
)
φ(XT )
]
, (3.13)
where (X, U ) := (Xk, Uk)k≥0 is the process induced by σ as in Definition 3.2 or Definition 3.3.
• and for strategies associated to the MDP starting at time t, the mixed payoff starting at time t:
J (T,σ)
t (x) := J T
t,x(X; U ) :=E
[(T −1
∑
`=t
( `−1
∏
m=t
αm(Xm, Um)
)
r`(X`, U`)
)
+
(T −1
∏
m=t
αm(Xm, Um)
)
φ(XT ) | Xt = x
]
. (3.14)
50


Theorem 3.20 (Dynamic programming equation for Markov decision problems with mixed functional and finite horizon). Assume that the maps φ, rk, αk, k ≥ 0 are bounded from above. Let vk be the value function of the Markov decision problem:
vk(x) := mσax J (T,σ)
k (x) ,
where the maximum is taken over all relaxed strategies starting at time k. Then, v satisfies the following backward recurrence, called the Bellman dynamic programming equation:
vk(x) = sup
u∈Ck (x)

rk(x, u) + αk(x, u)
∑
y∈E
M (k,u)
xy vk+1(y)

 ∀x ∈ E, 0 ≤ k ≤ T − 1 . (3.15)
with final condition
vT = φ .
Moreover, the values v obtained by optimizing over the restricted sets of pure strategies, Markov strategies, or feedback policies, coincide. Assume in addition that the maximum of (3.15) is attained for an action u ∈ Ck(x) and let us denote by πk(x) this action, then the feedback policy π = (πk)0≤k≤T −1 is an optimal strategy of the problem.
Remark 3.21. As above, when the MDP is stationary, that is Ck(x) and M (k,u)
x do not depend on k, and the reward rk and discount factor αk are independent of time k too, then B does not depend on k, and one can consider the value function as a function of the remaining time until the end:
v(t)(x) = mσax J (t,σ)
0 (x) .
Then, the backward Bellman equation for the value function is equivalent to the following forward Bellman equation:
v(k+1)(x) = sup
u∈C(x)

r(x, u) + α(x, u)
∑
y∈E
M (u)
xy v(k)(y)

 ∀x ∈ E, 0 ≤ k ≤ T − 1 ,
or equivalently to
v(k+1) = B(v(k)) ,
with initial condition v(0) = φ.
For the proof of Theorem 3.20, we shall use the Kolmogorov operators B(k,π) from RE to itself defined, for k ∈ N and v ∈ RE , by:
B(k,π)(v) = r(π)
k + A(k,π)v
where A(k,π) is the matrix with nonnegative entries such that A(k,π)
xy = αk(x, π(x))M (k,π)
xy . We also use the Bellman operators B(k) defined by (3.8). We have:
[B(k)(v)](x) = sup
u∈Ck (x)

rk(x, u) + αk(x, u)
∑
y∈E
M (k,u)
xy v(y)

 . (3.16)
51


Then, the dynamic programming equation (3.15) can be rewritten as in (3.7). As in the additive case, the supremum in (3.16) is finite as soon as E is finite, v ∈ RE , and the functions rk(x, ·) and αk(x, ·) are bounded from above, which implies that B(k) is well defined and is a map from RE to itself, and that all the value functions vk take finite real values. Moreover, the existence of an optimal control u in (3.15) for all x ∈ E is equivalent to the existence of a policy π ∈ Πk such that B(k)(vk+1) = B(k,π)(vk+1). The operators B(k,π) : RE → RE are affine, monotone, and thus the operators B(k) are convex and monotone. However, they are no more additively homogeneous. Nevertheless, when αk(x, u) ≤ β, for all x ∈ E and u ∈ Ck(x), the operators B(k,π) and B are β-subhomogeneous, where this property is defined as follows.
Definition 3.22. Let β > 0. We say that an operator B : RE → RE is additively β-subhomogeneous if it satisfies, for all v ∈ RE and λ ∈ R+,
B(v + λ1) ≤ B(v) + βλ1 .
When β = 1, we say that B is additively subhomogeneous
Proof of Theorem 3.20. We follow the same arguments as in the proof of Theorem 3.13, where Equation (3.15) is substituted to (3.5), and Kolmogorov equation of Theorem 2.23 is used instead of the one of Theorem 2.18. Indeed, Points 1 and 2 of the proof of Theorem 3.13 only use the monotonicity of the operators B(k,π) and B(k), Property (3.8) defining B(k) as a supremum of the B(k,π), and the Kolmogorov equation satisfied by the Markov chain Hk associated to any strategy, or the Markov chain Xk associated to any feedback policy. They thus can be followed similarly for the operators B(k,π) and B(k) of this section. For Point 3, we use the property that the functions αk are bounded from above. Let β be an upper bound. Then, from the above remarks, the operators B(k,π) and B(k) are additively βsubhomogeneous. Choose πk as in Point 3 of the proof of Theorem 3.13, and zk = wk − εγk, where the sequence γk is obtained by the backward induction γT = 0, and γk = 1 + βγk+1 (so γk = T − k if β = 1, and γk = (βT −k − 1)/(β − 1) otherwise). We obtain in the same way as in the proof of
Theorem 3.13 that B(k,πk)(zk+1) ≥ zk, and thus zk ≤ v(π)
k , and p(0)z0 ≤ v. Therefore
wk = zk + γkε ≤ v(π)
k + γkε ≤ vk + γkε
for all k ∈ {0, . . . , T }. Since this holds for all ε > 0, we deduce that wk ≤ vk for all k ∈ {0, . . . , T }. The rest of the proof remains.
Remark 3.23. Another way to prove Theorem 3.20 is to increase the state space by taking E′ := S × Z, with Z = R+, and considering the state sequence X′
k = (Xk, Zk), where Zk ∈ Z satisfies Zk+1 = αk(Xk, Uk)Zk starting at Z0 = 1. This corresponds to a MDP with transition probabilities
given by M ′(k,u)
x′y′ = M (k,u)
xy if x′ = (x, z) and y′ = (y, αk(x, u)z) and M (k,u)
x′y′ = 0 otherwise, and the
initial law p′(0) = p(0) ⊗ pz, where pz is the Dirac probability at Point 1 on Z. The only difficulty is that now the new state space is infinite not countable. One can reduce however the problem to a finite state space E′
k depending on time if we assume that the action spaces Ck(x) are finite. Consider
the reward functions defined for x′ = (x, z) ∈ E × Z, by r′
k(x′, u) = zrk(x, u), φ′(x′) = zφ(x), we
get that JtT,x(X; U ) is equal to the additive functional JT
t,(x,1)(X′; U ) defined with the new MDP
and rewards. We can then apply Theorem 3.13. We deduce that v = (p(0) ⊗ pz)(v′0) where v′
T = φ′
52


and v′
k satisfies the dynamic programming equation
v′
k(x′) = sup
u∈Ck (x)

r′
k(x′, u) +
∑
y′ ∈E ′
M (k,u)
x′y′ v′
k+1(y′)

 ∀x′ ∈ E′ .
Replacing r′
k and φ′ with their values, we obtain that, for all k ≥ 0, v′
k(x′) = zvk(x) for some
function vk, and that vk satisfies (3.15). Similarly v = p(0)v0.
The previous remark shows that one can always reduce a MDP with mixed functional to a MDP with additive functional by increasing the state space which may become infinite. One can also do the reverse operation, when the state has the form Xk = (X′
k, Zk), in which Zk is positively homogeneous, that is satisfies that Zk is transformed into λZk when Z0 is transformed into λZ0, if X′
k+1 does not depend on Zk. Indeed, in that case the state can be reduced to X′
k, then the additive functional becomes a mixed functional. When the discount factors αk are less than 1, we call the mixed functional a discounted functional. Another reduction can be obtained in that case, by adding a cemetery point, as in Proposition 2.24. This leads to the following result.
Proposition 3.24. The value function of a finite horizon discounted Markov Decision problem is equal to the restriction to E of the value function of a finite horizon MDP with additive criteria on the state space E ∪ {c}, where c 6∈ E is a cemetery point.
Proof. Consider a MDP with finite horizon mixed functional as above. Let c 6∈ E, and consider E′ = E ∪ {c}. We construct a MDP with additive functional on E′ as follows. We keep the same action spaces Ck(x) for x ∈ E and we take for Ck(c) any singleton subset of C, The initial law p′(0)
is p′(0)
x = p(0)
x for x ∈ E and p′(0)
c = 0, and the transition probabilities are
M ′(k,u)
xy = αk(x, u)M (k,u)
xy , when x, y ∈ E
M ′(k,u)
xc = 1 − αk(x, u), ∀x ∈ E
M ′(k,u)
cy = 0, ∀y ∈ E
M ′(k,u)
cc = 1 .
We extend rk and φ to E′ in r′
k and φ′ respectively by mapping (c, u) or c to 0. Let v′ be the value
function of this new MDP on E′ with additive functional (see (3.4)). By Theorem 3.13, we have v′ = p′(0)v′0 where v′
k ∈ RE′ is solution of the backward recurrence dynamic programming equation
v′
k(x) = sup
u∈Ck (x)

r′
k(x, u) +
∑
y∈E ′
M ′(k,u)
xy v′
k+1(y)

 ∀x ∈ E′, 0 ≤ k ≤ T − 1 ,
with final condition v′
T = φ′. Since φ(c) = rk(c, u) = 0 and M ′(k,u)
cc = 1, for all u ∈ Ck(c), we get that v′
k(c) = v′
k+1(c) for all k ≥ 0, hence v′
k(c) = 0. Therefore, if vk is the restriction of v′
k to E, we
obtain that v′ = p(0)v0 and that vk satisfies the dynamic programming equation (3.15). Moreover, v′ = v so the values of the two problems coincide.
53


3.6 Problems with exit time in finite horizon
Let be given a Markov decision process as in Definition 3.1 or Definition 3.8, and let B be a strict subset of E. Then, any strategy σ = (σn)n≥0 in Σ or ΣR induces the process (Xn, Un)n≥0 and the history process (Hn)n≥0. The later beeing a Markov chain, we can construct the filtration (Fn)n≥0 generated by (Hn)n≥0, which is in that case Fn = σa(Hn) = σa(X0, U0, . . . , Un−1, Xn). Then, the exit time of the sequence (Xn)n≥k from the set B:
τk
B := inf{n ≥ k | Xn 6∈ B}
is a stopping time with respect to the filtration (Fn)n≥k. When σ is a feedback policy, then (Xn)n≥0 is a Markov chain and τ k
B is also a stopping time with respect to the Markov chain (Xn)n≥k. Consider or denote:
• for all k ∈ N, the instantaneous/running reward/payoff at time k, which is a map rk : Ak → R;
• a final reward, which is a map φ : E → R;
• for all k ∈ N, an exit reward, which is a map ψk : E \ B → R, such that ψT is the restriction of φ on E \ B;
• for all strategies σ = (σk)k≥0 in Σ or ΣR, the payoff with exit stopping time and finite horizon T ≥ 1:
J (T,B,σ) :=J T,B(X; U ) := E




T ∧τ 0
B −1
∑
`=0
r`(X`, U`)

 + φ(XT )1T <τ0
B + ψτ 0
B (Xτ 0
B )1T ≥τ 0
B

,
(3.17)
where (X, U ) := (Xk, Uk)k≥0 is the process induced by σ as in Definition 3.2 or Definition 3.3, and τ 0
B is the exit time of the process (Xn)n≥0 from B.
• and for strategies associated to the MDP starting at time t, the payoff starting at time t with exit stopping time:
J (T,B,σ)
t (x) := J T,B
t,x (X; U ) := E




T ∧τ t
B −1
∑
`=t
r`(X`, U`)

 + ψT ∧τ t
B (XT ∧τ t
B ) | Xt = x

 , (3.18)
where we extend ψT to B by taking ψT = φ.
Theorem 3.25 (Dynamic programming equation for Markov decision problems with exit time in finite horizon). Assume that the maps φ, rk, k ≥ 0 are bounded from above. Let vk be the value function of the Markov decision problem:
vk(x) := mσax J (T,B,σ)
k (x) ,
where the maximum is taken over all relaxed strategies starting at time k. Then, v satisfies the following backward recurrence, called the Bellman dynamic programming equation:
vk(x) = sup
u∈Ck (x)

rk(x, u) +
∑
y∈E
M (k,u)
xy vk+1(y)

 ∀x ∈ B, 0 ≤ k ≤ T − 1 . (3.19a)
54


with boundary condition
vk(x) = ψk(x), ∀x 6∈ B, 0 ≤ k ≤ T , (3.19b)
and final condition
vT (x) = φ(x) = ψT (x), ∀x ∈ B . (3.19c)
Moreover, the values v obtained by optimizing over the restricted sets of pure strategies, Markov strategies, or feedback policies, coincide. Assume in addition that the maximum of (3.19) is attained for an action u ∈ Ck(x), for x ∈ B, and let us denote by πk(x) this action when x ∈ B, and choose any action πk(x) for x 6∈ B, then the feedback policy π = (πk)0≤k≤T −1 is an optimal strategy of the problem.
Theorem 3.25 can be deduced easily from Theorem 3.20 using the following property which is the same as Fact 2.34.
Fact 3.26. The functional J (T,B,σ)
t of (3.18) can rewritten as the mixed functional:
J (T,B,σ)
t (x) := J T,B
t,x (X; U ) :=E
[(T −1
∑
`=t
( `−1
∏
m=t
αm(Xm, Um)
)
r′
`(X`, U`)
)
+
(T −1
∏
m=t
αm(Xm, Um)
)
φ(XT ) | Xt = x
]
.
for the same Markov Decision process, with the same final reward φ, and the instantaneous rewards r′
k and variable discount factors αk given by:
r′
k(x, u) = rk(x, u), for x ∈ B, u ∈ Ck(x)
r′
k(x, u) = ψk(x), for x 6∈ B, u ∈ Ck(x)
αk(x, u) = 1, for x ∈ B, u ∈ Ck(x)
αk(x, u) = 0, for x 6∈ B, u ∈ Ck(x) .
The previous property shows that a Markov decision process with exit time in a finite horizon functional can be seen as a problem with mixed functional with discount factors ≤ 1, for the same MDP. By Proposition 3.24, such a problem can then be reduced to a problem with additive functional by adding a cemetery point to the state space. Another way to prove Theorem 3.25 or to reduce the exit time functional to an additive functional is to consider the process Xn∧τk
B which
corresponds to a slightly different MDP, in which the transition probabilities in E \ B are changed (see the proof of Theorem 2.33). The advantage of Fact 3.26 is that we do not need to change the MDP.
Example 3.27. Conversely, consider a Markov decision process with an additive criteria as (3.3)
including a cemetery point c ∈ E, as in the reductions of previous section. This means that
M (k,u)
cc = 1, and rk(c, u) = 0 for all u ∈ Ck(c). Then, one can rewrite the additive functional by using the exit stopping time τB from the set B = E \ {c} of the process (Xk)k≥0 induced by any strategy σ. Indeed, in that case rk(Xk, Uk) = 0 and Xk = Xk+1 = XτB for all k ≥ τB, and so
J (T,σ) := J T (X; U ) := E
[(T −1
∑
k=0
rk(Xk, Uk)
)
+ φ(XT )
]
=E
[(T ∧τB−1
∑
k=0
rk(Xk, Uk)
)
+ φ(XT ∧τB )
]
.
55


Again the advantage of this technique is that we do not need to change the MDP (the transition probabilities).
3.7 The example of optimal stopping time problems with finite
horizon
Consider
• a (fixed) Markov chain (Xn)n≥0 over a probability space (Ω, A, P ) taking its values in a finite state space E, with transition matrices M (n) at time n ∈ N, and initial probability law p(0).
• instantaneous/running rewards/payoffs rk ∈ RB (at any time k ≤ T − 1);
• final rewards φk ∈ RE (at any time k ≤ T );
• for all stopping times τ with respect to the Markov chain (Xn)n≥0, the finite horizon payoff with stopping time τ :
J (T,τ) :=E
[(τ ∧T −1
∑
`=0
r`(X`)
)
+ φτ∧T (Xτ∧T )
]
; (3.20)
• and for all t ≤ T , the finite horizon payoff with stopping time τ ≥ t, starting in x at time t:
J (T,τ )
t (x) :=E
[(τ ∧T −1
∑
`=t
r`(X`)
)
+ φτ∧T (Xτ∧T ) | Xt = x
]
. (3.21)
Definition 3.28. An Optimal stopping time problem with complete observation and finite horizon criteria consists in the following optimization problem:
mτax J (T,τ )
where the optimization holds over all stopping times τ with respect to the Markov chain (Xn)n≥0. The optimum of above criteria is called the value of the problem. An optimal solution τ is called an optimal stopping time.
Definition 3.29. For all x ∈ E, and t ≤ T , let vt(x) be the value of the optimal stopping time problem with initial state x at time t:
mτax J (T,τ )
t (x) .
The map v : {0, . . . , T } × E → R, (t, x) 7→ vt(x) is called the value function of the stopping time problem.
Theorem 3.30 (Dynamic programming equation for optimal stopping time problems with finite horizon criteria). Assume that the maps φk, rk, k ≥ 0, are bounded from above (or E finite). Let vk be the value function of the optimal stopping time problem with finite horizon:
vk(x) := mτax J (T,τ)
k (x) ,
56


where the maximum is taken over all stopping times τ with respect to the Markov chain (Xn)n≥k. Then, v satisfies the following backward recurrence, called the Bellman dynamic programming equation or variational inequality:
vk(x) = max
(
rk(x) +
∑
y∈E
M (k)
xy vk+1(y), φk(x)
)
∀x ∈ E, 0 ≤ k ≤ T − 1 . (3.22)
with final condition
vT = φT .
For all 0 ≤ k ≤ T , let Bk be the set of states in which the maximum in (3.22) is attained in the first term, that is
Bk := {x ∈ E | rk(x) +
∑
y∈E
M (k)
xy vk+1(y) ≥ φk(x)} ,
au define
τ = inf{k ≥ 0 | Xk 6∈ Bk or k = N } .
Then τ is an optimal stopping time.
Proof. For the proof, we reduce this problem to a Markov decision problem with finite horizon criteria. Consider the MDP in which
• the state space is E′ = E ∪ {c} where c 6∈ E;
• the control space C = {0, 1} (0 for stop, and 1 for not stop);
• the control space C(x) is such that C(x) = C if x ∈ E and C(x) = {0} if x = c.
• the states of the MDP, Yn, depend on the states of the Markov chain Xn and on the actions as follows:
Yn+1 = g(Xn+1, Un)
where g(x, u) = x if u = 1 and g(x, u) = c otherwise.
• Then, for all xi ∈ E′, ui ∈ Ci(xi), i ≥ 0, we have
P (Yk+1 = xk+1 | Yk = xk, Uk = uk, Yk−1 = xk−1, . . . , Y0 = x0, U0 = u0)
= 1 if c = xk+1 and uk = 0
= 0 if xk+1 ∈ E and uk = 0
= M (k)
xk,xk+1 if xk, xk+1 ∈ E and uk = 1
• This implies that
P (Yk+1 = xk+1 | Yk = xk, Uk = uk, Yk−1 = xk−1, . . . , Y0 = x0, U0 = u0)
= P (Yk+1 = xk+1 | Yk = xk, Uk = uk) .
which is the Markov property.
57


• The transition probabilities of the MDP are: M (k,1)
xy = M (k)
xy for all x, y ∈ E, M (k,1)
xy = 0 for
x ∈ E and y = c, M (k,0)
xy = 1 for x ∈ E′ and y = c, and M (k,0)
xy = 0 for x ∈ E′ and y ∈ E. Note that M (k,1)
cy , with y ∈ E′, is useless.
• Take then the rewards: r′
k(x, 1) = rk(x), r′
k(x, 0) = φk(x) for x ∈ E and r′
k(c, 0) = 0, and final reward φ(x) = φT (x) for x ∈ E and φ(c) = 0.
Then, the value of the finite horizon Markov Decision problem coincides with the value of the stopping time problem with finite horizon: Given a pure strategy, the associated process Ut satisfies Ut ∈ Ft = σa(X0, . . . , Xt), and so τ = inf{t ≥ 0 | Ut = 0} is a stopping time. Conversely, given a stopping time τ , consider the process such that Un = 1 for all n < τ and Un = 0 for n ≥ τ , we get that {Ut = 1} ∈ Ft = σa(X0, . . . , Xt), so that Ut can be written as a measurable function σt of X0, . . . , Xt. Then, the process (Ut)t≥0 is associated to the pure strategy (σt)t≥0. The Bellman equation of the Markov decision problem is then:
vk(x) = max
(
rk(x) +
∑
y∈E
M (k)
xy vk+1(y), φk(x) + vk+1(c)
)
∀x ∈ E ,
vk(c) = vk+1(c) ,
with the final condition vT (x) = φT (x) for x ∈ E and vT (c) = 0. Here the action u = 1 corresponds to the left term in the above maximum, and u = 0 corresponds to the right term. Therefore if πk : E → {0, 1} is an optimal policy given by the Bellman equation, we recover again the optimal stopping time by taking:
τ = inf{t ≥ 0 | πt(Xt) = 0}
or by taking τ as in the theorem.
Remark 3.31. As seen in the above result, the variational inequality (3.22) allows one to construct the set ∪k≥0{k}×Bk, for which the optimal stopping time is the exit time of the process (k, Xk)k≥0. The complementary of this set plays the same role as a free boundary in the continuous time and state setting. Contrarilly to the case of control problems with an exit time considered in Theorem 3.25, the above “boundary” is not known in advance, but is computed as the optimal boundary for a certain criterion.
3.8 Examples and Exercices
Example 3.32 (A random ressource allocation problem). Let us consider the ressource allocation problem described in Example 1.7 and Example 1.15, where we assume now that the reward obtained when one invests x units in the ith ressource is random, and can be written as Ri(x, Zi), where the Ri are deteministic maps and the Zi are independent random variables with values in some set Z. Assume that the investor is choosing the numbers of units ui he is investing in each ressource i in advance without knowing the values of the variables Zi. What is the maximal expected total reward of the investor ?
58


Assume now that the investor is choosing the amounts to be invested in the ressources sequentially, that is using some given order σ(i), i = 1, . . . , N , where σ is a permutation of {1, . . . , N }, and that when he decides to invest in ressource i, he is able to obtain the information on its reward, that is on Zi. Assume also that the set Z is finite. The investor wants to maximize his expected total reward. Write this problem as a Markov decision problem with state variable (x, z) with x ∈ {0, . . . , R} and z ∈ Z.
Show that this problem can be solved via the recursive equation with final value wN+1 = 0.
wn(x) = E
[
max
u∈N, 0≤u≤x[rσ(n)(u, Zσ(n)) + wn+1(x − u)]
]
, n = 1, . . . , N, x ∈ {0, . . . , R} ,
and that w1(R) gives the expected total reward of the investor. Show that if the maps ri are concave with respect to the first variable, then the maps wn are also concave (one can first consider the relaxed problem where x and the ui can take real values). Show that the solution depends on the permutation σ.
59


3.9 Problem: Airline Revenue Management
Consider an airline revenue manager who decide at each time of arrival of a demand of seats on a given flight, if he accept or reject this demand. We assume that the flight contains n > 1 classes of seats (a class may contain an information of level of the seat, of position in the plane, and of date of booking as well), numbered from 1 to n. The price of a seat of class k ∈ {1, . . . , n} will be denoted by pk, and we have p1 < · · · < pn. We assume also that demands of seats of class k arrive before demands of seats of class k + 1, this means that demands arrive in nonoverlapping intervals in the order of increasing prices (this is called the early bird hypotheses). If Dk denotes the total amount of demands of seats of class k, we also assume that the Dk are independent random variables. We consider a Markov decision problem with finite horizon (n stages), starting at stage 1, modelizing the sale of the seats of one flight of a plane only. At each stage k = 1, . . . , n, all the Dk demands of seats of class k arrive, and the manager decides to accept only a certain number of them Uk ≤ Dk. We add a final stage n + 1, at which no demand of seats is done, so Dn+1 = 0. Moreover, for each k = 1, . . . , n + 1, we denote by Xk the remaining capacity of the plane at stage k, that is the number of available seats. Thus, X1 is the total number of seats of the plane, and Xn+1 is the number of unsold seats at the end of the sale. The Markov decision problem will involve a Markov decision process such that at each stage k = 1, . . . , n + 1,
(Xk, Dk) : is the state of the MDP at stage k. Since the plane has a finite number of seats M , one can consider that the set of states E is of the form E = [M ] × N where [M ] := {0, . . . , M }, or even E = [M ] × [M ].
Uk : is the action of the MDP at stage k. Again, since Uk ≤ Dk, one can consider that the set of actions C is [M ], and that at stage k, Uk satisfies the constraint 0 ≤ Uk ≤ min(Xk, Dk), so that Uk ∈ C(Xk, Dk), with C(x, d) = [min(x, d)], for (x, d) ∈ E.
Moreover, the dynamics of the MDP is such that:
Xk+1 = Xk − Uk, k = 1, . . . , n ,
Dk are independent random variables with given laws qk(d) = P (Dk = d) , d ∈ N .
The payoff (criteria) of the MDP is the expected amount of money obtained after the end of the sale, that the manager want to maximize. So the instantaneous reward at stage k, state (x, d) and action u is given by
r(k; x, d; u) = pku ,
and the final reward at state (x, d) is
φ(x, d) = 0 .
Q 9.1. We denote by v(k; x, d) the value function of the MDP, when the starting stage is k = 1, . . . , n + 1 with starting state (x, d) ∈ E:
v(k; x, d) = sup E
[n ∑
`=k
r(`; X`, D`; U`) + φ(Xn+1, Dn+1) | Xk = x, Dk = d
]
,
where the supremum is taken over all (feedback) strategies (πk)k≥0 defining the admissible process (Uk)k≥1: Uk = πk(Xk, Dk). Write the dynamic programming equation satisfied by v.
60


Q 9.2. Denote wk(x) = E [v(k; x, Dk)]. Write a recurrence equation satisfied by wk.
Q 9.3. Show by induction on k, that wk : [M ] → R is concave, and that for each d ∈ N, x ∈ [M ] 7→ v(k; x, d) ∈ R is concave.
Q 9.4. For k = 2, . . . , n + 1, choose
yk ∈ Argmax
y∈[M ]
{−pk−1y + wk(y)} .
Show that πk(x, d) = max(min(x − yk+1, d), 0) = min(max(x − yk+1, 0), d)
is an optimal policy at time k for our problem.
Q 9.5. Explain the meaning of yk as a protection level for classes of levels ≥ k.
Q 9.6. For all k = 2, . . . , n + 1, write yk as a function of the map ∆wk : [M ] → R ∪ {+∞}, where
∆wk(x) = wk(x) − wk(x − 1) .
Q 9.7. To realize the previous policy, what should be the policy of acceptation of a single demand of a seat of class k?
Q 9.8. Compute yn as a function of pn−1, pn and the law qn of Dn.
Q 9.9. Show that ∆wk+1(y) ≤ ∆wk(y) for all y ∈ N and k ∈ {1, . . . , n}.
Q 9.10. Show that y1 ≥ · · · ≥ yn.
Some references for this problem:
[RM1] K. Littlewood. Forecasting and control of passenger bookings. In Proc. 12th AGIFORS Symposium. 1972. reprinted in Journal of Revenue and Pricing Management, Vol. 4 (2005).
[RM2] P.P. Belobaba. Air Travel Demand and Airline Seat Inventory Management. PhD thesis, Flight Transportation Laboratory. Cambridge, MIT, 1987.
[RM3] S.L. Brumelle and J.I. McGill. Airline seat allocation with multiple nested fare classes. Operations Research, (1):127–137, 1993.
[RM4] Kalyan T. Talluri and Garrett J. van Ryzin. The theory and practice of revenue management. International Series in Operations Research & Management Science, 68. Kluwer Academic Publishers, Boston, MA, 2004.
61


62


Chapter 4
Markov decision problems with
infinite horizon
4.1 Discounted infinite horizon problems
Assume given a stationary Markov decision process, that is
• a finite or discrete state space E;
• an action space C
• for all x ∈ E, the subset C(x) ⊂ C of all possible actions at any time k, when the state is equal to x;
• the set A := {(x, u) | x ∈ E, u ∈ C(x)} of all possibles couples (state, action) at any time k;
• an initial probability p(0) ∈ ∆E on E, or an initial state x0 ∈ E, which is equivalent to the case where p(0) is the Dirac measure at x0;
with either
• for all x ∈ E and u ∈ C(x), a probability row vector M (u)
x over E, the entries of which will be
denoted
(
M (u)
xy
)
y∈E
, that are the transition probabilities;
or
• a probability space (Ω, A, P ), a random variable X0 with values in E and law p(0), and a sequence of independent and identically distributed random variables (Wn)n≥0 with values in some discrete space W, independent from X0;
• with the dynamics at any time k, which is a map f : A × W → E.
Consider also the following (stationary) parameters:
• the instantaneous/running reward/payoff (at any time k), which is a map r : A → R, where, for all x ∈ E and u ∈ C(x), r(x, u) denotes the reward of the action u ∈ C in state x ∈ E;
• a (fixed) discount factor α ∈ [0, 1).
63


• for all strategies σ = (σk)k≥0 in Σ or ΣR, the discounted total additive payoff with infinite horizon:
J (σ)
α := Jα(X; U ) := E
[∞ ∑
k=0
αkr(Xk, Uk)
]
, (4.1)
where (X, U ) := (Xk, Uk)k≥0 is the process induced by σ as in Definition 3.2 or Definition 3.3.
• and the discounted total additive payoff with infinite horizon starting at x at time 0:
J (σ)
α (x) := Jα,x(X; U ) := E
[∞ ∑
k=0
αkr(Xk, Uk) | X0 = x
]
; (4.2)
Given the above parameters (in particular a stationary MDP, a stationary reward and a discount factor), and a stationary feedback policy π ∈ ΠS (the set of stationary feedback policies), we associate the (stationary) Markov transition matrix: M (π) given by
M (π)
xy := M (π(x))
xy , ∀x, y ∈ E .
We also associate the (stationary) reward vector r(π) ∈ RE with
r(π)
x := r(x, π(x)), for x ∈ E .
Fact 4.1. The associated stochastic process (Xk, Uk)k≥0 (that is satisfying Uk = π(Xk)) is such that (Xk)k≥0 is a stationary Markov chain with initial law p(0) and transition probability matrix M (π). Moreover, (Xk, Uk)k≥0 is also a Markov chain taking its values in E × C.
Using Kolmogorov equation for dicounted criteria (Theorem 2.25) and Fact 4.1, we deduce the following result.
Lemma 4.2. When σ = π is a stationary feedback policy, then the value function v := J(π)
α satisfies the Kolmogorov equation:
v = r(π) + αM (π)v .
Definition 4.3. A Markov decision problem with complete observation and infinite horizon discounted criteria consists in the following optimization problem:
mσax J (σ)
α
where the optimization holds over either all relaxed strategies σ ∈ ΣR, or all pure strategies, or all Markov strategies, or all feedback policies, or all stationary feedback policies. The optimum of above criteria is called the value of the problem. An optimal solution σ is called an optimal strategy, and the corresponding process Uk or (Xk, Uk) an optimal control process.
Moreover, maximization can be replaced by minimization.
64


4.1.1 The stationary dynamic programming equation
Definition 4.4. For all x ∈ E, let vα(x) or simply v(x) be the value of the Markov decision problem with an initial state x: mσax J (σ)
α (x) .
The map vα : E → R, x 7→ vα(x) is called the value function of the MDP.
Theorem 4.5 (Dynamic programming equation for Markov decision problems with discounted infinite horizon criteria). Assume that the map r is bounded from above. Let v be the value function of the Markov decision problem:
v(x) := mσax J (σ)
α (x) ,
where the maximum is taken over all relaxed strategies (starting at time 0). Then, v is the unique solution of the following fixed point equation, called the stationary Bellman dynamic programming equation:
v(x) = sup
u∈C(x)

r(x, u) + α
∑
y∈E
M (u)
xy v(y)

 ∀x ∈ E . (4.3)
Moreover, the values v obtained by optimizing over the restricted sets of pure strategies, Markov strategies, feedback policies, or stationary feedback policies coincide. Assume in addition that the maximum of (4.3) is attained for an action u ∈ C(x) and let us denote by π(x) this action, then the stationary feedback policy π (that is (πk)k≥0 with πk = π) is an optimal strategy of the problem.
We also have w = p(0)v for the value w of the Markov decision problem with infinite horizon discounted criteria. The right hand side of the Bellman equation (4.3):
v(x) = sup
u∈C(x)

r(x, u) + α
∑
y∈E
M (u)
xy v(y)

 ∀x ∈ E
can also be written as:
sup
u∈C(x)
(
r(x, u) + αE [v(X1) | X0 = x, U0 = u]
)
.
Moreover, in the case of the definition of a MDP using a i.i.d. random sequence (Wn) and a dynamics f such that Xn+1 = f (Xn, Un, Wn), the right hand side of the Bellman equation writes:
sup
u∈C(x)
(
r(x, u) + αE [v(f (x, u, W0))]
)
.
4.1.2 The Bellman operator
Definition 4.6. Let Bα : RE → RE be the map such that for all v ∈ RE , and x ∈ E, we have
[Bα(v)](x) = sup
u∈C(x)

r(x, u) + α
∑
y∈E
M (u)
xy v(y)

.
65


The map Bα is called the Bellman operator of the discounted infinite horizon Markov decision problem.
The dynamic programming equation can be rewritten in functional form as the fixed point equation of the Bellman operator Bα: v = Bα(v) .
Fact 4.7. The undiscounted Bellman operator B1 is monotone and additively homogenous.
Recall the proof given in the finite horizon case: Bα is the supremum of Kolmogorov operators
B(π)
α : v 7→ r(π) + αM (π)v .
The operators B(π)
1 are monotone and additively homogeneous, so is B1. One can also do the proof by “hand” on:
B1(v)(x) = sup
u∈C(x)
(
r(x, u) + E [v(f (x, u, W0))]
)
.
Expectation is monotone and it is additively homogenous because E [1] = 1. So is the previous expression as a function of v.
Corollary 4.8. The discounted Bellman operator Bα is Lipschitz continuous for the sup-norm with Lipschitz constant α, thus it is α-contracting when α < 1.
Corollary 4.9. When E is finite and α < 1, the operator Bα admits a unique fixed point v∗. Moreover, for any initial point v0 ∈ RE , the sequence vn+1 = Bα(vn) converges towards v∗:
‖vn − v∗‖∞ ≤ αn‖v0 − v∗‖∞ .
Definition 4.10. The algorithm constructing the sequence vn+1 = Bα(vn) is called value iterations.
4.1.3 Proof of the Stationary Dynamic programming equation
1. v is solution of the Bellman equation. Assume that α < 1. Let v∗ be the unique solution of the Bellman equation v = Bα(v), (by Fact 4.9). Let v(N) be the value function of the finite horizon problem:
v(N)(x) = mσax J (N,σ)
0 (x) ,
with
J (N,σ)
0 (x) := J N
0,x(X; U ) := E
[(N −1
∑
k=0
αkr(Xk, Uk)
)
+ 0 | X0 = x
]
.
From Theorem 3.13, v(N) = v(N)
0 with v(N)
k solution of the dynamic programming equation:
v(N )
N (x) = 0 ∀x ∈ E ,
v(N )
k (x) = sup{αkr(x, u) +
∑
y∈E
M (u)
xy v(N )
k+1(y) | u ∈ C(x)} ∀x ∈ E, k ≤ N − 1.
66


This can be rewritten as
v(N )
k = αkBα(v(N)
k+1/αk+1) .
Hence, v(N) = BαN (0) := Bα ◦ · · · ◦ Bα(0) (where the composition is done N times). Therefore,
limN→∞ v(N) = v∗ where the limit is uniform in E (limit for the sup-norm of RE ).
Let C be a bound of r. Then, for all strategies σ, the sum ∑∞
k=0 αkr(Xk, Uk) exists a.s. since
|αkr(Xk, Uk)| ≤ Cαk (the series is absolutely convergent). Hence the expectation J(σ)
α (x) also exists and is bounded. Therefore
v(x) := sup
σ
J (σ)
α (x) = sup
σ
E
[∞ ∑
k=0
αkr(Xk, Uk) | X0 = x
]
∈R
exists and satisfies
‖v − v(N)‖∞ ≤
∞
∑
k=N
αkC = αN C
1−α ,
so limN→∞ v(N) = v, which implies that v = v∗.
2. Optimality. Note that the bound ‖v − v(N)‖∞ ≤ αN C
1−α holds for the maximization over all nonstationary types of strategies: relaxed strategies, pure strategies, Markov strategies, or feedback policies. Since the value v(N) is the same if we maximize over all these types of strategies, the same property holds for v. To get the equality with stationary feedback strategies, and the optimality of the feedback policy given by Bellman equation, one proceed as for the proof of the finite horizon case. If π is the stationary policy obtained as in the theorem, that is if u = π(x) is optimal in (4.3),
then v satisfies the Kolmogorov equation associated to π, and so (by Theorem 2.25), v(x) = J(π)
α (x) which shows that π is optimal. If u = π(x) is only ε-optimal for (4.3), that is
v(x) − ε ≤

r(x, u) + α
∑
y∈E
M (u)
xy v(y)


Then, v − ε1 ≤ B(π)
α (v). Iterating this inequality and using that B(π)
1 is monotone and additively
homogenous, which, with B(π)
α (v) = B(π)
1 (αv), implies that B(π)
α is monotone and satisfies B(π)
α (λ1 +
v) = αλ1 + B(π)
α (v), we get
v ≤ ε + B(π)
α (v) ≤ ε(1 + α) + [B(π)
α ]2(v) ≤ · · · ≤ ε 1 − αn+1
1 − α + [B(π)
α )]n+1(v) .
Using that B(π)
α is contracting, we get that the limit of [B(π)
α )]n+1(v) is equal to the solution of
the stationary Kolmogorov equation, and is thus equal to J(π)
α . Passing to the limit when n → ∞, in the previous inequality, we deduce:
v≤ ε
1 − α + J (π)
α.
This shows that π is then ε/(1 − α)-optimal for the Markov decision problem. Since this holds for all ε > 0 (with a different π), we obtain that v is less or equal to the
supremum of J (π)
α over all stationary feedback policies. Since v is the supremum of J(σ)
α over all strategies, and is thus greater or equal to the one over all stationary feedback policies, we deduce that both suprema are equal.
67


4.2 Algorithms
4.2.1 Value iteration algorithm
Recall that the value iteration is the algorithm constructing
vn+1 = Bα(vn) .
that is the fixed point iterations associated to the α-contracting operator Bα.
• It satisfies
‖vn − v‖∞ ≤ αn‖v0 − v‖∞ .
• Denote rmax a bound on r (on both sides). Using the definition of v (as a maximum of the criterion (4.2), we found
‖v‖∞ ≤ rmax
1−α .
This can also be obtained from (4.3).
• To find a ε-solution, starting from 0, one need n iterations with αn rmax
1−α ≤ ε, so the complexity is in
O
( log( (1−α)ε
rmax )
log(α)
)
nm ,
where m = card(A) and n = card(E) (O(nm) is the maximal complexity of the computation of Bα(v) for some v).
• If α = 1 − η with η = p/q a small rational, then the length (number of bit) of α is in the order of log(q), whereas −1/ log(α) is in the order of q so is exponential in the length of α, so in the length of the input.
• Hence value iteration is only pseudo-polynomial.
• If α is fixed, and we consider that the entries ε, r(x, u) and M (u)
xy are rational numbers, we obtain that the complexity of value iteration is polynomial in the total length (number of bit) of the entries, so is a polynomial algorithm. Since the number of iterations depends on rmax, value iteration algorithm is not strongly polynomial.
• In practice one uses rather a variant similar to Gauss-Seidel algorithm (wrt to Jacobi) for the solution of linear systems, in order to avoid useless storage. The resulting algorithm is called Ford-Bellman algorithm. It depends on some ordering on E.
It has a similar convergence rate and complexity.
• When α < 1, or when α = 1 and the MDP is not deterministic, none of them converge in finite time.
• This is already the case with no control: C = {1}.
68


• For α < 1, take E = {1}, r = rmax, and v0 = 0, then vn ∈ R satisfies
vn+1 = rmax + αvn =⇒ vn = rmax
1 − αn
1−α
In this case, the number of iterations is equal to log( (1−α)ε
rmax )
log(α) . So the upper bound was tight.
• For α = 1, take E = {1, 2}, M =
[1/2 1/2 01
]
and r =
[1
0
]
. Then vn ∈ R2 satisfies
vn+1(1) = 1 + vn(1)/2 + vn(2)/2, vn+1(2) = vn(2).
• If v0(2) = 0, we get vn(1) = 2(1 − 2−n), and so the number of iterations is equal to log(ε/2)
log(1/2) .
4.2.2 Policy iteration algorithm
Recall that we want to solve
v(x) = sup
u∈C(x)

r(x, u) + α
∑
y∈E
M (u)
xy v(y)

 ∀x ∈ E .
and find an optimal strategy of the Markov decision problem. Assume that the action space C are finite, then for all v ∈ RE , and x ∈ E, the maximum in
sup
u∈C(x)

r(x, u) + α
∑
y∈E
M (u)
xy v(y)

 (4.4)
is attained by some action π(x) ∈ C(x). Theorem 4.5 shows that computing π with respect to the solution of the Bellman equation yields a feedback policy which is optimal among all strategies. The following algorithm has been introduced by Howard (1960) and is thus also called Howard algorithm.
Definition 4.11. The policy iteration algorithm applied to the Bellman equation v = Bα(v) consists in the following successive steps k ≥ 0, starting from a policy π0 ∈ Π:
1. wk is the unique solution of the Kolmogorov equation associated to the policy πk:
v(x) = r(x, πk(x)) + α
∑
y∈E
M (πk(x))
xy v(y) ∀x ∈ E .
2. πk+1 is an optimal policy for wk, that is an element π such that
π(x) ∈ Argmax
u∈C(x)

r(x, u) + α
∑
y∈E
M (u)
xy wk(y)

 ∀x ∈ E .
69


The Bellman equation can be put in the form:
v = Bα(v) ,
where Bα is the supremum of the Kolmogorov operators:
B(π)
α : v 7→ r(π) + αM (π)v .
If π is optimal for v in (4.4), then Bα(v) = B(π)
α (v)
which means that Bα(v) = max
π∈Π B(π)
α (v) .
Fact 4.12. The policy iteration algorithm applied to the Bellman equation v = Bα(v) consists in the following successive steps, starting from a policy π0 ∈ Π: for k ≥ 0, do
1. wk is the value of problem when the policy is freezed to πk that is the solution of the equation
w = B(πk)
α (w).
2. πk+1 is an optimal policy for wk, that is an element π of Π such that B(π)
α (wk) = Bα(wk).
Theorem 4.13. Assume that the optimization problems in Bellman equations can be solved, that
is, for all v ∈ RE , there exists π ∈ Π such that B(π)
α (v) = Bα(v). Denote by v the value function of the MDP with discounted criteria, that is the solution of v = Bα(v). Then, for all k ≥ 0, we have
wk ≤ wk+1 ≤ · · · ≤ v ,
and
kli→m∞ wk = v .
Moreover,
wk ≤ Bα(wk) ≤ wk+1 ,
which means that the policy iteration algorithm converges faster than the value iteration algorithm, and we have
‖wk+1 − v‖∞ ≤ α‖wk − v‖∞ .
The proofs of Policy iteration properties use the following properties that are of independent interest.
Proposition 4.14 (Sub or supersolutions). Let B be a monotone operator from RE to itself, which is contracting for the sup-norm. Let v be the unique fixed point of B. Then
w ≤ B(w) =⇒ w ≤ v (4.5)
w ≥ B(w) =⇒ w ≥ v . (4.6)
Proof. From w ≤ B(w) and monotonicity of B, we get, for all n ≥ 1, w ≤ · · · ≤ Bn(w). Since B is contracting, Bn(w) converges towards v, which implies w ≤ v.
70


Definition 4.15. A solution of w ≤ B(w) is called a subsolution of Bellman equation. A solution of w ≥ B(w) is called a supersolution of Bellman equation.
Proof of Theorem 4.13. We have
wk = B(πk)
α (wk) (4.7)
Bα(wk) = B(πk+1)
α (wk) (4.8)
wk+1 = B(πk+1)
α (wk+1) . (4.9)
Using the first and second equation together with the definition of Bα, we get
wk = B(πk)
α (wk) ≤ Bα(wk) = B(πk+1)
α (wk) .
Hence wk is a subsolution of the Kolmogorov equation w = B(πk+1)
α (w). From (4.5) applied to the
operator B(πk+1)
α , and (4.9), we deduce that wk ≤ wk+1. Since we also have wk ≤ Bα(wk), wk is a subsolution of the Bellman equation w = Bα(w). Applying (4.5) to Bα, we deduce that wk ≤ v, for all k ≥ 0.
From the above inequalities, we also get that wk ≤ Bα(wk) = B(πk+1)
α (wk), and so B(πk+1)
α (wk) ≤
(B(πk+1)
α )2(wk) ≤ · · · ≤ wk+1. This shows that wk ≤ Bα(wk) ≤ wk+1 ≤ v, which is the second assertion. With this we get that
0 ≤ v − wk+1 ≤ v − Bα(wk)
hence
‖v − wk+1‖∞ ≤ ‖v − Bα(wk)‖∞ = ‖Bα(v) − Bα(wk)‖∞ ≤ α‖v − wk‖∞ .
This shows that the sequence wk converges towards v and that the convergence is faster than the one of value iterations.
Theorem 4.16. Assume that the action spaces C(x) are all finite. Then, the policy iteration algorithm converges in a finite number of steps.
Proof. Since the sets C(x) are finite, the set of feedback policies, Π is finite. Therefore, there exists
k < ` such that πk = π`. From the uniqueness of the solution w to the equation B(πk)
α (w) = w, we get that wk = w`. Since the sequence wk is nondecreasing, wk ≤ wk+1 ≤ · · · ≤ w`, and satisfies wk = w`, we get the equality wk = wk+1. Then, using the definition of wk+1 and πk+1, and
wk = wk+1, we deduce wk = wk+1 = B(πk+1)
α (wk+1) = B(πk+1)
α (wk) = Bα(wk). Hence, wk is a fixed point solution of Bα and since Bα is contracting, the fixed point is unique, so wk = v. Moreover,
since πk+1 satisfies Bα(v) = B(π)
α (v), then it is optimal, and w` = v for all ` ≥ 1.
4.2.3 Additional properties of Policy iterations for discounted problems
In Theorem 4.18 below, we prove that the policy iterations coincide with Newton algorithm applied to the system of equations v − Bα(v) = 0. Let us first remark that if the map Bα is regular, then the latter Newton algorithm consists in the iterations (wk)k≥0 in which wk+1 is the solution of the tangent equation in point wk, that is
Dwk (I − Bα)(wk+1 − wk) + (I − Bα)(wk) = 0
71


which can be rewritten as:
wk+1 = Bα(wk) + Dwk (Bα)(wk+1 − wk) .
Moreover, one can weaken Newton algorithm by replacing the above differential by a subdifferential of Bα. Therefore, using the following result, one can see that policy iteration algorithm belongs to the class of generalized Newton algorithms, since then the above equation is equivalent to
wk+1 = rπwk
+ αM (πwk )wk+1 = Bπwk
α (wk+1) .
Lemma 4.17. For all x ∈ E, the map ψ : v ∈ RE 7→ [Bα(v)](x) ∈ R is convex, and for all v ∈ RE ,
and u ∈ Argmaxu∈C(x){r(x, u) + αM (u)
x v}, the row vector αM (u)
x is in the subdifferential of ψ at point v.
Proof. For all x ∈ E, the map ψ : v ∈ RE 7→ [Bα(v)](x) ∈ R is convex as a supremum of affine
maps: ψ(x) = [Bα(v)](x) = supu∈C(x) r(x, u) + αM (u)
x v. If u ∈ Argmaxu∈C(x){r(x, u) + αM (u)
x v}, we get that
ψ(w) − ψ(v) = [Bα(w)](x) − [Bα(v)](x) ≥ (r(x, u) + αM (u)
x w) − (r(x, u) + αM (u)
x v) = αM (u)
x (w − v) .
This shows that the row vector αM (u)
x is in the subdifferential of ψ at point v.
Theorem 4.18. Assume that E is finite, that the sets C(x) are compact spaces, that there exists a continuous map v ∈ RE 7→ πv ∈ Π such that, for all v ∈ RE , πv is the unique element of Π such
that B(πv)
α (v) = Bα(v), and that the map u ∈ C(x) 7→ M (u)
xy is continuous for all x, y ∈ E. Then, Bα is of class C1, the policy iteration algorithm coincides with Newton algorithm associated to the fixed point equation v = Bα(v), and its convergence is superlinear:
kli→m∞
‖wk+1 − v‖∞
‖wk − v‖∞
=0 .
Proof. Assume that a continuous map v ∈ RE 7→ πv ∈ Π exists with the property that πv is the
unique element of Π such that B(πv)
α (v) = Bα(v). This means that, for all v ∈ RE , x ∈ E, u = πv(x)
is the unique element of Argmaxu∈C(x){r(x, u) + αM (u)
x v}. Then, by Lemma 4.17, αM (u)
x is in the
subdifferential of ψ : v ∈ RE 7→ [Bα(v)](x) ∈ R at point v. Similarly, for all w ∈ RE , αM (πw(x))
x is in the subdifferential of ψ at w. Let p be any element of the subdifferential of ψ at v, which means that for all w ∈ RE , we have
ψ(w) − ψ(v) ≥ p(w − v) .
Then, we also have
ψ(w) − ψ(v) ≤ αM (πw(x))
x (w − v) ,
since αM (πw(x))
x is in the subdifferential of ψ at w. Therefore
p(w − v) ≤ αM (πw(x))
x (w − v) .
Consider the sequence wn = v + 1
n z, we deduce
pz ≤ αM (πwn (x))
x z.
72


Using that wn converges towards v, and that the maps w 7→ πw(x) ∈ C(x) and u ∈ C(x) 7→ M (u)
xy
are continuous, we deduce that αM (πwn(x))
x converges towards αM (πv(x))
x . This implies that pz ≤
αM (πv(x))
x z. Since this holds for all z ∈ RE , we deduce that p = αM (πv(x))
x . We have shown that the
subdifferential of ψ is reduced to a singleton. Then, the map is differentiable [6] and αM (πv(x))
x is the differential of v 7→ [Bα(v)](x). Applying this property for all x ∈ E, we get that Bα si differentiable
at v with differential equal to αM (πv). Since this differential is continuous with respect to v, this shows also that Bα is of class C1. Then, the above comments show that policy iteration algorithm coincides with Newton algorithm. Now let v be the value function. We have
(I − αM (πwk ))(wk+1 − v) = rπwk
+ αM (πwk )v − v (4.10)
= Bα(wk) − Bα(v) − αM (πwk )(wk − v) . (4.11)
We can show
‖wk+1 − v‖∞ ≤ 1
1 − α ‖Bα(wk) − Bα(v) − αM (πwk )(wk − v)‖∞ .
Since we already proved that Bα is of class C1 and that its differential at v is equal to αM (πv), we get that the right hand side of the above inequality is in o(‖wk − v‖∞), which gives the superlinear convergenve of Policy Iterations. Another proof is using directly the above subdifferential properties to show from (4.10):
α(M (πv) − M (πwk ))(wk − v) ≤ (I − αM (πwk ))(wk+1 − v) ≤ 0 .
Then, we obtain
‖wk+1 − v‖∞ ≤ α
1 − α ‖(M (πwk )−M(πv) )(wk − v)‖∞ ,
and since the map v 7→ M (πv) is continuous, the right hand side of the above inequality is in o(‖wk − v‖∞), which gives the superlinear convergenve of Policy Iterations.
Note that contrarilly to the general situation of the Newton algorithm, the policy iterations always converge towards the solution of the fixed point equation. This comes from the convexity of the maps v 7→ [B(v)](x), which implies the monotonicity of the sequence of value functions wk.
4.3 Optimal stopping time problems with infinite horizon
Consider
• a (fixed) stationary Markov chain (Xn)n≥0 over a probability space (Ω, A, P ) with values in a finite state space E and transition matrix M and initial probability law p(0).
• an instantaneous/running reward/payoff (at any time k), which is a map r : A → R;
• a (fixed) discount factor α ∈ [0, 1).
73


• for all stopping times τ with respect to the Markov chain (Xn)n≥0, the discounted infinite horizon payoff with stopping time τ :
J (τ )
α :=E
[(τ −1
∑
`=0
α`r(X`)
)
+ ατ φ(Xτ )
]
; (4.12)
• and the discounted infinite horizon payoff with stopping time τ , starting in x at time 0:
J (τ )
α :=E
[(τ −1
∑
`=0
α`r(X`)
)
+ ατ φ(Xτ ) | X0 = x
]
. (4.13)
Definition 4.19. An Optimal stopping time problem with complete observation and infinite horizon discounted criteria consists in the following optimization problem:
mτax J (τ)
α
where the optimization holds over all stopping times τ with respect to the Markov chain (Xn)n≥0. The optimum of above criteria is called the value of the problem. An optimal solution τ is called an optimal stopping time.
Definition 4.20. For all x ∈ E, let vα(x) or simply v(x) be the value of the optimal stopping time problem with an initial state x: mτax J (τ)
α (x) .
The map vα : E → R, x 7→ vα(x) is called the value function of the stopping time problem.
Theorem 4.21 (Dynamic programming equation for optimal stopping time problems with discounted infinite horizon criteria). Assume that the map r is bounded from above. Let v be the value function of the optimal stopping time problem:
v(x) := mτax J (τ)
α (x) ,
where the maximum is taken over all stopping times τ with respect to the Markov chain (Xn)n≥0. Then, v is the unique solution of the following fixed point equation, called stationary Bellman equation or variational inequality:
v(x) = max
(
r(x) + α
∑
y∈E
Mxyv(y), φ(x)
)
∀x ∈ E . (4.14)
Let B be the set of states in which the maximum in (4.14) is attained in the first term, that is
B := {x ∈ E | r(x) + α
∑
y∈E
Mxyv(y) ≥ φ(x)} .
Then an optimal stopping time τ is obtained by choosing for τ the exit time τB from B of the Markov chain.
Proof. Consider the MDP in which
74


• the state space is E′ = E ∪ {c} where c 6∈ E;
• the control space C = {0, 1} (0 for stop, and 1 for not stop);
• the control space C(x) is such that C(x) = C if x ∈ E and C(x) = {0} if x = c.
• the states of the MDP, Yn, depend on the states of the Markov chain Xn and on the actions as follows:
Yn+1 = g(Xn+1, Un)
where g(x, u) = x if u = 1 and g(x, u) = c otherwise.
Then, for all xi ∈ E′, ui ∈ Ci(xi), i ≥ 0, we have
P (Yk+1 = xk+1 | Yk = xk, Uk = uk, Yk−1 = xk−1, . . . , Y0 = x0, U0 = u0)
= 1 if c = xk+1 and uk = 0
= 0 if xk+1 ∈ E and uk = 0
= Mxk,xk+1 if xk, xk+1 ∈ E and uk = 1
This implies that
P (Yk+1 = xk+1 | Yk = xk, Uk = uk, Yk−1 = xk−1, . . . , Y0 = x0, U0 = u0)
= P (Yk+1 = xk+1 | Yk =k, Uk = uk) .
which is the Markov property. The transition vectors of the MDP are: M (1)
xy = Mxy for all x, y ∈ E,
M (1)
xy = 0 for all y = c, and M (0)
xy = 1 for y = c and 0 for y ∈ E.
Let us take the rewards: r′(x, 1) = r(x), r′(x, 0) = φ(x) for x ∈ E and r′(c, 0) = 0.
Then, the value of the discounted infinite horizon problem coincides with the value of the stopping time problem. Indeed, take τ = inf{t ≥ 0 | Ut = 0}, and conversely take Un = 1 for all n < τ and Un = 0 for n ≥ τ . Then, τ is a stopping time if and only if Un is given by a strategy.
The Bellman equation of the Markov decision problem is then:
v(x) = max
(
r(x) + α
∑
y∈E
Mxyv(y), φ(x) + v(c)
)
∀x ∈ E ,
v(c) = 0 .
Here the action u = 1 corresponds to the left term in the above maximum, and u = 0 corresponds to the right term. Therefore if π : E → {0, 1} is an optimal policy given by the Bellman equation, we recover again the optimal stopping time by taking:
τ = inf{t ≥ 0 | π(Xt) = 0}
or by taking τ = τB where B = {x ∈ E | π(x) = 1}.
75


4.4 Problems with variably discounted infinite horizon payoff
Assume given a stationary Markov decision process, and the following stationary parameters:
• the instantaneous/running reward/payoff (at any time k), which is a map r : A → R;
• a variable discount factor (at any time k), which is a map α : A → R+;
• for all strategies σ = (σk)k≥0 in Σ or ΣR, the variably discounted total additive payoff with infinite horizon:
J (σ) :=J (X; U ) := E
[∞ ∑
`=0
( `−1
∏
m=0
α(Xm, Um)
)
r(X`, U`)
]
, (4.15)
where (X, U ) := (Xk, Uk)k≥0 is the process induced by σ as in Definition 3.2 or Definition 3.3.
• and the variably discounted total additive payoff with infinite horizon, starting x at time 0:
J (σ)(x) := Jx(X; U ) := E
[∞ ∑
`=0
( `−1
∏
m=0
α(Xm, Um)
)
r(X`, U`) | Xk = x
]
, (4.16)
Theorem 4.22 (Dynamic programming equation for Markov decision problems with variably discounted infinite horizon criteria). Assume that the map r is bounded from above and that α(x, u) ≤ α ̄ for all (x, u) ∈ A, for some constant α ̄ < 1. Let v be the value function of the Markov decision problem associated to the above parameters:
v(x) := mσax J (σ)(x) ,
where the maximum is taken over all relaxed strategies (starting at time 0). Then, v is the unique solution of the following fixed point equation, called the stationary Bellman dynamic programming equation:
v(x) = sup
u∈C(x)

r(x, u) + α(x, u)
∑
y∈E
M (k,u)
xy v(y)

 ∀x ∈ E . (4.17)
Moreover, the values v obtained by optimizing over the restricted sets of pure strategies, Markov strategies, or feedback policies, or stationary feedback policies coincide. Assume in addition that the maximum of (4.17) is attained for an action u ∈ C(x) and let us denote by π(x) this action, then the stationary feedback policy π (that is (πk)k≥0 with πk = π) is an optimal strategy of the problem. problem.
The arguments of the proof are the same as when α is constant. Let us consider the Bellman operator of the variably discounted problem which is the map B : RE → RE such that, for all v ∈ RE , and x ∈ E, we have
[B(v)](x) = sup
u∈C(x)

r(x, u) + α(x, u)
∑
y∈E
M (u)
xy v(y)

.
The map B satisfies the following properties
76


Lemma 4.23. Under the assumptions of Theorem 4.22, B is monotone and α ̄-additively subhomogenous (Definition 3.22), meaning that for all v ∈ RE and λ ≥ 0, we have
B(v + λ1) ≤ B(v) + α ̄λ1 .
Therefore, it is Lipschitz continuous for the sup-norm with Lipschitz constant α ̄ < 1, thus it is α ̄-contracting.
Proof. The first assertion can be proved elementarily. Let us prove the second one using the first one. Let v, v′ ∈ RE . Denote λ = ‖v−v′‖∞. We have v(x) ≤ λ+v′(x) for all x ∈ E, that is v ≤ v′+λ1. Since B is monotone, we get B(v) ≤ B(v′ + λ1). Since B is additively subhomogenous with constant α ̄, we deduce B(v′ + λ1) ≤ B(v′) + α ̄λ. Then, B(v) ≤ B(v′) + α ̄λ1, hence maxx∈E [B(v)](x) − [B(v′)](x) ≤ α ̄λ. Exchanging v and v′ we get the other inequality: maxx∈E [B(v′)](x) − [B(v)](x) ≤ α ̄λ. Hence ‖B(v′) − B(v)‖∞ = maxx∈E max([B(v′)](x) − [B(v)](x), [B(v)](x) − [B(v′)](x)) ≤ α ̄λ = α ̄‖v′ − v‖∞. This shows the Lipschitz continuity.
Proof of Theorem 4.22. We use the same technique as for the constant discount factor case: first use of Bellman equation for finite horizon problems with mixed criterias, then take the limit using contraction, then use of the stationary Kolmogorov equation for infinite horizon criteria with variable discount factor.
Exercise 4.4.1. Show that one can reduce the above problem to an infinite horizon problem with constant discount factor equal to α ̄, by adding a cemetery point to the state space E.
Corollary 4.24. Under the assumptions of Theorem 4.22, the sequence v(T ) of value functions of finite horizon problems with mixed criteria (given in Remark 3.21) converges when T goes to infinity to the unique solution of the stationary Bellman dynamic programming equation (4.17).
4.5 Problems with exit time in infinite horizon
Assume given a stationary Markov decision process, and the following stationary parameters:
• a strict subset B of E;
• a final reward, which is a map φ : E → R;
• a (fixed) discount factor α ∈ [0, 1);
• the instantaneous/running reward/payoff (at any time k), which is a map r : A → R;
• for all strategies σ = (σk)k≥0 in Σ or ΣR, the payoff with exit time in infinite horizon:
J (B,σ) :=J B(X; U ) := E
[τB −1
∑
`=0
α`r(X`, U`) + ατB φ(XτB )1τB<+∞
]
, (4.18)
where (X, U ) := (Xk, Uk)k≥0 is the process induced by σ as in Definition 3.2 or Definition 3.3, and τB is the exit time of the process (Xn)n≥0 from B.
77


• and the payoff with exit time and infinite horizon, starting in x at time 0:
J (B,σ)(x) := J B
x (X; U ) := E
[τB −1
∑
`=0
α`r(X`, U`) + ατB φ(XτB )1τB<+∞ | X0 = x
]
. (4.19)
Theorem 4.25 (Dynamic programming equation for Markov decision problems with exit time in discounted infinite horizon). Assume that the maps φ, r are bounded from above. Let v be the value function of the Markov decision problem:
v(x) := mσax J (B,σ)(x) ,
where the maximum is taken over all relaxed strategies (starting at time 0). Then, v is the unique solution of the following fixed point equation, called the stationary Bellman dynamic programming equation:
v(x) = sup
u∈C(x)

r(x, u) + α
∑
y∈E
M (k,u)
xy v(y)

 ∀x ∈ B . (4.20a)
with boundary condition
v(x) = φ(x), ∀x 6∈ B . (4.20b)
Moreover, the values v obtained by optimizing over the restricted sets of pure strategies, Markov strategies, or feedback policies, coincide. Assume in addition that the maximum of (4.20) is attained for an action u ∈ C(x), for x ∈ B, and let us denote by π(x) this action when x ∈ B, and choose any action π(x) for x 6∈ B, then the stationary feedback policy π (that is (πk)k≥0 with πk = π) is an optimal strategy of the problem.
As for finite horizon problems, Theorem 4.25 can be deduced easily from Theorem 4.22 using the following property which is the same as Fact 2.34 and Fact 3.26.
Fact 4.26. The functional J(B,σ) of (4.19) can rewritten as the infinite horizon mixed functional:
J (B,σ)(x) := J B
x (X; U ) :=E
[∞ ∑
`=0
( `−1
∏
m=0
α(Xm, Um)
)
r′(X`, U`)
]
,
for the same Markov Decision process, with the instantaneous rewards r′ and variable discount factors α given by:
r′(x, u) = r(x, u), for x ∈ B, u ∈ C(x)
r′(x, u) = φ(x), for x 6∈ B, u ∈ C(x)
α(x, u) = α, for x ∈ B, u ∈ C(x)
α(x, u) = 0, for x 6∈ B, u ∈ C(x) .
78


4.6 Problem: Divorce of Birds
This problem is taken from the (ENSTA+M2) exam of 2016/2017. We consider the modelization of the decision of divorce of birds as a MDP. We assume that at each breeding (reproduction) season, the bird female has a mate, and that at the end of the season, she is taking the decision on whether to divorce her mate. Then, winter arrives and the female and the male may die. If the female has no mate after winter (she divorced, or the male died, or it is the first breeding season of the female), then she is choosing a mate among a “pool” of available males. The decision of the female is based on the qualities of the male and female, and also on the information on whether it is the first breeding season of the female, or the female divorced, or the male died during winter. We consider the following notations, parameters and assumptions:
• We consider one female during her life, and denote by Yk ∈ Y ⊂ R her quality at the begining of the breeding season of year k (one can start to number years after the female is able to breed). We shall assume that Y = [y ̄] := {0, . . . , y ̄}, where y = 0 means that the female is dead. We denote by Y∗ = Y \ {0}.
• We denote by Xk ∈ X ⊂ R the quality of the male choosen by the female at the begining of the breeding season of year k. Again, one may assume that X = [x ̄].
• We denote by Zk ∈ Z = {0, 1, M } the information on whether it is the first breeding season of the female or the female divorced (in which case Zk = 0), or the male died during winter (in which case Zk = M ), or the female mated with the same male the previous year (in which case Zk = 1).
• We denote by Uk ∈ C = {0, 1} the decision of the female to divorce: Uk = 1 if she decides to divorce and 0 otherwise.
• r(x, y, z) is the reproductive success, that is the expected number of children, during one season, when the qualities of the male and female are x and y and the information on whether it is the first breeding season of the female or the female divorced or the male died during winter is z.
• We assume that the pools of males in which the female is choosing a partner each year when needed are independent and that f (x) is the probability of finding a male with quality x in a pool.
• sf and sm are respectively the survival probabilities of a female and a male after winter. They are independent of age, constant and < 1.
• We assume that the quality of males and females do not vary with time until their death.
Q 6.1. Show that the sequence Yk is a Markov chain and compute its transition probability.
Q 6.2. Show that the sequences Xk, Yk, Zk, Uk define a MDP, precise what is the state and what is the control, and compute the transition probabilities.
Q 6.3. We assume that the aim of the female is to maximize her reproductive success, that is the expected total number of her children, during all her life. Write this as an infinite horizon criterion for the MDP.
79


Q 6.4. Let v(x, y, z) be the value of the previous problem when the initial qualities of male and female are x ∈ X and y ∈ Y∗ and the information on what happened before and during previous winter is z ∈ Z. What is the equation satisfied by the function v? How to find an optimal policy?
Q 6.5. Show that the equation of v is of the form v(x, y, z) = [F (v)](x, y, z) where F is an operator on the set of functions from X × Y∗ × Z to R of the form:
[F (v)](x, y, z) = r(x, y, z) + sf max
(
[M (1)v](y), [M (0)v](x, y)
)
where v is identified to a vector, for u = 0, 1, M (u) is a Markov matrix, and [M (0)v]x,y,z does not depend on z and [M (1)v]x,y,z does not depend on x and z.
Q 6.6. Deduce that the operator F is contracting for the sup-norm ‖v‖ = maxx,y,z |v(x, y, z)|.
Q 6.7. Show that the fixed point v of F is unique.
Q 6.8. Show that if r is nondecreasing with respect to x, then so does v.
Q 6.9. Show in that case that there exists x∗(y) such that the optimal policy of the female y is to divorce from the male x if and only if x < x∗(y).
Some references for this problem:
[BI1] John M. McNamara and Par Forslund. Divorce rates in birds: Predictions from an optimization model. The American Naturalist, 147(4):609–640, 1996.
80


Chapter 5
Long run average payoff problems
5.1 Motivation
Consider a stationary Markov Decision Process (Xk)k≥0, that is a MDP with a dynamics (transition probabilities) independent of time. This means that the following parameters (independent of time) are given:
• a state space E, which is a finite or countable set;
• a set of actions C, and possibly for each state x ∈ E, a nonempty subset C(x) of C, which is the set of possible actions when the state is equal to x;
• an initial probability p(0) on E, or an initial state x0, which is equivalent to the case where p(0) is the Dirac measure at x0;
• for all x ∈ E and u ∈ C(x), a probability vector M (u)
x over E, the entries of which will be
denoted
(
M (u)
xy
)
y∈E
.
Recall, that given the above parameters, and a pure strategy σ = (σk)k≥0, there exists two discrete time processes (Xk)k≥0 and (Uk)k≥0 taking their values in E and C respectively, with
transition probabilities M (u)
xy :
M (u)
xy = P (Xk+1 = y | Xk = x, Uk = u)
satisfying
Uk = σk(X0, U0, . . . , Xk−1, Uk−1, Xk) ,
and the Markov property:
P (Xk+1 = y | Xk = x, Uk = u, Xk−1 = xk−1, Uk−1 = uk−1, . . . , X0 = x0, U0 = u0)
= P (Xk+1 = y | Xk = x, Uk = u) , ∀x, y, xi ∈ E, u ∈ C(x), ui ∈ C(xi), for i ≥ 0 .
Moreover, the same holds for a relaxed strategy, in which case:
P (Uk ∈ B | X0, U0, . . . , Xk−1, Uk−1, Xk) = [σk(X0, U0, . . . , Xk−1, Uk−1, Xk)](B) .
81


We are interested here in the optimization of infinite horizon undiscounted criteria or in long run average criteria, for which we look for optimal strategies (among all strategies) that would be stationary and in feedback form (Markov). We assume given the stationary instantaneous/running reward/payoff (at any time k), which is a map r : A → R. For all strategies σ = (σk)k≥0 in Σ or ΣR, the undiscounted total additive payoff with infinite horizon (when it exists) is:
J (∞,σ) := J ∞(X; U ) := E
[∞ ∑
k=0
r(Xk, Uk)
]
, (5.1)
where (X, U ) := (Xk, Uk)k≥0 is the process induced by σ as in Definition 3.2 or Definition 3.3. This criteria is finite when for instance there exist a cemetery point c in which the state process arrives almost surely in a finite expected time and the reward there is zero. In that case, one can often transform the problem in a discounted control problem, in which the discount factor α is such that the probability to arrive in one step to the cemetery point is at least 1 − α. So we can generally apply the methods of the first part of the course. In particular, the value function
v∞(x) := max J (∞,σ) when X0 = x ,
is equal to the limit of the value function vT of the problem with finite horizon and (undiscounted) additive criteria: vT (x) = max J (T,σ)(x) , (5.2)
with
J (T,σ)(x) = E
[T ∑
k=0
r(Xk, Uk) | X0 = x
]
. (5.3)
The second type of criteria is the mean-payoff or long run time average payoff/reward, which is one of the following ones, for all strategies σ = (σk)k≥0 in Σ or ΣR
J (+,σ) := J +(X; U ) := lim sup
T →∞
{
1
TE
[T ∑
k=0
r(Xk, Uk)
]}
, (5.4a)
J (−,σ) := J −(X; U ) := lim inf
T →∞
{
1
TE
[T ∑
k=0
r(Xk, Uk)
]}
, (5.4b)
where (X, U ) := (Xk, Uk)k≥0 is the process induced by σ as in Definition 3.2 or Definition 3.3. The corresponding value functions will be denoted:
ζ±(x) = max J (±,σ) when X0 = x .
One may try to understand in which situations both criteria are equal and independent of the initial law of the MDP, and to compare the limit of (1 − α)Jασ when α goes to 1 from below. Moreover, we shall compare the optimum of the limit with the limit of the optimum. We also look for an optimal strategy which is a feedback stationary policy. We first study the uncontrolled case.
82


5.2 Long term behavior of Markov chains
5.2.1 Ergodicity of Markov chains
In this section, we shall study the uncontrolled case. We assume that (Xk)k≥0 is a stationary Markov chain on the finite state space E (for instance E = [n]) and we denote by M its transition probability matrix. Then, if X0 = x, we have
E
[T ∑
k=0
r(Xk)
]
=
T
∑
k=0
[M kr]x ,
and when X0 is random with law p(0), we have
E
[T ∑
k=0
r(Xk)
]
=
T
∑
k=0
p(0)M kr .
Hence, the value functions ζ± defined in the previous section reduce to
ζ (x) = J ((Xk)k≥0) :=



lim supT →∞
{1 T
∑T
k=0[M kr]x
}
if = +
lim infT →∞
{1 T
∑T
k=0[M kr]x
}
if = − .
(5.5)
Moreover, if we consider the case of a Markov chain (Xk) with initial law p(0) (not necessarily equal to the Dirac measure in some state x0), then we are looking for
ζ (p(0)) = J ((Xk)k≥0) :=



lim supT →∞
{1 T
∑T
k=0(p(0)M kr)
}
if = +
lim infT →∞
{1 T
∑T
k=0(p(0)M kr)
}
if = − .
(5.6)
Example 5.1. If (Xn)n≥0 is a sequence of independent random variables with values in E, then it is in particular a Markov chain with transition matrix M such that all rows are equal to the probability vector of X0, that is p(0). Then, (r(Xn))n≥0 is a sequence of i.i.d. random variables with expectation equal to p(0)r and the law of large numbers shows that
1
T
T
∑
k=0
r(Xk) T−→→∞ p(0)r a.s.
Taking the expectation, we get that ζ±(p(0)) = p(0)r.
In order to generalize the law of large numbers, or at least the easier expectation version above, to a Markov chain, one need the following notion.
Definition 5.2. We say that m ∈ ∆E is an invariant probability measure of the Markov chain (Xn)n≥0 on E with transition matrix M , or simply of the matrix M , if m satisfies mM = m.
As an example, if Xn are i.i.d with laws p, then Xn is a Markov chain with invariant probability measure p.
Fact 5.3. If the initial law of a Markov chain is an invariant probability measure, p(0) = m, then the law of Xn is equal to m for all n ≥ 0.
83


In this case, it is easy to see that ζ±(p(0)) = mr. This motivates the following definition.
Definition 5.4. We say that the Markov chain with Markov transition matrix M is ergodic if M has a unique invariant probability measure m.
To check the ergodicity of the chain or to find more generally the limit ζ±(p(0)), we need to study the spectral properties of the Markov matrix M . To do this, we can use general linear algebra techniques in particular Jordan normal form and/or the Perron-Frobenius theorem which is the tool for studying matrices with nonnegative entries. The latter result uses the properties of the graph associated to M , defined in the following section.
5.2.2 Graph properties of a Markov matrix
Recall (see Definition 2.13) that to a nonnegative matrix M over E, we associate a digraph denoted G(M ), with set of nodes equal to E and set of arcs A the set of (x, y) ∈ E × E such that Mxy > 0.
Definition 5.5. Given a directed graph G, with set of nodes E, we define the relations on E such that for x, y ∈ E,
• x → y if there exists a path from x to y of any length ≥ 0 in G (where a path of length 0 means that x = y).
• x ∼ y if x → y and y → x.
Proposition 5.6. For any digraph G with set of nodes E, ∼ is an equivalence relation and → a preorder on E. This defines a partition of E into equivalence classes for ∼, that are called strongly connected components of the graph G. If G = G(M ), where M is a Markov matrix, they are also called communication classes of M . Moreover, the relation → becomes a partial order on the set E/ ∼ of strongly connected components of G.
Proof. → is reflexive, x → x ∀x ∈ E, because paths of length 0 are allowed. It is transitive : (x → y and y → z ⇒ x → z) ∀x, y, z ∈ E, by concatenation of paths. So it is a preorder. Therefore ∼ is also reflexive and transitive. Moreover, it is symmetric by definition: (x ∼ y ⇔ y ∼ x) ∀x, y ∈ E.
Recall that the equivalence class of x for ∼ is defined as x ̄ = {y ∈ E, x ∼ y}, and that we have x ̄ = y ̄ if x ∼ y and x ̄ ∩ y ̄ = ∅ otherwise, so that equivalence classes define a partition of E. Since x ∼ y if x → y and y → x, we get that → can be defined on the quotient set E/ ∼, that is the set of equivalence classes for ∼, and that on this quotient set, we have (x ̄ → y ̄ and y ̄ → x ̄) if and only if x ̄ = y ̄, so that → becomes a (partial) order.
Definition 5.7. • A subset F of E is closed if (x ∈ F and x → y) ⇒ y ∈ F .
• A closed set with a unique element is absorbant.
• A maximal element for → in E/ ∼ is called a final class. This is a stongly connected component of G which is also a closed set.
• A transient state is an element of E which is not in a final class.
• The graph G is strongly connected if it has a unique strongly connected component, or equivalently if E is the only closed set.
84


• The matrix M is irreducible if G(M ) is strongly connected.
• The Markov chain (Xn)n≥0 is irreducible if its transition matrix is irreducible.
In a finite set E, final classes are equivalent to recurrence classes.
Example 5.8. The digraph G(M ) of the following matrix M is shown on the right:
M=

      
000001
1
2 0001
20 001000 100000
03
4
1
4000 000100

      
1 2 3 4 5 6
.
6
2
1
1
1
1/2
3/4
1/4
1
1
4
3
5
1/2
We see 3 strongly connected components: {1, 4, 6}, {3} and {2, 5}. The state 3 is absorbant. The closed sets are E, {3}, {1, 4, 6} and {1, 3, 4, 6}. The final classes are {3} and {1, 4, 6}.
Example 5.9 (Simple random walk). Given a directed graph G with set of nodes N and arcs A, and at least one arc from each node, a simple random walk is a Markov chain with transition probabilities: Mxy = 1/Nx if (x, y) is an arc and Nx ≥ 1 is the number of arcs starting from x, and Mxy = 0 otherwise. This means that the random walker is going with uniform probability in outgoing arcs. The graph of M coincides with G.
Example 5.10 (PageRank). Denote by E the set of Web pages. The graph of the Web is composed of E as set of nodes, and contains an arc (x, y) if there is a hyperlink from page x to page y. Assume that there is at least one hyperlink starting from any page. Otherwise, if a page x has no successor, then one add an arc from x to any page. Let P ∈ RE×E be the Markov transition matrix of a simple random walk on the Web graph: Pxy = 1/Nx if there is a hyperlink from x to y where Nx ≥ 1 is the number of hyperlinks from x, and Pxy = 0 otherwise. This matrix may not be irreducible. Google constructs the following Markov matrix:
M = γP + (1 − γ)1z
where
• 0 < γ < 1 is the damping factor: 1 − γ is the probability that a Web surfer is stopping clicking on following pages and is returning to the Google site (or any search engine) for instance or is going to any page randomly;
• z ∈ ∆E is a row probability vector with positive entries (zx > 0 for all x ∈ E, and z1 = 1) giving the probability for the Web surfer of going to any page when he is stopping clicking on following pages. This is the preference vector.
The PageRank computed by Google [EC1] is the invariant measure pM of M : that is a row probability vector (pM ∈ ∆E ) satisfying pM = pM M . Hence, the PageRank of a Web page x, PxM ,
85


corresponds to the expected frequency of visit of this page by the state of the Markov chain. Then the order on Web pages is defined as follows: a page x is better than page y if pxM ≥ pyM . In this context, a Website means a subset W of E. To “optimize” his Website, the owner of W would like to maximize a certain positive linear combination of the PageRank of W :
max
P ∈P
∑
x∈W
g(x)pM
x , (5.7)
where g ∈ R+W , and P is a set of possible Markov matrices. To solve this problem, we shall interpret
∑
x∈W
g(x)pM
x
as the value of a mean-payoff criteria, and in some particular cases the optimization as a Markov decision problem with mean-payoff criteria.
5.2.3 Perron-Frobenius theorem for irreducible matrices
The elements of this section can be found in [EC4]. Recall that we denote by ≤ the partial order on RE defined by entrywise inequalities: v ≤ w if vx ≤ wx for all x ∈ E; that 1 is the column vector of RE with all its entries equal to 1: 1x = 1 for all x ∈ E; and that we denote by ‖ · ‖∞ the sup-norm on vectors and the associated norm on matrices. Then, any Markov Matrix M over a finite set E satisfies ρ(M ) = ‖M ‖∞ = 1 (see Lemma 2.26). We also denote by < the relation on RE defined by entrywise strict inequalities: v < w if vx < wx for all x ∈ E.
Theorem 5.11 (Perron-Frobenius theorem). Let M be a matrix over E with nonnegative entries. Assume that M is irreducible. The following hold
1. ρ(M ) is an eigenvalue associated to an eigenvector v0 ∈ RE with positive entries (v0 > 0).
2. The eigenvalue ρ(M ) is (geomerically) simple, meaning that if M v = ρ(M )v, with v ∈ CE , then v = μv0 for some scalar μ ∈ C.
3. Any eigenvector v ≥ 0 is necessarily associated to the eigenvalue ρ(M ), and thus proportional to v0.
4. If M v ≤ μv and v > 0, then μ ≥ ρ(M ). (Collatz-Wielandt property)
5. If μv ≤ M v with μ ≥ 0, v ≥ 0 and v 6= 0, then μ ≤ ρ(M ).
6. If M v ≤ ρ(M )v with v ∈ RE (or M v ≥ ρ(M )v), then M v = ρ(M )v.
Then, any eigenvector v0 > 0 associated to the eigenvalue ρ(M ) is called a Perron vector.
Before giving the proof of Perron-Frobenius theorem, let us give some remark. Denote
ρ+(M ) = inf{μ > 0 | ∃v > 0, M v ≤ μv} = inf
v>0 max
x∈E
∑
y∈E Mxyvy
vx
. (5.8)
Note that this scalar is finite < +∞ by the second equivalent formula. Point 4 of Perron-Frobenius theorem 5.11 says that ρ+(M ) ≥ ρ(M ). Moreover, together with Point 1, it implies that the minimum is attained in the formula of ρ+(M ) and that ρ(M ) = ρ+(M ).
86


Proof of Perron-Frobenius theorem. The proof consists in several steps that are not necessarily in the same order as the points in the theorem. Let n be the cardinality of E and assume that E = {1, . . . , n}.
(1) If M v ≤ μv, with v ≥ 0 and v 6= 0, then v > 0 and μ > 0. Indeed, since M ≥ 0 is irreducible, then (I + M )n has positive entries. If M v ≤ μv, with v ≥ 0 and v 6= 0, then μ ≥ 0 (applying 0 ≤ [M v]x ≤ μvx to x such that vx 6= 0), and so (I + M )nv ≤ (1 + μ)nv (by using the monotonicity of M ). Therefore, v > 0 and so μ > 0.
(2) Point 4 or equivalently ρ+(M ) ≥ ρ(M ). Indeed, let v > 0 such that M v ≤ μv. Considering the diagonal matrix D such that Dxx = vx for all x ∈ E, we get that D1 = v, so M D1 ≤ μD1. Since v > 0, D is invertible and nonnegative, so D−1M D1 ≤ μ1. This implies that ‖D−1M D‖∞ ≤ μ, by the above formula for the sup-norm, and so ρ(M ) = ρ(D−1M D) ≤ μ, which shows Point 4.
(3) The infinum in (5.8) is a minimum. By definition of ρ+(M ), there exists μn > ρ+(M ) and vn > 0 such that limn→∞ μn = ρ+(M ) and μnvn ≥ M vn. One can choose vn such that vn · 1 = 1. Then, the sequence vn is bounded and thus admits a converging subsequence, that we also denote vn. Let v be the limit of this sequence. We have v ≥ 0 and v 6= 0 and M v ≤ ρ+(M )v. By Property (1) above, this implies that v > 0 and so ρ+(M ) is a minimum.
(4) If v is such that M v ≤ ρ+(M )v and v ≥ 0 then M v = ρ+(M )v. Denote w = ρ+(M )v − M v. We have w ≥ 0. Assume by contradiction that w 6= 0. Then, applying (I + M )n to w, we get that (I + M )nw > 0 and that (I + M )nw = ρ+(M )z − M z, with z = (I + M )nv > 0. Then, M z < ρ+(M )z and so there exists μ < ρ+(M ) such that M z ≤ μz, which contradicts the definition of ρ+(M ).
(5) Point 1. By (3), there exists v > 0 such that M v ≤ ρ+(M )v. By (4), this implies that M v = ρ+(M )v. Hence, ρ+(M ) ≤ ρ(M ) and since the reverse inequality holds by (2), we get that ρ+(M ) = ρ(M ) and that there exists v > 0 such that M v = ρ(M )v, which shows Point 1.
(6) Point 6. Let v ∈ RE such that M v ≤ ρ(M )v. Considering v0 as in Point 1. There exists μ ∈ R such that v ≥ μv0. Take μ as large as possible so that z = v − μv0 ≥ 0 and there exists an entry of z equal to zero. We have M z ≤ ρ(M )z and z ≥ 0 so by (1), if z 6= 0, this implies that z > 0 a contradiction. So z = 0, then v = μv0 and M v = ρ(M )v.
(7) Point 2. Let v ∈ CE \ {0} be such that M v = ρ(M )v. Taking the absolute value of the entries, we get that ρ(M )|v| = |M v| ≤ M |v|. Then, using Point 6, we deduce that ρ(M )|v| = M |v| and so |v| = μv0 for some constant μ ≥ 0. We also have (1 + ρ(M ))nv = (I + M )nv and so (1 + ρ(M ))n|v| = |(I + M )nv| ≤ (I + M )n|v| = (1 + ρ(M ))n|v|. Hence, |(I + M )nv| = (I + M )n|v|, in particular denoting αx = (I + M )1nx, we get | ∑n
x=1 αxvx| = ∑n
x=1 αx|vx|. Since all the αx are > 0, this shows that there exists β ∈ C, such that vx = β|vx|. So v = βμv0.
(8) Point 5. Let v ≥ 0 such that μv ≤ M v and v 6= 0. Then, by the monotonicity of M , we have μnv ≤ M nv.
Taking the sup-norm, we obtain that μ ≤ ‖M n‖1/n
∞ for all n ≥ 1. Taking the limit when n goes to infinity, we deduce that μ ≤ ρ(M ).
(9) Point 3. If v ≥ 0, v 6= 0 is such that M v = μv, then μ > 0 and v > 0, by (1). So by Points 4 and 5, we get
87


that μ = ρ(M ). Point 2 or 6 implies that v is proportional to v0.
Corollary 5.12. Let M be an irreducible Markov matrix. Then 1 is the unique eigenvector associated to the eigenvalue 1, up to a scalar factor, and there exists a unique invariant probability measure, that is a row vector m over E such that m ≥ 0, m1 = 1.
Proof. Since ρ(M ) = 1, M 1 = 1, and 1 > 0, Point 2 or 3 of Perron-Frobenius theorem 5.11 implies that 1 is a Perron vector and any eigenvector associated to the eigenvalue 1 is proportional to 1. Since the transpose matrix of M , denoted M T , is also nonnegative and ρ(M T ) = ρ(M ), then by Point 1 of Perron-Frobenius theorem 5.11, there exists a column vector w > 0 such that M T w = w. Choosing w such that w · 1 = 1, we get that m = wT is an invariant probability measure of M : m1 = 1 and mM = m. Moreover, by Point 2 or 3 of Perron-Frobenius theorem 5.11, if m and m′ are both invariant probability measures, then since m> and (m′)> are eigenvectors of M T associated to the eigenvalue 1, they are proportional. Then using the condition m1 = m′1 = 1, we get that they m = m′. So the invariant probability measure of M is unique.
Definition 5.13. We say that a Markov matrix M is primitive or acyclic if there exists k ≥ 1 such that M k is positive, meaning that all its entries are positive.
Proposition 5.14. Let M be a primitive Markov matrix over E. Then 1 is the unique eigenvalue with modulus 1.
Proof. Let λ be an eigenvalue with modulus 1 and v be an eigenvector associated to the eigenvalue λ. For any vector w in RE , we denote by |w| the vector with entries |wx|, x ∈ E. Then, |v| = |λv| = |M v| ≤ M |v|. By Point 6 of Theorem 5.11, this implies that |v| is proportional to the vector 1 and that |λv| = M |v|. Hence, |M kv| = |λkv| = M k|v| for all k ≥ 0. Let k be such that M k is positive, and let x ∈ E. Then, taking the equality |M kv| = M k|v| at x, we get that |
∑
y∈E [M k]xyvy| = ∑
y∈E [M k]xy|vy|. This implies that all the entries vy have same “sign”, that v is proportional to |v| and so to 1. Hence, λ = 1.
5.2.4 Linear algebra techniques and the multichain case
Proposition 5.15. Let M be a Markov matrix M over the state space E. Then all its eigenvalues of modulus 1 are semi-simple, meaning that they have no nilpotent.
Proof. Let M = QJQ−1 be the Jordan decomposition of M . Since ‖M ‖∞ = 1, and Jn = Q−1M nQ, we deduce that ‖Jn‖∞ ≤ C = ‖Q−1‖∞‖Q‖∞. If J′ is a block of J of size k corresponding to an eigenvalue λ of modulus 1, then J′ = λI + N where I is the identity matrix, and N is the nilpotent
matrix of order k (N k = 0) of the form N =
[ 0 1 0 ···
0 ... ...
··· 0 1
]
. Then, (J ′)n = ∑k−1
i=0
(n i
)λn−iN i (with
N 0 = I) and ‖J n‖∞ ≥ ‖(J ′)n‖∞ ≥ ( n
k−1
) − ∑k−2
i=0
(n i
). If k > 1, we get that ‖Jn‖∞ tends to +∞ when n goes to infinity, a contradiction. So k = 1 and the eigenvalue λ of M has no nilpotent.
Corollary 5.16. Let M be an irreducible Markov matrix. Then the eigenvalue 1 is algebraically simple.
Proof. By Corollary 5.12, there is a unique eigenvector associated to the eigenvalue 1, so 1 is geometrically simple. From Proposition 5.15, 1 has no nilpotent, so 1 is algebraically simple.
88


Proposition 5.17. Given a Markov matrix M over the state space E, and a Markov chain (Xn)n≥0 with transition matrix M and initial state X0 = x, we have
ζ±(x) = [P r]x, (5.9)
where P is the spectral projector of M for the eigenvalue 1, that is P is the unique matrix such that
P = P 2, Im P = ker(I − M ), ker P = Im(I − M ), and P = P M = M P .
This implies in particular, in the uncontrolled case, that vT
T converges towards ζ±.
Proof. Recall that ζ±(x) is the limsup or liminf of 1
T
∑T
k=0[M kr]x when T goes to infinity. Let
M = QJQ−1 be the Jordan decomposition of M . Then, 1
T
∑T
k=0 M k = Q
(1 T
∑T
k=0 J k)
Q−1. Let
J′ be a block of J corresponding to an eigenvalue λ. If |λ| < 1, then (J′)T tends to 0 when T goes to infinity, so does the Ces`aro mean 1
T
∑T
k=0 Jk. If |λ| = 1, then by Proposition 5.15, J′ is a block
of size 1 with entry λ. So 1
T
∑T
k=0 J k is a block of size 1 and entry 1
T
∑T
k=0 λk. If λ 6= 1, then
this entry is equal to (1 − λT +1)/(1 − λ)/T which tends to 0 when T goes to infinity. Otherwise,
the entry is equal to 1 + 1/T which tends to 1 when T goes to infinity. All together, we get that
1 T
∑T
k=0 Jk tends to the diagonal matrix D with ones at the places corresponding to the blocks of J associated to the eigenvalue 1 and 0 elsewhere. This diagonal matrix is exactly the spectral projector of J for the eigenvalue 1. Then, 1
T
∑T
k=0 M k tends to QDQ−1, which is the spectral projector P of M for the eigenvalue 1. Since 1 has no nilpotent, then P satisfies Im P = ker(I − M ) and ker P = Im(I − M ).
Corollary 5.18. Let M be an irreducible Markov matrix over the state space E, let m be its unique invariant probability measure and let (Xn)n≥0 be a Markov chain with transition matrix M and initial state X0 = x. We have
ζ±(x) = mr, ∀x ∈ E . (5.10)
Proof. Let M = QJQ−1 be the Jordan decomposition of M . Since the eigenvalue 1 of M is simple, the spectral projector P of M for the eigenvalue 1 is equal to QDQ−1, where D is the diagonal matrix with 1 in some position x and 0 elsewhere. So P is the product of the column x of Q and of the row x of Q−1, which are respectively equal to a column and row eigenvector of M with respect to the eigenvalue 1. These vectors are equal respectively to λ1 and μm, for some λ, μ ∈ C \ {0}. Since Q−1Q = I, and m1 = 1, we get that λμ = 1. So P = 1m and the result follows.
Theorem 5.19 (Decomposition of the spectral projector using final classes). Let M be a Markov matrix over the state space E. For each final class F ⊂ E, there exists a unique invariant probability measure m(F ) of M with support equal to F , and a unique fixed point v(F ) of M (that si satisfying M v(F ) = v(F )) such that [v(F )]x = 1 for x ∈ F and [v(F )]x = 0 for x ∈ F ′ and F ′ a final class 6= F . Moreover, the spectral projector of M for the eigenvalue 1 is equal to:
P=
∑
F final class
v(F )m(F ) .
Therefore, all invariant probability measures of M are convex combinations of the m(F ), and all fixed points of M are linear combinations of the v(F ). Given a Markov chain (Xn)n≥0 with transition
89


matrix M and initial state X0 = x, we have
ζ±(x) =
∑
F final class
(m(F )r)[v(F )]x . (5.11)
In particular
ζ±(x) = m(F )r ∀x ∈ F .
Proof. Let T be the set of transient states of M , that is the complementary of the union of final classes. Ordering the elements of E as x1, . . . , xn such that xi → xj for i < j, we get that the matrix M can be written in the following block form, where F1, . . . , Fm are the final classes:
M=

   
MT T MT F1 · · · MT Fm 0 MF1F1 0 0
0 0 ... 0
0 0 0 MFmFm

   
.
For each final class F ∈ {F1, . . . , Fm}, MF F is an irreducible Markov matrix on the set of states F , so that it has a unique invariant probability measure mF on F . For each final class, consider the row vector m(F ) = [0 · · · 0 mF 0 · · · 0], where mF corresponds to the restriction of m(F )
to the states in F . It is easy to see that m(F ) is an invariant probability measure of M with support equal to F . Conversely, if m is an invariant probability measure of M with support in F , then its restriction to F is an invariant probability measure of MF F so it is equal to mF and m = m(F ). The set T is equal to the union of the classes Ti, i = 1, . . . , k, of M that are transient (that is not final). Then, the eigenvalues of MT T are obtained by taking all the eigenvalues of the blocks MTiTi of MT T corresponding to the classes Ti. Since Ti is a transient class of M , we have that MTiTi1 ≤ 1 and MTiTi1 6= 1. Using the irreducibility of MTiTi and applying Point 6 of Perron-Frobenius Theorem 5.11, we deduce that ρ(MTiTi) < 1. Then, ρ(MT T ) = maxi=1,...,k ρ(MTiTi) < 1. Consider now the vector vF on T c = F1 ∪ · · · ∪ Fm with entries [vF ]x equal to 1 for x ∈ F and to 0 otherwise. Then, vF is a fixed point of the restriction MT cT c of M to T c. Consider
v(F ) =
[(IT T − MT T )−1MT F vF
vF
]
, which exists and has nonnegative entries, since ρ(MT T ) < 1. We
have that M v(F ) = v(F ) and that the entries [v(F )]x are equal to 1 for x ∈ F and to 0 for x ∈ F ′ with F ′ a final class 6= F . Conversely, if v satisfies these properties, then v = v(F ). This finish the proof of the first assertion of the theorem. In view of the block representation of M and of the properties that ρ(MT T ) < 1 and that the matrices MFjFj are irreducible and Markov, we get that 1 is an eigenvalue of (geometric and
algebraic) multiplicity m of M . We have already found m left eigenvectors of M , m(Fj), j = 1, . . . , m and m right eigenvectors of M , v(Fj), j = 1, . . . , m. Moreover, m(Fj)v(Fk) = δjk, so that one can construct a Jordan decomposition of M , M = QJQ−1, such that the m first diagonal entries of J are ones, the m first columns of Q are the eigenvectors v(Fj), j = 1, . . . , m, and the m first rows of Q−1 are the invariant probability measures m(Fj), j = 1, . . . , m. Then, the spectral projector is equal to P = QDQ−1, where D contains the Jordan blocks corresponding to the eigenvalue 1 of M , so is the diagonal matrix with its first m diagonal entries equal to 1 and remaining ones equal to 0. This leads to P = ∑
j=1,...,m v(Fj)m(Fj), that is the formula of P in the theorem. The last assertions follow from this formula.
90


The following result gives a characterization of ergodicity of a Markov chain.
Corollary 5.20. Let M be a Markov matrix over the state space E. Then, M has a unique final class F ⊂ E if and only if there exists a unique invariant probability measure m of M (that is the associated Markov chain is ergodic). In that case, F is the support of m and 1 is the unique fixed point of M . Moreover the spectral projector of M for the eigenvalue 1 is equal to P = 1m. Therefore, given a Markov chain (Xn)n≥0 with transition matrix M and initial state X0 = x, we have
ζ±(x) = mr ∀x ∈ E .
The previous results show that Cesa`ro means of M n converge. Using Proposition 5.14, one shows that, in the primitive case, the following stronger property holds.
Proposition 5.21. Let M be a primitive Markov matrix, and let ρ2 be the maximum of the modulus of the eigenvalues 6= 1, and let k be the maximal size of the Jordan block of such an eigenvalue. We have
M n = 1m + O(ρn
2 nk−1) .
The previous results, in particular Corollary 5.20 can be seen as a weak version of the ergodic theorem, stating a convergence in law. The following “strong” ergodic theorem states almost sure convergence. It can be proved using probabilistic techniques, and in particular using the law of large numbers, whereas it generalizes the law of large numbers. We state it without proof.
Theorem 5.22 (See []). Let M and m be as in Corollary 5.20. Given a Markov chain (Xn)n≥0 with transition matrix M , and any initial law p(0), we have
lim
T →∞
{
1
T
T
∑
k=0
r(Xk)
}
= mr, almost surely. (5.12)
5.2.5 The ergodic Kolmogorov equation
Consider first the case where M has a unique final (or recurrence) class. By Corollary 5.20, M has a unique invariant probability m, the support of m is the final class of M , and any right eigenvector of M associated to the eigenvalue 1 is a constant vector (M v = v =⇒ v = λ1 for some λ ∈ C). This implies that ζ±(x) is independent of the initial state x ∈ E, and equal to mr = ∑
x∈E mxr(x).
We can also characterize the value function ζ± as follows.
Proposition 5.23 (The ergodic Kolmogorov equation). Let E be a finite set, M be a Markov transition matrix on E and r ∈ RE . The following assertions hold:
1. Assume that there exists ρ ∈ R and v ∈ RE such that
ρ1 + v = r + M v . (5.13)
Then, ζ±(x) = ρ for all x ∈ E.
2. Assume that M has a unique final class, then there exists ρ ∈ R and v ∈ RE satisfying (5.13). Moreover, ρ ∈ R satisfying (5.13) is unique equal to mr, where m is the unique invariant probability measure of M and v satisfying (5.13) is unique up to an additive constant, meaning that if v, v′ satisfy (5.13), then v − v′ is a constant vector.
91


Proof. 1) We already know that ζ±(x) = [P r]x, where P is the spectral projector of M for the eigenvalue 1. Then, applying P to (5.13), we get that ρP 1 + P v = P r + P M v and since P = P M and P 1 = 1 (1 is a right eigenvector of M ), we obtain ρ1 = P r, so ζ±(x) = ρ for all x ∈ E. 2) If M has a unique final class, then P = 1m, where m is the unique invariant probability measure of M , and so ζ±(x) = [P r]x = mr, for all x ∈ E. Let ρ = mr, we get that m(r − ρ1) = 0 so r − ρ1 ∈ ker m = ker P = Im(I − M ). Hence, there exists v ∈ RS such that r − ρ1 = v − M v that is ρ and v satisfy (5.13). Conversely, if ρ and v satisfy (5.13), then by applying m to the equation, we get that ρ = mr so ρ is unique. Also if v, v′ satisfy (5.13), then (I − M )(v − v′) = 0 so v − v′ is a constant vector.
Example 5.24 (Pagerank (continued)). Let us come back to Example 5.10. Recall that to “optimize” his Website, the owner of W would like to maximize the criteria (5.7). Since M is irreducible (it has positive entries, since z > 0), we get that pM is unique pM > 0, and
∑
x∈W
g(x)pM
x = pM g = ρ
where g is extended by zero on W c, ρ = ζ±(x) for all x ∈ E with:
ζ (x) =



lim supT →∞
{1
TE
[
∑T
k=0 g(Xk) | X0 = x
]}
if = +
lim infT →∞
{1
TE
[
∑T
k=0 g(Xk) | X0 = x
]}
if = − .
and there exists v ∈ RE satisfying the ergodic Kolmogorov equation:
ρ1 + v = g + M v .
Moreover since M = γP + (1 − γ)1z, we have v = g + γP v and ρ = (1 − γ)zv.
Proposition 5.23 can be generalized as follows. The word “multichain” refers to the case of Markov chains with multiple final/recurrence classes.
Proposition 5.25 (The multichain Kolmogorov equation). Let E be a finite set, M be a Markov transition matrix on E and r ∈ RE , and let ζ = ζ± ∈ RE be defined as in (5.5) or (5.9). Then, there exists v ∈ RE such that (ζ, v) is solution to the following equations:
ζ + v = r + M v (5.14a)
ζ = M ζ (5.14b)
The solution ζ of (5.14) is unique and thus equal to P r, and v is unique up to the addition of any element of ker(I − M ) = ker(I − P ). In particular, there exists a unique (ζ, v) satisfying (5.14) together with the following condition:
mv = 0 for all invariant probability measures m of M . (5.15)
Note that (5.15) is equivalent to the condition that P v = 0.
92