Automated
Planning and Acting
Malik Ghallab
LAAS-CNRS, University of Toulouse, France
Dana Nau
University of Maryland, USA
Paolo Traverso
FBK ICT IRST, Trento, Italy
Manuscript of Automated Planning and Acting, c© 2016 by Malik Ghallab, Dana Nau and Paolo Traverso. Published by Cambridge University Press. Personal use only. Not for distribution. Do not post.
Manuscript, lecture slides, and other materials are at our web site.
July 5, 2016
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


To think is easy, to act is hard. The hardest is to act in accordance with your thinking.
Elective Affinities Johann Wolfgang von Goethe
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Contents
Table of Contents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iii List of Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix List of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi List of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiv Table of Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xv Foreword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xvi Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xviii
1 Introduction 1 1.1 Purpose and Motivations . . . . . . . . . . . . . . . . . . . . . . . . 1 1.1.1 First Intuition . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.1.2 Motivations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.1.3 Focus and Scope . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.2 Conceptual View of an Actor . . . . . . . . . . . . . . . . . . . . . . 6 1.2.1 A Simple Architecture . . . . . . . . . . . . . . . . . . . . . . 6 1.2.2 Hierarchical and Continual Online Deliberation . . . . . . . . 7 1.2.3 Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 1.3 Deliberation Models and Functions . . . . . . . . . . . . . . . . . . . 11 1.3.1 Descriptive and Operational Models of Actions . . . . . . . . 11 1.3.2 Description of States for Deliberation . . . . . . . . . . . . . 12 1.3.3 Planning Versus Acting . . . . . . . . . . . . . . . . . . . . . 14 1.3.4 Other Deliberation Functions . . . . . . . . . . . . . . . . . . 17 1.4 Illustrative Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 1.4.1 A Factotum Service Robot . . . . . . . . . . . . . . . . . . . 18 1.4.2 A Complex Operations Manager . . . . . . . . . . . . . . . . 20 1.5 Outline of the Book . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
2 Deliberation with Deterministic Models 24 2.1 State-Variable Representation . . . . . . . . . . . . . . . . . . . . . . 24 2.1.1 State-Transition Systems . . . . . . . . . . . . . . . . . . . . 25 2.1.2 Objects and State Variables . . . . . . . . . . . . . . . . . . . 27 2.1.3 Actions and Action Templates . . . . . . . . . . . . . . . . . 32 2.1.4 Plans and Planning Problems . . . . . . . . . . . . . . . . . . 35 2.2 Forward State-Space Search . . . . . . . . . . . . . . . . . . . . . . . 37
Authors’ manuscript. Published by Cambridge University Press. Do not distribute. iii


iv Contents
2.2.1 Breadth-First Search . . . . . . . . . . . . . . . . . . . . . . . 39 2.2.2 Depth-First Search . . . . . . . . . . . . . . . . . . . . . . . . 40 2.2.3 Hill Climbing . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 2.2.4 Uniform-Cost Search . . . . . . . . . . . . . . . . . . . . . . . 41 2.2.5 A* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 2.2.6 Depth-First Branch and Bound . . . . . . . . . . . . . . . . . 42 2.2.7 Greedy Best-First Search . . . . . . . . . . . . . . . . . . . . 43 2.2.8 Iterative Deepening . . . . . . . . . . . . . . . . . . . . . . . 43 2.2.9 Choosing a Forward-Search Algorithm . . . . . . . . . . . . . 44 2.3 Heuristic Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 2.3.1 Max-Cost and Additive Cost Heuristics . . . . . . . . . . . . 48 2.3.2 Delete-Relaxation Heuristics . . . . . . . . . . . . . . . . . . 51 2.3.3 Landmark Heuristics . . . . . . . . . . . . . . . . . . . . . . . 55 2.4 Backward Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 2.5 Plan-Space Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 2.5.1 Definitions and Algorithm . . . . . . . . . . . . . . . . . . . . 60 2.5.2 Planning Algorithm . . . . . . . . . . . . . . . . . . . . . . . 62 2.5.3 Search Heuristics . . . . . . . . . . . . . . . . . . . . . . . . . 64 2.6 Incorporating Planning into an Actor . . . . . . . . . . . . . . . . . . 68 2.6.1 Repeated Planning and Replanning . . . . . . . . . . . . . . 70 2.6.2 Online Planning . . . . . . . . . . . . . . . . . . . . . . . . . 72 2.7 Discussion and Historical Remarks . . . . . . . . . . . . . . . . . . . 74 2.7.1 Classical Domain Models . . . . . . . . . . . . . . . . . . . . 74 2.7.2 Generalized Domain Models . . . . . . . . . . . . . . . . . . . 77 2.7.3 Heuristic Search Algorithms . . . . . . . . . . . . . . . . . . . 78 2.7.4 Planning Graphs . . . . . . . . . . . . . . . . . . . . . . . . . 79 2.7.5 Converting Planning Problems into Other Problems . . . . . 80 2.7.6 Planning with Abstraction . . . . . . . . . . . . . . . . . . . . 80 2.7.7 HTN Planning . . . . . . . . . . . . . . . . . . . . . . . . . . 81 2.7.8 Temporal Logic . . . . . . . . . . . . . . . . . . . . . . . . . . 81 2.7.9 Domain-Independent Planning Heuristics . . . . . . . . . . . 83 2.7.10 Plan-Space Planning . . . . . . . . . . . . . . . . . . . . . . . 85 2.7.11 Online Planning . . . . . . . . . . . . . . . . . . . . . . . . . 86 2.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
3 Deliberation with Refinement Methods 94 3.1 Operational Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 3.1.1 Basic Ingredients . . . . . . . . . . . . . . . . . . . . . . . . . 96 3.1.2 Refinement Methods . . . . . . . . . . . . . . . . . . . . . . . 98 3.1.3 Illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 3.1.4 Updates of the Current State . . . . . . . . . . . . . . . . . . 103 3.2 A Refinement Acting Engine . . . . . . . . . . . . . . . . . . . . . . 104 3.2.1 Global View . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 3.2.2 Main Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . 106 3.2.3 Goals in RAE . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Contents v
3.2.4 Additional Features for RAE . . . . . . . . . . . . . . . . . . . 113 3.3 Refinement Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 3.3.1 Sequential Refinement Planning . . . . . . . . . . . . . . . . 116 3.3.2 Interleaved Plans . . . . . . . . . . . . . . . . . . . . . . . . . 121 3.4 Acting and Refinement Planning . . . . . . . . . . . . . . . . . . . . 127 3.4.1 Planning and Acting at Different Levels . . . . . . . . . . . . 127 3.4.2 Integrated Acting and Planning . . . . . . . . . . . . . . . . . 130 3.5 Discussion and Historical Remarks . . . . . . . . . . . . . . . . . . . 135 3.5.1 Refinement Acting . . . . . . . . . . . . . . . . . . . . . . . . 135 3.5.2 Refinement Planning . . . . . . . . . . . . . . . . . . . . . . . 137 3.5.3 Translating Among Multiple Domain Models . . . . . . . . . 139 3.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
4 Deliberation with Temporal Models 144 4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144 4.2 Temporal Representation . . . . . . . . . . . . . . . . . . . . . . . . 146 4.2.1 Assertions and Timelines . . . . . . . . . . . . . . . . . . . . 147 4.2.2 Actions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152 4.2.3 Methods and Tasks . . . . . . . . . . . . . . . . . . . . . . . . 154 4.2.4 Chronicles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158 4.3 Planning with Temporal Refinement Methods . . . . . . . . . . . . . 161 4.3.1 Temporal Planning Algorithm . . . . . . . . . . . . . . . . . . 162 4.3.2 Resolving Nonrefined Tasks . . . . . . . . . . . . . . . . . . . 163 4.3.3 Resolving Nonsupported Assertions . . . . . . . . . . . . . . . 164 4.3.4 Resolving Conflicting Assertions . . . . . . . . . . . . . . . . 164 4.3.5 Search Space . . . . . . . . . . . . . . . . . . . . . . . . . . . 165 4.3.6 Illustration . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166 4.3.7 Free Versus Task-Dependent Primitives . . . . . . . . . . . . 167 4.4 Constraint Management . . . . . . . . . . . . . . . . . . . . . . . . . 168 4.4.1 Consistency of Object Constraints . . . . . . . . . . . . . . . 169 4.4.2 Consistency of Temporal Constraints . . . . . . . . . . . . . . 170 4.4.3 Controllability of Temporal Constraints . . . . . . . . . . . . 173 4.5 Acting with Temporal Models . . . . . . . . . . . . . . . . . . . . . . 179 4.5.1 Acting with Atemporal Refinement Methods . . . . . . . . . 179 4.5.2 Acting with Temporal Refinement Methods . . . . . . . . . . 185 4.5.3 Acting and Planning with Temporal Methods . . . . . . . . . 188 4.6 Discussion and Historical Remarks . . . . . . . . . . . . . . . . . . . 189 4.6.1 Temporal Representation and Reasoning . . . . . . . . . . . . 189 4.6.2 Temporal Planning . . . . . . . . . . . . . . . . . . . . . . . . 190 4.6.3 Acting with Temporal Models . . . . . . . . . . . . . . . . . . 193 4.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


vi Contents
5 Deliberation with Nondeterministic Models 197 5.1 Introduction and Motivation . . . . . . . . . . . . . . . . . . . . . . . 198 5.2 The Planning Problem . . . . . . . . . . . . . . . . . . . . . . . . . . 200 5.2.1 Planning Domains . . . . . . . . . . . . . . . . . . . . . . . . 200 5.2.2 Plans as Policies . . . . . . . . . . . . . . . . . . . . . . . . . 202 5.2.3 Planning Problems and Solutions . . . . . . . . . . . . . . . . 204 5.3 And/Or Graph Search . . . . . . . . . . . . . . . . . . . . . . . . . . 210 5.3.1 Planning by Forward Search . . . . . . . . . . . . . . . . . . . 211 5.3.2 Planning by MinMax Search . . . . . . . . . . . . . . . . . . 213 5.4 Symbolic Model Checking Techniques . . . . . . . . . . . . . . . . . 216 5.4.1 Symbolic Representation of Sets of States . . . . . . . . . . . 218 5.4.2 Symbolic Representation of Actions and Transitions . . . . . 219 5.4.3 Planning for Safe Solutions . . . . . . . . . . . . . . . . . . . 221 5.4.4 Planning for Safe Acyclic Solutions . . . . . . . . . . . . . . . 224 5.4.5 BDD-based Representation . . . . . . . . . . . . . . . . . . . 226 5.5 Determinization Techniques . . . . . . . . . . . . . . . . . . . . . . . 227 5.5.1 Guided Planning for Safe Solutions . . . . . . . . . . . . . . . 228 5.5.2 Planning for Safe Solutions by Determinization . . . . . . . . 230 5.6 Online approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231 5.6.1 Lookahead . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233 5.6.2 Lookahead by Determinization . . . . . . . . . . . . . . . . . 235 5.6.3 Lookahead with a Bounded Number of Steps . . . . . . . . . 236 5.7 Refinement Methods with Nondeterministic Models . . . . . . . . . . 237 5.7.1 Tasks in Refinement Methods . . . . . . . . . . . . . . . . . . 238 5.7.2 Context-Dependent Plans . . . . . . . . . . . . . . . . . . . . 239 5.7.3 Search Automata . . . . . . . . . . . . . . . . . . . . . . . . . 242 5.7.4 Planning Based on Search Automata . . . . . . . . . . . . . . 247 5.8 Acting with Input/Output Automata . . . . . . . . . . . . . . . . . . 252 5.8.1 Input/Output Automata . . . . . . . . . . . . . . . . . . . . 253 5.8.2 Control Automata . . . . . . . . . . . . . . . . . . . . . . . . 255 5.8.3 Automated Synthesis of Control Automata . . . . . . . . . . 257 5.8.4 Synthesis of Control Automata by Planning . . . . . . . . . . 260 5.8.5 Acting by Interacting with Multiple Automata . . . . . . . . 263 5.9 Discussion and Historical Remarks . . . . . . . . . . . . . . . . . . . 267 5.9.1 Comparison among Different Approachess . . . . . . . . . . . 267 5.9.2 Historical Remarks . . . . . . . . . . . . . . . . . . . . . . . . 268 5.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272
6 Deliberation with Probabilistic Models 277 6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277 6.2 Stochastic Shortest-Path Problems . . . . . . . . . . . . . . . . . . . 278 6.2.1 Main Definitions . . . . . . . . . . . . . . . . . . . . . . . . . 279 6.2.2 Safe and Unsafe Policies . . . . . . . . . . . . . . . . . . . . . 280 6.2.3 Optimality Principle of Dynamic Programming . . . . . . . . 284 6.2.4 Policy Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . 286
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Contents vii
6.2.5 Value Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . 288 6.3 Heuristic Search Algorithms . . . . . . . . . . . . . . . . . . . . . . . 296 6.3.1 A General Heuristic Search Schema . . . . . . . . . . . . . . 296 6.3.2 Best-First Search . . . . . . . . . . . . . . . . . . . . . . . . . 300 6.3.3 Depth-First Search . . . . . . . . . . . . . . . . . . . . . . . . 305 6.3.4 Iterative Deepening Search . . . . . . . . . . . . . . . . . . . 306 6.3.5 Heuristics and Search Control . . . . . . . . . . . . . . . . . . 309 6.4 Online Probabilistic Approaches . . . . . . . . . . . . . . . . . . . . 311 6.4.1 Lookahead Methods . . . . . . . . . . . . . . . . . . . . . . . 312 6.4.2 Lookahead with Deterministic Search . . . . . . . . . . . . . 314 6.4.3 Stochastic Simulation Techniques . . . . . . . . . . . . . . . . 315 6.4.4 Sparse Sampling and Monte Carlo Search . . . . . . . . . . . 318 6.5 Acting with Probabilistic Models . . . . . . . . . . . . . . . . . . . . 323 6.5.1 Using Deterministic Methods to Refine Policy Steps . . . . . 323 6.5.2 Acting with Probabilistic Methods . . . . . . . . . . . . . . . 324 6.6 Representations of Probabilistic Models . . . . . . . . . . . . . . . . 326 6.6.1 Probabilistic Precondition-Effect Operators . . . . . . . . . . 326 6.6.2 Dynamic Bayesian Networks . . . . . . . . . . . . . . . . . . 328 6.7 Domain Modeling and Practical Issues . . . . . . . . . . . . . . . . . 333 6.7.1 Sources of Nondeterminism . . . . . . . . . . . . . . . . . . . 333 6.7.2 Sparse Probabilistic Domains . . . . . . . . . . . . . . . . . . 334 6.7.3 Goals and Objectives . . . . . . . . . . . . . . . . . . . . . . . 335 6.7.4 Domain Decomposition and Hierarchization . . . . . . . . . . 337 6.7.5 Probabilistic Versus Nondeterministic Approaches . . . . . . 339 6.7.6 Practical Considerations . . . . . . . . . . . . . . . . . . . . . 341 6.8 Discussion and Historical Remarks . . . . . . . . . . . . . . . . . . . 342 6.8.1 Foundations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342 6.8.2 SSP Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343 6.8.3 Partially Observable Models . . . . . . . . . . . . . . . . . . . 343 6.8.4 Other Extended MDP Models . . . . . . . . . . . . . . . . . . 345 6.8.5 Algorithms and Heuristics . . . . . . . . . . . . . . . . . . . . 346 6.8.6 Factored and Hierarchical MDPs . . . . . . . . . . . . . . . . 348 6.9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349
7 Other Deliberation Functions 352 7.1 Perceiving . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353 7.1.1 Planning and Acting with Information Gathering . . . . . . . 353 7.1.2 Planning to Perceive . . . . . . . . . . . . . . . . . . . . . . . 354 7.1.3 Symbol Anchoring . . . . . . . . . . . . . . . . . . . . . . . . 355 7.1.4 Event and Situation Recognition . . . . . . . . . . . . . . . . 356 7.2 Monitoring and Goal Reasoning . . . . . . . . . . . . . . . . . . . . . 358 7.2.1 Platform Monitoring . . . . . . . . . . . . . . . . . . . . . . . 358 7.2.2 Action and Plan Monitoring . . . . . . . . . . . . . . . . . . . 360 7.2.3 Goal Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . 364 7.3 Learning and Model Acquisition . . . . . . . . . . . . . . . . . . . . 366
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


viii Contents
7.3.1 Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . 366 7.3.2 Learning from Demonstrations, Advice and Partial Programs 371 7.3.3 Acquiring Descriptive Models and Heuristics . . . . . . . . . 373 7.4 Hybrid Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374 7.4.1 Hybrid Automata . . . . . . . . . . . . . . . . . . . . . . . . 375 7.4.2 Input/Output Hybrid Automata . . . . . . . . . . . . . . . . 378 7.4.3 Planning as Model Checking . . . . . . . . . . . . . . . . . . 380 7.4.4 Flow Tubes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383 7.4.5 Discretization Techniques . . . . . . . . . . . . . . . . . . . . 386 7.5 Ontologies for Planning and Acting . . . . . . . . . . . . . . . . . . . 388 7.5.1 Ontologies and Description Logic . . . . . . . . . . . . . . . . 388 7.5.2 Ontologies and Planning . . . . . . . . . . . . . . . . . . . . . 389 7.5.3 Planning and Acting Based on Description Logic . . . . . . . 390 7.5.4 Semantic Mapping for Hierarchical Representations . . . . . . 392
8 Concluding Remarks 394
A Search Algorithms 396 A.1 Nondeterministic State-Space Search . . . . . . . . . . . . . . . . . . 396 A.2 And/Or Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 400
B Strongly Connected Components of a Graph 401
Bibliography 403
Index 447
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


List of Algorithms
2.1 Forward-search planning schema. . . . . . . . . . . . . . . . . . . . . 37 2.2 Deterministic-Search, a deterministic version of Forward-search. . . . . 38 2.3 HFF, an algorithm to compute the Fast-Forward heuristic. . . . . . . 53 2.4 Backward-search planning schema. . . . . . . . . . . . . . . . . . . . . 57 2.5 PSP, plan-space planning. . . . . . . . . . . . . . . . . . . . . . . . . 62 2.6 Run-Lookahead replans before every action. . . . . . . . . . . . . . . 70 2.7 Run-Lazy-Lookahead replans only when necessary. . . . . . . . . . . . 70 2.8 Run-Concurrent-Lookahead does acting and replanning concurrently. . 70
3.1 Main procedure of the Refinement Acting Engine (RAE). . . . . . . . 107 3.2 RAE: progressing a refinement stack. . . . . . . . . . . . . . . . . . . 108 3.3 RAE: trying alternative methods for a task. . . . . . . . . . . . . . . 109 3.4 SeRPE, the Sequential Refinement Planning Engine. . . . . . . . . . 117 3.5 IRT, a refinement-planning algorithm that can do interleaving. . . . 124 3.6 Subroutine of IRT to simulate the next step in a method. . . . . . . 125 3.7 Replanning before every action. . . . . . . . . . . . . . . . . . . . . . 128 3.8 Replanning only when necessary. . . . . . . . . . . . . . . . . . . . . 128 3.9 Replanning concurrently with acting. . . . . . . . . . . . . . . . . . . 128 3.10 Main procedure of REAP. . . . . . . . . . . . . . . . . . . . . . . . . 131 3.11 REAP’s procedure for progressing a refinement stack. . . . . . . . . 132 3.12 REAP’s version of RAE’s Retry subroutine. . . . . . . . . . . . . . . 133
4.1 TemPlan, a temporal planner . . . . . . . . . . . . . . . . . . . . . . 162 4.2 Path consistency algorithm for simple constraint networks . . . . . . 171 4.3 A dispatching function for eRAE. . . . . . . . . . . . . . . . . . . . . 183
5.1 Procedure for performing the actions of a policy. . . . . . . . . . . . 203 5.2 Planning for solutions by forward search. . . . . . . . . . . . . . . . 211 5.3 Planning for safe solutions by forward search. . . . . . . . . . . . . . 212 5.4 Planning for safe acyclic solutions by forward search. . . . . . . . . . 213 5.5 Planning for safe acyclic solutions by MinMax Search. . . . . . . . . 214 5.6 The policy with minimal cost over actions. . . . . . . . . . . . . . . . 215 5.7 Planning for safe solutions by symbolic model checking. . . . . . . . 222
Authors’ manuscript. Published by Cambridge University Press. Do not distribute. ix


x List of Algorithms
5.8 PruneUnconnected: Removing unconnected states. . . . . . . . . . . . 222 5.9 RemoveNonProgress: Removing states/actions . . . . . . . . . . . . . 224 5.10 Planning for acyclic solutions by symbolic model checking . . . . . . 225 5.11 Guided planning for safe solutions. . . . . . . . . . . . . . . . . . . . 229 5.12 Planning for safe solutions by determinization. . . . . . . . . . . . . 231 5.13 Transformation of a sequential plan into a corresponding policy. . . . 232 5.14 Interleaving planning and execution by lookahead. . . . . . . . . . . 234 5.15 Online determinization planning and acting algorithm. . . . . . . . . 236 5.16 MinMax Learning Real Time A*. . . . . . . . . . . . . . . . . . . . . 237 5.17 Planning based on search automata . . . . . . . . . . . . . . . . . . . 248 5.18 Associating states to contexts. . . . . . . . . . . . . . . . . . . . . . . 249
6.1 A simple procedure to run a policy. . . . . . . . . . . . . . . . . . . . 282 6.2 Policy Iteration algorithm. . . . . . . . . . . . . . . . . . . . . . . . . 287 6.3 Synchronous Value Iteration algorithm . . . . . . . . . . . . . . . . . 288 6.4 Asynchronous Value Iteration algorithm. . . . . . . . . . . . . . . . . 289 6.5 Bellman update procedure. . . . . . . . . . . . . . . . . . . . . . . . 289 6.6 A guaranteed approximation procedure for VI. . . . . . . . . . . . . . 293 6.7 Find&Revise schema. . . . . . . . . . . . . . . . . . . . . . . . . . . . 298 6.8 Best-first search algorithm AO∗ and LAO∗. . . . . . . . . . . . . . . . 300 6.9 Bottom-up update for AO∗. . . . . . . . . . . . . . . . . . . . . . . . 301 6.10 A “VI-like” update for LAO∗. . . . . . . . . . . . . . . . . . . . . . . 304 6.11 ILAO∗, a variant of LAO∗ . . . . . . . . . . . . . . . . . . . . . . . . 305 6.12 A heuristic depth-first search algorithm for SSPs. . . . . . . . . . . . 306 6.13 Solved-SCC: labelling solved strongly connected components . . . . . 307 6.14 Algorithm LDFSa. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308 6.15 Acting with the guidance of lookahead search. . . . . . . . . . . . . . 312 6.16 A determinization planning algorithm. . . . . . . . . . . . . . . . . . 315 6.17 Algorithm LRTDP. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316 6.18 Procedure to check and label solve states for LRTDP. . . . . . . . . . 317 6.19 Sampling lookahead Tree to Estimate. . . . . . . . . . . . . . . . . . 320 6.20 A recursive UCT procedure. . . . . . . . . . . . . . . . . . . . . . . . 322 6.21 Compression framework for sparse probabilistic domains. . . . . . . 334
7.1 Monitoring of the progression of a plan . . . . . . . . . . . . . . . . . 361 7.2 Q-learning, a reinforcement learning algorithm. . . . . . . . . . . . . 368
A.1 Equivalent iterative and recursive versions of nondeterministic search. 397 A.2 A deterministic counterpart to Nondeterministic-Search. . . . . . . . . 398 A.3 A generic nondeterministic And/Or search algorithm. . . . . . . . . 400
B.1 Tarjan’s algorithm for finding strongly connected components. . . . . 402
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


List of Figures
1.1 Conceptual view of an actor. . . . . . . . . . . . . . . . . . . . . . . 7 1.2 Multiple levels of abstraction in deliberative acting. . . . . . . . . . . 8 1.3 Planning as a combination of prediction and search. . . . . . . . . . 15 1.4 Receding horizon scheme for planning and acting. . . . . . . . . . . . 17 1.5 Deliberation components for a Harbor Operations Manager. . . . . . 21
2.1 A two-dimensional network of locations connected by roads. . . . . . 26 2.2 Geometric model of a workpiece. . . . . . . . . . . . . . . . . . . . . 27 2.3 A few states and transitions in a simple state-transition system. . . . 29 2.4 Initial state and goal for Example 2.21. . . . . . . . . . . . . . . . . 47 2.5 Computation of hmax(s1, g) and hmax(s2, g). . . . . . . . . . . . . . . 49 2.6 Computation of hadd(s1, g) and hadd(s2, g). . . . . . . . . . . . . . . 50 2.7 Computation of HFF(Σ, s1, g) = 2. . . . . . . . . . . . . . . . . . . . 54 2.8 Computation of HFF(Σ, s2, g) = 3. . . . . . . . . . . . . . . . . . . . 55 2.9 Initial state and goal for Example 2.32. . . . . . . . . . . . . . . . . 63 2.10 The initial partial plan contains dummy actions. . . . . . . . . . . . 64 2.11 Resolving ag’s open-goal flaws. . . . . . . . . . . . . . . . . . . . . . 65 2.12 Resolving a1’s open-goal flaws. . . . . . . . . . . . . . . . . . . . . . 65 2.13 Resolving a2’s open-goal flaws. . . . . . . . . . . . . . . . . . . . . . 66 2.14 Resolving a3’s open-goal flaws. . . . . . . . . . . . . . . . . . . . . . 66 2.15 Action templates for Example 2.33. . . . . . . . . . . . . . . . . . . . 68 2.16 Planning problem for Exercise 2.7. . . . . . . . . . . . . . . . . . . . 89 2.17 Partial plan for swapping the values of two variables. . . . . . . . . . 90 2.18 Blocks world planning domain and a planning problem. . . . . . . . 91 2.19 Partial plan for Exercise 2.12. . . . . . . . . . . . . . . . . . . . . . . 92
3.1 A simple architecture for a refinement acting engine. . . . . . . . . . 105 3.2 A simple environment . . . . . . . . . . . . . . . . . . . . . . . . . . 109 3.3 Refinement tree of tasks, methods, and commands. . . . . . . . . . . 110 3.4 The state s0 in Equation 3.1. . . . . . . . . . . . . . . . . . . . . . . 119 3.5 Refinement trees for two solution plans. . . . . . . . . . . . . . . . . 120 3.6 The initial state s0 from Example 2.5. . . . . . . . . . . . . . . . . . 121 3.7 An interleaved refinement tree corresponding to π5. . . . . . . . . . . 123
Authors’ manuscript. Published by Cambridge University Press. Do not distribute. xi


xii List of Figures
3.8 Translation of a command . . . . . . . . . . . . . . . . . . . . . . . . 126
4.1 State-oriented versus time-oriented views. . . . . . . . . . . . . . . . 146 4.2 A timeline for the state variable loc(r1). . . . . . . . . . . . . . . . . 149 4.3 Assertions, actions, and subtasks of a refinement method. . . . . . . 156 4.4 Temporally qualified actions of two robots, r1 and r2. . . . . . . . . . 160 4.5 A planning problem involving two robots, r1 and r2. . . . . . . . . . 166 4.6 A simple temporal network. . . . . . . . . . . . . . . . . . . . . . . . 171 4.7 A consistent STN. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172 4.8 An uncontrollable network. . . . . . . . . . . . . . . . . . . . . . . . 174 4.9 A dynamically controllable STNU. . . . . . . . . . . . . . . . . . . . 176 4.10 Basic constraints for dynamic controllability. . . . . . . . . . . . . . 177 4.11 Part of a temporal plan given to eRAE for execution. . . . . . . . . . 184
5.1 A simple nondeterministic planning domain model. . . . . . . . . . . 201 5.2 Reachability graph for policy π1. . . . . . . . . . . . . . . . . . . . . 205 5.3 Reachability graph for policy π2. . . . . . . . . . . . . . . . . . . . . 206 5.4 Reachability graph for policy π3. . . . . . . . . . . . . . . . . . . . . 207 5.5 Different kinds of solutions: class diagram. . . . . . . . . . . . . . . 209 5.6 BDD for pos(item) = gate1 ∨ pos(item) = gate2 . . . . . . . . . . . . 217 5.7 Two BDDs for the formula (a1 ↔ b1) ∧ (a2 ↔ b2) ∧ (a3 ↔ b3). . . . . 228 5.8 Offline versus run-time search spaces. . . . . . . . . . . . . . . . . . . 232 5.9 An example of nondeterministic planning domain . . . . . . . . . . . 240 5.10 A context-dependent plan. . . . . . . . . . . . . . . . . . . . . . . . . 241 5.11 Search automaton for safe acyclic solutions. . . . . . . . . . . . . . . 244 5.12 Search automaton for safe cyclic solutions. . . . . . . . . . . . . . . . 245 5.13 Search automaton for primitive actions. . . . . . . . . . . . . . . . . 245 5.14 Search automaton for the sequence t1; t2. . . . . . . . . . . . . . . . 246 5.15 Search automaton for conditional task if p then t1 else t2. . . . . . . 247 5.16 Search automaton for loop task while p do t1. . . . . . . . . . . . . . 247 5.17 Search automaton for test point test p. . . . . . . . . . . . . . . . . . 248 5.18 Search automaton for failure-handling task if t1 fails then t2. . . . . 248 5.19 Nondeterministic model for an open-door method. . . . . . . . . . . 252 5.20 Input/output automaton for an open-door method. . . . . . . . . . . 253 5.21 Input/Output (I/O) automaton to control the robot I/O automaton. 256 5.22 Input/output automaton for approaching a door. . . . . . . . . . . . 258 5.23 An abstracted system . . . . . . . . . . . . . . . . . . . . . . . . . . 261 5.24 A planning domain generated from an I/O automaton . . . . . . . . 263 5.25 Robot and door interacting input/output automata. . . . . . . . . . 264 5.26 A controller for the robot and door input/output automata. . . . . . 265 5.27 A nondeterministic state-transition system. . . . . . . . . . . . . . . 273 5.28 A descriptive model of a nondeterministic planning domain . . . . . 274
6.1 Part of the state space for the problem in Example 6.4. . . . . . . . 281 6.2 A safe solution for Example 6.4 and its Graph(s0, π). . . . . . . . . . 281
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


List of Figures xiii
6.3 Partition of the set of states with respect to solutions. . . . . . . . . 284 6.4 A very simple domain. . . . . . . . . . . . . . . . . . . . . . . . . . . 290 6.5 Connectivity graph of a simple environment. . . . . . . . . . . . . . 294 6.6 Example of an acyclic search space. . . . . . . . . . . . . . . . . . . . 302 6.7 (a) Single Monte Carlo rollout; (b) Multiple rollout. . . . . . . . . . 319 6.8 Sparse sampling tree of procedure SLATE. . . . . . . . . . . . . . . . 321 6.9 Probabilistic model for an open-door method. . . . . . . . . . . . . . 325 6.10 A DBN for action take in the domain PAMq. . . . . . . . . . . . . . 329 6.11 Conditional probability trees for the ctrs state variables. . . . . . . . 330 6.12 DBN for action take in domain PAMo. . . . . . . . . . . . . . . . . . 331 6.13 Conditional probability table and tree. . . . . . . . . . . . . . . . . . 332 6.14 Comparing features of probabilistic and nondeterministic models. . . 340 6.15 An SSP problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350
7.1 Hybrid automaton for a thermostat. . . . . . . . . . . . . . . . . . . 376 7.2 A charging station for two plants. . . . . . . . . . . . . . . . . . . . . 378 7.3 A linear flow tube. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383 7.4 A flow tube. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385 7.5 Planning with flow tubes. . . . . . . . . . . . . . . . . . . . . . . . . 385 7.6 Open door: semantic mapping of state variables and actions. . . . . 393
A.1 Search tree for Nondeterministic-Search. Each branch represents one of the possible refinements. . . . . . . . . . . . . . . . . . . . . . . . 397 A.2 And/Or search tree. . . . . . . . . . . . . . . . . . . . . . . . . . . . 399
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


List of Tables
4.1 Constraint propagation rules for dynamic controllability. . . . . . . . 178
5.1 Solutions: different terminologies in the literature. . . . . . . . . . . 210
6.1 V (l) after the first three and last three iterations of VI. . . . . . . . 295 6.2 Iterations of AO∗ on the example of Figure 6.6. . . . . . . . . . . . . 303
Authors’ manuscript. Published by Cambridge University Press. Do not distribute. xiv


Table of Notation
Notation Meaning
a, A action, set of actions α, A action template, set of action templates cost(a), cost(s, a) cost of a, cost of a in state s cost(π), cost(s, π) cost of π, cost of π in state s Dom(f ), Dom(π) domain of a function or plan eff(a) effects of action a g, Sg goal conditions, goal states γ(s, a) progression, i.e., predicted result of applying a in s γ−1(g, a) regression, i.e., conditions needed for a to produce s ̂γ(s0, π) {all states reachable from s0 using π}, if π is a policy ̂γ(s0, π) sequence of states π produces from s0, if π is a plan h heuristic function m, M refinement method, set of methods head(a) a’s name and argument list Pr(s′|s, a) probability of transition to s′ if a is used in s P , P planning problem π, Π plan or policy, set of plans or policies pre(a) preconditions of action a Range(v) range of a function or variable s, S predicted state, set of states for the planner ξ, Ξ actual state, set of states for the actor s0, S0 initial state, set of initial states Σ = (S, A, γ) planning domain Σ = (B, R, X, I, A) state-variable representation of a planning domain: a.π, π.a, π.π′ concatenation of actions and plans (a, b, . . . , u) tuple 〈a, b, . . . , u〉 sequence
s |= g, s 6|= g s satisfies g, s does not satisfy g
Authors’ manuscript. Published by Cambridge University Press. Do not distribute. xv


Foreword
Over ten years ago, Malik Ghallab, Dana Nau, and Paolo Traverso gave us the first—and to date only—comprehensive textbook dedicated to the field of Automated Planning, providing a much needed resource for students, researchers and practitioners. Since then, this rich field has continued to evolve rapidly. There is now a unified understanding of what once seemed disparate work on classical planning. Models and methods to deal with time, resources, continuous change, multiple agents, and uncertainty have substantially matured. Cross-fertilization with other fields such as software verification, optimization, machine learning, and robotics has become the rule rather than the exception. A phenomenal range of applications could soon be within reach—given the right future emphasis for the field. Today, the authors are back with a new book, Automated Planning and Acting. As the title indicates, this is not a mere second edition of the older book. In line with the authors’ analysis of where the future emphasis should lie for the field to realize its full impact, the book covers deliberative computational techniques for both planning and acting, that is for deciding which actions to perform and also how to perform them. Automated Planning and Acting is more than a graduate textbook or a reference book. Not only do the authors outstandingly discharge their duties of educating the reader about the basics and much of the recent progress in the field, but they also propose a new framework from which the community can start to intensify research on deliberative acting and its integration with planning. These aims are reflected in the book’s content. The authors put the integration of planning and acting at the forefront by dedicating an entire chapter to a unified hierarchical model and refinement procedures that suit the needs of both planning and acting functions. Each chapter devoted to a particular class of representations also includes significant material on the integration of planning and acting using these representations. Overall, the book is more focused than its predecessor, and explores in even greater depth models and approaches motivated by the needs of planning and acting in
Authors’ manuscript. Published by Cambridge University Press. Do not distribute. xvi


Foreword xvii
the real world, such as handling time and uncertainty. At the same time, the authors successfully balance breadth and depth by providing an elegant, concise synthesis of a larger body of work than in their earlier text. There is no doubt that Automated Planning and Acting will be the text I require my students to read when they first start, and the goto book on my shelf for my own reference. As a timely source of motivation for gamechanging research on the integration of planning and acting, it will also help shape the field for the next decade.
Sylvie Thi ́ebaux The Australian National University
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Preface
This book is about methods and techniques that a computational agent can use for deliberative planning and acting, that is, for deciding both which actions to perform and how to perform them, to achieve some objective. The study of deliberation has several scientific and engineering motivations. Understanding deliberation is an objective for most cognitive sciences. In artificial intelligence research, this is done by modeling deliberation through computational approaches to enable it and to allow it to be explained. Furthermore, the investigated capabilities are better understood by mapping concepts and theories into designed systems and experiments to test empirically, measure, and qualify the proposed models. The engineering motivation for studying deliberation is to build systems that exhibit deliberation capabilities and develop technologies that address socially useful needs. A technological system needs deliberation capabilities if it must autonomously perform a set of tasks that are too diverse – or must be done in environments that are too diverse – to engineer those tasks into innate behaviors. Autonomy and diversity of tasks and environments is a critical feature in many applications, including robotics (e.g., service and personal robots; rescue and exploration robots; autonomous space stations, satellites, or vehicles), complex simulation systems (e.g., tutoring, training or entertainment), or complex infrastructure management (e.g., industrial or energy plants, transportation networks, urban facilities).
Motivation and Coverage
The coverage of this book derives from the view we advocated in our previous work [230], which we now briefly summarize. Automated planning is a rich technical field, which benefits from the work of an active and growing research community. Some areas in this field are extensively explored and correspond to a number of already mature
Authors’ manuscript. Published by Cambridge University Press. Do not distribute. xviii


Preface xix
techniques. However, there are other areas in which further investigation is critically needed if automated planning is to have a wider impact on a broader set of applications. One of the most important such areas, in our view, is the integration of planning and acting. This book covers several different kinds of models and approaches – deterministic, hierarchical, temporal, nondeterministic and probabilistic – and for each of them, we discuss not only the techniques themselves but also how to use them in the integration of planning and acting. The published literature on automated planning is large, and it is not feasible to cover all of it in detail in a single book. Hence our choice of what to cover was motivated by putting the integration of planning and acting at the forefront. The bulk of research on automated planning is focused on a restricted form called classical planning, an understanding of which is prerequisite introductory material, and we cover it in part of Chapter 2. But we have devoted large parts of the book to extended classes of automated planning and acting that relax the various restrictions required by classical planning. There are several other kind of deliberation functions, such as monitoring, reasoning about one’s goals, reasoning about sensing and informationgathering actions, and learning and otherwise acquiring deliberation models. Although these are not our focus, we cover them briefly in Chapter 7. The technical material in this book is illustrated with examples inspired from concrete applications. However, most of the technical material is theoretical. Case studies and application-oriented work would certainly enrich the integration of planning and acting view developed in here. We plan to devote a forthcoming volume to automated planning and acting applications.
Using This Book
This work started as a textbook project, to update our previous textbook on automated planning [231]. Our analysis of the state of the art led us quickly to embrace the objective of covering planning and acting and their integration and, consequently, to face two obstacles:
• The first problem was how to cover a domain whose scope is not easily amenable to a sharp definition and that requires integrating conceptually heterogenous models and approaches. In contrast to our previous book, which was focused on planning, this one proved harder to converge into a reasonably united perspective.
• The second problem was how to combine a textbook approach, that
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


xx Preface
is, a coherent synthesis of the state of the art, with the development of new material. Most of this new material is presented in comprehensive detail (e.g., in Chapter 3) consistent with a textbook use. In a few parts (e.g., Section 4.5.3), this new material is in preliminary form and serves as an invitation for further research.
This book can be used as a graduate-level textbook and as an information source for scientists and professionals in the field. We assume the reader to be familiar with the basic concepts of algorithms and data structures at the level that one might get in an undergraduate-level computer science curriculum. Prior knowledge of heuristic search techniques would also be helpful, but is not strictly necessary because the appendices provide overviews of needed tools. A complete set of lecture slides for this book and other auxiliary materials are available online.
Acknowledgments
We are thankful to several friends and colleagues who gave us very valuable feedback on parts of this book. Among these are Hector Geffner, Robert Goldman, Patrik Haslum, J ̈org Hoffmann, Joachim Hertzberg, Felix Ingrand, Ugur Kuter, Marco Pistore, Mak Roberts, Vikas Shivashankar, Sylvie Thi ́ebaux, and Qiang Yang. We also wish to acknowledge the support of our respective organizations, which provided the support and facilities that helped to make this work possible: LAAS-CNRS in Toulouse, France; the University of Maryland in College Park, Maryland; and FBK ICT-IRST in Trento, Italy. Dana Nau thanks ONR for their support of his planning work, and the students who took courses from rough drafts of this book. Finally, we wish to acknowledge the support of our families, who remained patient during a project that consumed much more of our time and attention than we had originally anticipated.
About the Authors
Malik Ghallab is Directeur de Recherche at CNRS, the National Center for Scientific Research, France. For most of his career he has been with LAASCNRS, University of Toulouse. His research activity is mainly focused on planning, acting and learning in robotics and AI. He has co-authored several books and more than two hundred scientific publications. He has been head
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Preface xxi
of a French research program in Robotics, director of LAAS-CNRS and CEO for Science and Technology of INRIA. He is an ECCAI Fellow.
Dana Nau is a Professor at the University of Maryland, in the Department of Computer Science and the Institute for Systems Research. He has more than 300 refereed technical publications. Some of his best-known research contributions include the discovery of pathological game trees, the strategic planning algorithm that Bridge Baron used to win the 1997 world championship of computer bridge, applications of AI in automated manufacturing, automated planning systems such as SHOP, SHOP2, and Pyhop, and evolutionary game-theoretic studies of cultural evolution. He is an AAAI Fellow and an ACM Fellow.
Paolo Traverso is the Director of FBK ICT IRST, the Research Center at Fondazione Bruno Kessler (FBK). Paolo has worked in the advanced technology groups of companies in Chicago, London, and Milan, leading projects in safety critical systems, data and knowledge management, and service oriented applications. His research activity is mainly focused on planning and acting under uncertainty. His contributions to research in automated planning include the technique called “planning via symbolic model checking.” He has published more than one hundred papers in artificial intelligence. He is an ECCAI Fellow.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Chapter 1
Introduction
This chapter introduces informally the concepts and technical material developed in the rest of the book. It discusses in particular the notion of deliberation, which is at the core of the interaction between planning and acting. Section 1.1 motivates our study of deliberation from a computational viewpoint and delineates the scope of the book. We then introduce a conceptual view of an artificial entity, called an actor, capable of acting deliberately on its environment, and discuss our main assumptions. Deliberation models and functions are presented next. Section 1.4 describes two application domains that will be simplified into illustrative examples of the techniques covered in rest of the book.
1.1 Purpose and Motivations
1.1.1 First Intuition
What is deliberative acting? That is the question we are studying in this book. We address it by investigating the computational reasoning principles and mechanisms supporting how to choose and perform actions. We use the word action to refer to something that an agent does, such as exerting a force, a motion, a perception or a communication, in order to make a change in its environment and own state. An agent is any entity capable of interacting with its environment. An agent acting deliberately is motivated by some intended objective. It performs one or several actions that are justifiable by sound reasoning with respect to this objective. Deliberation for acting consists of deciding which actions to undertake and how to perform them to achieve an objective. It refers to a reasoning
Authors’ manuscript. Published by Cambridge University Press. Do not distribute. 1


2 Chapter 1
process, both before and during acting, that addresses questions such as the following:
• If an agent performs an action, what will the result be?
• Which actions should an agent undertake, and how should the agent perform the chosen actions to produce a desired effect?
Such reasoning allows the agent to predict, to decide what to do and how do it, and to combine several actions that contribute jointly to the objective. The reasoning consists of using predictive models of the agent’s environment and capabilities to simulate what will happen if the agent performs an action. Let us illustrate these abstract notions intuitively.
Example 1.1. Consider a bird in the following three scenes:
• To visually track a target, the bird moves its eyes, head, and body.
• To get some food that is out of reach, the bird takes a wire rod, finds a wedge to bend the wire into a hook, uses the hook to get the food.
• To reach a worm floating in a pitcher, the bird picks up a stone and drops it into the pitcher, repeats with other stones until the water has risen to a reachable level, and then picks up the worm.
Example 1.1 mentions actions such as moving, sensing, picking, bending and throwing. The first scene illustrates a precise coordination of motion and sensing that is called visual servoing. This set of coordinated actions is certainly purposeful: it aims at keeping the target in the field of view. But it is more reactive than deliberative. The other two scenes are significantly more elaborate: they demand reasoning about causal relations among interdependent actions that transform objects, and the use of these actions to achieve an objective. They illustrate our intuitive notion of acting deliberately. The mechanisms for acting deliberately have always been of interest to philosophy.1 They are a subject of intense research in several scientific disciplines, including biology, neuroscience, psychology, and cognitive sciences. The deliberative bird behaviors of Example 1.1 have been observed and studied from the viewpoint of how deliberative capabilities are developed, in species of corvids such as crows [597] or rooks [71, 70]. Numerous other animal species have the ability to simulate their actions and deliber
1In particular, the branch of philosophy called action theory, which explores questions such as, “What is left over if I subtract the fact that my arm goes up from the fact that I raise my arm?” [610].
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 1.1 3
ate on the basis of such simulations.2 The sophisticated human deliberation faculties are the topic of numerous research, in particular regarding their development in infants and babies, starting from the work of Piaget (as in [478, 479]) to the recent diversity of more formal psychology models (e.g., [563, 19, 461]).
We are interested here in the study of computational deliberation capabilities that allow an artificial agent to reason about its actions, choose them, organize them purposefully, and act deliberately to achieve an objective. We call this artificial agent an actor. This is to underline the acting functions on which we are focusing and to differentiate them from the broader meaning of the word “agent.” We consider physical actors such as robots, as well as abstract actors that act in simulated or virtual environments, for example, through graphic animation or electronic Web transactions. For both kinds of actors, sensory-motor functions designate in a broad sense the low-level functions that implement the execution of actions.
1.1.2 Motivations
We address the issue of how an actor acts deliberately by following the approaches and methods of artificial intelligence (AI). Our purpose proceeds from the usual motivations of AI research, namely:
• To understand, through effective formal models, the cognitive capabilities that correspond to acting deliberately.
• To build actors that exhibit these capabilities.
• To develop technologies that address socially useful needs.
Understanding deliberation is an objective for most cognitive sciences. The specifics of AI are to model deliberation through computational approaches that allow us to explain as well as to generate the modeled capabilities. Furthermore, the investigated capabilities are better understood by mapping concepts and theories into designed systems and experiments to test empirically, measure, and qualify the proposed models. The technological motivation for endowing an artificial actor with deliberation capabilities stems from two factors:
• autonomy, meaning that the actor performs its intended functions without being directly operated by a person, and
2In the interesting classification of Dennett [150], these species are called Popperian, in reference to the epistemologist Karl Popper.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


4 Chapter 1
• diversity in the tasks the actor can perform and the environments in which it can operate.
Without autonomy, a directly operated or teleoperated device does not usually need to deliberate. It simply extends the acting and sensing capabilities of a human operator who is in charge of understanding and decision making, possibly with the support of advice and planning tools, for example, as in surgical robotics and other applications of teleoperation.
An autonomous system may not need deliberation if it operates only in the fully specified environment for which it has been designed. Manufacturing robots autonomously perform tasks such as painting, welding, assembling, or servicing a warehouse without much deliberation. Similarly, a vending machine or a driverless train operates autonomously without a need for deliberation. For these and similar examples of automation, deliberation is performed by the designer. The system and its environment are engineered so that the only variations that can occur are those accounted for at the design stage in the system’s predefined functioning envelope. Diversity in the environment is not expected. A state outside of the functioning envelope puts the system into a failure mode in which a person takes deliberate actions.
Similarly, a device designed for a unique specialized task may perform it autonomously without much deliberation, as long the variations in its environment are within its designed range. For example, a vacuum-cleaning or lawn mowing robot does not deliberate, but it can cope autonomously with its specialized tasks in a reasonable range of lawns or floors. However, it may cease to function properly when it encounters a slippery floor, a steep slope, or any condition outside of the range for which it was designed.
When a designer can account, within some functioning envelope, for all the environments and tasks a system will face and when a person can be in charge of deliberating outside of this envelope, by means of teleoperation or reprogramming, then deliberation generally is not needed in the system itself. Such a system will be endowed with a library of reactive behaviors (e.g., as the bird’s visual target tracking in Example 1.1) that cover efficiently its functioning envelope. However, when an autonomous actor has to face a diversity of tasks, environments and interactions, then achieving its purpose will require some degree of deliberation. This is the case in many robotics applications, such as service and personal robots, rescue and exploration robots, autonomous space stations and satellites, or even driverless cars. This holds also for complex simulation systems used in entertainment (e.g., video games) or educational applications (serious games). It is equally ap
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 1.1 5
plicable to many control systems that manage complex infrastructures such as industrial or energy plants, transportation networks, and urban facilities (smart cities). Autonomy, diversity in tasks and environments, and the need for deliberation are not binary properties that are either true or false. Rather, the higher the need for autonomy and diversity, the higher the need for deliberation. This relationship is not restricted to artificial systems. Numerous natural species (plants and some invertebrates such as sponges or worms) have been able to evolve to fit into stable ecological niches, apparently without much deliberation. Species that had to face rapid changes in their environment and to adapt to a wide range of living conditions had to develop more deliberation capabilities.
1.1.3 Focus and Scope
We address deliberation from an AI viewpoint. Our focus is on the reasoning functions required for acting deliberately. This focus involves two restrictions:
• We are not interested in actions that consists solely of internal computations, such as adding “2 + 3” or deducing that “Socrates is mortal.” These computations are not actions that change the state of the world.3 They can be used as part of the actor’s deliberation, but we take them as granted and outside of our scope.
• We are not concerned with techniques for designing the sensing, actuation, and sensory-motor control needed for the low-level execution of actions. Sensory-motor control (e.g., the visual servoing of Example 1.1) can be essential for acting, but its study is not within our scope. We assume that actions are performed with a set of primitives, which we will call commands, that implement sensory-motor control. The actor performs its actions by executing commands. To deliberate, it relies on models of how these commands work.
The scope of this book is not limited to the most studied deliberation function, which is planning what actions to perform. Planning consists of choosing and organizing the actions that can achieve a given objective. In many situations, there is not much need for planning: the actions to perform are known. But there is a need for significant deliberation in deciding how to perform each action, given the context and changes in the environment.
3The borderline between computational operations and actions that change the external world is not as sharp for an abstract actor as for a physical one.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


6 Chapter 1
We develop the view that planning can be needed for deliberation but is seldom sufficient. We argue that acting goes beyond the execution of lowlevel commands.
Example 1.2. Dana finishes breakfast in a hotel restaurant, and starts going back to his room. On the way, he notices that the elevator is not on his floor and decides to walk up the stairs. After a few steps he becomes aware that he doesn’t have his room key which he left on the breakfast table. He goes back to pick it up.
In this example, the actor does not need to plan the simple task of going to his room. He continually deliberates while acting: he makes opportunistic choices, simulates in advance and monitors his actions, stops when needed and decides on alternate actions. Deliberation consists of reasoning with predictive models as well as acquiring these models. An actor may have to learn how to adapt to new situations and tasks, as much as to use the models it knows about for its decision making. Further, even if a problem can be addressed with the actor’s generic models, it can be more efficient to transform the explicit computations with these models into low-level sensory-motor functions. Hence, it is natural to consider learning to act as a deliberation function. Section 7.3 offers a brief survey on learning and model acquisition for planning and acting. However, our focus is on deliberation techniques using predefined models.
1.2 Conceptual View of an Actor
1.2.1 A Simple Architecture
An actor interacts with the external environment and with other actors. In a simplified architecture, depicted in Figure 1.1(a), the actor has two main modules: a set of deliberation functions and an execution platform.
The actor’s sensory-motor functions are part of its execution platform. They transform the actor’s commands into actuations that execute its actions (e.g., the movement of a limb or a virtual character). The execution platform also transforms sensed signals into features of the world (e.g., to recognize a physical or virtual object, or to query information from the Web). The capabilities of the platform are explicitly described as models of the available commands. Deliberation functions implement the reasoning needed to choose, organize, and perform actions that achieve the actor’s objectives, to react
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 1.2 7
Deliberation functions
Execution platform
Commands Percepts
Other actors
Objectives
Messages
External World
Actuations Signals
(a)
Execution platform
Commands Percepts
Other actors
Objectives
Messages
External World
Actuations Signals
Acting
Planning
Queries Plans
(b)
Figure 1.1: Conceptual view of an actor (a); its restriction to planning and acting (b).
adequately to changes in the environment, and to interact with other actors, including human operators. To choose and execute commands that ultimately achieve its objectives, the actor needs to perform a number of deliberation functions. For example, the actor must commit to intermediate goals, plan for those goals, refine each planned action into commands, react to events, monitor its activities to compare the predicted and observed changes, and decide whether recovery actions are needed. These deliberation functions are depicted in Figure 1.1(b) as two main functions: planning and acting. The acting function is in charge of refining actions into commands, reacting to events, and monitoring.
1.2.2 Hierarchical and Continual Online Deliberation
The view presented in Section 1.2.1 can be a convenient first approach for describing an actor, but one must keep in mind that it is an oversimplification.
Example 1.3. To respond to a user’s request, a robot has to bring an object o7 to a location room2 (see Figure 1.2). To do that, it plans a sequence of abstract actions such as “navigate to,” “fetch,” and “deliver.” One of these refines into “move to door,” “open door,” “get out,” and “close door.” Once the robot is at the door, it refines the “open door” action appropriately for how it perceives that particular door.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


8 Chapter 1
ungrasp
grasp knob
turn knob
maintain
move back
pull
monitor
identify type of door
pull
monitor
move close to knob
open door
......
get out close door
respond to user requests
...
... bring o7 to room2
go to hallway
deliver
o7
... ...... ...
...
move to door
fetch
o7
navigate
to room2
navigate
to room1
Figure 1.2: Multiple levels of abstraction in deliberative acting. Each solid red arrow indicates a refinement of an abstract action into more concrete ones. Each dashed blue arrow maps a task into a plan of actions.
The robot’s deliberation can be accomplished by a collection of hierarchically organized components. In such a hierarchy, a component receives tasks from the component above it, and decides what activities need to be performed to carry out those tasks. Performing a task may involve refining it into lower-level steps, issuing subtasks to other components below it in the hierarchy, issuing commands to be executed by the platform, and reporting to the component that issued the task. In general, tasks in different parts of the hierarchy may involve concurrent use of different types of models and specialized reasoning functions.
This example illustrates two important principles of deliberation: hierarchical organization and continual online processing.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 1.2 9
• Hierarchically organized deliberation. Some of the actions the actor wishes to perform do not map directly into a command executable by its platform. An action may need further refinement and planning. This is done online and may require different representations, tools, and techniques from the ones that generated the task. A hierarchized deliberation process is not intended solely to reduce the search complexity of offline plan synthesis. It is needed mainly to address the heterogeneous nature of the actions about which the actor is deliberating, and the corresponding heterogeneous representations and models that such deliberations require.
• Continual online deliberation. Only in exceptional circumstances will the actor do all of its deliberation offline before executing any of its planned actions. Instead, the actor generally deliberates at runtime about how to carry out the tasks it is currently performing. The deliberation remains partial until the actor reaches its objective, including through flexible modification of its plans and retrials. The actor’s predictive models are often limited. Its capability to acquire and maintain a broad knowledge about the current state of its environment is very restricted. The cost of minor mistakes and retrials are often lower than the cost of extensive modeling, information gathering, and thorough deliberation. Throughout the acting process, the actor refines and monitors its actions; reacts to events; and extends, updates, and repairs its plan on the basis of its perception focused on the relevant part of the environment.
Different parts of the actor’s hierarchy often use different representations of the state of the actor and its environment. These representations may correspond to different amounts of detail in the description of the state and different mathematical constructs. In Figure 1.2, a graph of discrete locations may be used at the upper levels, while the lower levels may use vectors of continuous configuration variables for the robot limbs. Finally, because complex deliberations can be compiled down by learning into low-level commands, the frontier between deliberation functions and the execution platform is not rigid; it evolves with the actor’s experience.
1.2.3 Assumptions
We are not seeking knowledge representation and reasoning approaches that are effective across every kind of deliberation problem and at every level of a hierarchically organized actor. Neither are we interested in highly specialized
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


10 Chapter 1
actors tailored for a single niche, because deliberation is about facing diversity. Instead, we are proposing a few generic approaches that can be adapted to different classes of environments and, for a given actor, to different levels of its deliberation. These approaches rely on restrictive assumptions that are needed from a computational viewpoint, and that are acceptable for the class of environments and tasks in which we are interested. Deliberation assumptions are usually about how variable, dynamic, observable, and predictable the environment is, and what the actor knows and perceives about it while acting. We can classify them into assumptions related to the dynamics of the environment, its observability, the uncertainty managed in models, and how time and concurrency are handled.
• Dynamics of the environment. An actor may assume to be in a static world except for its own actions, or it may take into account exogenous events and changes that are expected and/or observed. In both cases the dynamics of the world may be described using discrete, continuous or hybrid models. Of these, hybrid models are the most general. Acting necessarily involves discontinuities in the interaction with the environment,4 and these are best modeled discretely. But a purely discrete model abstracts away continuous processes that may also need to be modeled.
• Observability of the environment. It is seldom the case that all the information needed for deliberation is permanently known to the actor. Some facts or parameters may be always known, others may be observable if specific sensing actions are performed, and others will remain hidden. The actor may have to act on the basis of reasonable assumptions or beliefs regarding the latter.
• Uncertainty in knowledge and predictions. No actor is omniscient. It may or may not be able to extend its knowledge with specific actions. It may or may not be able to reason about the uncertainty regarding the current state of the world and the predicted future (e.g., with nondeterministic or probabilistic models). Abstracting away uncertainty during a high-level deliberation can be legitimate if the actor can handle it at a lower level and correct its course of action when needed.
• Time and concurrency. Every action consumes time. But deliberation may or may not need to model it explicitly and reason about its flow for the purpose of meeting deadlines, synchronizing, or handling
4Think of the phases in a walking or grasping action.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 1.3 11
concurrent activities.
Different chapters of the book make different assumptions about time, concurrency, and uncertainty. Except for Section 7.4 on hybrid models, we’ll restrict ourself to discrete approaches. This is consistent with the focus and scope discussed in Section 1.1.3, because it is primarily in sensory-motor functions and commands that continuous models are systematically needed.
1.3 Deliberation Models and Functions
1.3.1 Descriptive and Operational Models of Actions
An actor needs predictive models of its actions to decide what actions to do and how to do them. These two types of knowledge are expressed with, respectively, descriptive and operational models.
• Descriptive models of actions specify the actor’s “know what.” They describe which state or set of possible states may result from performing an action or command. They are used by the actor to reason about what actions may achieve its objectives.
• Operational models of actions specify the actor’s “know how.” They describe how to perform an action, that is, what commands to execute in the current context, and how organize them to achieve the action’s intended effects. The actor relies on operational models to perform the actions that it has decided to perform.
In general, descriptive models are more abstract than operational models. Descriptive models abstract away the details, and focus on the main effects of an action; they are useful at higher levels of a deliberation hierarchy. This abstraction is needed because often it is too difficult to develop very detailed predictive models, and because detailed models require information that is unknown at planning time. Furthermore, reasoning with detailed models is computationally very complex. For example, if you plan to take a book from a bookshelf, at planning time you will not be concerned with the available space on the side or on the top of the book to insert your fingers and extract the book from the shelf. The descriptive model of the action will abstract away these details. It will focus on where the book is, whether it is within your reach, and whether you have a free hand with which to pick it up. The simplifications allowed in a descriptive model are not possible in an operational model. To actually pick up the book, you will have to determine
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


12 Chapter 1
precisely where the book is located in the shelf, which positions of your hand and fingers are feasible, and which sequences of precise motions and manipulations will allow you to perform the action. Furthermore, operational models may need to include ways to respond to exogenous events, that is, events that occur because of external factors beyond the actor’s control. For example, someone might be standing in front of the bookshelf, the stool that you intended to use to reach the book on a high shelf might be missing, or any of a potentially huge number of other possibilities might interfere with your plan. In principle, descriptive models can take into account the uncertainty caused by exogenous events, for example, through nondeterministic or probabilistic models (see Chapters 5 and 6), but the need to handle exogenous events is much more compelling for operational models. Indeed, exogenous events are often ignored in descriptive models because it is impractical to try to model all of the possible joint effects of actions and exogenous events, or to plan in advance for all of the contingencies. But operational models must have ways to respond to such events if they happen, because they can interfere with the execution of an action. In the library example, you might need to ask someone to move out of the way, or you might have to stand on a chair instead of the missing stool. Finally, an actor needs descriptive models of the available commands in order to use them effectively, but in general it does not need their operational models. Indeed, commands are the lower-level sensory-motor primitives embedded in the execution platform; their operational models correspond to what is implemented in these primitives. Taking this remark to the extreme, if one assumes that every known action corresponds to an executable command, then all operational models are embedded in the execution platform and can be ignored at the deliberation level. This assumption seldom holds.
1.3.2 Description of States for Deliberation
To specify both descriptive and operational models of actions, we will use representational primitives that define the state of an actor and its environment; these are called state variables. A state variable associates a relevant attribute of the world with a value that changes over time. The definition of a state with state variables needs to include enough details for the actor’s deliberations, but it does not need to be, nor can it be, exhaustive. In a hierarchically organized actor, different deliberative activities may need different amounts of detail in the state description. For example, in actions such as “grasp knob” and “turn knob” at the bottom of Figure 1.2,
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 1.3 13
to choose the commands for grasping and operating the handle, the actor needs to reason about detailed parameters such as the robot’s configuration coordinates and the position and shape of the door handle. Higher up, where the actor refines “bring o7 to room2” into actions such as “go to hallway” and “navigate to room1,” such details are not needed. It is more convenient there to reason about the values of more abstract variables, such as location(robot) = room1 or position(door) = closed. To establish correspondences between these abstract variables and the detailed ones, the actor could have definitions saying, for example, that location(robot) = room1 corresponds to a particular area in an Euclidean reference frame.
The precise organization of a hierarchy of data structures and state representations is a well-known area in computer science (e.g., [522]). It may take different forms in application domains such as robotics, virtual reality, or geographic information systems. Here, we’ll keep this point as simple as possible and assume that at each part of an actor’s deliberation hierarchy, the state representation includes not only the variables used in that part of the hierarchy (e.g., the robot’s configuration coordinates at the bottom of Figure 1.2), but also the variables used higher up in the hierarchy (e.g., location(robot)).
An important issue is the distinction and correspondence between predicted states and observed states. When an actor reasons about what might happen and simulates changes of state to assess how desirable a course of action is, it uses predicted states. When it reasons about how to perform actions in some context, it relies on observed states; it may contrast its observations with its expectations. Predicted states are in general less detailed than the observed one; they are obtained as a result of one or several predictions starting from an abstraction of the current observed state. To keep the distinction clear, we’ll use different notations:
• s ∈ S is a predicted state;
• ξ ∈ Ξ is an observed state.
Because of partial and inaccurate observations, there can be uncertainty about the present observed state as well as about the future predicted states. Furthermore, information in a dynamic environment is ephemeral. Some of the values in ξ may be out-of-date: they may refer to things that the actor previously observed but that it cannot currently observe. Thus, ξ is the state of the actor’s knowledge, rather than the true state of the world. In general, the actor should be endowed with appropriate means to manage the uncertainty and temporality of the data in ξ.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


14 Chapter 1
Observability is an additional issue. As underlined in Section 1.2.3, some information relevant to the actor’s behavior can be momentarily or permanently hidden; it must be indirectly inferred. In the general case, the design of an actor should include the following distinctions among state variables:
• A variable is invisible if it is not observable but can only be estimated from observations and a priori information.
• A variable is observable if its value can be obtained by performing appropriate actions. At various points, it may be either visible if its value is known to the actor, or hidden if the actor must perform an observation action to get its value.
For simplicity, we’ll start out by assuming that the values of all state variables are precisely known at every moment while acting. Later in the book, we’ll consider more realistically that some state variables are observable but can only be observed by performing some specific actions. In Chapter 5, we deal with a specific case of partial observability: in Section 5.8.4, we transform a partially observable domain into an abstracted domain whose states are sets of states. We also examine (in Chapter 6) the case in which some state variables are permanently or momentarily observable but others remain hidden. The class of models known as partially observable models, in which every state variable is assumed to be either always known or always hidden, is discussed in Section 6.8.3.
1.3.3 Planning Versus Acting
The simple architecture of Figure 1.1(b) introduces planning and acting as respectively finding what actions to perform and how to refine chosen actions into commands. Here, we further discuss these two functions, how they differ, and how they can be associated in the actor’s deliberation. The purpose of planning is to synthesize an organized set of actions to carry out some activity. For instance, this can be done by a lookahead procedure that combines prediction steps (Figure 1.3: when in state s, action a is predicted to produce state s′) within a search through alternative sets of actions for a set that leads to a desired goal state. Planning problems vary in the kinds of actions to be planned for, the kinds of predictive models that are needed, and the kinds of plans that are considered satisfactory. For some kinds of problems, domain-specific planning methods have been developed that are tailor-made for that kind of problem. For instance, motion planning synthesizes a geometric and kinematic trajectory for moving a mobile system (e.g., a truck, a robot,
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 1.3 15
9
s s’
Predict
Search
a
Figure 1.3: Planning as a combination of prediction steps and a search mechanism.
or a virtual character); perception planning synthesizes an organized set of sensing and interpretation actions to recognize an object or to build a threedimensional model of a scene; infrastructure planning synthesizes plans to deploy and organize facilities, such as a public transportation infrastructure, to optimize their usage or to meet the needs of a community. Many other such examples can be given, such as flight navigation planning, satellite configuration planning, logistics planning, or industrial process planning. There are, however, commonalities to many forms of planning. Domainindependent planning tries to grasp these commonalities at an abstract level, in which actions are generic state transformation operators over a widely applicable representation of states as relations among objects. Domain-independent and domain-specific planning complement each other. In a hierarchically organized actor, planning takes place at multiple levels of the hierarchy. At high levels, abstract descriptions of a problem can be tackled using domain-independent planning techniques. The example shown in Figure 1.2 may require a path planner (for moving to locations), a manipulation planner (for grasping the door handle), and a domain-independent planner at the higher levels of the hierarchy. Acting involves deciding how to perform the chosen actions (with or without the help of a planner) while reacting to the context in which the activity takes place. Each action is considered as an abstract task to be refined, given the current context, progressively into actions or commands that are more concrete. Whereas planning is a search over predicted states, acting requires a continual assessment of the current state ξ, to contrast it with a predicted state s and adapt accordingly. Consequently, acting also includes reacting to unexpected changes and exogenous events, which are
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


16 Chapter 1
independent from the actor’s activity.
The techniques used in planning and acting can be compared as follows. Planning can be organized as an open-loop search, whereas acting needs to be a closed-loop process. Planning relies on descriptive models (know-what); acting uses mostly operational models (know-how). Domainindependent planners can be developed to take advantage of commonalities among different forms of planning problems, but this is less true for acting systems, which require more domain-specific programming.
The relationship between planning and acting is more complex than a simple linear sequence of “plan then act.” Seeking a complete plan before starting to act is not always feasible, and not always needed. It is feasible when the environment is predictable and well modeled, for example, as for a manufacturing production line. It is needed when acting has a high cost or risk, and when actions are not reversible. Often in such applications, the designer has to engineer out the environment to reduce diversity as much as possible beyond what is modeled and can be predicted.
In dynamic environments where exogenous events can take place and are difficult to model and predict beforehand, plans should be expected to fail if carried out blindly until the end. Their first steps are usually more reliable than the rest and steer toward the objectives. Plan modification and replanning are normal and should be embedded in the design of an actor. Metaphorically, planning is useful to shed light on the road ahead, not to lay an iron rail all the way to the goal.
The interplay between acting and planning can be organized in many ways, depending on how easy it is to plan and how quickly the environment changes. A general paradigm is the receding horizon scheme, which is illustrated in Figure 1.4. It consists of repeating the two following steps until the actor has accomplished its goal:
(i) Plan from the current state toward the goal, but not necessarily all the way to the goal.
(ii) Act by refining one or a few actions of the synthesized plan into commands to be executed.
A receding horizon approach can be implemented in many ways. Options include various planning horizon, number of actions to perform at each planning stage, and what triggers replanning. Furthermore, the planning and acting procedures can be run either sequentially or in parallel with synchronization.
Suppose an actor does a depth-first refinement of the hierarchy in Fig
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 1.3 17
8
Planning stage
Acting stage
Figure 1.4: Receding horizon scheme for planning and acting.
ure 1.2. Depending on the actor’s planning horizon, it may execute each command as soon as one is planned or wait until the planning proceeds a bit farther. Recall from Section 1.3.2 that the observed state ξ may differ from the predicted one. Furthermore, ξ may evolve even when no commands are being executed. Such situations may invalidate what is being planned, necessitating replanning. The interplay between acting and planning is relevant even if the planner synthesizes alternative courses of action for different contingencies (see Chapters 5 and 6). Indeed, it may not be worthwhile to plan for all possible contingencies, or the planner may not know in advance what all of them are.
1.3.4 Other Deliberation Functions
We have mentioned deliberation functions other than planning and acting: perceiving, monitoring, goal reasoning, communicating, and learning. These functions (surveyed in Chapter 7) are briefly described here. Perceiving goes beyond sensing, even with elaborate signal processing and pattern matching methods. Deliberation is needed in bottom-up processes for getting meaningful data from sensors, and in top-down activities such as focus-of-attention mechanisms, reasoning with sensor models, and planning how to do sensing and information gathering. Some of the issues include how to maintain a mapping between sensed data and deliberation symbols, where and how to use the platform sensors, or how to recognize actions and plans of other actors. Monitoring consists of comparing observations of the environment with what the actor’s deliberation has predicted. It can be used to detect and interpret discrepancies, perform diagnosis, and trigger initial recovery actions when needed. Monitoring may require planning what observation actions to perform, and what kinds of diagnosis tests to perform. There is a strong
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


18 Chapter 1
relationship between planning techniques and diagnosis techniques. Goal reasoning is monitoring of the actor’s objectives or mission, to keep the actor’s commitments and goals in perspective. It includes assessing their relevance, given the observed evolutions, new opportunities, constraints or failures, using this assessment to decide whether some commitments should be abandoned, and if so, when and how to update the current goals. Communicating and interacting with other actors open numerous deliberation issues such as communication planning, task sharing and delegation, mixed initiative planning, and adversarial interaction. Learning may allow an actor to acquire, adapt, and improve through experience the models needed for deliberation and to acquire new commands to extend and improve the actor’s execution platform. Conversely, techniques such as active learning may themselves require acting for the purpose of better learning.
1.4 Illustrative Examples
To illustrate particular representations and algorithms, we’ll introduce a variety of examples inspired by two application domains: robotics and operations management. We’ll use highly simplified views of these applications to include only the features that are relevant for the issue we’re trying to illustrate. In this section, we provide summaries of the real-world context in which our simple examples might occur.
1.4.1 A Factotum Service Robot
We will use the word factotum to mean a general-purpose service robot that consists of a mobile platform equipped with several sensors (lasers, cameras, etc.) and actuators (wheels, arms, forklift) [329]. This robot operates in structured environments such as a mall, an office building, a warehouse or a harbor. It accomplishes transportation and logistics tasks autonomously (e.g., fetching objects, putting them into boxes, assembling boxes into containers, moving them around, delivering them or piling them up in storage areas). This robot platform can execute parameterized commands, such as localize itself in the map, move along a path, detect and avoid obstacles, identify and locate items, grasp, ungrasp and push items. It knows about a few actions using these commands, for example, map the environment (extend or update the map), goto a destination, open a door, search for or fetch an item.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 1.4 19
These actions and commands are specified with descriptive and operational models. For example, move works if it is given waypoints in free space or an obstacle-free path that meet kinematics and localization constraints; the latter are, for example, visual landmarks required by action localize. These conditions need to be checked and monitored by the robot while performing the actions. Concurrency has to be managed. For example, goto should run in parallel with detect, avoid, and localize. Factotum needs domain-specific planners, for example, a motion planner for move, a manipulation planner for grasp (possibly using locate, push, and move actions). Corresponding plans are more than a sequence or a partially ordered set of commands; they require closed-loop control and monitoring. At the mission-preparation stage (the upper levels in Figure 1.2), it is legitimate to view a logistics task as an organized set of abstract subtasks for collecting, preparing, conveying, and delivering the goods. Each subtask may be further decomposed into a sequence of still abstract actions such as goto, take, and put. Domain-independent task planning techniques are needed here. However, deliberation does not end with the mission preparation stage. A goto action can be performed in many ways depending on the environment properties: it may or may not require a planned path; it may use different localization, path following, motion control, detection, and avoidance methods (see the “goto” node in Figure 1.2). A goto after a take is possibly different from the one before because of the held object. To perform a goto action in different contexts, the robot relies on a collection of skills defined formally by methods. A method specifies a way to refine an action into commands. The same goto may start with a method (e.g., follow GPS waypoints) but may be pursued with more adapted methods when required by the environment (indoor without GPS signal) or the context. Such a change between methods may be a normal progression of the goto action or a retrial due to complications. The robot also has methods for take, put, open, close, and any other actions it may need to perform. These methods endow the robot with operational models (its know-how) and knowledge about how to choose the most adapted method with the right parameters. The methods for performing actions may use complex control constructs with concurrent processes (loops, conditionals, semaphores, multithread and real-time locks). They can be developed from formal specifications in some representation and/or with plan synthesis techniques. Different representations may be useful to cover the methods needed by the factotum robot. Machine learning techniques can be used for improving the methods, acquiring their models, and adapting the factotum to a new trade.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


20 Chapter 1
In addition to acting with the right methods, the robot has to monitor its activity at every level, including possibly at the goal level. Prediction of what is needed to correctly perform and monitor foreseen activities should be made beforehand. Making the right predictions from the combined models of actions and models of the environment is a difficult problem that involves heterogeneous representations.
Finally, the robot requires extended perception capabilities: reasoning on what is observable and what is not, integrating knowledge-gathering actions to environment changing actions, acting in order to maintain sufficient knowledge for the task at hand with a consistent interpretation of self and the world.
1.4.2 A Complex Operations Manager
A Harbor Operations Manager (HOM) is a system that supervises and controls all the tasks performed in a harbor.5 Examples of such tasks include unloading cars from ships, parking them in storage areas, moving them to a repair area, performing the repair, preparing the delivery of cars according to orders, and loading them onto trucks when the trucks arrive at the harbor. Some of these operations are performed by human workers, others automatically by machines such as the factotum robot of previous section. This complex environment has several features that require deliberation:
• It is customizable: for example, delivery procedures can be customized according to the car brand, model, or retailer-specific requirements.
• It is variable: procedures for unloading/loading cars depend on the car brands; storage areas have different parking procedures, for example.
• It is dynamic: ships, cars, trucks, and orders arrive dynamically.
• It is partially predictable and controllable: cars may be damaged and need repair, storage areas may not be available, orders have unpredictable requirements, ships and trucks have random delays, for example.
At a high level, an HOM has to carry out a simple sequence of abstract tasks: 〈unload, unpack, store, wait-for-order, treatment, delivery〉 (see Figure 1.5). This invariant plan is easily specified by hand. The deliberation problem in an HOM is not in the synthesis of this plan but in the dynamic refinement of its tasks in more concrete subtasks. For example, an HOM
5Example inspired from a facility developed for the port of Bremen, Germany [76, 100].
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 1.4 21
...
...
manage incoming shipment
unload unpack store
......
...
... ......
registration
manager storage
assignment manager
release manager
booking manager navigation
...
...
await order prepare deliver
storage area C manager
storage area B manager
storage area A manager
Figure 1.5: Deliberation components for a Harbor Operations Manager.
refines the abstract task store of Figure 1.5 into subtasks for registering a car to be stored, moving it, and other tasks, down to executable commands.
Moreover, the tasks to be refined and controlled are carried out by different components, for example, ships, gates, and storage or repair areas. Each ship has its own procedure to unload cars to a gate. A gate has its own procedure to accept cars that are unloaded to the deck. A natural design option is therefore to model the HOM in a distributed way, as a set of interacting deliberation components. The interactions between ships and gates, gates and trucks, and trucks and storage areas must be controlled with respect to the global constraints and objectives of the system. To do that, HOM must deal with uncertainty and nondeterminism due to exogenous events, and to the fact that each component may – from the point of view of the management facility – behave nondeterministically. For instance, in the task to synchronize a ship with a gate to unload cars, the ship may send a request for unloading cars to the unloading manager, and the gate may reply either that the request meets its requirements and the unloading operation
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


22 Chapter 1
can proceed according to some unloading specifications, or that the request cannot be handled. The management facility may not know a priori what the request, the unloading specifications, and reply will be. In summary, an HOM relies on a collection of interacting components, each implementing its own procedures. It refines the abstract tasks of the high-level plan into a composition of these procedures to address each new object arrival and adapt to each exogenous event. The refinement and adaptation mechanisms can be designed through an approach in which the HOM is an actor organized into a hierarchy of components, each abstract action is a task to be further refined and planned for, and online planning and acting are performed continually to adapt and repair plans. The approach embeds one or several planners within these components, which are called at run-time, when the system has to refine an abstract action to adapt to a new context. It relies on refinement mechanisms that can be triggered at run-time whenever an abstract action in a procedure needs to be refined or an adaptation needs to be taken into account.
1.5 Outline of the Book
This chapter has provided a rather abstract and broad introduction. Chapter 2 offers more concrete material regarding deliberation with deterministic models and full knowledge about a static environment. It covers the “classical planning” algorithms and heuristics, with state-space search, forward and backward, and plan-space search. It also presents how these planning techniques can be integrated online with acting. Chapter 3 is focused on refinement methods for acting and planning. It explores how a unified representation can be used for both functions, at different levels of the deliberation hierarchy, and in different ways. It also discusses how the integration of planning and acting can be performed. Chapter 4 is about deliberation with explicit time models using a representation with timelines and chronicles. A temporal planner, based on refinement methods, is presented together with the constraint management techniques needed for handling temporal data. Using the techniques from Chapter 3, we also discuss the integration of planning and acting with temporal models. Uncertainty in deliberation is addressed in Chapters 5 and 6. The main planning techniques in nondeterministic search spaces are covered in Chapter 5, together with model checking and determinization approaches. In this chapter, we present online lookahead methods for the interleaving of plan
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 1.5 23
ning and acting. We also show how nondeterministic models can be used with refinements techniques that intermix plans, actions, and goals. We discuss the integration of planning and acting with input/output automata to cover cases such as the distributed deliberation in the HOM example. We cover probabilistic models in Chapter 6. We develop heuristic search techniques for stochastic shortest path problems. We present online approaches for planning and acting, discuss refinement methods for acting with probabilistic models, and analyze the specifics of descriptive models of actions in the probabilistic case together with several practical issues for modeling probabilistic domains. Chapters 2 through 6 are devoted to planning and acting. Chapter 7 briefly surveys the other deliberation functions introduced in Section 1.3.4: perceiving, monitoring, goal reasoning, interacting, and learning. It also discusses hybrid models and ontologies for planning and acting.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Chapter 2
Deliberation with
Deterministic Models
Having considered the components of an actor and their relation to the actor’s environment we now need to develop some representational and algorithmic tools for performing the actor’s deliberation functions. In this chapter we develop a simple kind of descriptive model for use in planning, describe some planning algorithms that can use this kind of model, and discuss some ways for actors to use those algorithms.
This chapter is organized as follows. Section 2.1 develops state-variable representations of planning domains. Sections 2.2 and 2.3 describe forwardsearch planning algorithms, and heuristics to guide them. Sections 2.4 and 2.5 describe backward-search and plan-space planning algorithms. Section 2.6 describes some ways for an actor to use online planning. Sections 2.7 and 2.8 contain the discussion and historical remarks, and the student exercises.
2.1 State-Variable Representation
The descriptive models used by planning systems are often called planning domains. However, it is important to keep in mind that a planning domain is not an a priori definition of the actor and its environment. Rather, it is necessarily an imperfect approximation that must incorporate trade-offs among several competing criteria: accuracy, computational performance, and understandability to users.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute. 24


Section 2.1 25
2.1.1 State-Transition Systems
In this chapter, we use a simple planning-domain formalism that is similar to a finite-state automaton:
Definition 2.1. A state-transition system (also called a classical planning domain) is a triple Σ = (S, A, γ) or 4-tuple Σ = (S, A, γ, cost), where
• S is a finite set of states in which the system may be.
• A is a finite set of actions that the actor may perform.
• γ : S × A → S is a partial function called the prediction function or state-transition function. If (s, a) is in γ’s domain (i.e., γ(s, a) is defined), then a is applicable in s, with γ(s, a) being the predicted outcome. Otherwise a is inapplicable in s.
• cost : S × A → [0, ∞) is a partial function having the same domain as γ. Although we call it the cost function, its meaning is arbitrary: it may represent monetary cost, time, or something else that one might want to minimize. If the cost function isn’t given explicitly (i.e., if Σ = (S, A, γ)), then cost(s, a) = 1 whenever γ(s, a) is defined.
To avoid several of the difficulties mentioned in Chapter 1, Definition 2.1 requires a set of restrictive assumptions called the classical planning assumptions:
1. Finite, static environment. In addition to requiring the sets of states and actions to be finite, Definition 2.1 assumes that changes occur only in response to actions: if the actor does not act, then the current state remains unchanged. This excludes the possibility of actions by other actors, or exogenous events that are not due to any actor.
2. No explicit time, no concurrency. There is no explicit model of time (e.g., when to start performing an action, how long a state or action should last, or how to perform other actions concurrently). There is just a discrete sequence of states and actions 〈s0, a1, s1, a2, s2, . . .〉.1
3. Determinism, no uncertainty. Definition 2.1 assumes that we can predict with certainty what state will be produced if an action a is performed in a state s. This excludes the possibility of accidents or
1This does not prohibit one from encoding some kinds of time-related information (e.g., timestamps) into the actions’ preconditions and effects. However, to represent and reason about actions that have temporal durations, a more sophisticated planning-domain formalism is usually needed, such as that discussed in Chapter 4.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


26 Chapter 2
loc1
loc3
loc2
loc6
loc5 loc4 loc7
loc8
x
y
4
3
2
1
0123456
loc9
loc0
Figure 2.1: A two-dimensional network of locations connected by roads.
execution errors, as well as nondeterministic actions, such as rolling a pair of dice.
In environments that do not satisfy the preceding assumptions, classical domain models may introduce errors into the actor’s deliberations but this does not necessarily mean that one should forgo classical models in favor of other kinds of models. The errors introduced by a classical model may be acceptable if they are infrequent and do not have severe consequences, and models that do not use the above assumptions may be much more complex to build and to reason with. Let us consider the computational aspects of using a state-transition system. If S and A are small enough, it may be feasible to create a lookup table that contains γ(s, a) and cost(s, a) for every s and a, so that the outcome of each action can be retrieved directly from the table. For example, we could do this to represent an actor’s possible locations and movements in the road network shown in Figure 2.1. In cases in which Σ is too large to specify every instance of γ(s, a) explicitly, the usual approach is to develop a generative representation in which there are procedures for computing γ(s, a) given s and a. The specification of Σ may include an explicit description of one (or a few) of the states in S; other states can be computed using γ. The following is an example of a domain-specific representation, that is, one designed specifically for a given planning domain. We then develop a domain-independent approach for representing any classical planning domain.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 2.1 27
(a) (b)
Figure 2.2: Geometric model of a workpiece, (a) before and (b) after computing the effects of a drilling action.
Example 2.2. Consider the task of using machine tools to modify the shape of a metal workpiece. Each state might include a geometric model of the workpiece (see Figure 2.2), and information about its location and orientation, the status and capabilities of each machine tool, and so forth. A descriptive model for a drilling operation might include the following:
• The operation’s name and parameters (e.g., the dimensions, orientation, and machining tolerances of the hole to be drilled).
• The operation’s preconditions, that is, conditions that are necessary for it to be used. For example, the desired hole should be perpendicular to the drilling surface, the workpiece should be mounted on the drilling machine, the drilling machine should have a drill bit of the proper size, and the drilling machine and drill bit need to be capable of satisfying the machining tolerances.
• The operation’s effects, that is, what it will do. These might include a geometric model of the modified workpiece (see Figure 2.2(b)) and estimates of how much time the action will take and how much it will cost.
The advantage of domain-specific representations is that one can choose whatever data structures and algorithms seem best for a given planning domain. The disadvantage is that a new representation must be developed for each new planning domain. As an alternative, we now develop a domainindependent way to represent classical planning domains.
2.1.2 Objects and State Variables
In a state-transition system, usually each state s ∈ S is a description of the properties of various objects in the planner’s environment. We will say that
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


28 Chapter 2
a property is rigid if it remains the same in every state in S, and it is varying if it may differ from one state to another. To represent the objects and their properties, we will use three sets B, R, and X, which we will require to be finite:
• B is a set of names for all of the objects, plus any mathematical constants that may be needed to represent properties of those objects. We will usually divide B into various subsets (robots, locations, mathematical constants, and so forth).
• To represent Σ’s rigid properties, we will use a set R of rigid relations. Each r ∈ R will be an n-ary (for some n) relation over B.
• To represent Σ’s varying properties, we will use a set X of syntactic terms called state variables, such that the value of each x ∈ X depends solely on the state s.
Which objects and properties are in B, R, and X depends on what parts of the environment the planner needs to reason about. For example, in Figure 1.2, the orientation of the robot’s gripper may be essential for deliberating about a low-level task such as “open door,” but irrelevant for a high-level task such as “bring 07 to room2.” In a hierarchically organized actor, these tasks may be described using two state spaces, S and S′ whose states describe different kinds of objects and properties.
Here are examples of B and R. We will say more about X shortly.
Example 2.3. Figure 2.3 depicts some states in a simple state-transition system. B includes two robots, three loading docks, three containers, three piles (stacks of containers), the Boolean constants T and F, and the constant nil:
B = Robots ∪ Docks ∪ Containers ∪ Piles ∪ Booleans ∪ {nil};
Booleans = {T, F};
Robots = {r1, r2};
Docks = {d1, d2, d3};
Containers = {c1, c2, c3};
Piles = {p1, p2, p3}.
We will define two rigid properties: each pair of loading docks is adjacent if there is a road between them, and each pile is at exactly one loading dock.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 2.1 29
unload(r1,c1,c2,p1,d1)
load(r1,c1,c2,p1,d1)
move(r1,d1,d3)
move(r1,d3,d1)
d1
d3
d2
p1 p2
p3
s0:
c2
c1 c3
r1 r2
d1
d3
d2
p1 p2
p3
s2:
c2
c1 c3
r1
r2
d1
d3
d2
p1 p2
p3
s1:
c2 c3
r1 r2
c1
Figure 2.3: A few of the states and transitions in a simple state-transition system. Each robot can hold at most one container, and at most one robot can be at each loading dock.
To represent these properties, R = {adjacent, at}, where
adjacent = {(d1, d2), (d2, d1), (d2, d3), (d3, d2), (d3, d1), (d1, d3)};
at = {(p1, d1), (p2, d2), (p3, d2)}.
In the subsequent examples that build on this one, we will not need to reason about objects such as the roads and the robots’ wheels, or properties such as the colors of the objects. Hence B and R do not include them.
Definition 2.4. A state variable over B is a syntactic term
x = sv(b1, . . . , bk), (2.1)
where sv is a symbol called the state variable’s name, and each bi is a member of B. Each state variable x has a range,2 Range(x) ⊆ B, which is the set of all possible values for x.
2We use range rather than domain to avoid confusion with planning domain.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


30 Chapter 2
Example 2.5. Continuing Example 2.3, let
X = {cargo(r), loc(r), occupied(d), pile(c), pos(c), top(p)
| r ∈ Robots, d ∈ Docks, c ∈ Containers, p ∈ Piles},
where the state variables have the following interpretations:
• Each robot r can carry at most one container at a time. We let cargo(r) = c if r is carrying container c, and cargo(r) = nil otherwise. Hence Range(cargo(r)) = Containers ∪ {nil}.
• loc(r) is robot r’s current location, which is one of the loading docks. Hence Range(loc(r)) = Docks.
• Each loading dock d can be occupied by at most one robot at a time. To indicate whether d is occupied, Range(occupied(d)) = Booleans.
• pos(c) is container c’s position, which can be a robot, another container, or nil if c is at the bottom of a pile. Hence Range(pos(c)) = Containers ∪ Robots ∪ {nil}.
• If container c is in a pile p then pile(c) = p, and if c is not in any pile then pile(c) = nil. Hence Range(pile(c)) = Piles ∪ {nil}.
• Each pile p is a (possibly empty) stack of containers. If the stack is empty then top(p) = nil, and otherwise top(p) is the container at the top of the stack. Hence Range(top(p)) = Containers ∪ {nil}.
A variable-assignment function over X is a function s that maps each xi ∈ X into a value zi ∈ Range(xi). If X = {x1, . . . , xn}, then because a function is a set of ordered pairs, we have
s = {(x1, z1), . . . , (xn, zn)}, (2.2)
which we often will write as a set of assertions:
s = {x1 = z1, x2 = z2, . . . , xn = zn}. (2.3)
Because X and B are finite, so is the number of variable-assignment functions.
Definition 2.6. A state-variable state space is a set S of variable-assignment functions over some set of state variables X. Each variable-assignment function in S is called a state in S.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 2.1 31
If the purpose of S is to represent some environment E, then we will want each state in s to have a sensible interpretation in E. Without getting into the formal details, an interpretation is a function I that maps B, R, and X to sets of objects, rigid properties, and variable properties in some environment E, in such a way that each s ∈ S corresponds to a situation (roughly, a combination of the objects and properties in the image of I) that can occur in E.3 If a variable-assignment function does not correspond to such a situation, then should not be a state in S.4
Example 2.7. Continuing Example 2.5, let us define the state-variable state space S depicted in Figure 2.3. The state s0 is the following variableassignment function:
s0 = {cargo(r1) = nil, cargo(r2) = nil, loc(r1) = d1, loc(r2) = d2, occupied(d1) = T, occupied(d2) = T, occupied(d3) = F, pile(c1) = p1, pile(c2) = p1, pile(c3) = p2, pos(c1) = c2, pos(c2) = nil, pos(c3) = nil, top(p1) = c1, top(p2) = c3, top(p3) = nil}.
(2.4)
In the same figure, the state s1 is identical to s0 except that cargo(r1) = c1, pile(c1) = nil, pos(c1) = r1, and top(p1) = c2.
In Example 2.5, the sizes of the state variables’ ranges are
|Range(cargo(r1))| = |Range(cargo(r2))| = 4,
|Range(loc(r1))| = |Range(loc(r2))| = 3,
|Range(occupied(d1))| = |Range(occupied(d2))| = |Range(occupied(d3))| = 2,
|Range(pile(c1))| = |Range(pile(c2))| = |Range(pile(c3))| = 4,
|Range(pos(c1))| = |Range(pos(c2))| = |Range(pos(c3))| = 6,
|Range(top(p1))| = |Range(top(p2))| = |Range(top(p3))| = 4.
Thus the number of possible variable-assignment functions is
42 × 32 × 23 × 43 × 63 × 43 = 1, 019, 215, 872.
3The details are quite similar to the definition of an interpretation in first-order logic [535, 517]. However, in first-order logic, E is a static domain rather than a dynamic environment, hence the interpretation maps a single state into a single situation. 4This is ideally how an interpretation should work, but in practice it is not always feasible to define an interpretation that satisfies those requirements completely. As we said in Section 2.1, a planning domain is an imperfect approximation of the actor and its environment, not an a priori definition.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


32 Chapter 2
However, fewer than 750 of these functions are states in S. A state-variable assignment function is a state in S if and only if it has an interpretation in the environment depicted in Figure 2.3. One way to specify the members of S is to give a set of consistency constraints (i.e., restrictions on what combinations of variable assignments are possible) and to say that a state-variable assignment function is a state in S if and only if it satisfies all of the constraints. Here are some examples of consistency constraints for S. A state s cannot have both loc(r1) = d1 and loc(r2) = d1, because a loading dock can only accommodate one robot at a time; s cannot have both pos(c1) = c3 and pos(c2) = c3, because two containers cannot have the same physical location; and s cannot have both pos(c1) = c2 and pos(c2) = c1, because two containers cannot be on top of each other. Exercise 2.2 is the task of finding a complete set of consistency constraints for S.
The preceding example introduced the idea of using consistency constraints to determine which variable-assignment functions are states but said nothing about how to represent and enforce such constraints. Throughout most of this book, we avoid the need to represent such constraints explicitly, by writing action models in such a way that if s is a state and a is an action that is applicable in s, then γ(s, a) is also a state. However, in Chapter 4, we will use a domain representation in which some of the constraints are represented explicitly and the planner must make sure never to use an action that would violate them.
2.1.3 Actions and Action Templates
To develop a way to write action models, we start by introducing some terminology borrowed loosely from first-order logic with equality:
Definition 2.8. A positive literal, or atom (short for atomic formula), is an expression having either of the following forms:
rel(z1, . . . , zn) or sv(z1, . . . , zn) = z0,
where rel is the name of a rigid relation, sv is a state-variable name, and each zi is either a variable (an ordinary mathematical variable, not a state variable) or the name of an object. A negative literal is an expression having either of the following forms:
¬rel(z1, . . . , zn) or sv(z1, . . . , zn) 6= z0.
A literal is ground if it contains no variables, and unground otherwise.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 2.1 33
In the atom sv(z1, . . . , zn) = z0, we will call sv(z1, . . . , zn) the atom’s target. Thus in Equation 2.3, a state is a set of ground atoms such that every state variable x ∈ X is the target of exactly one atom.
Definition 2.9. Let l be an unground literal, and Z be any subset of the variables in l. An instance of l is any expression l′ produced by replacing each z ∈ Z with a term z′ that is either an element of Range(z) or a variable with Range(z′) ⊆ Range(z).
Definition 2.9 generalizes straightforwardly to any syntactic expression that contains literals. We will say that such an expression is ground if it contains no variables and it is unground otherwise. If it is unground, then an instance of it can be created as described in Definition 2.9.
Definition 2.10. Let R and X be sets of rigid relations and state variables over a set of objects B, and S be a state-variable state space over X. An action template5 for S is a tuple α = (head(α), pre(α), eff(α), cost(α)) or α = (head(α), pre(α), eff(α)), the elements of which are as follows:
• head(α) is a syntactic expression6 of the form
act(z1, z2, . . . , zk),
where act is a symbol called the action name, and z1, z2, . . . , zk are variables called parameters. The parameters must include all of the variables (here we mean ordinary variables, not state variables) that appear anywhere in pre(α) and eff(α). Each parameter zi has a range of possible values, Range(zi) ⊆ B.
• pre(α) = {p1, . . . , pm} is a set of preconditions, each of which is a literal.
• eff(α) = {e1, . . . , en} is a set of effects, each of which is an expression of the form
sv(t1, . . . , tj) ← t0 (2.5)
where sv(t1, . . . , tj) is the effect’s target, and t0 is the value to be assigned. No target can appear in eff(α) more than once.
5In the artificial intelligence planning literature, these are often called planning operators or action schemas; see Section 2.7.1. 6The purpose of head(α) is to provide a convenient and unambiguous way to refer to actions. An upcoming example is load(r1, c1, c2, p1, d1) at the end of Example 2.12.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


34 Chapter 2
• cost(α) is a number c > 0 denoting the cost of applying the action.7 If it is omitted, then the default is cost(α) = 1.
We usually will write action templates in the following format (e.g., see Example 2.12). The “cost” line may be omitted if c = 1.
act(z1, z2, . . . , zk) pre: p1, . . . , pm eff: e1, . . . , en cost: c
Definition 2.11. A state-variable action is a ground instance a of an action template α that satisfies the following requirements: all rigid-relation literals in pre(a) must be true in R, and no target can appear more than once in eff(a). If a is an action and a state s satisfies pre(a), then a is applicable in s, and the predicted outcome of applying it is the state
γ(s, a) = {(x, w) | eff(a) contains the effect x ← w}
∪ {(x, w) ∈ s | x is not the target of any effect in eff(a)}. (2.6)
If a isn’t applicable in s, then γ(s, a) is undefined.
Thus if a is applicable in s, then
(γ(s, a))(x) =
{
w, if eff(a) contains an effect x ← w,
s(x), otherwise. (2.7)
Example 2.12. Continuing Example 2.5, suppose each robot r has an execution platform that can perform the following commands:
• if r is at a loading dock and is not already carrying anything, r can load a container from the top of a pile;
• if r is at a loading dock and is carrying a container, r can unload the container onto the top of a pile; and
• r can move from one loading dock to another if the other dock is unoccupied and there is a road between the two docks.
To model these commands, let A comprise the following action templates:
7This can be generalized to make cost(α) a numeric formula that involves α’s parameters. In this case, most forward-search algorithms and many domain-specific heuristic functions will still work, but most domain-independent heuristic functions will not, nor will backward-search and plan-space search algorithms (Sections 2.4 and 2.5).
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 2.1 35
load(r, c, c′, p, d)
pre: at(p, d), cargo(r) = nil, loc(r) = d, pos(c) = c′, top(p) = c eff: cargo(r) = c, pile(c) ← nil, pos(c) ← r, top(p) ← c′
unload(r, c, c′, p, d)
pre: at(p, d), pos(c) = r, loc(r) = d, top(p) = c′
eff: cargo(r) ← nil, pile(c) ← p, pos(c) ← c′, top(p) ← c
move(r, d, d′)
pre: adjacent(d, d′), loc(r) = d, occupied(d′) = F eff: loc(r) ← d′, occupied(d) ← F, occupied(d′) ← T
In the action templates, the parameters have the following ranges:
Range(c) = Containers; Range(c′) = Containers ∪ Robots ∪ {nil}; Range(d) = Docks; Range(d′) = Docks; Range(p) = Piles; Range(r) = Robots.
Let a1 be the state-variable action load(r1, c1, c2, p1, d1). Then
pre(a1) =
{at(p1, d1), cargo(r1) = nil, loc(r1) = d1, pos(c1) = c2, top(p1) = c1}.
Let s0 and s1 be in Example 2.5 and Figure 2.3. Then a1 is applicable in s0, and γ(s0, a1) = s1.
2.1.4 Plans and Planning Problems
Definition 2.13. Let B, R, X, and S be as in Section 2.1.2. Let A be a set of action templates such that for every α ∈ A, every parameter’s range is a subset of B, and let A = {all state-variable actions that are instances of members of A}. Finally, let γ be as in Equation 2.6. Then Σ = (S, A, γ, cost) is a state-variable planning domain.
Example 2.14. If B, R, X, S, A and γ are as in Examples 2.3, 2.5, 2.7, and 2.12, then (S, A, γ) is a state-variable planning domain.
Just after Definition 2.6, we discussed the notion of an interpretation of a state space S. We now extend this to include planning domains. An interpretation I of a state-variable planning domain Σ in an environment E is an interpretation of S in E that satisfies the following additional requirement: under I, each a ∈ A corresponds to an activity in E such that whenever
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


36 Chapter 2
a is applicable in a state s ∈ S, performing that activity in a situation corresponding to s will produce a situation corresponding to γ(s, a).8
Definition 2.15. A plan is a finite sequence of actions
π = 〈a1, a2, . . . , an〉.
The plan’s length is |π| = n, and its cost is the sum of the action costs: cost(π) = ∑n
i=1 cost(ai). As a special case, 〈〉 is the empty plan, which contains no actions. Its length and cost are both 0.
Definition 2.16. Let π = 〈a1, . . . , an〉 and π′ = 〈a′1, . . . , a′
n′〉 be plans and a be an action. We define the following concatenations:
π.a = 〈a1, . . . , an, a〉;
a.π = 〈a, a1, . . . , an〉;
π.π′ = 〈a1, . . . , an, a′
1, . . . , a′
n′ 〉;
π.〈〉 = 〈〉.π = π.
Definition 2.17. A plan π = 〈a1, a2, . . . , an〉 is applicable in a state s0 if there are states s1, . . . , sn such that γ(si−1, ai) = si for i = 1, . . . , n. In this case, we define
γ(s0, π) = sn;
̂γ(s0, π) = 〈s0, . . . , sn〉.
As a special case, the empty plan 〈〉 is applicable in every state s, with γ(s, 〈〉) = s and ̂γ(s, 〈〉) = 〈s〉.
In the preceding, ̂γ is called the transitive closure of γ. In addition to the predicted final state, it includes all of the predicted intermediate states.
Definition 2.18. A state-variable planning problem is a triple P = (Σ, s0, g), where Σ is a state-variable planning domain, s0 is a state called the initial state, and g is a set of ground literals called the goal. A solution for P is any plan π = 〈a1, . . . , an〉 such that the state γ(s0, π) satisfies g. Alternatively, one may write P = (Σ, s0, Sg), where Sg is a set of goal states. In this case, a solution for P is any plan π such that γ(s0, π) ∈ Sg.
8Ideally one would like to put a similar requirement on the interpretation of the action’s cost, but we said earlier that its interpretation is arbitrary.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 2.2 37
Forward-search (Σ, s0, g) s ← s0; π ← 〈〉 loop if s satisfies g, then return π A′ ← {a ∈ A | a is applicable in s} if A′ = ∅, then return failure
nondeterministically choose a ∈ A′ (i) s ← γ(s, a); π ← π.a
Algorithm 2.1: Forward-search planning schema.
For a planning problem P , a solution π is minimal if no subsequence of π is also a solution for P , shortest if there is no solution π′ such that |π′| < |π|, and cost-optimal (or just optimal, if it is clear from context) if
cost(π) = min{cost(π′) | π′ is a solution for P }.
Example 2.19. Let P = (Σ, s0, g), where Σ is the planning domain in Example 2.12 and Figure 2.3, s0 is as in Equation 2.4, and g = {loc(r1) = d3}. Let
π1 = 〈move(r1, d1, d3)〉;
π2 = 〈move(r2, d2, d3), move(r1, d1, d2), move(r2, d3, d1), move(r1, d2, d3)〉;
π3 = 〈load(r1, c1, c2, p1, d1), unload(r1, c1, c2, p1, d1), move(r1, d1, d3)〉.
Then π1 is a minimal, shortest, and cost-optimal solution for P ; π2 is a minimal solution but is neither shortest nor cost-optimal; and π3 is a solution but is neither minimal nor shortest nor cost-optimal.
2.2 Forward State-Space Search
Many planning algorithms work by searching forward from the initial state to try to construct a sequence of actions that reaches a goal state. Forwardsearch, Algorithm 2.1, is a procedural schema for a wide variety of such algorithms. In line (i), the nondeterministic choice is an abstraction that allows us to ignore the precise order in which the algorithm tries the alternative values of a (see Appendix A). We will use nondeterministic algorithms in many places in the book to discuss properties of all algorithms that search the same search space, irrespective of the order in which they visit the nodes.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


38 Chapter 2
Deterministic-Search(Σ, s0, g)
Frontier ← {(〈〉, s0)} // (〈〉, s0) is the initial node Expanded ← ∅ while Frontier 6= ∅ do
select a node ν = (π, s) ∈ Frontier (i) remove ν from Frontier and add it to Expanded if s satisfies g then (ii) return π Children ← {(π.a, γ(s, a)) | s satisfies pre(a)} prune (i.e., remove and discard) 0 or more nodes
from Children, Frontier and Expanded (iii) Frontier ← Frontier ∪ Children (iv) return failure
Algorithm 2.2: Deterministic-Search, a deterministic version of Forwardsearch.
Deterministic-Search, Algorithm 2.2, is a deterministic version of Forwardsearch. Frontier is a set of nodes that are candidates to be visited, and Expanded is a set of nodes that have already been visited. During each loop iteration, Deterministic-Search selects a node, generates its children, prunes some unpromising nodes, and updates Frontier to include the remaining children.
In the Deterministic-Search pseudocode, each node is written as a pair ν = (π, s), where π is a plan and s = γ(s0, π). However, in most implementations ν includes other information, for example, pointers to ν’s parent and possibly to its children, the value of cost(π) so that it will not need to be computed repeatedly, and the value of h(s) (see Equation 2.8 below). The “parent” pointers make it unnecessary to store π explicitly in ν; instead, ν typically contains only the last action of π, and the rest of π is computed when needed by following the “parent” pointers back to s0.
Many forward-search algorithms can be described as instances of Deterministic-Search by specifying how they select nodes in line (i) and prune nodes in line (iii). Presently we will discuss several such algorithms; but first, here are some basic terminology and concepts.
The initial or starting node is (〈〉, s0), that is, the empty plan and the initial state. The children of a node ν include all nodes (π.a, γ(s, a)) such that a is applicable in s. The successors or descendants of ν include all of ν’s children and, recursively, all of the children’s successors. The ancestors
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 2.2 39
of ν include all nodes ν′ such that ν is a successor of ν′. A path in the search space is any sequence of nodes 〈ν0, ν1, . . . , νn〉 such that each νi is a child of νi−1. The height of the search space is the length of the longest acyclic path that starts at the initial node. The depth of a node ν is the length of the path from the initial node to ν. The maximum branching factor is the maximum number of children of any node. To expand a node ν means to generate all of its children. Most forward-search planning algorithms attempt to find a solution without exploring the entire search space, which can be exponentially large.9 To make informed guesses about which parts of the search space are more likely to lead to solutions, node selection (line (i) of Deterministic-Search) often involves a heuristic function h : S → R that returns an estimate of the minimum cost of getting from s to a goal state:
h(s) ≈ h∗(s) = min{cost(π) | γ(s, π) satisfies g}. (2.8)
For information on how to compute such an h, see Section 2.3. If 0 ≤ h(s) ≤ h∗(s) for every s ∈ S, then h is said to be admissible. Notice that if h is admissible, then h(s) = 0 whenever s is a goal node. Given a node ν = (π, s), some forward-search algorithms will use h to compute an estimate f (ν) of the minimum cost of any solution plan that begins with π:
f (ν) = cost(π) + h(s) ≈ min{cost(π.π′) | γ(s0, π.π′) satisfies g}. (2.9)
If h is admissible, then f (ν) is a lower bound on the cost of every solution that begins with π. In many forward-search algorithms, the pruning step (line (iii) of Deterministic-Search) often includes a cycle-checking step:
remove from Children every node (π, s) that has an ancestor (π′, s′) such that s′ = s.
In classical planning problems (and any other planning problems where the state space is finite), cycle-checking guarantees that the search will always terminate.
2.2.1 Breadth-First Search
Breadth-first search can be written as an instance of Deterministic-Search in which the selection and pruning are done as follows:
9The worst-case computational complexity is expspace-equivalent (see Section 2.7), although the complexity of a specific planning domain usually is much less.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


40 Chapter 2
• Node selection. Select a node (π, s) ∈ Children that minimizes the length of π. As a tie-breaking rule if there are several such nodes, choose one that minimizes h(s).
• Pruning. Remove from Children and Frontier every node (π, s) such that Expanded contains a node (π′, s). This keeps the algorithm from expanding s more than once.
In classical planning problems, breadth-first search will always terminate and will return a solution if one exists. The solution will be shortest but not necessarily cost-optimal.
Because breadth-first search keeps only one path to each node, its worstcase memory requirement is O(|S|), where |S| is the number of nodes in the search space. Its worst-case running time is O(b|S|), where b is the maximum branching factor.
2.2.2 Depth-First Search
Although depth-first search (DFS) is usually written as a recursive algorithm, it can also be written as an instance of Deterministic-Search in which the node selection and pruning are done as follows:
• Node selection. Select a node (π, s) ∈ Children that maximizes the length of π. As a tie-breaking rule if there are several such nodes, choose one that minimizes h(s).
• Pruning. First do cycle-checking. Then, to eliminate nodes that the algorithm is done with, remove ν from Expanded if it has no children in Frontier ∪ Expanded, and do the same with each of ν’s ancestors until no more nodes are removed. This garbage-collection step corresponds to what happens when a recursive version of depth-first search returns from a recursive call.
In classical planning problems, depth-first search will always terminate and will return a solution if one exists, but the solution will not necessarily be shortest or cost-optimal. Because the garbage-collection step removes all nodes except for those along the current path, the worst-case memory requirement is only O(bl), where b is the maximum branching factor and l is the height of the state space. However, the worst-case running time is O(bl), which can be much worse than O(|S|) if there are many paths to each state in S.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 2.2 41
2.2.3 Hill Climbing
A hill climbing (or greedy) search is a depth-first search with no backtracking:
• Node selection. Select a node (π, s) ∈ Children that minimizes h(s).
• Pruning. First, do cycle-checking. Then assign Frontier ← ∅, so that line (iv) of Algorithm 2.2 will be the same as assigning Frontier ← Children.
The search follows a single path, and prunes all nodes not on that path. It is guaranteed to terminate on classical planning problems, but it is not guaranteed to return an optimal solution or even a solution at all. Its worst-case running time is O(bl) and its the worst-case memory requirement is O(l), where l is the height of the search space and b is the maximum branching factor.
2.2.4 Uniform-Cost Search
Like breadth-first search, uniform-cost (or least-cost first) search does not use a heuristic function. Unlike breadth-first search, it does node selection using the accumulated cost of each node:
• Node selection. Select a node (π, s) ∈ Children that minimizes cost(π).
• Pruning. Remove from Children and Frontier every node (π, s) such that Expanded contains a node (π′, s). In classical planning problems (and any other problems in which all costs are nonnegative), it can be proved that cost(π′) ≤ cost(π), so this step ensures that the algorithm only keeps the least costly path to each node.
In classical planning problems, the search is guaranteed to terminate and to return an optimal solution. Like breadth-first search, its worst-case running time and memory requirement are O(b|S|) and O(|S|), respectively.
2.2.5 A*
A* is similar to uniform-cost search, but uses a heuristic function:
• Node selection. Select a node ν ∈ Children that minimizes f (ν) (defined in Equation 2.9).
• Pruning. For each node (π, s) ∈ Children, if A* has more than one plan that goes to s, then keep only the least costly one. More specifically,
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


42 Chapter 2
let
Vs = {(π′, s′) ∈ Children ∪ Frontier ∪ Expanded | s′ = s};
and if Vs contains any nodes other than (π, s) itself, let (π′, s) be the one for which cost(π′) is smallest (if there is a tie, choose the oldest such node). For every node ν ∈ Vs other than (π′, s), remove ν and all of its descendants from Children, Frontier , and Expanded.
Here are some of A*’s properties:
• Termination, completeness, and optimality. On any classical planning problem, A* will terminate and return a solution if one exists; and if h is admissible, then this solution will be optimal.
• Epsilon-optimality. If h is -admissible (i.e., if there is an > 0 such that 0 ≤ h(s) ≤ h∗(s) + for every s ∈ S), then the solution returned by A* will be within of optimal [491].
• Monotonicity. If h(s) ≤ cost(γ(s, a)) + h(γ(s, a)) for every state s and applicable action a, then h is said to be monotone or consistent. In this case, f (ν) ≤ f (ν′) for every child ν′ of a node ν, from which it can be shown that A* will never prune any nodes from Expanded, and will expand no state more than once.
• Informedness. Let h1 and h2 be admissible heuristic functions such that h2 dominates h1, i.e., 0 ≤ h1(s) ≤ h2(s) ≤ h∗(s) for every s ∈ S.10 Then A* will never expand more nodes with h2 than with h1,11 and in most cases, it will expand fewer nodes with h2 than with h1.
A*’s primary drawback is its space requirement: it needs to store every state that it visits. Like uniform-cost search, A*’s worst-case running time and memory requirement are O(b|S|) and O(|S|). However, with a good heuristic function, A*’s running time and memory requirement are usually much smaller.
2.2.6 Depth-First Branch and Bound
Depth-first branch and bound (DFBB) is a modified version of depth-first search that uses a different termination test than the one in line (ii) of Algorithm 2.2. Instead of returning the first solution it finds, DFBB keeps
10Dominance has often been described by saying that “h2 is more informed than h1,” but that phrase is somewhat awkward because h2 always dominates itself. 11Here, we assume that A* always uses the same tie-breaking rule during node selection if two nodes have the same f -value.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 2.2 43
searching until Frontier is empty. DFBB maintains two variables π∗ and c∗, which are the least costly solution that has been found so far, and the cost of that solution. Each time DFBB finds a solution (line (ii) of DeterministicSearch), it does not return the solution but instead updates the values of π∗ and c∗. When Frontier is empty, if DFBB has found at least one solution then it returns π∗, and otherwise it returns failure. Node selection and pruning are the same as in depth-first search, but an additional pruning step occurs during node expansion: if the selected node ν has f (ν) ≥ c∗, DFBB discards ν rather than expanding it. If the first solution found by DFBB has a low cost, this can prune large parts of the search space. DFBB has the same termination, completeness, and optimality properties as A*. Because the only nodes stored by DFBB are the ones in the current path, its space requirement is usually much lower than A*’s. However, because it does not keep track of which states it has visited, it may regenerate each state many times if there are multiple paths to the state; hence its running time may be much worse than A*’s. In the worst case, its running time and memory requirement are O(bl) and O(bl), the same as for DFS.
2.2.7 Greedy Best-First Search
For classical planning problems where nonoptimal solutions are acceptable, the search algorithm that is used most frequently is Greedy Best-First Search (GBFS). It works as follows:
• Node selection. Select a node (π, s) ∈ Children that minimizes h(s).
• Pruning. Same as in A*.
Like hill climbing, GBFS continues to expand nodes along its current path as long as that path looks promising. But like A*, GBFS stores every state that it visits. Hence it can easily switch to a different path if the current path dead-ends or ceases to look promising (see Exercise 2.4). Like A*, GBFS’s worst-case running time and memory requirement are O(b|S|) and O(|S|). Unlike A*, GBFS is not guaranteed to return optimal solutions; but in most cases, it will explore far fewer paths than A* and find solutions much more quickly.
2.2.8 Iterative Deepening
There are several search algorithms that do forward-search but are not instances of Deterministic-Search. Several of these are iterative-deepening al
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


44 Chapter 2
gorithms, which gradually increase the depth of their search until they find a solution. The best known of these is iterative deepening search (IDS), which works as follows:
for k = 1 to ∞, do a depth-first search, backtracking at every node of depth k if the search found a solution, then return it if the search generated no nodes of depth k, then return failure
On classical planning problems, IDS has the same termination, completeness, and optimality properties as breadth-first search. Its primary advantage over breadth-first search is that its worst-case memory requirement is only O(bd), where d is the depth of the solution returned if there is one, or the height of the search space otherwise. If the number of nodes at each depth k grows exponentially with k, then IDS’s worst-case running time is O(bd), which can be substantially worse than breadth-first search if there are many paths to each state. A closely related algorithm, IDA*, uses a cost bound rather than a depth bound:
c←0 loop do a depth-first search, backtracking whenever f (ν) > c if the search found a solution, then return it if the search did not generate an f (ν) > c, then return failure c ← the smallest f (ν) > c where backtracking occurred
On classical planning problems, IDA*’s termination, completeness, and optimality properties are the same as those of A*. IDA*’s worst-case memory requirement is O(bl), where l is the height of the search space. If the number of nodes grows exponentially with c (which usually is true in classical planning problems but less likely to be true in nonclassical ones), then IDA*’s worst-case running time is O(bd), where d is the depth of the solution returned if there is one or the height of the search space otherwise. However, this is substantially worse than A* if there are many paths to each state.
2.2.9 Choosing a Forward-Search Algorithm
It is difficult to give any hard-and-fast rules for choosing among the forwardsearch algorithms presented here, but here are some rough guidelines. If a nonoptimal solution is acceptable, often the best choice is to develop a planning algorithm based on GBFS (e.g., [510, 613]). There are no
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 2.3 45
guarantees as to GBFS’s performance; but with a good heuristic function, it usually works quite well. If one needs a solution that is optimal (or within of optimal) and has a good heuristic function that is admissible (or -admissible), then an A*-like algorithm is a good choice if the state space is small enough that every node can be held in main memory. If the state space is too large to hold in main memory, then an algorithm such as DFBB or IDA* may be worth trying, but there may be problems with excessive running time. For integration of planning into acting, an important question is how to turn any of these algorithms into online algorithms. This is discussed further in Section 2.6.
2.3 Heuristic Functions
Recall from Equation 2.8 that a heuristic function is a function h that returns an estimate h(s) of the minimum cost h∗(s) of getting from the state s to a goal state and that h is admissible if 0 ≤ h(s) ≤ h∗(s) for every state s (from which it follows that h(s) = 0 whenever s is a goal node). The simplest possible heuristic function is h0(s) = 0 for every state s. It is admissible and trivial to compute but provides no useful information. We usually will want a heuristic function that provides a better estimate of h∗(s) (e.g., see the discussion of dominance at the end of Section 2.2.5). If a heuristic function can be computed in a polynomial amount of time and can provide an exponential reduction in the number of nodes examined by the planning algorithm, this makes the computational effort worthwhile. The best-known way of producing heuristic functions is relaxation. Given a planning domain Σ = (S, A, γ) and planning problem P = (Σ, s0, g), relaxing them means weakening some of the constraints that restrict what the states, actions, and plans are; restrict when an action or plan is applicable and what goals it achieves; and increase the costs of actions and plans. This produces a relaxed domain Σ′ = (S′, A′, γ′) and problem P ′ = (Σ′, s′0, g′)
having the following property: for every solution π for P , P ′ has a solution π′ such that cost′(π′) ≤ cost(π). Given an algorithm for solving planning problems in Σ′, we can use it to create a heuristic function for P that works as follows: given a state s ∈ S, solve (Σ′, s, g′) and return the cost of the solution. If the algorithm always finds optimal solutions, then the heuristic function will be admissible. Just as domain representations can be either domain-specific or domainindependent, so can heuristic functions. Here is an example of the former:
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


46 Chapter 2
Example 2.20. Let us represent the planning domain in Figure 2.1 as follows. The objects include a set of locations and a few numbers:
B = Locations ∪ Numbers;
Locations = {loc1, . . . , loc9};
Numbers = {1, . . . , 9}.
There is a rigid relation adjacent that includes every pair of locations that have a road between them, and rigid relations x and y that give each location’s x and y coordinates:
adjacent = {(loc0, loc1), (loc0, loc6), (loc1, loc0), (loc1, loc3), . . .};
x = {(loc0, 2), (loc1, 0), (loc2, 4), . . .};
y = {(loc0, 4), (loc1, 3), (loc2, 4), . . .}.
There is one state variable loc with Range(loc) = Locations, and 10 states:
si = {loc = loci}, i = 0, . . . , 9.
There is one action template:
move(l, m) pre: adjacent(l, m), loc = l eff: loc ← m cost: distance(l, m)
where Range(l) = Range(m) = Locations, and distance(l, m) is the Euclidean distance between l and m:
distance(l, m) = √(x(l) − x(m))2 + (y(l) − y(m))2.
Consider the planning problem (Σ, s0, s8). One possible heuristic function is the Euclidean distance from loc to the goal location,
h(s) = distance(s(loc), loc8),
which is the length of an optimal solution for a relaxed problem in which the actor is not constrained to follow roads. This is a lower bound on the length of every route that follows roads to get to loc8, so h is admissible.
It is possible to define a variety of domain-independent heuristic functions that can be used in any state-variable planning domain. In the following subsections, we describe several such heuristic functions, and illustrate each of them in the following example.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 2.3 47
d3
g:
d1
d3
d2
c1
s0: r1
r1 c1
Figure 2.4: Initial state and goal for Example 2.21.
Example 2.21. Figure 2.4 shows a planning problem P = (Σ, s0, g) in a planning domain Σ = (B, R, X, A) that is a simplified version of the one in Figure 2.3. B includes one robot, one container, three docks, no piles, and the constant nil:
B = Robots ∪ Docks ∪ Containers ∪ {nil};
Robots = {r1};
Docks = {d1, d2, d3};
Containers = {c1}.
There are no rigid relations, that is, R = ∅. There are two state variables, X = {cargo(r1), loc(c1)}, with
Range(cargo(r1)) = {c1, nil};
Range(loc(c1)) = {d1, d2, d3, r1}.
A contains three action templates:
load(r, c, l)
pre: cargo(r) = nil, loc(c) = l, loc(r) = l eff: cargo(r) ← c, loc(c) ← r cost: 1
unload(r, c, l)
pre: cargo(r) = c, loc(r) = l
eff: cargo(r) ← nil, loc(c) ← l cost: 1
move(r, d, e)
pre: loc(r) = d eff: loc(r) ← e cost: 1
The action templates’ parameters have the following ranges:
Range(c) = Containers; Range(d) = Range(e) = Docks; Range(l) = Locations; Range(r) = Robots.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


48 Chapter 2
P ’s initial state and goal are
s0 = {loc(r1) = d3, cargo(r1) = nil, loc(c1) = d1};
g = {loc(r1) = d3, loc(c1) = r1}.
Suppose we are running GBFS (see Section 2.2.7) on P . In s0, there are two applicable actions: a1 = move(r1, d3, d1) and a2 = move(r1, d3, d2). Let
s1 = γ(s0, a1) = {loc(r1) = d1, cargo(r1) = nil, loc(c1) = d1}; (2.10)
s2 = γ(s0, a2) = {loc(r1) = d2, cargo(r1) = nil, loc(c1) = d1}. (2.11)
In line (i) of Algorithm 2.2, GBFS chooses between a1 and a2 by evaluating h(s1) and h(s2). The following subsections describe several possibilities for what h might be.
2.3.1 Max-Cost and Additive Cost Heuristics
The max-cost of a set of literals g = {g1, . . . , gk} is defined recursively as the largest max-cost of each gi individually, where each gi’s max-cost is the minimum, over all actions that can produce gi, of the action’s cost plus the max-cost of its preconditions. Here are the equations:
∆max(s, g) = max
gi∈g ∆max(s, gi);
∆max(s, gi) =
{
0, if gi ∈ s,
min{∆max(s, a) | a ∈ A and gi ∈ eff(a)}, otherwise;
∆max(s, a) = cost(a) + ∆max(s, pre(a)).
In a planning problem P = (Σ, s0, g), the max-cost heuristic is
hmax(s) = ∆max(s, g).
As shown in the following example, the computation of hmax can be visualized as an And/Or search going backward from g. At the beginning of Section 2.3, we said that most heuristics are derived by relaxation. One way to describe hmax is that it is the cost of an optimal solution to a relaxed problem in which a goal (i.e., a set of literals such as g or the preconditions of an action) can be reached by achieving just one of the goal’s literals, namely, the one that is the most expensive to achieve.
Example 2.22. In Example 2.21, suppose GBFS’s heuristic function is hmax. Figure 2.5 shows the computation of hmax(s1) = 1 and hmax(s2) = 2. Because hmax(s1) < hmax(s2), GBFS will choose s1.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 2.3 49
true in s1
loc(r1)=d3 loc(c1)=r1
g = {loc(r1)=d3, loc(c1)=r1}
move(r1,d1,d3)
move(r1,d2,d3)
pre:
loc(r1)=d1
pre:
loc(r1)=d2
true in s1
...0
0
>0
min(1,(>1)) = 1
pre:
cargo(r1)=nil
true in s1
loc(r1)=d1
loc(c1)=d1
true in s1
0
0
load(r1,c1,d1)
0+1 = 1
0+1 = 1 (>0)+1 > 1
load(r1,c2,d2)
load(r1,c3,d3)
...
...
>0
>0
(>0) +1 > 1
(>0)+1 > 1
min(1,(>1),(>1)) = 1
max(0,0,0) = 0
hmax(s1) = Δmax(s1,g) = max(1,1) = 1
max
max
hmax(s2) = Δmax(s2,g) = max(1,2) = 2
move(r1,d2,d1)
loc(r1)=d3 loc(c1)=r1
g = {loc(r1)=d3, loc(c1)=r1}
move(r1,d1,d3)
move(r1,d2,d3)
pre:
loc(r1)=d1
pre:
loc(r1)=d2
...
true in s2 0
>0
0
min(1,(>1)) = 1
pre:
cargo(r1)=nil
true in s2
loc(r1)=d1
loc(c1)=d1
true in s2
0 0+1 = 1
load(r1,c1,d1)
1+1 = 2
(>0)+1 > 1 0+1 = 1
load(r1,c2,d2)
load(r1,c3,d3)
...
...
>1
>1
(>1) +1 > 2
(>1)+1 > 2
min(2,(>2),(>2)) = 2
max(0,1,0) = 1
pre:
loc(r1)=d2
true in s2
0
max
max
Figure 2.5: Computation of hmax(s1, g) and hmax(s2, g).
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


50 Chapter 2
true in s1
loc(r1)=d3 loc(c1)=r1
g = {loc(r1)=d3, loc(c1)=r1}
move(r1,d1,d3)
move(r1,d2,d3)
pre:
loc(r1)=d1
pre:
loc(r1)=d2
true in s1
...0
0
>0
min(1,(>1)) = 1
pre:
cargo(r1)=nil
true in s1
loc(r1)=d1
loc(c1)=d1
true in s1
0
0
load(r1,c1,d1)
0+1 = 1
0+1 = 1 (>0)+1 > 1
load(r1,c2,d2)
load(r1,c3,d3)
...
...
>0
>0
(>0) +1 > 1
(>0)+1 > 1
min(1,(>1),(>1)) = 1
0+0+0 = 0
hadd(s1) = Δadd(s1,g) = 1+1 = 2
add
add
hadd(s2) = Δadd(s2,g) = 1+2 = 3
loc(r1)=d3 loc(c1)=r1
g = {loc(r1)=d3, loc(c1)=r1}
move(r1,d1,d3)
move(r1,d2,d3)
min(1,(>1)) = 1
load(r1,c1,d1)
1+1 = 2
(>0)+1 > 1 0+1 = 1
load(r1,c2,d2)
load(r1,c3,d3)
...
...
>1
>1
(>1) +1 > 2
(>1)+1 > 2
min(2,(>2),(>2)) = 2
move(r1,d2,d1)
pre:
loc(r1)=d1
pre:
loc(r1)=d2
...
true in s2 0
>0
0
pre:
cargo(r1)=nil
true in s2
loc(r1)=d1
loc(c1)=d1
true in s2
0 0+1 = 1
0+1+0 = 1
pre:
loc(r1)=d2
true in s2
0
add
add
Figure 2.6: Computation of hadd(s1, g) and hadd(s2, g).
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 2.3 51
Although hmax is admissible, it is not very informative. A closely related heuristic, the additive cost heuristic, is not admissible but generally works better in practice. It is similar to hmax but adds the costs of each set of literals rather than taking their maximum. It is defined as
hadd(s) = ∆add(s, g),
where
∆add(s, g) =
∑
gi∈g
∆add(s, gi);
∆add(s, gi) =
{
0, if gi ∈ s,
min{∆add(s, a) | a ∈ A and gi ∈ eff(a)}, otherwise;
∆add(s, a) = cost(a) + ∆add(s, pre(a)).
As shown in the following example, the computation of hadd can be visualized as an And/Or search nearly identical to the one for hmax.
Example 2.23. In Example 2.21, suppose GBFS’s heuristic function is hadd. Figure 2.6 shows the computation of hadd(s1) = 2 and hadd(s2) = 3. Because hadd(s1) < hadd(s2), GBFS will choose s1. To see that hadd is not admissible, notice that if a single action a could achieve both loc(r1)=d3 and loc(c1)=r1, then hadd(g) would be higher than h∗(g), because hadd would count a’s cost twice.
Both hmax and hadd have the same time complexity. Their running time is nontrivial, but it is polynomial in |A|+∑
x∈X |Range(x)|, the total number of actions and ground atoms in the planning domain.
2.3.2 Delete-Relaxation Heuristics
Several heuristic functions are based on the notion of delete-relaxation, a problem relaxation in which applying an action never removes old atoms from a state, but simply adds new ones.12 If a state s includes an atom x = v and an applicable action a has an effect x ← w, then the delete-relaxed result of applying a will be a “state” γ+(s, a) that includes both x = v and x = w. We will make the following definitions:
12The hadd and hmax heuristics can also be explained in terms of delete-relaxation; see Section 2.7.9.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


52 Chapter 2
• A relaxed state (or r-state, for short) is any set sˆ of ground atoms such that every state variable x ∈ X is the target of at least one atom in sˆ. It follows that every state is also an r-state.
• A relaxed state sˆ r-satisfies a set of literals g if S contains a subset s ⊆ sˆ that satisfies g.
• An action a is r-applicable in an r-state sˆ if sˆ r-satisfies pre(a). In this case, the predicted r-state is
γ+(sˆ, a) = sˆ ∪ γ(s, a). (2.12)
• By extension, a plan π = 〈a1, . . . , an〉 is r-applicable in an r-state sˆ0 if there are r-states sˆ1, . . . , sˆn such that
sˆ1 = γ+(sˆ0, a1), sˆ2 = γ+(sˆ1, a2), . . . , sˆn = γ+(sˆn−1, an).
In this case, γ+(sˆ0, π) = sˆn.
• A plan π is a relaxed solution for a planning problem P = (Σ, s0, g) if γ+(s0, π) r-satisfies g. Thus the cost of the optimal relaxed solution is
∆+(s, g) = min{cost(π) | γ+(s, π) r-satisfies g}.
For a planning problem P = (Σ, s0, g), the optimal relaxed solution heuristic is
h+(s) = ∆+(s, g).
Example 2.24. Let P be the planning problem in Example 2.21. Let sˆ1 = γ+(s0, move(r1, d3, d1)) and sˆ2 = γ+(sˆ1, load(r1, c1, d1)). Then
sˆ1 = {loc(r1) = d1, loc(r1) = d3, cargo(r1) = nil, loc(c1) = d1};
sˆ2 = {loc(r1) = d1, loc(r1) = d3, cargo(r1) = nil, cargo(r1) = c1,
loc(c1) = d1, loc(c1) = r1}.
The r-state sˆ2 r-satisfies g, so the plan π = 〈move(r1, d3, d1), load(r1, c1, d1)〉 is a relaxed solution for P . No shorter plan is a relaxed solution for P , so h+(s) = ∆+(s0, g).
Because every ordinary solution for P is also a relaxed solution for P , it follows that h+(s) ≤ h∗(s) for every s. Thus h+ is admissible, so h+ can be used with algorithms such as A* to find an optimal solution for P . On the
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 2.3 53
HFF(Σ, s, g)
sˆ0 = s; A0 = ∅ for k = 1 by 1 until a subset of sˆk r-satisfies g do (i) Ak ← {all actions that are r-applicable in sˆk−1} sˆk ← γ+(sˆk−1, Ak)
if sˆk = sˆk−1 then // (Σ, s, g) has no solution (ii) return ∞ gˆk ← g
for i = k down to 1 do (iii) arbitrarily choose a minimal set of actions ˆai ⊆ Ai such that γ+(sˆi, ˆai) satisfies gˆi gˆi−1 ← (gˆi − eff(ˆai)) ∪ pre(ˆai)
πˆ ← 〈ˆa1, ˆa2, . . . , ˆak〉 (iv) return ∑{cost(a) | a is an action in πˆ}
Algorithm 2.3: HFF, an algorithm to compute the Fast-Forward heuristic.
other hand, h+ is expensive to compute: the problem of finding an optimal relaxed solution for a planning problem P is NP-hard [68].13 We now describe an approximation to h+ that is easier to compute. It is based on the fact that if A is a set of actions that are all r-applicable in a relaxed state sˆ, then they will produce the same predicted r-state regardless of the order in which they are applied. This r-state is
γ+(sˆ, A) = sˆ ∪
⋃
a∈A
eff(a). (2.13)
HFF, Algorithm 2.3, starts at an initial r-state sˆ0 = s, and uses Equation 2.13 to generate a sequence of successively larger r-states and sets of applicable actions,
sˆ0, A1, sˆ1, A2, sˆ2 . . . ,
until it generates an r-state that r-satisfies g. From this sequence, HFF extracts a relaxed solution and returns its cost. Line (ii) whether the sequence has converged to an r-state that does not r-satisfy g, in which case the planning problem is unsolvable. The Fast-Forward heuristic, hFF(s), is defined to be the value returned by HFF.14 The definition of hFF is ambiguous, because the returned value
13If we restrict P to be ground (see Section 2.7.1), then the problem is NP-complete. 14The name comes from the FF planner in which this heuristic was introduced; see Section 2.7.9.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


54 Chapter 2
from ŝ0:
Atoms in ŝ0: Actions in A1: Atoms in ŝ1:
loc(r1) = d1
loc(c1) = d1
cargo(r1) = nil
move(r1,d1,d3)
move(r1,d1,d2)
loc(r1) = d1
loc(c1) = d1
cargo(r1) = nil
loc(r1) = d3
loc(r1) = d2
load(r1,c1,d1)
cargo(r1) = c1
loc(c1) = r1
Figure 2.7: Computation of HFF(Σ, s1, g) = 2. The solid lines indicate the actions’ preconditions and effects. The elements of gˆ0, aˆ1, and gˆ1 are shown in boldface.
may vary depending on HFF’s choices of ˆak, ˆak−1, . . . , ˆa1 in the loop (iii). Furthermore, because there is no guarantee that these choices are the optimal ones, hFF is not admissible. As with hmax and hadd, the running time for HFF is polynomial in |A| +
∑
x∈X |Range(x)|, the number of actions and ground atoms in the planning domain.
Example 2.25. In Example 2.21, suppose GBFS’s heuristic function is hFF, as computed by HFF. To compute hFF(s1), HFF begins with sˆ0 = s1, and computes A1 and sˆ1 in the loop at line (i). Figure 2.7 illustrates the computation: the lines to the left of each action show which atoms in sˆ0 satisfy its preconditions, and the lines to the right of each action show which atoms in sˆ1 are its effects. For the loop at line (iii), HFF begins with gˆ1 = g and computes aˆ1 and gˆ0; these sets are shown in boldface in Figure 2.7. In line (iv), the relaxed solution is
πˆ = 〈ˆa1〉 = 〈{move(r1, d1, d3), load(r1, c1, d1)}〉.
Thus HFF returns hFF(s1) = cost(πˆ) = 2. Figure 2.8 is a similar illustration of HFF’s computation of hFF(s2). For the loop at line (i), HFF begins with sˆ0 = s2 and computes the sets A1, sˆ1, A2, and sˆ2. For the loop at line (iii), HFF begins with gˆ2 = g and computes ˆa2, gˆ1, aˆ1, and gˆ0, which are shown in boldface in Figure 2.8. In line (iv), the relaxed solution is
πˆ = 〈ˆa1, ˆa2〉 = 〈{move(r1, d2, d1)〉}, {〈move(r1, d1, d3), load(r1, c1, d1)}〉,
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 2.3 55
from ŝ0:
Atoms in ŝ2:
Actions in A2:
Actions in A1: Atoms in ŝ1:
Atoms in ŝ0:
loc(r1) = d2
loc(c1) = d1
cargo(r1) = nil
move(r1,d2,d3)
move(r1,d2,d1)
from ŝ1:
loc(r1) = d2
loc(c1) = d1
cargo(r1) = nil
loc(r1) = d3
loc(r1) = d1
move(r1,d1,d2)
move(r1,d3,d2)
move(r1,d1,d3)
move(r1,d2,d3)
move(r1,d2,d1)
move(r1,d3,d1)
load(r1,c1,d1)
loc(r1) = d2
loc(c1) = d1
cargo(r1) = nil
loc(r1) = d3
loc(r1) = d1
cargo(r1) = c1
loc(c1) = r1
Figure 2.8: Computation of HFF(Σ, s2, g) = 3. The solid lines indicate the actions’ preconditions and effects. The atoms and actions in each gˆi and ˆai are shown in boldface.
so HFF returns hFF(s2) = cost(πˆ) = 3. Thus hFF(s1) < hFF(s2), so GBFS will choose to expand s1 next.
The graph structures in Figures 2.7 and 2.8 are called relaxed planning graphs.
2.3.3 Landmark Heuristics
Let P = (Σ, s0, g) be a planning problem, and let φ = φ1 ∨ . . . ∨ φm be a disjunction of atoms. Then φ is a disjunctive landmark for P if every solution plan produces an intermediate state (i.e., a state other than s0 and g) in which φ is true. The problem of deciding whether an arbitrary φ is a disjunctive landmark is PSPACE-complete [281]. However, that is a worst-case result; many disjunctive landmarks can often be efficiently discovered by reasoning about relaxed planning graphs [281, 509]. One way to to do this is as follows. Let s be the current state, and g be the goal; but instead of requiring g to be a set of atoms, let it be a set g = {φ1, . . . , φk} such that each φi is a disjunction of one or more atoms. For each φi, let Ri = {every action whose effects include at least one of the atoms in φi}. Let from Ri every action a for which we can show (using a relaxed-planning-graph computation) that a’s preconditions cannot be achieved without Ri, and let Ni = {a1, a2, . . . , ak} be the remaining set of actions. If we pick a precondition pj of each aj in N , then φ′ = p1 ∨ . . . ∨ pk is a disjunctive landmark. To avoid a combinatorial explosion, we will not
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


56 Chapter 2
want to compute every such φ′; instead we will only compute landmarks consisting of no more than four atoms (the number 4 being more-or-less arbitrary). The computation can be done by calling RPG-landmark(s, φi) once for each φi, as follows:
RPG-landmark(s, φ) takes two arguments: a state s, and a disjunction φ of one or more atoms such that φ is false in s (i.e., s contains none of the atoms in φ). It performs the following steps:
1. Let Relevant = {every action whose effects include at least one member of φ}. Then achieving φ will require at least one of the actions in Relevant. If some action a ∈ Relevant has all of its preconditions satisfied in s, then 〈a〉 is a solution, and the only landmark is φ itself, so return φ.
2. Starting with s, and using only the actions in A \ Relevant (i.e., the actions that cannot achieve φ), construct a sequence of r-states and r-actions sˆ0, A1, sˆ1, A2, sˆ2, . . . as in the HFF algorithm. But instead of stopping when HFF does, keep going until an r-state sˆk is reached such that sˆk = sˆk−1. Then sˆk includes every atom that can be produced without using the actions in Relevant.
3. Let Necessary = {all actions in Relevant that are applicable in sˆk}. Then achieving φ will require at least one of the actions in Necessary. If Necessary = ∅ then φ cannot be achieved, so return failure.
4. Consider every disjunction of atoms φ′ = p1 ∨ . . . ∨ pm having the following properties: m ≤ 4 (as we noted earlier, this is an arbitrary limit to avoid a combinatorial explosion), every pi in φ′ is a precondition of at least one action in Necessary, every action in Necessary has exactly one of p1, . . . , pm as a precondition, and s0 contains none of p1, . . . , pm. Then none of the actions in Necessary will be applicable until φ′ is true, so φ′ is a disjunctive landmark.
5. For every landmark φ′ found in the previous step, recursively call RPG-landmark(s, φ′) to find additional landmarks.15 These landmarks precede φ′, that is, they must be achieved before φ′. Return every φ′ and all of the landmarks found in the recursive calls.
The simple landmark heuristic is
hsl(s) = the total number of landmarks found by the preceding algorithm.
15In implementations, this usually is done only if every atom in φ′ has the same type, for example, φ′ = loc(r1) = d1 ∨ loc(r2) = d1.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 2.4 57
Backward-search(Σ, s0, g0)
π ← 〈〉; g ← g0 (i) loop if s0 satisfies g then return π A′ ← {a ∈ A | a is relevant for g} if A′ = ∅ then return failure
nondeterministically choose a ∈ A′
g ← γ−1(g, a) (ii) π ← a.π (iii)
Algorithm 2.4: Backward-search planning schema. During each loop iteration, π is a plan that achieves g from any state that satisfies g.
Although the algorithm is more complicated than the HFF algorithm, its running time is still polynomial. Better landmark heuristics can be devised by doing additional computations to discover additional landmarks and by reasoning about the order in which to achieve the landmarks. We discuss this further in Section 2.7.9.
Example 2.26. As before, consider the planning problem in Example 2.21. To compute hsl(s1), we count the number of landmarks between s1 and g. If we start in s1, then every solution plan must include a state in which cargo(r1) = c1. We will skip the computational details, but this is the only landmark that the landmark computation will find for s1. Thus hsl(s1) = 1. If we start in state s2, then the landmark computation will find two landmarks: cargo(s1) = c1 as before, and loc(r1) = d1 (which was not a landmark for s1 because it was already true in s1). Thus hsl(s2) = 2.
2.4 Backward Search
Backward-search, Algorithm 2.4, does a state-space search backward from the goal. As with Forward-search, it is a nondeterministic algorithm that has many possible deterministic versions. The variables in the algorithm are as follows: π is the current partial solution, g′ is a set of literals representing all states from which π can achieve g, Solved is a set of literals representing all states from which a suffix of π can achieve g, and A′ is the set of all actions that are relevant for g′, as defined next. Informally, we will consider an action a to be relevant for achieving a goal g if a does not make any of the conditions in g false and makes at least
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


58 Chapter 2
one of them true. More formally:
Definition 2.27. Let g = {x1 = c1, . . . , xk = ck}, where each xi is a state variable and each ci is a constant. An action a is relevant for g if the following conditions hold:
• For at least one i ∈ {1, . . . , k}, effa contains xi ← ci.
• For i = 1, . . . , k, effa contains no assignment statement xi ← c′
i such
that c′
i 6= ci.
• For each xi that is not affected by a, pre(a) does not contain the precondition xi 6= ci, nor any precondition xi = c′
i such that c′
i 6= ci.
In line (ii) of Backward-search, γ−1(g, a) is called the regression of g through a. It is a set of conditions that is satisfied by every state s such that γ(s, a) satisfies g. It includes all of the literals in pre(a), and all literals in g that a does not achieve:
γ−1(g, a) = pre(a) ∪ {(xi, ci) ∈ g | a does not affect xi} (2.14)
We can incorporate loop-checking into Backward-search by inserting the following line after line (i):
Solved ← {g}
and adding these two lines after line (iii):
if g ∈ Solved then return failure Solved ← Solved ∪ {g}
We can make the loop-checking more powerful by replacing the preceding two lines with the following subsumption test:
if g ∈ Solved then return failure if ∃g′ ∈ Solved s.t. g′ ⊆ g then return failure
Here, Solved represents the set of all states that are “already solved,” that is, states from which π or one of π’s suffixes will achieve g0; and g′ represents the set of all states from which the plan a.π will achieve g0. If every state that a.π can solve is already solved, then it is useless to prepend a to π. For any solution that we can find this way, another branch of the search space will contain a shorter solution that omits a.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 2.5 59
Example 2.28. Suppose we augment Backward-search to incorporate loop checking and call it on the planning problem in Example 2.21. The first time through the loop,
g = {cargo(r1) = c1, loc(r1) = d3},
and there are three relevant actions: move(r1, d1, d3), move(r1, d2, d3), and load(r1, c1, d3). Suppose Backward-search’s nondeterministic choice is move(r1, d1, d3). Then in lines 7–10,
g ← γ−1(g, move(r1, d1, d3)) = {loc(r1) = d1, cargo(r1) = c1};
π ← 〈move(r1, d1, d3)〉;
Solved ← {{cargo(r1) = c1, loc(r1) = d3}, {loc(r1) = d1, cargo(r1) = c1}}.
In its second loop iteration, Backward-search chooses nondeterministically among three relevant actions in line 6: move(r1, d2, d1), move(r1, d3, d1), and load(r1, c1, d1). Let us consider two of these choices. If Backward-search chooses move(r1, d3, d1), then in lines 7–9,
g ← γ−1(g, move(r1, d3, d1)) = {loc(r1) = d3, cargo(r1) = c1};
π ← 〈move(r1, d3, d1), move(r1, d1, d3)〉;
g ∈ Solved, so Backward-search returns failure.
If Backward-search instead chooses load(r1, c1, d1), then in lines 7–10,
g ← γ−1(g, load(r1, c1, d1)) = {loc(r1) = d1, cargo(r1) = nil};
π ← 〈load(r1, c1, d1), move(r1, d1, d3)〉;
Solved ← {{cargo(r1) = c1, loc(r1) = d3}, {loc(r1) = d1, cargo(r1) = c1},
{loc(r1) = d1, cargo(r1) = nil}}.
Consequently, one of the possibilities in Backward-search’s third loop iteration is to set
π ← 〈move(r1, d1, d3), load(r1, c1, d1), move(r1, d1, d3)〉.
If Backward-search does this, then it will return π at the start of the fourth loop iteration.
To choose among actions in A, Backward-search can use many of the same heuristic functions described in Section 2.3, but with the following modification: rather than using them to estimate the cost of getting from the current state to the goal, what should be estimated is the cost of getting from s0 to γ−1(g, a).
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


60 Chapter 2
2.5 Plan-Space Search
Another approach to plan generation is to formulate planning as a constraint satisfaction problem and use constraint-satisfaction techniques to produce solutions that are more flexible than linear sequences of ground actions. For example, plans can be produced in which the actions are partially ordered, along with a guarantee that every total ordering that is compatible with this partial ordering will be a solution plan. Such flexibility allows some of the ordering decisions to be postponed until the plan is being executed, at which time the actor may have a better idea about which ordering will work best. Furthermore, the techniques are a first step toward planning concurrent execution of actions, a topic that we will develop further in Chapter 4.
2.5.1 Definitions and Algorithm
The PSP algorithm, which we will describe shortly, solves a planning problem by making repeated modifications to a “partial plan” in which the actions are partially ordered and partially instantiated, as defined here. A partially instantiated action is any instance of an action template. It may be either ground or unground. Informally, a partially ordered plan is a plan in which the actions are partially ordered. However, some additional complication is needed to make it possible (as it is in ordinary plans) for actions to occur more than once. The mathematical definition is as follows:
Definition 2.29. A partially ordered plan is a triple π = (V, E, act) in which V and E are the nodes and edges of an acyclic digraph, and each node v ∈ V contains an action act(v).16 The edges in E represent ordering constraints on the nodes in V , and we define v ≺ v′ if v 6= v′ and (V, E) contains a path from v to v′. A total ordering of π is any (ordinary) plan π′ = 〈act(v1), . . . , act(vn)〉 such that v1 ≺ v2 ≺ . . . ≺ vn and {v1, . . . , vn} = V.
A partially ordered solution for a planning problem P is a partially ordered plan π such that every total ordering of π is a solution for P .
Definition 2.30. A partial plan is a 4-tuple π = (V, E, act, C), where (V, E, act) is the same as in Definition 2.29 except that each action act(v)
16For readers familiar with partially ordered multisets [233], we essentially are defining a partially ordered plan to be a pomset in which act(.) is the labeling function.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 2.5 61
may be partially instantiated, and C is a set of constraints. Each constraint in C is either an inequality constraint or a causal link:
• An inequality constraint is an expression of the form y 6= z, where y and z may each be either a variable or a constant.
• A causal link is an expression v1
x=b
99K v2, where v1 and v2 are two nodes such that v1 ≺ v2, x = b is a precondition of act(v2), and x ← b is an effect of act(v1).
The purpose of a causal link is to designate act(v1) as the (partially instantiated) action that establishes act(v2)’s precondition x = b. Consequently, for every node such that v1 ≺ v3 ≺ v2, we will say that v3 violates the causal link if x is the target of one of act(v3)’s effects, even if the effect is x ← b.17
A partial plan π = (V, E, act, C) is inconsistent if (V, E) contains a cycle, C contains a self-contradictory inequality constraint (e.g., y 6= y) or a violated causal link, or an action act(v) has an illegal argument. Otherwise π is consistent.
Definition 2.31. If π = (V, E, act, C) is a consistent partial plan, then a refinement of π is any sequence ρ of the following modifications to π:
• Add an edge (v, v′) to E. This produces a partial plan (V, E′, act, C) in which v ≺ v′.
• Instantiate a variable x. This means replacing all occurrences of x with an object b ∈ Range(x) or a variable y with Range(y) ⊆ Range(x). This produces a partial plan (V, E, act′, C′), where C′ and act′ are the instances of C and act produced by replacing x.
• Add a constraint c. This produces a partial plan (V, E, act, C ∪ {c}).
• Add a new node v containing a partially instantiated action a. This produces a partial plan π′ = (V ′, E, act′, C), where V ′ = V ∪ {v} and act′ = act ∪ {(v, a)}.
A refinement ρ is feasible for π if it produces a consistent partial plan.
17The reason for calling this a violation even if the effect is x ← b is to ensure that PSP (Algorithm 2.5) performs a systematic search [411, 336], that is, it does not generate the same partial plan several times in different parts of the search space.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


62 Chapter 2
PSP(Σ, π) loop if Flaws(π) = ∅ then return π arbitrarily select f ∈ Flaws(π) (i) R ← {all feasible resolvers for f } if R = ∅ then return failure
nondeterministically choose ρ ∈ R (ii) π ← ρ(π) return π
Algorithm 2.5: PSP, plan-space planning.
2.5.2 Planning Algorithm
The PSP algorithm is Algorithm 2.5. Its arguments include a state-variable planning domain Σ = (B, R, X, A) and a partial plan π = (V, E, act, C) that represents a planning problem P = (Σ, s0, g). The initial value of π is as follows, where v0 and vg are nodes containing two dummy actions that PSP uses to represent the initial state and goal:
• V = {v0, vg} and E = {(v0, vg)},
• act(v0) is a dummy action a0 that has pre(a0) = ∅ and eff(a0) = s0.
• act(vg) is a dummy action ag that has pre(ag) = g and eff(ag) = ∅.
• C = ∅, that is, there are not (yet) any constraints.
The reason for calling a0 and ag “dummy actions” is that they look syntactically like actions but are not instances of action templates in A. Their sole purpose is to represent s0 and g in a way that is easy for PSP to work with. PSP repeatedly makes feasible refinements to π in an effort to produce a partially ordered solution for P . PSP does this by finding flaws (things that prevent π from being a solution to P ) and for each flaw applying a resolver (a refinement that removes the flaw). In PSP, Flaws(π) is the set of all flaws in π. There are two kinds of flaws: open goals and threats. These and their resolvers are described next.
Open goals. If a node v ∈ V has a precondition p ∈ pre(act(v)) for which there is no causal link, then p is an open goal. There are two kinds of resolvers for this flaw:
• Establish p using an action in π. Let v′ be any node of π such that
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 2.5 63
d1 d2
r1
r2 d1
d3
d2
r1 r2
r1
s0 g
Figure 2.9: Initial state and goal for Example 2.32.
v 6≺ v′. If act(v) has an effect e that can be unified with p (i.e., made syntactically identical to p by instantiating variables), then the following refinement is a resolver for p: instantiate variables if necessary
to unify p and e; add a causal link v′ e′
99K v (where e′ is the unified expression); and add (v′, v) to E unless v′ ≺ v already.
• Establish p by adding a new action. Let a′ be a standardization of an action template a ∈ A (i.e., a′ is a partially instantiated action produced by renaming the variables in a to prevent name conflicts with the variables already in π). If eff(a′) has an effect e that can be unified with p, then the following refinement is a resolver for p: add a new node v′ to V ; add (v′, a′) to act; instantiate variables if necessary
to unify p and e; add a causal link v′ e′
99K v; make v0 ≺ v′ by adding (v0, v′) to E; and add (v′, v) to E.
Threats. Let v1
x=b
99K v2 be any causal link in π, and v3 ∈ V be any node such that v2 6≺ v3 and v3 6≺ v1 (hence it is possible for v3 to come between v1 and v2). Suppose act(v3) has an effect y ← w that is unifiable with x, that is, π has an instance (here we extend Definition 2.9 to plans) in which both x and y are the same state variable). Then v3 is a threat to the causal link. There are three kinds of resolvers for such a threat:
• Make v3 ≺ v1, by adding (v3, v1) to E.
• Make v2 ≺ v3, by adding (v2, v3) to E.
• Prevent x and y from unifying, by adding to C an inequality constraint on their parameters.
Example 2.32. Figure 2.9 shows the initial state and goal for a simple planning problem in which there are two robots and three loading docks, that is, B = Robots ∪ Docks, where Robots = {r1, r2} and Docks = {d1, d2, d3}. There are no rigid relations. There is one action template,
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


64 Chapter 2
occupied(d1) = r1
loc(r2) = d1
loc(r1) = d2
loc(r2) = d2
loc(r1) = d1
occupied(d2) = r2
occupied(d3) = nil
ag
a0
Figure 2.10: The initial partial plan contains dummy actions a0 and ag that represent s0 and g. There are two flaws: ag’s two preconditions are open goals.
move(r, d, d′)
pre: loc(r) = d, occupied(d′) = F eff: loc(r) ← d′,
where r ∈ Robots and d, d′ ∈ Docks. The initial state and the goal (see Figure 2.9) are
s0 = {loc(r1) = d1, loc(r2) = d2, occupied(d1) = T,
occupied(d2) = T, occupied(d3) = F};
g = {loc(r1) = d2, loc(r2) = d1}.
Figure 2.10 shows the initial partial plan, and Figures 2.11 through 2.14 show successive snapshots of one of PSP’s nondeterministic execution traces. Each action’s preconditions are written above the action, and the effects are written below the action. Solid arrows represent edges in E, dashed arrows represent causal links, and dot-dashed arrows represent threats. The captions describe the refinements and how they affect the plan.
2.5.3 Search Heuristics
Several of the choices that PSP must make during its search are very similar to the choices that a backtracking search algorithm makes in order to solve constraint-satisfaction problems (CSPs); for example, see [517]. Consequently, some of the heuristics for guiding CSP algorithms can be translated into analogous heuristics for guiding PSP. For example:
• Flaw selection (line (i) of PSP) is not a nondeterministic choice, because all of the flaws must eventually be resolved, but the order in
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 2.5 65
a1!=!move(r1,d,d2)
occupied(d1)!=!r1
loc(r2)!=!d1
loc(r1)!=!d2
loc(r2)!=!d2
loc(r1)!=!d1
occupied(d2)!=!r2
occupied(d3)!=!nil
ag
a0
occupied(d)!=!nil occupied(d2)!=!r1
loc(r1)!=!d2
occupied(d2)!=!nil
loc(r1)!=!d
occupied(d')!=!nil occupied(d1)!=!r2
loc(r2)!=!d1
occupied(d1)!=!nil loc(r2)!=!d' a2!=!move(r2,d',d1)
Figure 2.11: Resolving ag’s open-goal flaws. For one of them, PSP adds a1 and a causal link. For the other, PSP adds a2 and another causal link.
a1!=!move(r1,d1,d2)
occupied(d1)!=!r1
loc(r2)!=!d1
loc(r1)!=!d2
loc(r2)!=!d2
loc(r1)!=!d1
occupied(d2)!=!r2
occupied(d3)!=!nil
ag
a0
occupied(d1)!=!nil occupied(d2)!=!r1
loc(r1)!=!d2
occupied(d2)!=!nil
loc(r1)!=!d1
a3!=!move(r,d2,d'')
occupied(d'')!=!nil
loc(r)!=!d''
occupied(d'')!=!r
occupied(d2)!=!nil
loc(r)!=!d2
a2!=!move(r2,d',d1)
occupied(d')!=!nil occupied(d1)!=!r2
loc(r2)!=!d1
occupied(d1)!=!nil loc(r2)!=!d'
Figure 2.12: Resolving a1’s open-goal flaws. For one of them, PSP substitutes d1 for d (which also resolves a1’s free-variable flaw) and adds a causal link from a0. For the other, PSP adds a3 and a causal link. The new action a3 causes two threats (shown as red dashed-dotted lines).
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


66 Chapter 2
a1!=!move(r1,d1,d2)
occupied(d1)!=!r1
loc(r2)!=!d1
loc(r1)!=!d2
loc(r2)!=!d2
loc(r1)!=!d1
occupied(d2)!=!r2
occupied(d3)!=!nil
occupied(d')!=!nil occupied(d1)!=!r2
loc(r2)!=!d1
occupied(d1)!=!nil loc(r2)!=!d'
ag
a0
occupied(d1)!=!nil occupied(d2)!=!r1
loc(r1)!=!d2
occupied(d2)!=!nil
loc(r1)!=!d1
a3!=!move(r2,d2,d')
occupied(d')!=!nil
loc(r2)!=!d'
occupied(d')!=!r2
occupied(d2)!=!nil
loc(r2)!=!d2
a2!=!move(r2,d',d1)
Figure 2.13: Resolving a2’s open-goal flaws. For one of them, PSP substitutes r2 for r and d′ for d′′, and adds a causal link from a3. For the other, PSP adds a causal link from a1. As a side effect, these changes resolve the two threats.
a1!=!move(r1,d1,d2)
occupied(d1)!=!r1
loc(r2)!=!d1
loc(r1)!=!d2
loc(r2)!=!d2
loc(r1)!=!d1
occupied(d2)!=!r2
occupied(d3)!=!nil
occupied(d3)!=!nil occupied(d1)!=!r2
loc(r2)!=!d1
occupied(d1)!=!nil loc(r2)!=!d3
ag
a0
occupied(d1)!=!nil occupied(d2)!=!r1
loc(r1)!=!d2
occupied(d2)!=!nil
loc(r1)!=!d1
a3!=!move(r2,d2,d3)
occupied(d3)!=!nil
loc(r2)!=!d3
occupied(d3)!=!r2
occupied(d2)!=!nil
loc(r2)!=!d2
a2!=!move(r2,d3,d1)
Figure 2.14: Resolving a3’s open-goal flaws. For one of them, PSP adds a causal link. For the other, PSP substitutes d3 for d′ and adds a causal link. The resulting partially ordered plan contains no flaws and hence solves the planning problem.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 2.5 67
which PSP selects the flaws can affect the size of the search space generated by PSP’s nondeterministic choices in line (ii). Flaw selection is analogous to variable ordering in CSPs, and the Minimum Remaining Values heuristic for CSPs (choose the variable with the fewest remaining values) is analogous to a PSP heuristic called Fewest Alternatives First: select the flaw with the fewest resolvers.
• Resolver selection (line (ii) of PSP) is analogous to value ordering in CSPs. The Least Constraining Value heuristic for CSPs (choose the value that rules out the fewest values for the other variables) translates into the following PSP heuristic: choose the resolver that rules out the fewest resolvers for the other flaws.
The preceding heuristic ignores an important difference between planspace planning and CSPs. Ordinarily, the number of variables in a CSP is fixed in advance, hence the search tree is finite and all solutions are at exactly the same depth. If one of PSP’s resolvers introduces a new action that has n new preconditions to achieve, this is like introducing n new variables (and a number of new constraints) into a CSP, which could make the CSP much harder to solve.
One way of adapting this heuristic to PSP is by first looking for resolvers that do not introduce open goals, and if there are several such resolvers, then to choose the one that rules out the fewest resolvers for the other flaws.
Although the preceding heuristics can help speed PSP’s search, implementations of PSP tend to run much more slowly than the fastest statespace planners. Generally the latter are GBFS algorithms that are guided by heuristics like the ones in Section 2.3, and there are several impediments to developing an analogous version of PSP. Because plan spaces have no explicit states, the heuristics in Section 2.3 are not directly applicable, nor is it clear how to develop similar plan-space heuristics. Even if such heuristics were available, a depth-first implementation of PSP would be problematic because plan spaces generally are infinite. Consequently, for solving problems like the ones in the International Planning Competitions [291], most automated-planning researchers have abandoned PSP in favor of forwardsearch algorithms.
On the other hand, some important algorithms for temporal planning (see Chapter 4) are extensions of PSP and are useful for maintaining flexibility of execution in unpredictable environments. An understanding of PSP is useful to provide the necessary background for understanding those
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


68 Chapter 2
go(r, l, l′)
pre: adjacent(l, l′), loc(r) = l eff: loc(r) ← l′
navigate(r, l, l′) pre: ¬adjacent(l, l′), loc(r) = l eff: loc(r) ← l′
take(r, l, o)
pre: loc(r) = l, pos(o) = l, cargo(r) = nil
eff: pos(o) ← r, cargo(r) ← o
Figure 2.15: Action templates for Example 2.33.
algorithms.
2.6 Incorporating Planning into an Actor
We now consider what is needed for actors to utilize the planning algorithms in this chapter. Because it is quite unlikely that the environment will satisfy all of the assumptions in Section 2.1.1, a planning domain will almost never be a fully accurate model of the actor’s environment. Hence if a planning algorithm predicts that a plan π will achieve a goal g, this does not ensure that π will achieve g when the actor performs the actions in π.
Example 2.33. To illustrate some of the things that can go wrong, suppose a robot, rbt, is trying to accomplish the task “bring o7 to loc2” near the top of Figure 1.2. To create an abstract plan for this task, suppose rbt calls a planner on a planning problem P = (Σ, s0, g) in which Σ contains the action templates shown in Figure 2.15, and
s0 = {loc(rbt) = loc3, pos(o7) = loc1, cargo(rbt) = nil},
g = {pos(o7) = loc2}.
The planner will return a solution plan π = 〈a1, a2, a3, a4, a5〉 in which the actions are slightly more detailed versions of the ones near the top of Figure 1.2:
a1 = go(rbt,loc3,hall), a2 = navigate(rbt,hall,loc1),
a3 = take(rbt,loc1,o7), a4 = navigate(rbt,loc1,loc2),
a5 = put(rbt,loc2,o7).
When rbt tries to perform π, several kinds of problems may occur:
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 2.6 69
1. Execution failures. Suppose rbt’s refinement of a1 involves opening a door, as in Figure 1.2. Then a1 will succeed if the lower-level actions work correctly or if there is a fixable problem (e.g., rbt’s gripper may slip on the doorknob, but rbt may be able to reposition its gripper and continue). However, if there is a problem that rbt cannot fix (e.g., the door is locked or broken), then a1 will fail, and rbt will need to revise π (e.g., by taking an alternate route to loc1).
2. Unexpected events. Suppose that once rbt finishes a1 and reaches the hallway, someone puts an object o6 onto rbt. Then a2 is still applicable, but a3 is not, because rbt can only hold one object at a time. Depending on what o6 is and why it was put there, some possible courses of action might be to remove o6 and then go to loc1, to take o6 to loc1 and remove it there, or to take o6 somewhere else before going to loc1.
3. Incorrect information. Suppose that when rbt tries to perform a2, a navigation error causes it to go to a different location, loc4. To recover, it will need to navigate from loc4 to loc1.
4. Partial information. Suppose loc1 is where o7 is normally stored, but rbt cannot observe whether o7 is there except by going there. Because state-variable representations assume that the current state is always fully known, a planner that uses this formalism cannot create a conditional plan such as
look for o7 in loc1; and if it’s not there then look for it in loc4.
As a work-around, if rbt thinks o7 is likely to be at loc1, then it could include pos(o7) = loc1 in s0 when calling the planner. If rbt reaches loc1 and o7 is not there, then rbt could call the planner with another guess for o7’s location; and so forth.
Alternatively, we might want to give rbt a planner that can create conditional plans or policies (see Chapters 5 and 6). But even then, situations can arise in which the planner did not plan for all of the possible contingencies because it did not know they were possible. Thus rbt may still need work-arounds such as that just described.
Consequently, actors need ways to change their plans when problems are detected. The following section describes some ways to do that.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


70 Chapter 2
Run-Lookahead(Σ, g)
while (s ← abstraction of observed state ξ) 6|= g do π ← Lookahead(Σ, s, g)
if π = failure then return failure
a ← pop-first-action(π); perform(a)
Algorithm 2.6: Run-Lookahead replans before every action.
Run-Lazy-Lookahead(Σ, g)
s ← abstraction of observed state ξ while s 6|= g do
π ← Lookahead(Σ, s, g)
if π = failure then return failure while π 6= 〈〉 and s 6|= g and Simulate(Σ, s, g, π) 6= failure do a ← pop-first-action(π); perform(a) s ← abstraction of observed state ξ
Algorithm 2.7: Run-Lazy-Lookahead replans only when necessary.
Run-Concurrent-Lookahead(Σ, g)
π ← 〈〉; s ← abstraction of observed state ξ thread 1: // threads 1 and 2 run concurrently loop
π ← Lookahead(Σ, s, g) thread 2: loop if s |= g then return success else if π = failure then return failure else if π 6= 〈〉 and Simulate(Σ, s, g, π) 6= failure then a ← pop-first-action(π); perform(a) s ← abstraction of observed state ξ
Algorithm 2.8: Run-Concurrent-Lookahead does acting and replanning concurrently.
2.6.1 Repeated Planning and Replanning
Algorithms 2.6 through 2.8 illustrate some ways for an actor to use a planner. In each of them, (Σ, s, g) is a planning problem, and Lookahead is an
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 2.6 71
online planning algorithm, that is, a planning algorithm that incorporates modifications (which we discuss in Section 2.6.2) to facilitate interaction between planning and acting. An important consequence of these modifications is that the plan returned by Lookahead is not guaranteed to solve (Σ, s, g). Ideally we might like it to be at least a partial solution, that is, a plan that can be extended to produce a solution—but even that cannot be guaranteed.
Recall from Section 1.3.2 that the planner’s initial state s is an abstraction that may differ from the actor’s current state ξ. It may omit parts of ξ that are irrelevant for planning and may include hypothesized values of state variables that the actor cannot currently observe, or it may be a hypothetical future state. Similarly, the goal g in Algorithms 2.6–2.8 is for planning purposes and may sometimes differ from what the actor ultimately wants to achieve. For example, it may be a subgoal (see Section 2.6.2).
In each algorithm, pop-first-action removes and returns the first action in π; and perform calls the actor’s acting component—which may execute the action if it is a command to the execution platform or else refine it into lower-level actions and commands.
Here are some comparisons among the procedures:
• Run-Lookahead is a simple version of the receding-horizon approach in Figure 1.4. Each time it calls Lookahead, it performs only the first action of the plan that Lookahead returned. This is useful, for example, in unpredictable or dynamic environments in which some of the states are likely to be different from what the planner predicted.
The biggest disadvantage of Run-Lookahead is that repeatedly waiting for Lookahead may be impractical if Lookahead has a large running time, and may be unnecessary if the action models are known to give very accurate predictions.
• Run-Lazy-Lookahead executes each plan π as far as possible, calling Lookahead again only when π ends or a plan simulator says that π will no longer work properly. This can be useful in environments where it is computationally expensive to call Lookahead and the actions in π are likely to produce the predicted outcomes.
Simulate is the plan simulator, which may use the planner’s prediction function γ or may do a more detailed computation (e.g., a physicsbased simulation) that would be too time-consuming for the planner to use. Simulate should return failure if its simulation indicates that π will not work properly – for example, if it finds that an action in π
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


72 Chapter 2
will have an unsatisfied precondition or if π is supposed to achieve g and the simulation indicates that it will not do so.
The biggest disadvantage of Run-Lazy-Lookahead is that sometimes it can be difficult to predict that replanning is needed without actually doing the replanning to find out. In such cases, Run-Lazy-Lookahead may fail to detect problems until it is too late to fix them easily. For example, in Example 2.33, suppose rbt uses Run-Lazy-Lookahead, and Lookahead returns the partial solution 〈a1, a2〉. In problem 2 of the example, rbt will take o6 to loc1 without considering whether to leave o6 in the hallway or take it elsewhere.
• Run-Concurrent-Lookahead is a receding-horizon procedure in which the acting and planning processes run concurrently. Each time an action is performed, the action comes from the most recent plan that Lookahead has provided. This avoids Run-Lookahead’s problem with waiting for Lookahead to return. Like Run-Lazy-Lookahead, it risks continuing with an old plan in situations where it might be better to wait for a new one, but the risk is lower because the plan is updated more frequently.
The foregoing procedures are not the only possibilities. For example, there are variants of Run-Lazy-Lookahead that maintain information [196] about which actions in π establish the preconditions of other actions in π. This information can be used to detect situations where an action can be removed from π because it is no longer needed, or where a specific part of π needs to be revised.
2.6.2 Online Planning
Most of the planning algorithms earlier in this chapter were designed to run off-line. We now discuss how to adapt them for use with the acting procedures in Section 2.6.1, which need to interact with planners that run online. The biggest issue is that the planning algorithms were designed to find plans that (according to the planner’s domain model) are complete (and in some cases, optimal) solutions to the planning problem. In online planning, the actor may need to start acting before such a plan can be found. Most of the planning algorithms presented earlier – especially the ones that use forward search – can be modified to end their search early and return the best “partial solution” that they have found, and we will now discuss several techniques for how to do that. The term partial solution is somewhat misleading because there is no
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 2.6 73
guarantee that the plan will actually lead to a goal. But neither can we guarantee that an actor will reach the goal if it uses a purported “complete solution plan.” As we discussed in Section 2.6.1, acting procedures may need to deal with a variety of problems that were not in the planner’s domain model.
Subgoaling. In each of the algorithms in the previous section, the goal g′ given to the planner does not have to be the actor’s ultimate goal g; instead it may be a subgoal. If g′ is a subgoal, then once it has been achieved, the actor may formulate its next subgoal and ask the planner to solve it. How to formulate these subgoals is somewhat problematic, but one can imagine several possible techniques. The elements of a compound goal g = {g1, . . . , gk} could be used as subgoals, if one can decide on a reasonable order in which to try to achieve them. Another possibility may be to compute an ordered set of landmarks and choose the earliest one as a subgoal. In practical applications, g′ usually is selected in a domain-specific manner. For example, subgoaling with short-term objectives such as “get to shelter” is used to plan actions for the computerized opponents in Killzone 2, a “first-person shooter” video game [585, 113]. The acting algorithm is similar to Run-Concurrent-Lookahead, and the planner is similar to the SeRPE algorithm that we discuss in Chapter 3. The actor runs the planner several times per second, and the planner generates plans that are typically about four or five actions long. The main purpose of the planner is to generate credible humanlike actions for the computerized opponents, and it would not work well to do more elaborate planning because the current state changes quickly as the game progresses.
Limited-horizon planning. Recall that in the receding-horizon technique, the interaction between the actor and planner is as depicted in Figure 1.4. Each time the actor calls Lookahead, the planner starts at the current state and searches until it either reaches a goal or exceeds some kind of limit, and then it returns the best solution or partial solution it has found. Several of the algorithms in this section can easily be modified to do that. Following are some examples. We can modify A* (Section 2.2.5) to return if the least costly node in Frontier has a cost that exceeds a limit cmax, by putting the following step immediately after line (i) of Algorithm 2.2:
if cost(π) + h(s) > cmax, then return π
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


74 Chapter 2
Here is a modified version of IDS (Section 2.2.8) that uses a depth limit,
kmax:
for k = 1 to kmax: do a depth-first search, backtracking at every node of depth k, and keeping track of which node ν = (π, s) at depth k has the lowest value f (ν) if the search finds a solution, then return it return π
Both A* and IDS can also be modified to use a time limit, by having them throw an exception when time runs out. When the exception is thrown, IDS would return the plan π mentioned in the preceding pseudocode, and A* would return the plan found in the node ν = (π, s) ∈ Frontier that minimizes f (ν).
Sampling. In a sampling search, the planner uses a modified version of hill climbing (see Section 2.2.3) in which the node selection is randomized. The choice can be purely random, or it can be weighted toward the actions in Actions that produce the best values for h(γ(s, a)), using techniques similar to the ones that we describe later in Section 6.4.4. The modified algorithm could do this several times to generate multiple solutions and either return the one that looks best or return the n best solutions so that the actor can evaluate them further. Such a technique is used in the UCT algorithm (Algorithm 6.20) in Chapter 6.
2.7 Discussion and Historical Remarks
2.7.1 Classical Domain Models
Classical representations. Problem representations based on state variables have long been used in control-system design [244, 528, 161] and operations research [559, 4, 285], but their use in automated-planning research came much later [29, 31, 215]. Instead, most automated-planning research has used representation and reasoning techniques derived from mathematical logic. This began with the early work on GPS [451] and the situation calculus [413] and continued with the STRIPS planning system [197] and the widely used classical 18 representations [460, 472, 384, 231, 517].
18These are also called STRIPS representations but are somewhat simpler than the representation used in the STRIPS planner [197].
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 2.7 75
In a classical representation, all atoms have the same syntax as our rigid relations. Each state s is represented as the set of all atoms that are true in s, hence any atom not in this set is false in s. Each planning operator (the classical counterpart of an action template) has preconditions and effects that are literals.
Example 2.34. Here is a classical representation of s0 in Equation 2.4:
s0 = {loc(r1, d1), loc(r2, d2), occupied(d1), occupied(d2), pile(c1, p1), pile(c2, p1), pile(c3, p2), pos(c1, c2), pos(c2, nil), pos(c3, nil), top(p1, c1), top(p2, c3), top(p3, nil)}.
Here is a classical planning operator corresponding to the load action template in Example 2.12:
load(r, c, c′, p, d)
pre: at(p, d), ¬cargo(r), loc(r, d), pos(c, c′), top(p, c)
eff: cargo(r), ¬pile(c, p), pile(c, nil), ¬pos(c, c′), pos(c, r), ¬top(p, c), top(p, c′)
The well-known PDDL planning language ([204, 216]) is based on a classical representation but incorporates a large number of extensions. Classical planning domains can be translated to state-variable planning domains, and vice versa, with at most a linear increase in size:
• Translating a classical planning operator into an action template involves converting each logical atom p(t1, . . . , tn) into a Boolean-valued state variable xp(t1, . . . , tn). This can be done by replacing each negative literal ¬p(t1, . . . , tn) with xp(t1, . . . , tn) = F, and each positive literal p(t1, . . . , tn) with xp(t1, . . . , tn) = T. This produces an action template that has the same numbers of parameters, preconditions, and effects as the classical operator.
• Translating an action template α into a classical planning operator involves converting each state-variable x(t1, . . . , tn) into a set of logical atoms
{px(t1, . . . , tn, v) | v ∈ Range(x(t1, . . . , tn)}.
The conversion can be done as follows. For each expression x(t1, . . . , tn) = v or x(t1, . . . , tn) 6= v in α’s preconditions, replace it with px(t1, . . . , tn, v) or ¬px(t1, . . . , tn, v), respectively. For each expression x(t1, . . . , tn) ← v′ in α’s effects, replace it with px(t1, ..., tn, v′),
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


76 Chapter 2
and also do the following. If α’s preconditions include px(t1, . . . , tn, v) for some v, then add to α a new effect ¬px(t1, . . . , tn, v). Otherwise, add to α a new parameter u, a new precondition px(t1, . . . , tn, u), and a new effect ¬px(t1, . . . , xn, u).
Note that the planning operator may have twice as many effects and parameters as the action template. The reason is that each state variable x(t1, . . . , tn) has only one value at a time, so the planning operator must ensure that px(t1, . . . , tn, v) is true for only one v at a time. In the state-variable representation, this happens automatically; but in the classical representation, asserting a new value requires explicitly deleting the old one.
The classical and state-variable representation schemes are expspaceequivalent [182, 231]. In both of them, the time needed to solve a classical planning problem may be exponential in the size of the problem description. We emphasize, however, that this is a worst-case result; most classical planning problems are considerably easier.
Ground representations. A classical representation is ground if it contains no unground atoms. With this restriction, the planning operators have no parameters; hence each planning operator represents just a single action. Ground classical representations usually are called propositional representations [105], because the ground atoms can be rewritten as propositional variables. Every classical representation can be translated into an equivalent propositional representation by replacing each planning operator with all of its ground instances (i.e., all of the actions that it represents), but this incurs a combinatorial explosion in the size of the representation. For the load operator in Example 2.34, if r, c, p, and d are the numbers of robots, containers, piles, and locations, then the the number of load actions represented by the operator is rc2pd. More generally, if a planning operator has p parameters and each parameter has v possible values, then there are vp ground instances. Each of them must be written explicitly, so the ground classical representation is larger by a multiplicative factor of vp.
A ground state-variable representation is one in which all of the state variables are ground. Each ground state variable can be rewritten as a state variable that has no arguments (like an ordinary mathematical variable) [31, 267, 510]. Every state-variable representation can be translated into an equivalent ground state-variable representation, with a combinatorial explo
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 2.7 77
sion like the one in the classical-to-propositional conversion. If an action template has p parameters and each parameter has v possible values, then the ground representation is larger by a factor of vp. The propositional and ground state-variable representation schemes are both pspace-equivalent [104, 30]. They can represent exactly the same set of planning problems as classical and state-variable representations; but as we just discussed, they may require exponentially more space to do so. This lowers the complexity class because computational complexity is expressed as a function of the size of the input. In a previous work [231, Section 2.5.4], we claimed that propositional and ground state-variable representations could each be converted into the other with at most a linear increase in size, but that claim was only partially correct. Propositional actions can be converted to state-variable actions with at most a linear increase in size, using a procedure similar to the one we used to convert planning operators to action templates. For converting in the reverse direction, the worst-case size increase is polynomial but superlinear.19 The literature contains several examples of cases in which the problem representation and the computation of heuristic functions can be done more easily with state variables than with propositions [268, 510]. Helmert [267, Section 1.3] advances a number of arguments for considering ground statevariable representations superior to propositional representations.
2.7.2 Generalized Domain Models
The state-variable representation in Section 2.1 can be generalized to let states be arbitrary data structures, and an action template’s preconditions, effects, and cost be arbitrary computable functions operating on those data structures. Analogous generalizations can be made to the classical representation by allowing a predicate’s arguments to be functional terms whose values are calculated procedurally rather than inferred logically (see Fox and Long [204]). Such generalizations can make the domain models applicable to a much larger variety of application domains.
19We believe it is a multiplicative factor between lg v and v, where v is the maximum size of any state variable’s range. The lower bound follows from the observation that if there are n state variables, then representing the states may require n lg v propositions, with commensurate increases in the size of the planning operators. The upper bound follows from the existence of a conversion procedure that replaces each action’s effect x(c1, . . . , cn) ← d with the following set of literals: {px(c1, . . . , cn, d)} ∪ {¬x(c1, . . . , cn, d′) | d′ ∈ Range(x(c1, . . . , cn)) \ {d}}.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


78 Chapter 2
With the preceding modifications, the forward-search algorithms in Section 2.2 will still work correctly [460, 357, 287], but they will not be able to use the domain-independent heuristic functions in Section 2.3, because those heuristics work by manipulating the syntactic elements of state-variable and classical representations. Instead, domain-specific heuristic functions will be needed. One way to generalize the action model while still allowing the use of domain-independent heuristics is to write each action as a combination of two parts – a “classical” part that uses a classical or state-variable representation and a “nonclassical” part that uses some other kind of representation – and write separate algorithms to reason about the classical and nonclassical parts. Ivankovic et al. [295] coordinate the two parts in a manner somewhat like planning with abstraction (see Section 2.7.6). Gregory et al. [246] use a “planning modulo theories” approach that builds on recent work on SAT modulo theories [456, 41]. The action models in Section 2.1.3 can also be generalized in several other ways, for example, to explicitly model the actions’ time requirements or to model uncertainty about the possible outcomes. Such generalizations are discussed in Chapters 4, 5, and 6.
2.7.3 Heuristic Search Algorithms
Heuristic functions that estimated the distance to the goal were first developed in the mid-1960s [450, 387, 160], and the A* algorithm was developed a few years later by Hart et al. [255, 256]. A huge amount of subsequent work has been done on A* and other heuristic search algorithms. Nilsson [460] and Russell and Norvig [517]20 give tutorial introductions to some of these algorithms, and Pearl [467] provides a comprehensive analysis of a large number of algorithms and techniques. Our definition of problem relaxation in Section 2.3 is based on Pearl’s. Branch-and-bound algorithms have been widely used in combinatorial optimization problems [373, 33, 425, 508]. DFBB (Section 2.2.6) is the bestknown version, but most forward-search algorithms (including, for example, A*) can be formulated as special cases of branch-and-bound [290, 356, 447]. Although some related ideas were explored much earlier by Pohl [491], the first version of GBFS that we know of is the algorithm that Russell and Norvig [517] called “greedy search.” We believe the name “greedy best-first search” was coined by Bonet and Geffner [82].
20The version of A* in Russell and Norvig [517] does not guarantee optimality unless h is monotone (see Section 2.2.5) because of a subtle flaw in its pruning rule.
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.


Section 2.7 79
Computer programs for games such as chess and checkers typically use an acting procedure similar to Run-Lookahead (Algorithm 2.8). In these programs, the Lookahead subroutine is similar to the time-limited version of depth-first iterative deepening in Section 2.6.2, except that the depth-first search is the well-known alpha-beta algorithm [338, 460, 517]. The IDA* algorithm in Section 2.2.8 is attributable to Korf [351]. Iterative-deepening algorithms are a special case of node-regeneration algorithms that retract nodes to save space and regenerate them later if they need to examine them again. There are several other search algorithms (e.g., the RBFS algorithm [353]) that do node regeneration in one way or another. Zhang [625] provides a survey of such algorithms.
2.7.4 Planning Graphs
A planning graph is similar to HFF’s relaxed planning graphs (see Figures 2.7 and 2.8), but it also includes various mutex (i.e., mutual exclusion) conditions: for example, two actions are mutex if they change the same state variable to different values. Rather than including all r-applicable actions, each Ak only includes the ones whose preconditions are not mutex in sˆk. Weld [598] gives a good tutorial account of this. Planning graphs were first used in Blum and Furst’s GraphPlan algorithm [74]. Graphplan does an iterative-deepening search to generate successively larger r-states. For each r-state sˆk such that the atoms of g are non-mutex in sˆk, GraphPlan uses a backward-search backtracking algorithm to look for a relaxed solution π such that the actions in each aˆi are nonmutex. Such a π is often called a parallel plan or layered plan, and it is a partially ordered solution (although not necessarily an optimal one). It can be proven that if a planning problem P has a solution, then a sufficiently large planning graph will contain a solution to P . Hence Graphplan is complete. Furthermore, because GraphPlan’s backward search is restricted to the planning graph, it usually can solve classical planning problems much faster than planners based on Backward-search or PSP [598]. GraphPlan inspired a large amount of follow-up research on planninggraph techniques. These can be classified roughly as follows. Some of them extend planning graphs in various nonclassical directions, such as conformant planning [548], sensing [600], temporal planning [549, 221, 395], resources [340, 341, 556], probabilities [73], soft constraints [422], and distributed planning [296]. Others modify the planning-graph techniques to obtain improved performance on classical-planning problems. Kautz and Selman’s BlackBox
Authors’ manuscript. Published by Cambridge University Press. Do not distribute.