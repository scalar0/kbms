Bernhard Korte • Jens Vygen
Combinatorial Optimization
Theory and Algorithms
Sixth Edition
123


Bernhard Korte Research Institute for Discrete Mathematics University of Bonn Bonn, Germany
Jens Vygen Research Institute for Discrete Mathematics University of Bonn Bonn, Germany
ISSN 0937-5511 ISSN 2197-6783 (electronic) Algorithms and Combinatorics ISBN 978-3-662-56038-9 ISBN 978-3-662-56039-6 (eBook) https://doi.org/10.1007/978-3-662-56039-6
Library of Congress Control Number: 2017958030
© Springer-Verlag GmbH Germany 2000, 2002, 2006, 2008, 2012, 2018


Preface to the Sixth Edition
After six years, it was again time for a new edition. Besides updates, new exercises, and a few corrections, it contains the following new material. Section 7.4 is devoted to shallow-light trees. Section 14.6 contains the recent two-factor approximation algorithm for submodular function maximization. Section 17.5 discusses the Nemhauser-Ullmann algorithm and smoothed analysis. In Section 20.3, we present the (ln 4 + )-factor approximation algorithm for the Steiner tree problem. Finally, Section 20.7 contains the VPN theorem. There are also small additions, e.g. on the integrality ratio in Section 5.1 and kernelization in Section 15.7. We would like to thank Maxim Babenko, Steffen Böhmer, Ulrich Brenner, György Dósa, Michael Etscheid, Jean Fonlupt, Michel Goemans, Stephan Held, Stefan Hougardy, Jochen Könemann, Solomon Lo, Jens Maßberg, Neil Olver, Dieter Rautenbach, Heiko Röglin, Jan Schneider, Sophie Spirkl, and Uri Zwick for feedback on the previous edition or proofreading new parts. We hope that, with this new edition, the book remains useful for research and teaching for many years to come.
Bonn, Germany Bernhard Korte September 2017 Jens Vygen


Preface to the Fifth Edition
When preparing the first edition of this book, more than 10 years ago, we tried to accomplish two objectives: it should be useful as an advanced graduate textbook but also as a reference work for research. With each new edition, we have to decide how the book can be improved further. Of course, it is less and less possible to describe the growing area comprehensively. If we included everything that we like, the book would grow beyond a single volume. Since the book is used for many courses, now even sometimes at undergraduate level, we thought that adding some classical material might be more useful than including a selection of the latest results. In this edition, we added a proof of Cayley’s formula, more details on blocking flows, the new faster b-matching separation algorithm, an approximation scheme for multidimensional knapsack, and results concerning the multicommodity max-flow min-cut ratio and the sparsest cut problem. There are further small improvements in numerous places and more than 60 new exercises. Of course, we also updated the references to point to the most recent results and corrected some minor errors that were discovered. We would like to thank Takao Asano, Maxim Babenko, Ulrich Brenner, Benjamin Bolten, Christoph Buchheim, Jean Fonlupt, András Frank, Michael Gester, Stephan Held, Stefan Hougardy, Hiroshi Iida, Klaus Jansen, Alexander Karzanov, Levin Keller, Alexander Kleff, Niko Klewinghaus, Stefan Knauf, Barbara Langfeld, Jens Maßberg, Marc Pfetsch, Klaus Radke, Rabe von Randow, Tomás Salles, Jan Schneider, Christian Schulte, András Sebo ̋, Martin Skutella, Jácint Szabó, and Simon Wedeking for valuable feedback on the previous edition. We are pleased that this book has been received so well, and further translations are on their way. Editions in Japanese, French, Italian, German, Russian, and Chinese have appeared since 2009 or are scheduled to appear soon. We hope that our book will continue to serve its purpose in teaching and research in combinatorial optimization.
Bonn, Germany Bernhard Korte September 2011 Jens Vygen


Preface to the Fourth Edition
With four English editions, and translations into four other languages forthcoming, we are very happy with the development of our book. Again, we have revised, updated, and significantly extended it for this fourth edition. We have added some classical material that may have been missed so far, in particular on linear programming, the network simplex algorithm, and the max-cut problem. We have also added a number of new exercises and up-to-date references. We hope that these changes serve to make our book an even better basis for teaching and research. We gratefully acknowledge the continuous support of the Union of the German Academies of Sciences and Humanities and the North Rhine-Westphalian (NRW) Academy of Sciences via the long-term research project “Discrete Mathematics and Its Applications.” We also thank those who gave us feedback on the third edition, in particular Takao Asano, Christoph Bartoschek, Bert Besser, Ulrich Brenner, Jean Fonlupt, Satoru Fujishige, Marek Karpinski, Jens Maßberg, Denis Naddef, Sven Peyer, Klaus Radke, Rabe von Randow, Dieter Rautenbach, Martin Skutella, Markus Struzyna, Jürgen Werber, Minyi Yue, and Guochuan Zhang, for their valuable comments. At http://www.or.uni-bonn.de/∼vygen/co.html, we will continue to maintain updated information about this book.
Bonn, Germany Bernhard Korte August 2007 Jens Vygen


Preface to the Third Edition
After 5 years, it was time for a thoroughly revised and substantially extended edition. The most significant feature is a completely new chapter on facility location. No constant-factor approximation algorithms were known for this important class of NP-hard problems until 8 years ago. Today there are several interesting and very different techniques that lead to good approximation guarantees, which makes this area particularly appealing also for teaching. In fact, the chapter has arisen from a special course on facility location. Many of the other chapters have also been extended significantly. The new material includes Fibonacci heaps, Fujishige’s new maximum flow algorithm, flows over time, Schrijver’s algorithm for submodular function minimization, and the Robins-Zelikovsky Steiner tree approximation algorithm. Several proofs have been streamlined, and many new exercises and references have been added. We thank those who gave us feedback on the second edition, in particular Takao Asano, Yasuhito Asano, Ulrich Brenner, Stephan Held, Tomio Hirata, Dirk Müller, Kazuo Murota, Dieter Rautenbach, Martin Skutella, Markus Struzyna, and Jürgen Werber, for their valuable comments. Eminently, Takao Asano’s notes and Jürgen Werber’s proofreading of Chapter 22 helped to improve the presentation at various places. Again we would like to mention the Union of the German Academies of Sciences and Humanities and the North Rhine-Westphalian Academy of Sciences. Their continuous support via the long-term project “Discrete Mathematics and Its Applications” funded by the German Federal Ministry of Education and Research and the state of North Rhine-Westphalia is gratefully acknowledged.
Bonn, Germany Bernhard Korte May 2005 Jens Vygen


Preface to the Second Edition
It was more than a surprise to us that the first edition of this book already went out of print about a year after its first appearance. We were flattered by the many positive and even enthusiastic comments and letters from colleagues and the general readership. Several of our colleagues helped us in finding typographical and other errors. In particular, we thank Ulrich Brenner, András Frank, Bernd Gärtner, and Rolf Möhring. Of course, all errors detected so far have been corrected in this second edition, and references have been updated. Moreover, the first preface had a flaw. We listed all individuals who helped us in preparing this book. But we forgot to mention the institutional support, for which we make amends here. It is evident that a book project which took 7 years benefited from many different grants. We would like to mention explicitly the bilateral Hungarian-German Research Project, sponsored by the Hungarian Academy of Sciences and the Deutsche Forschungsgemeinschaft, two Sonderforschungsbereiche (special research units) of the Deutsche Forschungsgemeinschaft, the Ministère Français de la Recherche et de la Technologie and the Alexander von Humboldt Foundation for support via the Prix Alexandre de Humboldt, and the Commission of the European Communities for participation in two projects of DONET. Our most sincere thanks go to the Union of the German Academies of Sciences and Humanities and to the North Rhine-Westphalian Academy of Sciences. Their long-term project “Discrete Mathematics and Its Applications” supported by the German Federal Ministry of Education and Research (BMBF) and the state of North Rhine-Westphalia was of decisive importance for this book.
Bonn, Germany Bernhard Korte October 2001 Jens Vygen


Preface to the First Edition
Combinatorial optimization is one of the youngest and most active areas of discrete mathematics and is probably its driving force today. It became a subject in its own right about 50 years ago. This book describes the most important ideas, theoretical results, and algorithms in combinatorial optimization. We have conceived it as an advanced graduate text which can also be used as an up-to-date reference work for current research. The book includes the essential fundamentals of graph theory, linear and integer programming, and complexity theory. It covers classical topics in combinatorial optimization as well as very recent ones. The emphasis is on theoretical results and algorithms with provably good performance. Applications and heuristics are mentioned only occasionally. Combinatorial optimization has its roots in combinatorics, operations research, and theoretical computer science. A main motivation is that thousands of real-life problems can be formulated as abstract combinatorial optimization problems. We focus on the detailed study of classical problems which occur in many different contexts, together with the underlying theory. Most combinatorial optimization problems can be formulated naturally in terms of graphs and as (integer) linear programs. Therefore this book starts, after an introduction, by reviewing basic graph theory and proving those results in linear and integer programming which are most relevant for combinatorial optimization. Next, the classical topics in combinatorial optimization are studied: minimum spanning trees, shortest paths, network flows, matchings, and matroids. Most of the problems discussed in Chapters 6–14 have polynomial-time (“efficient”) algorithms, while most of the problems studied in Chapters 15–21 are NP-hard, i.e., a polynomial-time algorithm is unlikely to exist. In many cases, one can at least find approximation algorithms that have a certain performance guarantee. We also mention some other strategies for coping with such “hard” problems. This book goes beyond the scope of a normal textbook on combinatorial optimization in various aspects. For example, we cover the equivalence of optimization and separation (for full-dimensional polytopes), O(n3)-implementations of matching algorithms based on ear decompositions, Turing machines, the perfect graph theorem, MAXSNP-hardness, the Karmarkar-Karp algorithm for bin packing, recent approximation algorithms for multicommodity flows, survivable network design,


and the Euclidean traveling salesman problem. All results are accompanied by detailed proofs. Of course, no book on combinatorial optimization can be absolutely comprehensive. Examples of topics which we mention only briefly or do not cover at all are tree decompositions, separators, submodular flows, path matchings, delta-matroids, the matroid parity problem, location and scheduling problems, nonlinear problems, semidefinite programming, the average-case analysis of algorithms, advanced data structures, parallel and randomized algorithms, and the theory of probabilistically checkable proofs (we cite the PCP theorem without proof). At the end of each chapter, there are a number of exercises containing additional results and applications of the material in that chapter. Some exercises which might be more difficult are marked with an asterisk. Each chapter ends with a list of references, including texts recommended for further reading. This book arose from several courses on combinatorial optimization and from special classes on topics like polyhedral combinatorics or approximation algorithms. Thus, material for basic and advanced courses can be selected from this book. We have benefited from discussions and suggestions of many colleagues and friends and – of course – from other texts on this subject. Especially we owe sincere thanks to András Frank, László Lovász, András Recski, Alexander Schrijver, and Zoltán Szigeti. Our colleagues and students in Bonn, Christoph Albrecht, Ursula Bünnagel, Thomas Emden-Weinert, Mathias Hauptmann, Sven Peyer, Rabe von Randow, André Rohe, Martin Thimm, and Jürgen Werber, have carefully read several versions of the manuscript and helped to improve it. Last but not least, we thank Springer-Verlag for the most efficient cooperation.
Bonn, Germany Bernhard Korte January 2000 Jens Vygen


1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.1 Enumeration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.2 Running Time of Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.3 Linear Optimization Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.4 Sorting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
2 Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.1 Basic Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.2 Trees, Circuits, and Cuts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 2.3 Connectivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 2.4 Eulerian and Bipartite Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 2.5 Planarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 2.6 Planar Duality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
3 Linear Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 3.1 Polyhedra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 3.2 The Simplex Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 3.3 Implementation of the Simplex Algorithm . . . . . . . . . . . . . . . . . . . . 62 3.4 Duality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 3.5 Convex Hulls and Polytopes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
4 Linear Programming Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 4.1 Size of Vertices and Faces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 4.2 Continued Fractions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78 4.3 Gaussian Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 4.4 The Ellipsoid Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 4.5 Khachiyan’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
Contents


4.6 Separation and Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
5 Integer Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 5.1 The Integer Hull of a Polyhedron . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 5.2 Unimodular Transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109 5.3 Total Dual Integrality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 5.4 Totally Unimodular Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 5.5 Cutting Planes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120 5.6 Lagrangean Relaxation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
6 Spanning Trees and Arborescences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 6.1 Minimum Spanning Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134 6.2 Minimum Weight Arborescences . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140 6.3 Polyhedral Descriptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144 6.4 Packing Spanning Trees and Arborescences . . . . . . . . . . . . . . . . . . . 147 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
7 Shortest Paths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159 7.1 Shortest Paths From One Source . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160 7.2 Shortest Paths Between All Pairs of Vertices . . . . . . . . . . . . . . . . . . 165 7.3 Minimum Mean Cycles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167 7.4 Shallow-Light Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
8 Network Flows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177 8.1 Max-Flow-Min-Cut Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178 8.2 Menger’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182 8.3 The Edmonds-Karp Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184 8.4 Dinic’s, Karzanov’s, and Fujishige’s Algorithm . . . . . . . . . . . . . . . . . 186 8.5 The Goldberg-Tarjan Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190 8.6 Gomory-Hu Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194 8.7 The Minimum Capacity of a Cut in an Undirected Graph . . . . . . . 201 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210
9 Minimum Cost Flows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215 9.1 Problem Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215 9.2 An Optimality Criterion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218 9.3 Minimum Mean Cycle-Cancelling Algorithm . . . . . . . . . . . . . . . . . . 220 9.4 Successive Shortest Path Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 223 9.5 Orlin’s Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227


9.6 The Network Simplex Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232 9.7 Flows Over Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
10 Maximum Matchings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245 10.1 Bipartite Matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246 10.2 The Tutte Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248 10.3 Tutte’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250 10.4 Ear-Decompositions of Factor-Critical Graphs . . . . . . . . . . . . . . . . . 253 10.5 Edmonds’ Matching Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
11 Weighted Matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277 11.1 The Assignment Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278 11.2 Outline of the Weighted Matching Algorithm . . . . . . . . . . . . . . . . . 280 11.3 Implementation of the Weighted Matching Algorithm . . . . . . . . . . 283 11.4 Postoptimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296 11.5 The Matching Polytope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302
12 b-Matchings and T-Joins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305 12.1 b-Matchings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305 12.2 Minimum Weight T -Joins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309 12.3 T -Joins and T -Cuts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313 12.4 The Padberg-Rao Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
13 Matroids . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325 13.1 Independence Systems and Matroids . . . . . . . . . . . . . . . . . . . . . . . . . 325 13.2 Other Matroid Axioms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329 13.3 Duality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334 13.4 The Greedy Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337 13.5 Matroid Intersection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343 13.6 Matroid Partitioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347 13.7 Weighted Matroid Intersection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355
14 Generalizations of Matroids . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359 14.1 Greedoids . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359 14.2 Polymatroids . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363 14.3 Minimizing Submodular Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . 367 14.4 Schrijver’s Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370


14.5 Symmetric Submodular Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . 373 14.6 Submodular Function Maximization . . . . . . . . . . . . . . . . . . . . . . . . . . 375 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382
15 NP-Completeness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385 15.1 Turing Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386 15.2 Church’s Thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388 15.3 P and NP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393 15.4 Cook’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397 15.5 Some Basic NP-Complete Problems . . . . . . . . . . . . . . . . . . . . . . . . . . 401 15.6 The Class coNP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408 15.7 NP-Hard Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 410 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419
16 Approximation Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423 16.1 Set Covering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424 16.2 The Max-Cut Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429 16.3 Colouring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435 16.4 Approximation Schemes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 443 16.5 Maximum Satisfiability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446 16.6 The PCP Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451 16.7 L-Reductions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 464
17 The Knapsack Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 471 17.1 Fractional Knapsack and Weighted Median Problem . . . . . . . . . . . 471 17.2 A Pseudopolynomial Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 474 17.3 A Fully Polynomial Approximation Scheme . . . . . . . . . . . . . . . . . . 476 17.4 Multi-Dimensional Knapsack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 479 17.5 The Nemhauser-Ullmann Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 481 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 484 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485
18 Bin-Packing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489 18.1 Greedy Heuristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 490 18.2 An Asymptotic Approximation Scheme . . . . . . . . . . . . . . . . . . . . . . 495 18.3 The Karmarkar-Karp Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 499 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 502 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 505
19 Multicommodity Flows and Edge-Disjoint Paths . . . . . . . . . . . . . . . . . 509 19.1 Multicommodity Flows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 510 19.2 Algorithms for Multicommodity Flows . . . . . . . . . . . . . . . . . . . . . . . 515 19.3 Sparsest Cut and Max-Flow Min-Cut Ratio . . . . . . . . . . . . . . . . . . . 519


19.4 The Leighton-Rao Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 521 19.5 Directed Edge-Disjoint Paths Problem . . . . . . . . . . . . . . . . . . . . . . . . 524 19.6 Undirected Edge-Disjoint Paths Problem . . . . . . . . . . . . . . . . . . . . . . 527 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 533 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 537
20 Network Design Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 543 20.1 Steiner Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 544 20.2 The Robins-Zelikovsky Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 549 20.3 Rounding the Directed Component LP . . . . . . . . . . . . . . . . . . . . . . . 555 20.4 Survivable Network Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561 20.5 A Primal-Dual Approximation Algorithm . . . . . . . . . . . . . . . . . . . . . 564 20.6 Jain’s Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 572 20.7 The VPN Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 578 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 581 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 585
21 The Traveling Salesman Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 591 21.1 Approximation Algorithms for the TSP . . . . . . . . . . . . . . . . . . . . . . 591 21.2 Euclidean TSP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 596 21.3 Local Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603 21.4 The Traveling Salesman Polytope . . . . . . . . . . . . . . . . . . . . . . . . . . . . 610 21.5 Lower Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 616 21.6 Branch-and-Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 618 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 621 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624
22 Facility Location . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 629 22.1 The Uncapacitated Facility Location Problem . . . . . . . . . . . . . . . . . 629 22.2 Rounding Linear Programming Solutions . . . . . . . . . . . . . . . . . . . . . 631 22.3 Primal-Dual Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 633 22.4 Scaling and Greedy Augmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 639 22.5 Bounding the Number of Facilities . . . . . . . . . . . . . . . . . . . . . . . . . . . 642 22.6 Local Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 645 22.7 Capacitated Facility Location Problems . . . . . . . . . . . . . . . . . . . . . . . 651 22.8 Universal Facility Location . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 654 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 661 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 662
Notation Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 667
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 671
Subject Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 683


1 Introduction
Let us start with two examples. A company has a machine which drills holes into printed circuit boards. Since it produces many of these boards it wants the machine to complete one board as fast as possible. We cannot optimize the drilling time but we can try to minimize the time the machine needs to move from one point to another. Usually drilling machines can move in two directions: the table moves horizontally while the drilling arm moves vertically. Since both movements can be done simultaneously, the time needed to adjust the machine from one position to another is proportional to the maximum of the horizontal and the vertical distance. This is often called the ∞distance. (Older machines can only move either horizontally or vertically at a time; in this case the adjusting time is proportional to the 1-distance, the sum of the horizontal and the vertical distance.) An optimum drilling path is given by an ordering of the hole positions
p1, . . . , pn such that ∑n−1
i=1 d( pi , pi+1) is minimum, where d is the ∞-distance:
for two points p = (x, y) and p′ = (x′, y′) in the plane we write d( p, p′) := max{|x − x ′|, |y − y′|}. An order of the holes can be represented by a permutation, i.e. a bijection π : {1, . . . , n} → {1, . . . , n}. Which permutation is best of course depends on the hole positions; for each list of hole positions we have a different problem instance. We say that one instance of our problem is a list of points in the plane, i.e. the coordinates of the holes to be drilled. Then the problem can be stated formally as follows:
DRILLING PROBLEM
Instance: A set of points p1, . . . , pn ∈ R2.
Task: Find a permutation π : {1, . . . , n} → {1, . . . , n} such that
∑n−1
i=1 d( pπ(i), pπ(i+1)) is minimum.
We now explain our second example. We have a set of jobs to be done, each having a specified processing time. Each job can be done by a subset of the employees, and we assume that all employees who can do a job are equally efficient. Several employees can contribute to the same job at the same time, and one employee can contribute to several jobs (but not at the same time). The objective is to get all jobs done as early as possible.


In this model it suffices to prescribe for each employee how long he or she should work on which job. The order in which the employees carry out their jobs is not important, since the time when all jobs are done obviously depends only on the maximum total working time we have assigned to one employee. Hence we have to solve the following problem:
JOB ASSIGNMENT PROBLEM
Instance: A set of numbers t1, . . . , tn ∈ R+ (the processing times for n jobs), a number m ∈ N of employees, and a nonempty subset Si ⊆ {1, . . . , m} of employees for each job i ∈ {1, . . . , n}.
Task: Find numbers xij ∈ R+ for all i = 1, . . . , n and j ∈ Si such that
∑
j ∈Si xi j = ti for i = 1, . . . , n and max j ∈{1,...,m}
∑
i: j ∈Si xi j is
minimum.
These are two typical problems arising in combinatorial optimization. How to model a practical problem as an abstract combinatorial optimization problem is not described in this book; indeed there is no general recipe for this task. Besides giving a precise formulation of the input and the desired output it is often important to ignore irrelevant components (e.g. the drilling time which cannot be optimized or the order in which the employees carry out their jobs). Of course we are not interested in a solution to a particular drilling problem or job assignment problem in some company, but rather we are looking for a way how to solve all problems of these types. We first consider the DRILLING PROBLEM.
1.1 Enumeration
How can a solution to the DRILLING PROBLEM look like? There are infinitely many instances (finite sets of points in the plane), so we cannot list an optimum permutation for each instance. Instead, what we look for is an algorithm which, given an instance, computes an optimum solution. Such an algorithm exists: Given a set of n points, just try all possible n! orders, and for each compute the ∞-length of the corresponding path. There are different ways of formulating an algorithm, differing mostly in the level of detail and the formal language they use. We certainly would not accept the following as an algorithm: “Given a set of n points, find an optimum path and output it.” It is not specified at all how to find the optimum solution. The above suggestion to enumerate all possible n! orders is more useful, but still it is not clear how to enumerate all the orders. Here is one possible way: We enumerate all n-tuples of numbers 1, . . . , n, i.e. all nn vectors of {1, . . . , n}n. This can be done similarly to counting: we start with (1, . . . , 1, 1), (1, . . . , 1, 2) up to (1, . . . , 1, n) then switch to (1, . . . , 1, 2, 1), and so on. At each step we increment the last entry unless it is already n, in which case we go back to the last entry that is smaller than n, increment it and set all subsequent entries to 1.


This technique is sometimes called backtracking. The order in which the vectors of {1, . . . , n}n are enumerated is called the lexicographical order:
Definition 1.1. Let x, y ∈ Rn be two vectors. We say that a vector x is lexicographically smaller than y if there exists an index j ∈ {1, . . . , n} such that xi = yi for i = 1, . . . , j − 1 and x j < y j .
Knowing how to enumerate all vectors of {1, . . . , n}n we can simply check for each vector whether its entries are pairwise distinct and, if so, whether the path represented by this vector is shorter than the best path encountered so far. Since this algorithm enumerates nn vectors it will take at least nn steps (in fact, even more). This is not best possible. There are only n! permutations of {1, . . . , n},
and n! is significantly smaller than nn. (By Stirling’s formula n! ≈ √2π n nn
en
(Stirling [1730]); see Exercise 1.) We shall show how to enumerate all paths in approximately n2 · n! steps. Consider the following algorithm which enumerates all permutations in lexicographical order:
PATH ENUMERATION ALGORITHM
Input: A natural number n ≥ 3. A set { p1, . . . , pn} of points in the plane.
Output: A permutation π∗ : {1, . . . , n} → {1, . . . , n} with
cost (π ∗) := ∑n−1
i=1 d( pπ∗(i), pπ∗(i+1)) minimum.
©1 Set π(i ) := i and π∗(i ) := i for i = 1, . . . , n. Set i := n − 1.
©2 Let k := min({π(i ) + 1, . . . , n + 1} \ {π(1), . . . , π(i − 1)}).
©3 If k ≤ n then:
Set π(i ) := k.
If i = n and cost (π) < cost (π ∗) then set π∗ := π . If i < n then set π(i + 1) := 0 and i := i + 1. If k = n + 1 then set i := i − 1. If i ≥ 1 then go to ©2 .
Starting with (π(i ))i=1,...,n = (1, 2, 3, . . . , n−1, n) and i = n−1, the algorithm finds at each step the next possible value of π(i ) (not using π(1), . . . , π(i − 1)). If there is no more possibility for π(i ) (i.e. k = n + 1), then the algorithm decrements i (backtracking). Otherwise it sets π(i ) to the new value. If i = n, the new permutation is evaluated, otherwise the algorithm will try all possible values for π(i + 1), . . . , π(n) and starts by setting π(i + 1) := 0 and incrementing i . So all permutation vectors (π(1), . . . , π(n)) are generated in lexicographical order. For example, the first iterations in the case n = 6 are shown below:


π := (1, 2, 3, 4, 5, 6), i := 5 k := 6, π := (1, 2, 3, 4, 6, 0), i := 6
k := 5, π := (1, 2, 3, 4, 6, 5), cost (π) < cost (π∗)? k := 7, i := 5 k := 7, i := 4 k := 5, π := (1, 2, 3, 5, 0, 5), i := 5 k := 4, π := (1, 2, 3, 5, 4, 0), i := 6
k := 6, π := (1, 2, 3, 5, 4, 6), cost (π) < cost (π∗)?
Since the algorithm compares the cost of each path to π∗, the best path encountered so far, it indeed outputs the optimum path. But how many steps will this algorithm perform? Of course, the answer depends on what we call a single step. Since we do not want the number of steps to depend on the actual implementation we ignore constant factors. On any reasonable computer, ©1 will take at least 2n +1 steps (this many variable assignments are done) and at most cn steps for some constant c. The following common notation is useful for ignoring constant factors:
Definition 1.2. Let f, g : D → R+ be two functions. We say that f is O(g) (and sometimes write f = O(g), and also g = ( f )) if there exist constants α, β > 0 such that f (x) ≤ αg(x) + β for all x ∈ D. If f = O(g) and g = O( f ) we also say that f = (g) (and of course g = ( f )). In this case, f and g have the same rate of growth.
Note that the use of the equation sign in the O-notation is not symmetric. To illustrate this definition, let D = N, and let f (n) be the number of elementary steps in ©1 and g(n) = n (n ∈ N). Clearly we have f = O(g) (in fact f = (g)) in this case; we say that ©1 takes O(n) time (or linear time). A single execution of ©3 takes a constant number of steps (we speak of O(1) time or constant time) except in the case k ≤ n and i = n; in this case the cost of two paths have to be compared, which takes O(n) time. What about ©2 ? A naive implementation, checking for each j ∈ {π(i ) + 1, . . . , n} and each h ∈ {1, . . . , i − 1} whether j = π(h), takes O((n − π(i ))i ) steps, which can be as big as (n2). A better implementation of ©2 uses an auxiliary array indexed by 1, . . . , n:
©2 For j := 1 to n do aux( j ) := 0. For j := 1 to i − 1 do aux(π( j )) := 1. Set k := π(i ) + 1.
While k ≤ n and aux(k) = 1 do k := k + 1.
Obviously with this implementation a single execution of ©2 takes only O(n) time. Simple techniques like this are usually not elaborated in this book; we assume that the reader can find such implementations himself or herself. Having computed the running time for each single step we now estimate the total amount of work. Since the number of permutations is n! we only have to estimate the amount of work which is done between two permutations. The counter i might move back from n to some index i ′ where a new value π(i ′) ≤ n is found. Then it moves forward again up to i = n. While the counter i is constant each


of ©2 and ©3 is performed once, except in the case k ≤ n and i = n; in this case ©2 and ©3 are performed twice. So the total amount of work between two permutations consists of at most 4n times ©2 and ©3 , i.e. O(n2). So the overall running time of the PATH ENUMERATION ALGORITHM is O(n2n!). One can do slightly better; a more careful analysis shows that the running time is only O(n · n!) (Exercise 4). Still the algorithm is too time-consuming if n is large. The problem with the enumeration of all paths is that the number of paths grows exponentially with the number of points; already for 20 points there are 20! = 2432902008176640000 ≈ 2.4 · 1018 different paths and even the fastest computer needs several years to evaluate all of them. So complete enumeration is impossible even for instances of moderate size. The main subject of combinatorial optimization is to find better algorithms for problems like this. Often one has to find the best element of some finite set of feasible solutions (in our example: drilling paths or permutations). This set is not listed explicitly but implicitly depends on the structure of the problem. Therefore an algorithm must exploit this structure. In the case of the DRILLING PROBLEM all information of an instance with n points is given by 2n coordinates. While the naive algorithm enumerates all n! paths it might be possible that there is an algorithm which finds the optimum path much faster, say in n2 computation steps. It is not known whether such an algorithm exists (though results of Chapter 15 suggest that it is unlikely). Nevertheless there are much better algorithms than the naive one.
1.2 Running Time of Algorithms
One can give a formal definition of an algorithm, and we shall in fact give one in Section 15.1. However, such formal models lead to very long and tedious descriptions as soon as algorithms are a bit more complicated. This is quite similar to mathematical proofs: Although the concept of a proof can be formalized nobody uses such a formalism for writing down proofs since they would become very long and almost unreadable. Therefore all algorithms in this book are written in an informal language. Still the level of detail should allow a reader with a little experience to implement the algorithms on any computer without too much additional effort. Since we are not interested in constant factors when measuring running times we do not have to fix a concrete computing model. We count elementary steps, but we are not really interested in how elementary steps look like. Examples of elementary steps are variable assignments, random access to a variable whose index is stored in another variable, conditional jumps (if – then – go to), and simple arithmetic operations like addition, subtraction, multiplication, division and comparison of numbers. An algorithm consists of a set of valid inputs and a sequence of instructions each of which can be composed of elementary steps, such that for each valid input


the computation of the algorithm is a uniquely defined finite series of elementary steps which produces a certain output. Usually we are not satisfied with finite computation but rather want a good upper bound on the number of elementary steps performed, depending on the input size. The input to an algorithm usually consists of a list of numbers. If all these numbers are integers, we can code them in binary representation, using O(log(|a|+ 2)) bits for storing an integer a. Rational numbers can be stored by coding the numerator and the denominator separately. The input size size(x) of an instance x with rational data is the total number of bits needed for the binary representation.
Definition 1.3. Let A be an algorithm which accepts inputs from a set X, and let f : N → R+. If there exist constants α, β > 0 such that A terminates its computation after at most α f (size(x)) + β elementary steps (including arithmetic operations) for each input x ∈ X, then we say that A runs in O( f ) time. We also say that the running time (or the time complexity) of A is O( f ).
Definition 1.4. An algorithm with rational input is said to run in polynomial time if there is an integer k such that it runs in O(nk) time, where n is the input size, and all numbers in intermediate computations can be stored with O(nk) bits. An algorithm with arbitrary input is said to run in strongly polynomial time if there is an integer k such that it runs in O(nk) time for any input consisting of n numbers and it runs in polynomial time for rational input. In the case k = 1 we have a linear-time algorithm.
An algorithm which runs in polynomial but not strongly polynomial time is called weakly polynomial.
Note that the running time might be different for several instances of the same size (this was not the case with the PATH ENUMERATION ALGORITHM). We consider the worst-case running time, i.e. the function f : N → N where f (n) is the maximum running time of an instance with input size n. For some algorithms we do not know the rate of growth of f but only have an upper bound. The worst-case running time might be a pessimistic measure if the worst case occurs rarely. In some cases an average-case running time with some probabilistic model might be appropriate, but we shall not consider this. If A is an algorithm which for each input x ∈ X computes the output f (x) ∈ Y , then we say that A computes f : X → Y . If a function is computed by some polynomial-time algorithm, it is said to be computable in polynomial time. Polynomial-time algorithms are sometimes called “good” or “efficient”. This concept was introduced by Cobham [1964] and Edmonds [1965]. Table 1.1 motivates this by showing hypothetical running times of algorithms with various time complexities. For various input sizes n we show the running time of algorithms that take 100n log n, 10n2, n3.5, nlog n, 2n, and n! elementary steps; we assume that one elementary step takes one nanosecond. As always in this book, log denotes the logarithm with basis 2.


Table 1.1.
n 100n log n 10n2 n3.5 nlog n 2n n!
10 3 μs 1 μs 3 μs 2 μs 1 μs 4 ms
20 9 μs 4 μs 36 μs 420 μs 1 ms 76 years
30 15 μs 9 μs 148 μs 20 ms 1 s 8 · 1015 y.
40 21 μs 16 μs 404 μs 340 ms 1100 s
50 28 μs 25 μs 884 μs 4 s 13 days
60 35 μs 36 μs 2 ms 32 s 37 years
80 50 μs 64 μs 5 ms 1075 s 4 · 107 y.
100 66 μs 100 μs 10 ms 5 hours 4 · 1013 y.
200 153 μs 400 μs 113 ms 12 years
500 448 μs 2.5 ms 3 s 5 · 105 y.
1000 1 ms 10 ms 32 s 3 · 1013 y.
104 13 ms 1 s 28 hours
105 166 ms 100 s 10 years
106 2 s 3 hours 3169 y.
107 23 s 12 days 107 y.
108 266 s 3 years 3 · 1010 y.
1010 9 hours 3 · 104 y.
1012 46 days 3 · 108 y.
As Table 1.1 shows, polynomial-time algorithms are faster for large enough instances. The table also illustrates that constant factors of moderate size are not very important when considering the asymptotic growth of the running time. Table 1.2 shows the maximum input sizes solvable within one hour with the above six hypothetical algorithms. In (a) we again assume that one elementary step takes one nanosecond, (b) shows the corresponding figures for a ten times faster machine. Polynomial-time algorithms can handle larger instances in reasonable time. Moreover, even a speedup by a factor of 10 of the computers does not increase the size of solvable instances significantly for exponential-time algorithms, but it does for polynomial-time algorithms.
Table 1.2.
100n log n 10n2 n3.5 nlog n 2n n!
(a) 1.19 · 109 60000 3868 87 41 15
(b) 10.8 · 109 189737 7468 104 45 16


(Strongly) polynomial-time algorithms, if possible linear-time algorithms, are what we look for. There are some problems where it is known that no polynomialtime algorithm exists, and there are problems for which no algorithm exists at all. (For example, a problem which can be solved in finite time but not in polynomial time is to decide whether a so-called regular expression defines the empty set; see Aho, Hopcroft and Ullman [1974]. A problem for which there exists no algorithm at all, the HALTING PROBLEM, is discussed in Exercise 1 of Chapter 15.) However, almost all problems considered in this book belong to the following two classes. For the problems of the first class we have a polynomial-time algorithm. For each problem of the second class it is an open question whether a polynomialtime algorithm exists. However, we know that if one of these problems has a polynomial-time algorithm, then all problems of this class do. A precise formulation and a proof of this statement will be given in Chapter 15. The JOB ASSIGNMENT PROBLEM belongs to the first class, the DRILLING PROBLEM belongs to the second class. These two classes of problems divide this book roughly into two parts. We first deal with tractable problems for which polynomial-time algorithms are known. Then, starting with Chapter 15, we discuss hard problems. Although no polynomialtime algorithms are known, there are often much better methods than complete enumeration. Moreover, for many problems (including the DRILLING PROBLEM), one can find approximate solutions within a certain percentage of the optimum in polynomial time.
1.3 Linear Optimization Problems
We now consider our second example given initially, the JOB ASSIGNMENT PROBLEM, and briefly address some central topics which will be discussed in later chapters. The JOB ASSIGNMENT PROBLEM is quite different to the DRILLING PROBLEM since there are infinitely many feasible solutions for each instance (except for trivial cases). We can reformulate the problem by introducing a variable T for the time when all jobs are done:
min T
s.t.
∑
j ∈Si
xij = ti (i ∈ {1, . . . , n})
xij ≥ 0 (i ∈ {1, . . . , n}, j ∈ Si )
∑
i: j ∈Si
xij ≤ T ( j ∈ {1, . . . , m})
(1.1)
The numbers ti and the sets Si (i = 1, . . . , n) are given, the variables xij and T are what we look for. Such an optimization problem with a linear objective


function and linear constraints is called a linear program. The set of feasible solutions of (1.1), a so-called polyhedron, is easily seen to be convex, and one can prove that there always exists an optimum solution which is one of the finitely many extreme points of this set. Therefore a linear program can, theoretically, also be solved by complete enumeration. But there are much better ways as we shall see later. Although there are several algorithms for solving linear programs in general, such general techniques are usually less efficient than special algorithms exploiting the structure of the problem. In our case it is convenient to model the sets Si , i = 1, . . . , n, by a graph. For each job i and for each employee j we have a point (called vertex), and we connect employee j with job i by an edge if he or she can contribute to this job (i.e. if j ∈ Si ). Graphs are a fundamental combinatorial structure; many combinatorial optimization problems are described most naturally in terms of graph theory. Suppose for a moment that the processing time of each job is one hour, and we ask whether we can finish all jobs within one hour. So we look for numbers xij
(i ∈ {1, . . . , n}, j ∈ Si ) such that 0 ≤ xij ≤ 1 for all i and j , ∑
j∈Si xij = 1 for
i = 1, . . . , n, and ∑
i: j∈Si xij ≤ 1 for j = 1, . . . , n. One can show that if such a solution exists, then in fact an integral solution exists, i.e. all xij are either 0 or 1. This is equivalent to assigning each job to one employee, such that no employee has to do more than one job. In the language of graph theory we then look for a matching covering all jobs. The problem of finding optimal matchings is one of the best-known combinatorial optimization problems. We review the basics of graph theory and linear programming in Chapters 2 and 3. In Chapter 4 we prove that linear programs can be solved in polynomial time, and in Chapter 5 we discuss integral polyhedra. In the subsequent chapters we discuss some classical combinatorial optimization problems in detail.
1.4 Sorting
Let us conclude this chapter by considering a special case of the DRILLING PROBLEM where all holes to be drilled are on one horizontal line. So we are given just one coordinate for each point pi , i = 1, . . . , n. Then a solution to the drilling problem is easy, all we have to do is sort the points by their coordinates: the drill will just move from left to right. Although there are still n! permutations, it is clear that we do not have to consider all of them to find the optimum drilling path, i.e. the sorted list. It is very easy to sort n numbers in nondecreasing order in O(n2) time. To sort n numbers in O(n log n) time requires a little more skill. There are several algorithms accomplishing this; we present the well-known MERGE-SORT ALGORITHM. It proceeds as follows. First the list is divided into two sublists of approximately equal size. Then each sublist is sorted (this is done recursively by the same algorithm). Finally the two sorted sublists are merged together. This


general strategy, often called “divide and conquer”, can be used quite often. See e.g. Section 17.1 for another example. We did not discuss recursive algorithms so far. In fact, it is not necessary to discuss them, since any recursive algorithm can be transformed into a sequential algorithm without increasing the running time. But some algorithms are easier to formulate (and implement) using recursion, so we shall use recursion when it is convenient.
MERGE-SORT ALGORITHM
Input: A list a1, . . . , an of real numbers.
Output: A permutation π : {1, . . . , n} → {1, . . . , n} such that aπ(i) ≤ aπ(i+1) for all i = 1, . . . , n − 1.
©1 If n = 1 then set π(1) := 1 and stop (return π).
©2 Set m := ⌊ n
2
⌋.
Let ρ :=MERGE-SORT(a1, . . . , am). Let σ :=MERGE-SORT(am+1, . . . , an).
©3 Set k := 1, l := 1.
While k ≤ m and l ≤ n − m do:
If aρ(k) ≤ am+σ (l) then set π(k + l − 1) := ρ(k) and k := k + 1
else set π(k + l − 1) := m + σ (l) and l := l + 1. While k ≤ m do: Set π(k + l − 1) := ρ(k) and k := k + 1. While l ≤ n − m do: Set π(k + l − 1) := m + σ (l) and l := l + 1.
As an example, consider the list “69,32,56,75,43,99,28”. The algorithm first splits this list into two, “69,32,56” and “75,43,99,28” and recursively sorts each of the two sublists. We get the permutations ρ = (2, 3, 1) and σ = (4, 2, 1, 3) corresponding to the sorted lists “32,56,69” and “28,43,75,99”. Now these lists are merged as shown below: k := 1, l := 1 ρ(1) = 2, σ (1) = 4, aρ(1) = 32, aσ (1) = 28, π(1) := 7, l := 2 ρ(1) = 2, σ (2) = 2, aρ(1) = 32, aσ (2) = 43, π(2) := 2, k := 2 ρ(2) = 3, σ (2) = 2, aρ(2) = 56, aσ (2) = 43, π(3) := 5, l := 3 ρ(2) = 3, σ (3) = 1, aρ(2) = 56, aσ (3) = 75, π(4) := 3, k := 3 ρ(3) = 1, σ (3) = 1, aρ(3) = 69, aσ (3) = 75, π(5) := 1, k := 4 σ (3) = 1, aσ (3) = 75, π(6) := 4, l := 4 σ (4) = 3, aσ (4) = 99, π(7) := 6, l := 5
Theorem 1.5. The MERGE-SORT ALGORITHM works correctly and runs in O(n log n) time.
Proof: The correctness is obvious. We denote by T (n) the running time (number of steps) needed for instances consisting of n numbers and observe that T (1) = 1 and T (n) = T ( n
2 )+T( n
2 ) + 3n + 6. (The constants in the term 3n + 6 depend on how exactly a computation step is defined; but they do not really matter.)


We claim that this yields T (n) ≤ 12n log n + 1. Since this is trivial for n = 1 we proceed by induction. For n ≥ 2, assuming that the inequality is true for 1, . . . , n − 1, we get
T (n) ≤ 12
⌊n
2
⌋
log
(2
3n
)
+ 1 + 12
⌈n
2
⌉
log
(2
3n
)
+ 1 + 3n + 6
= 12n(log n + 1 − log 3) + 3n + 8
≤ 12n log n − 13
2 n + 3n + 8 ≤ 12n log n + 1,
because log 3 ≥ 37
24 .
Of course the algorithm works for sorting the elements of any totally ordered set, assuming that we can compare any two elements in constant time. Can there be a faster, a linear-time algorithm? Suppose that the only way we can get information on the unknown order is to compare two elements. Then we can show that any algorithm needs at least (n log n) comparisons in the worst case. The outcome of a comparison can be regarded as a zero or one; the outcome of all comparisons an algorithm does is a 0-1-string (a sequence of zeros and ones). Note that two different orders in the input of the algorithm must lead to two different 0-1-strings (otherwise the algorithm could not distinguish between the two orders). For an input of n elements there are n! possible orders, so there must be n! different 01-strings corresponding to the computation. Since the number of 0-1-strings with
length less than ⌊ n
2 log n
2
⌋ is 2 n
2 log n
2 − 1 < 2n
2 log n
2 = (n
2)n
2 ≤ n! we conclude that the maximum length of the 0-1-strings, and hence of the computation, must be at least n
2 log n
2 = (n log n). In the above sense, the running time of the MERGE-SORT ALGORITHM is optimal up to a constant factor. However, there is an algorithm for sorting integers (or sorting strings lexicographically) whose running time is linear in the input size; see Exercise 8. An algorithm to sort n integers in O(n log log n) time was proposed by Han [2004]. Lower bounds like the one above are known only for very few problems (except trivial linear bounds). Often a restriction on the set of operations is necessary to derive a superlinear lower bound.
Exercises
1. Prove that for all n ∈ N:
e
(n
e
)n ≤ n! ≤ en
(n
e
)n .
Hint: Use 1 + x ≤ ex for all x ∈ R. 2. Prove that log(n!) = (n log n). 3. Prove that n log n = O(n1+ ) for any > 0.


4. Show that the running time of the PATH ENUMERATION ALGORITHM is O(n · n!).
5. Show that there is a polynomial-time algorithm for the DRILLING PROBLEM where d is the 1-distance if and only if there is one for ∞-distance. Note: Both is unlikely as the problems were proved to be NP-hard (this will be explained in Chapter 15) by Garey, Graham and Johnson [1976]. 6. Suppose we have an algorithm whose running time is (n(t + n1/t )), where n is the input length and t is a positive parameter we can choose arbitrarily. How should t be chosen (depending on n) such that the running time (as a function of n) has a minimum rate of growth? 7. Let s, t be binary strings, both of length m. We say that s is lexicographically smaller than t if there exists an index j ∈ {1, . . . , m} such that si = ti for i = 1, . . . , j − 1 and s j < t j . Now given n strings of length m, we want to sort them lexicographically. Prove that there is a linear-time algorithm for this problem (i.e. one with running time O(nm)). Hint: Group the strings according to the first bit and sort each group. 8. Describe an algorithm which sorts a list of natural numbers a1, . . . , an in linear time; i.e. which finds a permutation π with aπ(i) ≤ aπ(i+1) (i = 1, . . . , n − 1) and runs in O(log(a1 + 1) + · · · + log(an + 1)) time. Hint: First sort the strings encoding the numbers according to their length. Then apply the algorithm of Exercise 7. Note: The algorithm discussed in this and the previous exercise is often called radix sorting.
References
General Literature
Cormen, T.H., Leiserson, C.E., Rivest, R.L., and Stein, C. [2009]: Introduction to Algorithms. Third Edition. MIT Press, Cambridge 2009 Hougardy, S., and Vygen, J. [2016]: Algorithmic Mathematics. Springer, Cham 2016 Knuth, D.E. [1968]: The Art of Computer Programming; Vol. 1. Fundamental Algorithms. Addison-Wesley, Reading 1968 (third edition: 1997) Mehlhorn, K., and Sanders, P. [2008]: Algorithms and Data Structures: The Basic Toolbox. Springer, Berlin 2008
Cited References
Aho, A.V., Hopcroft, J.E., and Ullman, J.D. [1974]: The Design and Analysis of Computer Algorithms. Addison-Wesley, Reading 1974 Cobham, A. [1964]: The intrinsic computational difficulty of functions. Proceedings of the 1964 Congress for Logic Methodology and Philosophy of Science (Y. Bar-Hillel, ed.), North-Holland, Amsterdam 1964, pp. 24–30


Edmonds, J. [1965]: Paths, trees, and flowers. Canadian Journal of Mathematics 17 (1965), 449–467 Garey, M.R., Graham, R.L., and Johnson, D.S. [1976]: Some NP-complete geometric problems. Proceedings of the 8th Annual ACM Symposium on the Theory of Computing (1976), 10–22 Han, Y. [2004]: Deterministic sorting in O(n log log n) time and linear space. Journal of Algorithms 50 (2004), 96–105 Stirling, J. [1730]: Methodus Differentialis. London 1730


2 Graphs
Graphs are a fundamental combinatorial structure used throughout this book. In this chapter we not only review the standard definitions and notation, but also prove some basic theorems and mention some fundamental algorithms. After some basic definitions in Section 2.1 we consider fundamental objects occurring very often in this book: trees, circuits, and cuts. We prove some important properties and relations, and we also consider tree-like set systems in Section 2.2. The first graph algorithms, determining connected and strongly connected components, appear in Section 2.3. In Section 2.4 we prove Euler’s Theorem on closed walks using every edge exactly once. Finally, in Sections 2.5 and 2.6 we consider graphs that can be drawn in the plane without crossings.
2.1 Basic Definitions
An undirected graph is a triple (V , E, ), where V is a nonempty finite set, E is a finite set, and : E → {X ⊆ V : |X | = 2}. A directed graph or digraph is a triple (V, E, ), where V is a nonempty finite set, E is a finite set, and : E → {(v, w) ∈ V × V : v = w}. By a graph we mean either an undirected graph or a digraph. The elements of V are called vertices, the elements of E are the edges.
Edges e, e′ with e = e′ and (e) = (e′) are called parallel. Graphs without parallel edges are called simple. For simple graphs we usually identify an edge e
with its image (e) and write G = (V (G), E(G)), where E(G) ⊆ (X
2
) := {X ⊆ V (G) : |X| = 2} or E(G) ⊆ V (G) × V (G). We often use this simpler notation even in the presence of parallel edges, then the “set” E(G) may contain several “identical” elements. |E(G)| denotes the number of edges, and for two edge sets E
and F we always have |E .∪ F| = |E| + |F| even if parallel edges arise. We write e = {v, w} or e = (v, w) for each edge e with (e) = {v, w} or (e) = (v, w), respectively. We say that an edge e = {v, w} or e = (v, w) joins v and w. In this case, v and w are adjacent. v is a neighbour of w (and vice versa). v and w are the endpoints of e. If v is an endpoint of an edge e, we say that v is incident with e. In the directed case we say that e = (v, w) leaves v (the tail of e) and enters w (the head of e). Two edges which share at least one endpoint are called adjacent.


This terminology for graphs is not the only one. Sometimes vertices are called nodes or points, other names for edges are arcs (especially in the directed case) or lines. In some texts, a graph is what we call a simple undirected graph, in the presence of parallel edges they speak of multigraphs. Sometimes edges whose endpoints coincide, so-called loops, are considered. However, unless otherwise stated, we do not use them. For a digraph G we sometimes consider the underlying undirected graph, i.e. the undirected graph G′ on the same vertex set which contains an edge {v, w} for each edge (v, w) of G (so |E(G′)| = |E(G)|). We also say that G is an orientation of G′. A subgraph of a graph G = (V (G), E(G)) is a graph H = (V (H ), E(H )) with V (H ) ⊆ V (G) and E(H ) ⊆ E(G). We also say that G contains H . H is an induced subgraph of G if it is a subgraph of G and E(H ) = {{x, y} ∈ E(G) : x, y ∈ V (H )} or E(H ) = {(x, y) ∈ E(G) : x, y ∈ V (H )}. Here H is the subgraph of G induced by V (H ). We also write H = G[V (H )]. A subgraph H of G is called spanning if V (H ) = V (G). If v ∈ V (G), we write G − v for the subgraph of G induced by V (G) \ {v}. If e ∈ E(G), we define G − e := (V (G), E(G) \ {e}). We also use this notation for deleting a set X of vertices or edges and write G − X . Furthermore, the addition
of a new edge e is abbreviated by G + e := (V (G), E(G) .∪ {e}). If G and H are two graphs, we denote by G + H the graph with V (G + H ) = V (G) ∪ V (H ) and E(G + H ) being the disjoint union of E(G) and E(H ) (parallel edges may arise). A family of graphs is called vertex-disjoint or edge-disjoint if their vertex sets or edge sets are pairwise disjoint, respectively. Two graphs G and H are called isomorphic if there are bijections V : V (G) → V (H ) and E : E(G) → E(H ) such that E ((v, w)) = ( V (v), V (w)) for all (v, w) ∈ E(G), or E ({v, w}) = { V (v), V (w)} for all {v, w} ∈ E(G) in the undirected case. We normally do not distinguish between isomorphic graphs; for example we say that G contains H if G has a subgraph isomorphic to H . Suppose we have an undirected graph G and X ⊆ V (G). By contracting (or shrinking) X we mean deleting the vertices in X and the edges in G[X ], adding a new vertex x and replacing each edge {v, w} with v ∈ X , w ∈/ X by an edge {x, w} (parallel edges may arise). Similarly for digraphs. We often call the result G/X.
For a graph G and X, Y ⊆ V (G) we define E(X, Y ) := {{x, y} ∈ E(G) : x ∈ X \ Y, y ∈ Y \ X } if G is undirected and E+(X, Y ) := {(x, y) ∈ E(G) : x ∈ X \ Y, y ∈ Y \ X } if G is directed. For undirected graphs G and X ⊆ V (G) we define δ(X) := E(X, V (G) \ X ). The set of neighbours of X is defined by (X) := {v ∈ V (G) \ X : E(X, {v}) = ∅}. For digraphs G and X ⊆ V (G) we define δ+(X ) := E+(X, V (G) \ X ), δ−(X ) := δ+(V (G) \ X ) and δ(X ) := δ+(X )∪δ−(X ). We use subscripts (e.g. δG (X )) to specify the graph G if necessary. For singletons, i.e. one-element vertex sets {v} (v ∈ V (G)) we write δ(v) := δ({v}), (v) := ({v}), δ+(v) := δ+({v}) and δ−(v) := δ−({v}). The degree of


a vertex v is |δ(v)|, the number of edges incident to v. In the directed case, the in-degree is |δ−(v)|, the out-degree is |δ+(v)|, and the degree is |δ+(v)|+|δ−(v)|. A vertex with degree zero is called isolated. A graph where all vertices have degree k is called k-regular.
For any graph, ∑
v∈V (G) |δ(v)| = 2|E(G)|. In particular, the number of vertices
with odd degree is even. In a digraph, ∑
v∈V (G) |δ+(v)| = ∑
v∈V (G) |δ−(v)|. To prove these statements, please observe that each edge is counted twice on each side of the first equation and once on each side of the second equation. With just a little more effort we get the following useful statements:
Lemma 2.1. For a digraph G and any two sets X, Y ⊆ V (G):
(a) |δ+(X )| + |δ+(Y )| = |δ+(X ∩ Y )| + |δ+(X ∪ Y )| + |E+(X, Y )| + |E+(Y, X )|; (b) |δ−(X )| + |δ−(Y )| = |δ−(X ∩ Y )| + |δ−(X ∪ Y )| + |E+(X, Y )| + |E+(Y, X )|.
For an undirected graph G and any two sets X, Y ⊆ V (G):
(c) |δ(X)| + |δ(Y )| = |δ(X ∩ Y )| + |δ(X ∪ Y )| + 2|E(X, Y )|; (d) |δ(X)| + |δ(Y )| = |δ(X \ Y )| + |δ(Y \ X )| + 2|E(X ∩ Y, V (G) \ (X ∪ Y ))|; (e) | (X)| + | (Y )| ≥ | (X ∩ Y )| + | (X ∪ Y )|.
Proof: All parts can be proved by simple counting arguments. Let Z := V (G) \ (X ∪ Y ).
To prove (a), observe that |δ+(X )|+|δ+(Y )| = |E+(X, Z )|+|E+(X, Y \ X )|+ |E+(Y, Z )| + |E+(Y, X \ Y )| = |E+(X ∪ Y, Z )| + |E+(X ∩ Y, Z )| + |E+(X, Y \ X )| + |E+(Y, X \ Y )| = |δ+(X ∪ Y )| + |δ+(X ∩ Y )| + |E+(X, Y )| + |E+(Y, X )|. (b) follows from (a) by reversing each edge (replace (v, w) by (w, v)). (c) follows from (a) by replacing each edge {v, w} by a pair of oppositely directed edges (v, w) and (w, v). Substituting Y by V (G) \ Y in (c) yields (d). To show (e), observe that | (X )| + | (Y )| = | (X ∪ Y )| + | (X ) ∩ (Y )| + | (X) ∩ Y | + | (Y ) ∩ X | ≥ | (X ∪ Y )| + | (X ∩ Y )|.
A function f : 2U → R (where U is some finite set and 2U denotes its power set) is called
• submodular if f (X ∩ Y ) + f (X ∪ Y ) ≤ f (X ) + f (Y ) for all X, Y ⊆ U ; • supermodular if f (X ∩ Y ) + f (X ∪ Y ) ≥ f (X ) + f (Y ) for all X, Y ⊆ U ; • modular if f (X ∩ Y ) + f (X ∪ Y ) = f (X ) + f (Y ) for all X, Y ⊆ U .
So Lemma 2.1 implies that |δ+|, |δ−|, |δ| and | | are submodular. This will be useful later. A complete graph is a simple undirected graph where each pair of vertices is adjacent. We denote the complete graph on n vertices by Kn. The complement of a simple undirected graph G is the graph H for which V (G) = V (H ) and G + H is a complete graph. A matching in an undirected graph G is a set of pairwise disjoint edges (i.e. the endpoints are all different). A vertex cover in G is a set S ⊆ V (G) of vertices such that every edge of G is incident to at least one vertex in S. An edge cover in


G is a set F ⊆ E(G) of edges such that every vertex of G is incident to at least one edge in F. A stable set in G is a set of pairwise non-adjacent vertices. A graph containing no edges is called empty. A clique is a set of pairwise adjacent vertices.
Proposition 2.2. Let G be a graph and X ⊆ V (G). Then the following three statements are equivalent:
(a) X is a vertex cover in G, (b) V (G) \ X is a stable set in G, (c) V (G) \ X is a clique in the complement of G.
If F is a family of sets or graphs, we say that F is a minimal element of F if F contains F but no proper subset/subgraph of F. Similarly, F is maximal in F if F ∈ F and F is not a proper subset/subgraph of any element of F . When we speak of a minimum or maximum element, we mean one of minimum/maximum cardinality. For example, a minimal vertex cover is not necessarily a minimum vertex cover (see e.g. the graph in Figure 13.1), and a maximal matching is in general not maximum. The problems of finding a maximum matching, stable set or clique, or a minimum vertex cover or edge cover in an undirected graph will play important roles in later chapters. The line graph of a simple undirected graph G is the graph (E(G), F), where F = {{e1, e2} : e1, e2 ∈ E(G), |e1 ∩ e2| = 1}. Obviously, matchings in a graph G correspond to stable sets in the line graph of G. For the following notation, let G be a graph, directed or undirected. An edge progression W in G (from v1 to vk+1) is a sequence v1, e1, v2, . . . , vk , ek , vk+1 such that k ≥ 0, and ei = (vi , vi+1) ∈ E(G) or ei = {vi , vi+1} ∈ E(G) for i = 1, . . . , k. If in addition ei = e j for all 1 ≤ i < j ≤ k, W is called a walk in G. W is closed if v1 = vk+1. A path is a graph P = ({v1, . . . , vk+1}, {e1, . . . , ek}) such that vi = v j for 1 ≤ i < j ≤ k + 1 and the sequence v1, e1, v2, . . . , vk , ek, vk+1 is a walk. P is also called a path from v1 to vk+1 or a v1-vk+1-path. v1 and vk+1 are the endpoints of P, v2, . . . , vk are its internal vertices. By P[x,y] with x , y ∈ V (P) we mean the (unique) subgraph of P which is an x-y-path. For any graph G and vertices v, w ∈ V (G), there is an edge progression from v to w in G if and only if there is a v-w-path in G (meaning: contained in G as a subgraph). For undirected graphs this defines an equivalence relation. A circuit or a cycle is a graph ({v1, . . . , vk }, {e1, . . . , ek}) such that the sequence v1, e1, v2, . . . , vk , ek, v1 is a (closed) walk with k ≥ 2 and vi = v j for 1 ≤ i < j ≤ k. An easy induction argument shows that the edge set of a closed walk can be partitioned into edge sets of circuits.
By an undirected path or an undirected circuit in a digraph, we mean a subgraph corresponding to a path or circuit, respectively, in the underlying undirected graph.


The length of a path or circuit is the number of its edges. If it is a subgraph of G, we speak of a path or circuit in G. A spanning path in G is called a Hamiltonian path while a spanning circuit in G is called a Hamiltonian circuit or a tour. A graph containing a Hamiltonian circuit is a Hamiltonian graph. For two vertices v and w we write dist(v, w) or distG (v, w) for the length of a shortest v-w-path (the distance from v to w) in G. If there is no v-w-path at all, i.e. w is not reachable from v, we set dist(v, w) := ∞. In the undirected case, dist(v, w) = dist(w, v) for all v, w ∈ V (G).
We shall often have a weight (or cost) function c : E(G) → R. Then for
F ⊆ E(G) we write c(F) := ∑
e∈F c(e) (and c(∅) = 0). This extends c to a
modular function c : 2E(G) → R. Moreover, dist(G,c)(v, w) denotes the minimum c(E(P)) over all v-w-paths P in G.
2.2 Trees, Circuits, and Cuts
An undirected graph G is called connected if there is a v-w-path for all v, w ∈ V (G); otherwise G is disconnected. A digraph is called connected if the underlying undirected graph is connected. The maximal connected subgraphs of a graph are its connected components. Sometimes we identify the connected components with the vertex sets inducing them. A set of vertices X is called connected if the subgraph induced by X is connected. A vertex v with the property that G − v has more connected components than G is called an articulation vertex. An edge e is called a bridge if G − e has more connected components than G. An undirected graph without a circuit (as a subgraph) is called a forest. A connected forest is a tree. A vertex of degree at most 1 in a tree is called a leaf. A star is a tree where at most one vertex is not a leaf. In the following we shall give some equivalent characterizations of trees and their directed counterparts, arborescences. We need the following connectivity criterion:
Proposition 2.3.
(a) An undirected graph G is connected if and only if δ(X ) = ∅ for all ∅ = X ⊂ V (G). (b) Let G be a digraph and r ∈ V (G). Then there exists an r -v-path for every v ∈ V (G) if and only if δ+(X ) = ∅ for all X ⊂ V (G) with r ∈ X .
Proof: (a): If there is a set X ⊂ V (G) with r ∈ X , v ∈ V (G) \ X , and δ(X ) = ∅, there can be no r -v-path, so G is not connected. On the other hand, if G is not connected, there is no r -v-path for some r and v. Let R be the set of vertices reachable from r . We have r ∈ R, v ∈/ R and δ(R) = ∅. (b) is proved analogously.


Theorem 2.4. Let G be an undirected graph on n vertices. Then the following statements are equivalent:
(a) G is a tree (i.e. is connected and has no circuits). (b) G has n − 1 edges and no circuits. (c) G has n − 1 edges and is connected. (d) G is connected and every edge is a bridge. (e) G satisfies δ(X ) = ∅ for all ∅ = X ⊂ V (G), but deleting any edge would destroy this property. (f) G is a forest, but the addition of an arbitrary edge would create a circuit. (g) G contains a unique path between any pair of vertices.
Proof: (a)⇒(g) follows from the fact that the union of two distinct paths with the same endpoints contains a circuit. (g)⇒(e)⇒(d) follows from Proposition 2.3(a). (d)⇒(f) is trivial. (f)⇒(b)⇒(c): This follows from the fact that for forests with n vertices, m edges and p connected components n = m + p holds. (The proof is a trivial induction on m.) (c)⇒(a): Let G be connected with n −1 edges. As long as there are any circuits in G, we destroy them by deleting an edge of the circuit. Suppose we have deleted k edges. The resulting graph G′ is still connected and has no circuits. G′ has m = n − 1 − k edges. So n = m + p = n − 1 − k + 1, implying k = 0.
In particular, (d)⇒(a) implies that a graph is connected if and only if it contains a spanning tree (a spanning subgraph which is a tree). A digraph is a branching if the underlying undirected graph is a forest and each vertex v has at most one entering edge. A connected branching is an arborescence. By Theorem 2.4 an arborescence with n vertices has n − 1 edges, hence it has exactly one vertex r with δ−(r ) = ∅. This vertex is called its root; we also speak of an arborescence rooted at r . For a vertex v in a branching, the vertices w for which (v, w) is an edge are called the children of v. For a child w of v, v is called the parent or predecessor of w. Vertices without children are called leaves.
Theorem 2.5. Let G be a digraph on n vertices. Then the following statements are equivalent:
(a) G is an arborescence rooted at r (i.e. a connected branching with δ−(r ) = ∅). (b) G is a branching with n − 1 edges and δ−(r ) = ∅. (c) G has n − 1 edges and every vertex is reachable from r . (d) Every vertex is reachable from r , but deleting any edge would destroy this property. (e) G satisfies δ+(X ) = ∅ for all X ⊂ V (G) with r ∈ X , but deleting any edge would destroy this property. (f) δ−(r ) = ∅, and there is a unique walk from r to v for each v ∈ V (G) \ {r }. (g) δ−(r ) = ∅, |δ−(v)| = 1 for all v ∈ V (G) \ {r }, and G contains no circuit.


Proof: (a)⇒(b) and (c)⇒(d) follow from Theorem 2.4. (b)⇒(c): We have that |δ−(v)| = 1 for all v ∈ V (G) \ {r }. So for any v we have an r -v-path (start at v and always follow the entering edge until r is reached). (d)⇔(e) is implied by Proposition 2.3(b). (d)⇒(f): Any edge in δ−(r ) could be deleted without destroying reachability from r . Suppose that, for some v ∈ V (G), there are two r -v-walks P and Q. Let e be the last edge of P that does not belong to Q (if such an edge does not exist, exchange P and Q). Then after deleting e, every vertex is still reachable from r . (f)⇒(g): If every vertex is reachable from r and |δ−(v)| > 1 for some vertex v ∈ V (G) \ {r }, then we have two walks from r to v. If G contains a circuit C, let v ∈ V (C), consider the r -v-path P, and let x be the first vertex on P belonging to C. Then there are two walks from r to x : P[r,x], and P[r,x] plus C.
(g)⇒(a): If |δ−(v)| ≤ 1 for all v ∈ V (G), then every undirected circuit is a (directed) circuit.
A cut in an undirected graph G is an edge set of type δ(X ) for some ∅ = X ⊂ V (G). In a digraph G, δ+(X ) is a directed cut if ∅ = X ⊂ V (G) and δ−(X ) = ∅, i.e. no edge enters the set X . We say that an edge set F ⊆ E(G) separates two vertices s and t if t is reachable from s in G but not in (V (G), E(G) \ F). An s-t-cut in an undirected graph is a cut δ(X ) for some X ⊂ V (G) with s ∈ X and t ∈/ X . In a digraph, an s-t-cut is an edge set δ+(X ) with s ∈ X and t ∈/ X . An r-cut in a digraph is an edge set δ+(X ) for some X ⊂ V (G) with r ∈ X . An undirected cut in a digraph is an edge set corresponding to a cut in the underlying undirected graph, i.e., δ(X ) for some ∅ = X ⊂ V (G).
Lemma 2.6. (Minty [1960]) Let G be a digraph and e ∈ E(G). Suppose e is coloured black, while all other edges are coloured red, black or green. Then exactly one of the following statements holds:
(a) There is an undirected circuit containing e and only red and black edges such that all black edges have the same orientation. (b) There is an undirected cut containing e and only green and black edges such that all black edges have the same orientation.
Proof: Let e = (x, y). We label the vertices of G by the following procedure. First label y. In case v is already labelled and w is not, we label w if there is a black edge (v, w), a red edge (v, w) or a red edge (w, v). In this case, we write pr ed(w) := v.
When the labelling procedure stops, there are two possibilities: Case 1: x has been labelled. Then the vertices x, pr ed(x), pr ed( pr ed(x)), . . . , y form an undirected circuit with the property (a). Case 2: x has not been labelled. Then let R consist of all labelled vertices. Obviously, the undirected cut δ+(R) ∪ δ−(R) has the property (b). Suppose that an undirected circuit C as in (a) and an undirected cut δ+(X ) ∪ δ−(X ) as in (b) both exist. All edges in their (nonempty) intersection are black,


they all have the same orientation with respect to C, and they all leave X or all enter X. This is a contradiction.
A digraph is called strongly connected if there is a path from s to t and a path from t to s for all s, t ∈ V (G). The strongly connected components of a digraph are the maximal strongly connected subgraphs.
Corollary 2.7. In a digraph G, each edge belongs either to a (directed) circuit or to a directed cut. Moreover the following statements are equivalent:
(a) G is strongly connected. (b) G contains no directed cut. (c) G is connected and each edge of G belongs to a circuit.
Proof: The first statement follows directly from Minty’s Lemma 2.6 by colouring all edges black. This also proves (b)⇒(c). (a)⇒(b) follows from Proposition 2.3(b). (c)⇒(a): Let r ∈ V (G) be an arbitrary vertex. We prove that there is an r -vpath for each v ∈ V (G). Suppose this is not true, then by Proposition 2.3(b) there is some X ⊂ V (G) with r ∈ X and δ+(X ) = ∅. Since G is connected, we have δ+(X ) ∪ δ−(X ) = ∅ (by Proposition 2.3(a)), so let e ∈ δ−(X ). But then e cannot belong to a circuit since no edge leaves X .
Corollary 2.7 and Theorem 2.5 imply that a digraph is strongly connected if and only if it contains for each vertex v a spanning arborescence rooted at v. A digraph is called acyclic if it contains no (directed) circuit. So by Corollary 2.7 a digraph is acyclic if and only if each edge belongs to a directed cut. Moreover, a digraph is acyclic if and only if its strongly connected components are the singletons. The vertices of an acyclic digraph can be ordered in a nice way:
Definition 2.8. Let G be a digraph. A topological order of G is an order of the vertices V (G) = {v1, . . . , vn} such that for each edge (vi , v j ) ∈ E(G) we have i < j.
Proposition 2.9. A digraph has a topological order if and only if it is acyclic.
Proof: If a digraph has a circuit, it clearly cannot have a topological order. We show the converse by induction on the number of edges. If there are no edges, every order is topological. Otherwise let e ∈ E(G); by Corollary 2.7 e belongs to a directed cut δ+(X ). Then a topological order of G[X ] followed by a topological order of G − X (both exist by the induction hypothesis) is a topological order of G.
Circuits and cuts also play an important role in algebraic graph theory. For a graph G we associate a vector space RE(G) whose elements are vectors (xe)e∈E(G) with |E(G)| real components. Following Berge [1985] we shall now briefly discuss two linear subspaces which are particularly important.


Let G be a digraph. We associate a vector ζ (C) ∈ {−1, 0, 1}E(G) with each undirected circuit C in G by setting ζ (C)e = 0 for e ∈/ E(C), and setting ζ (C)e ∈ {−1, 1} for e ∈ E(C) such that reorienting all edges e with ζ (C)e = −1 results
in a directed circuit. Similarly, we associate a vector ζ (D) ∈ {−1, 0, 1}E(G) with each undirected cut D = δ(X ) in G by setting ζ (D)e = 0 for e ∈/ D, ζ (D)e = −1
for e ∈ δ−(X ) and ζ (D)e = 1 for e ∈ δ+(X ). Note that these vectors are properly defined only up to multiplication by −1. However, the subspaces of the vector space RE(G) generated by the set of vectors associated with the undirected circuits and by the set of vectors associated with the undirected cuts in G are properly defined; they are called the cycle space and the cocycle space of G, respectively.
Proposition 2.10. The cycle space and the cocycle space are orthogonal to each other.
Proof: Let C be any undirected circuit and D = δ(X ) be any undirected cut. We claim that the scalar product of ζ (C) and ζ (D) is zero. Since reorienting any edge does not change the scalar product we may assume that D is a directed cut. But then the result follows from observing that any circuit enters a set X the same number of times as it leaves X .
We shall now show that the sum of the dimensions of the cycle space and the cocycle space is |E(G)|, the dimension of the whole space. A set of undirected circuits (undirected cuts) is called a cycle basis (a cocycle basis) if the associated vectors form a basis of the cycle space (the cocycle space, respectively). Let G be a graph (directed or undirected) and T a maximal subgraph without an undirected circuit. For each e ∈ E(G) \ E(T ) we call the unique undirected circuit in T + e the fundamental circuit of e with respect to T . Moreover, for each e ∈ E(T ) there is a set X ⊆ V (G) with δG(X ) ∩ E(T ) = {e} (consider a component of T − e); we call δG (X ) the fundamental cut of e with respect to T .
Theorem 2.11. Let G be a digraph and T a maximal subgraph without an undirected circuit. The |E(G) \ E(T )| fundamental circuits with respect to T form a cycle basis of G, and the |E(T )| fundamental cuts with respect to T form a cocycle basis of G.
Proof: The vectors associated with the fundamental circuits are linearly independent since each fundamental circuit contains an element not belonging to any other. The same holds for the fundamental cuts. Since the vector spaces are orthogonal to each other by Proposition 2.10, the sum of their dimensions cannot exceed |E(G)| = |E(G) \ E(T )| + |E(T )|.
The fundamental cuts have a nice property which we shall exploit quite often and which we shall discuss now. Let T be a digraph whose underlying undirected graph is a tree. Consider the family F := {Ce : e ∈ E(T )}, where for e = (x, y) ∈ E(T ) we denote by Ce the connected component of T − e containing y (so δ(Ce) is the fundamental cut of e with respect to T ). If T is an arborescence, then any two elements of F are either disjoint or one is a subset of the other. In general F is at least cross-free:


a bc d e f g
a b,c
d
e
f
g
(a) (b)
Fig. 2.1.
Definition 2.12. A set system is a pair (U, F ), where U is a nonempty finite set and F a family of subsets of U . (U, F ) is cross-free if for any two sets X, Y ∈ F , at least one of the four sets X \ Y , Y \ X, X ∩ Y , U \ (X ∪ Y ) is empty. (U, F ) is laminar if for any two sets X, Y ∈ F , at least one of the three sets X \ Y , Y \ X, X ∩ Y is empty.
In the literature set systems are also known as hypergraphs. See Figure 2.1(a) for an illustration of the laminar family {{a}, {b, c}, {a, b, c}, {a, b, c, d}, { f }, { f, g}}. Another word used for laminar is nested. Whether a set system (U, F ) is laminar does not depend on U , so we sometimes simply say that F is a laminar family. However, whether a set system is cross-free can depend on the ground set U . If U contains an element that does not belong to any set of F , then F is cross-free if and only if it is laminar. Let r ∈ U be arbitrary. It follows directly from the definition that a set system (U, F ) is cross-free if and only if
F ′ := {X ∈ F : r ∈ X } ∪ {U \ X : X ∈ F , r ∈ X }
is laminar. Hence cross-free families are sometimes depicted similarly to laminar families: for example, Figure 2.2(a) shows the cross-free family {{b, c, d, e, f }, {c}, {a, b, c}, {e}, {a, b, c, d, f }, {e, f }}; a square corresponds to the set containing all elements outside. While oriented trees lead to cross-free families the converse is also true: every cross-free family can be represented by a tree in the following sense:
Definition 2.13. Let T be a digraph such that the underlying undirected graph is a tree. Let U be a finite set and φ : U → V (T ). Let F := {Se : e ∈ E(T )}, where for e = (x, y) we define
Se := {s ∈ U : φ(s) is in the same connected component of T − e as y}.
Then (T, φ) is called a tree-representation of (U, F ).


a bc d e f
a
b
c
d
e
f
(a) (b)
Fig. 2.2.
See Figures 2.1(b) and 2.2(b) for examples.
Proposition 2.14. Let (U, F ) be a set system with a tree-representation (T, φ). Then (U, F ) is cross-free. If T is an arborescence, then (U, F ) is laminar. Moreover, every cross-free family has a tree-representation, and for laminar families, an arborescence can be chosen as T .
Proof: If (T, φ) is a tree-representation of (U, F ) and e = (v, w), f = (x, y) ∈ E(T ), we have an undirected v-x-path P in T (ignoring the orientations). There are four cases: If w, y ∈/ V (P) then Se ∩ S f = ∅ (since T contains no circuit). If w ∈/ V (P) and y ∈ V (P) then Se ⊆ S f . If y ∈/ V (P) and w ∈ V (P) then S f ⊆ Se. If w, y ∈ V (P) then Se ∪ S f = U . Hence (U, F ) is cross-free. If T is an arborescence, the last case cannot occur (otherwise at least one vertex of P would have two entering edges), so F is laminar.
To prove the converse, let F first be a laminar family. We define V (T ) := F .∪ {r } and E(T ) :=
{(X, Y ) ∈ F × F : X ⊃ Y = ∅ and there is no Z ∈ F with X ⊃ Z ⊃ Y }
∪ {(r, X) : X = ∅ ∈ F or X is a maximal element of F } .
We set φ(x) := X , where X is the minimal set in F containing x, and φ(x) := r if no set in F contains x. Obviously, T is an arborescence rooted at r , and (T, φ) is a tree-representation of F . Now let F be a cross-free family of subsets of U . Let r ∈ U . As noted above,
F ′ := {X ∈ F : r ∈ X } ∪ {U \ X : X ∈ F , r ∈ X }
is laminar, so let (T, φ) be a tree-representation of (U, F′). Now for an edge e ∈ E(T ) there are three cases: If Se ∈ F and U \ Se ∈ F , we replace the edge e = (x, y) by two edges (x, z) and (y, z), where z is a new vertex. If Se ∈ F and U \ Se ∈ F , we replace the edge e = (x, y) by (y, x). If Se ∈ F and U \ Se ∈ F ,


we do nothing. Let T ′ be the resulting graph. Then (T ′, φ) is a tree-representation of (U, F ).
The above result is mentioned by Edmonds and Giles [1977] but was probably known earlier.
Corollary 2.15. A laminar family of distinct subsets of U has at most 2|U | elements. A cross-free family of distinct subsets of U has at most 4|U | − 2 elements.
Proof: We first consider a laminar family F of distinct nonempty proper subsets of U . We prove that |F | ≤ 2|U | − 2. Let (T, φ) be a tree-representation, where T is an arborescence whose number of vertices is as small as possible. For every w ∈ V (T ) we have either |δ+(w)| ≥ 2 or there exists an x ∈ U with φ(x) = w or both. (For the root this follows from U ∈/ F , for the leaves from ∅ ∈/ F , for all other vertices from the minimality of T .) There can be at most |U | vertices w with φ(x) = w for some x ∈ U and at most
⌊ |E(T )| 2
⌋
vertices w with |δ+(w)| ≥ 2. So |E(T )| + 1 = |V (T )| ≤ |U | + |E(T )|
2
and thus |F | = |E(T )| ≤ 2|U | − 2. Now let (U, F ) be a cross-free family with ∅, U ∈/ F , and let r ∈ U . Since
F′ := {X ∈ F : r ∈ X } ∪ {U \ X : X ∈ F , r ∈ X }
is laminar, we have |F′| ≤ 2|U | − 2. Hence |F | ≤ 2|F ′| ≤ 4|U | − 4. The proof is concluded by taking ∅ and U as possible members of F into account.
2.3 Connectivity
Connectivity is a very important concept in graph theory. For many problems it suffices to consider connected graphs, since otherwise we can solve the problem for each connected component separately. So it is a fundamental task to detect the connected components of a graph. The following simple algorithm finds a path from a specified vertex s to all other vertices that are reachable from s. It works for both directed and undirected graphs. In the undirected case it builds a maximal tree containing s; in the directed case it constructs a maximal arborescence rooted at s.
GRAPH SCANNING ALGORITHM
Input: A graph G (directed or undirected) and some vertex s.
Output: The set R of vertices reachable from s, and a set T ⊆ E(G) such that (R, T ) is an arborescence rooted at s, or a tree.
©1 Set R := {s}, Q := {s} and T := ∅.
©2 If Q = ∅ then stop,
else choose a v ∈ Q.


©3 Choose a w ∈ V (G) \ R with e = (v, w) ∈ E(G) or e = {v, w} ∈ E(G). If there is no such w then set Q := Q \ {v} and go to ©2 .
©4 Set R := R ∪ {w}, Q := Q ∪ {w} and T := T ∪ {e}. Go to ©2 .
Proposition 2.16. The GRAPH SCANNING ALGORITHM works correctly.
Proof: At any time, (R, T ) is a tree or an arborescence rooted at s. Suppose at the end there is a vertex w ∈ V (G) \ R that is reachable from s. Let P be an s-w-path, and let {x, y} or (x, y) be an edge of P with x ∈ R and y ∈/ R. Since x has been added to R, it also has been added to Q at some time during the execution of the algorithm. The algorithm does not stop before removing x from Q. But this is done in ©3 only if there is no edge {x, y} or (x, y) with y ∈/ R.
Since this is the first graph algorithm in this book we discuss some implementation issues. The first question is how the graph is given. There are several natural ways. For example, one can think of a matrix with a row for each vertex and a column for each edge. The incidence matrix of an undirected graph G is the matrix A = (av,e)v∈V (G), e∈E(G) where
av,e =
{
1 if v ∈ e
0 if v ∈ e .
The incidence matrix of a digraph G is the matrix A = (av,e)v∈V (G), e∈E(G) where
av,(x,y) =
⎧⎪⎨
⎪⎩
−1 if v = x
1 if v = y
0 if v ∈ {x, y}
.
Of course this is not very efficient since each column contains only two nonzero entries. The space needed for storing an incidence matrix is obviously O(nm), where n := |V (G)| and m := |E(G)|. A better way is a matrix whose rows and columns are indexed by the vertex set. The adjacency matrix of a simple graph G is the 0-1-matrix A = (av,w)v,w∈V (G) with av,w = 1 iff {v, w} ∈ E(G) or (v, w) ∈ E(G). For graphs with parallel edges we can define av,w to be the number of edges from v to w. An adjacency matrix requires O(n2) space for simple graphs. The adjacency matrix is appropriate if the graph is dense, i.e. has (n2) edges (or more). For sparse graphs, say with O(n) edges only, one can do much better. Besides storing the number of vertices we can simply store a list of the edges, for each edge noting its endpoints. If we address each vertex by a number from 1 to n, the space needed for each edge is O(log n). Hence we need O(m log n) space altogether. Just storing the edges in an arbitrary order is not very convenient. Almost all graph algorithms require finding the edges incident to a given vertex. Thus one


should have a list of incident edges for each vertex. In case of directed graphs, two lists, one for entering edges and one for leaving edges, are appropriate. This data structure is called adjacency list; it is the most customary one for graphs. For direct access to the list(s) of each vertex we have pointers to the heads of all lists; these can be stored with O(n log m) additional bits. Hence the total number of bits required for an adjacency list is O(n log m + m log n). Whenever a graph is part of the input of an algorithm in this book, we assume that the graph is given by an adjacency list. As for elementary operations on numbers (see Section 1.2), we assume that elementary operations on vertices and edges take constant time only. This includes scanning an edge, identifying its ends and accessing the head of the adjacency list for a vertex. The running time will be measured by the parameters n and m, and an algorithm running in O(m + n) time is called linear. We shall always use the letters n and m for the number of vertices and the number of edges. For many graph algorithms it causes no loss of generality to assume that the graph at hand is simple and connected; hence n − 1 ≤ m < n2. Among parallel edges we often have to consider only one, and different connected components can often be analyzed separately. The preprocessing can be done in linear time in advance; see Exercise 17 and the following. We can now analyze the running time of the GRAPH SCANNING ALGORITHM:
Proposition 2.17. The GRAPH SCANNING ALGORITHM can be implemented to run in O(m) time. The connected components of an undirected graph can be determined in linear time.
Proof: We assume that G is given by an adjacency list. Implement Q by a simple list, such that ©2 takes constant time. For each vertex x that we insert into Q we introduce a pointer current(x), indicating the current edge in the list containing all edges in δ(x) or δ+(x) (this list is part of the input). Initially current(x) is set to the first element of the list. In ©3 , the pointer moves forward. When the end of the list is reached, x is removed from Q and will never be inserted again. So the overall running time is proportional to the number of vertices reachable from s plus the number of edges, i.e. O(m). To identify the connected components of a graph, we apply the algorithm once and check if R = V (G). If so, the graph is connected. Otherwise R is a connected component, and we apply the algorithm to (G, s′) for an arbitrary vertex s′ ∈ V (G) \ R (and iterate until all vertices have been scanned, i.e. added to R). Again, no edge is scanned more than twice, so the overall running time remains linear.
An interesting question is in which order the vertices are chosen in ©3 . Obviously we cannot say much about this order if we do not specify how to choose a v ∈ Q in ©2 . Two methods are frequently used; they are called DEPTH-FIRST SEARCH (DFS) and BREADTH-FIRST SEARCH (BFS). In DFS we choose the v ∈ Q that was the last to enter Q. In other words, Q is implemented as a LIFO-stack (last


in-first-out). In BFS we choose the v ∈ Q that was the first to enter Q. Here Q is implemented by a FIFO-queue (first-in-first-out). An algorithm similar to DFS has been described already before 1900 by Trémaux and Tarry; see König [1936]. BFS seems to have been mentioned first by Moore [1959]. Trees (in the directed case: arborescences) (R, T ) computed by DFS and BFS are called DFS-tree and BFS-tree, respectively. For BFS-trees we note the following important property:
Proposition 2.18. A BFS-tree contains a shortest path from s to each vertex reachable from s. The values distG (s, v) for all v ∈ V (G) can be determined in linear time.
Proof: We apply BFS to (G, s) and add two statements: initially (in ©1 of the GRAPH SCANNING ALGORITHM) we set l(s) := 0, and in ©4 we set l(w) := l(v) + 1. We obviously have that l(v) = dist(R,T )(s, v) for all v ∈ R, at any stage of the algorithm. Moreover, if v is the currently scanned vertex (chosen in ©2 ), at this time there is no vertex w ∈ R with l(w) > l(v) + 1 (because the vertices are scanned in an order with nondecreasing l-values). Suppose that when the algorithm terminates there is a vertex w ∈ V (G) with distG (s, w) < dist(R,T )(s, w); let w have minimum distance from s in G with this property. Let P be a shortest s-w-path in G, and let e = (v, w) or e = {v, w} be the last edge in P. We have distG (s, v) = dist(R,T )(s, v), but e does not belong to T . Moreover, l(w) = dist(R,T )(s, w) > distG (s, w) = distG (s, v) + 1 = dist(R,T )(s, v)+ 1 = l(v)+ 1. This inequality combined with the above observation proves that w did not belong to R when v was removed from Q. But this contradicts ©3 because of edge e.
This result will also follow from the correctness of DIJKSTRA’S ALGORITHM for the SHORTEST PATH PROBLEM, which can be thought of as a generalization of BFS to the case where we have nonnegative weights on the edges (see Section 7.1). We now show how to identify the strongly connected components of a digraph. Of course, this can easily be done by using n times DFS (or BFS). However, it is possible to find the strongly connected components by visiting every edge only twice:
STRONGLY CONNECTED COMPONENT ALGORITHM
Input: A digraph G.
Output: A function comp : V (G) → N indicating the membership of the strongly connected components.
©1 Set R := ∅. Set N := 0.
©2 For all v ∈ V (G) do: If v ∈/ R then VISIT1(v).
©3 Set R := ∅. Set K := 0.
©4 For i := |V (G)| down to 1 do:
If ψ−1(i ) ∈/ R then set K := K + 1 and VISIT2(ψ−1(i )).


VISIT1(v)
©1 Set R := R ∪ {v}.
©2 For all w with (v, w) ∈ E(G) do: If w ∈/ R then VISIT1(w).
©3 Set N := N + 1, ψ(v) := N and ψ−1(N) := v.
VISIT2(v)
©1 Set R := R ∪ {v}.
©2 For all w with (w, v) ∈ E(G) do: If w ∈/ R then VISIT2(w).
©3 Set comp(v) := K .
Figure 2.3 shows an example: The first DFS scans the vertices in the order a, g, b, d, e, f and produces the arborescence shown in the middle; the numbers are the ψ-labels. Vertex c is the only one that is not reachable from a; it gets the highest label ψ(c) = 7. The second DFS starts with c but cannot reach any other vertex via a reverse edge. So it proceeds with vertex a because ψ(a) = 6. Now b, g and f can be reached. Finally e is reached from d. The strongly connected components are {c}, {a, b, f, g} and {d, e}. In summary, one DFS is needed to find an appropriate numbering, while in the second DFS the reverse graph is considered and the vertices are processed in decreasing order with respect to this numbering. Each connected component of the second DFS-forest is an anti-arborescence, a graph arising from an arborescence by reversing every edge. We show that these anti-arborescences identify the strongly connected components.
Theorem 2.19. The STRONGLY CONNECTED COMPONENT ALGORITHM identifies the strongly connected components correctly in linear time.
b b1
b
cc 7
c
aa 6
a
d 3d d
e e2 e
f f4 f
gg
5
g
Fig. 2.3.


Proof: The running time is obviously O(n + m). Of course, vertices of the same strongly connected component are always in the same component of any DFS-forest, so they get the same comp-value. We have to prove that two vertices u and v with comp(u) = comp(v) indeed lie in the same strongly connected component. Let r (u) and r (v) be the vertex reachable from u and v with the highest ψ-label, respectively. Since comp(u) = comp(v), i.e. u and v lie in the same anti-arborescence of the second DFS-forest, r := r (u) = r (v) is the root of this anti-arborescence. So r is reachable from both u and v. Since r is reachable from u and ψ(r ) ≥ ψ(u), r has not been added to R after u in the first DFS, and the first DFS-forest contains an r -u-path. In other words, u is reachable from r . Analogously, v is reachable from r . Altogether, u is reachable from v and vice versa, proving that indeed u and v belong to the same strongly connected component.
It is interesting that this algorithm also solves another problem: finding a topological order of an acyclic digraph. Observe that contracting the strongly connected components of any digraph yields an acyclic digraph. By Proposition 2.9 this acyclic digraph has a topological order. In fact, such an order is given by the numbers com p(v) computed by the STRONGLY CONNECTED COMPONENT ALGORITHM:
Theorem 2.20. The STRONGLY CONNECTED COMPONENT ALGORITHM determines a topological order of the digraph resulting from contracting each strongly connected component of G. In particular, we can for any given digraph either find a topological order or decide that none exists in linear time.
Proof: Let X and Y be two strongly connected components of a digraph G, and suppose the STRONGLY CONNECTED COMPONENT ALGORITHM computes comp(x) = k1 for x ∈ X and comp(y) = k2 for y ∈ Y with k1 < k2. We claim
that E+
G (Y, X ) = ∅.
Suppose that there is an edge (y, x) ∈ E(G) with y ∈ Y and x ∈ X . All vertices in X are added to R in the second DFS before the first vertex of Y is added. In particular we have x ∈ R and y ∈/ R when the edge (y, x) is scanned in the second DFS. But this means that y is added to R before K is incremented, contradicting comp(y) = comp(x). Hence the com p-values computed by the STRONGLY CONNECTED COMPONENT ALGORITHM determine a topological order of the digraph resulting from contracting the strongly connected components. The second statement of the theorem now follows from Proposition 2.9 and the observation that a digraph is acyclic if and only if its strongly connected components are the singletons.
A linear-time algorithm that identifies the strongly connected components was first given by Karzanov [1970] and Tarjan [1972]. The problem of finding a topological order (or deciding that none exists) was solved earlier (Kahn [1962], Knuth [1968]). Both BFS and DFS occur as subroutines in many other combinatorial algorithms. Some examples will appear in later chapters.


Sometimes one is interested in higher connectivity. Let k ≥ 2. An undirected graph with more than k vertices and the property that it remains connected even if we delete any k − 1 vertices, is called k-connected. A graph with at least two vertices is k-edge-connected if it remains connected after deleting any k −1 edges. So a connected graph with at least three vertices is 2-connected (2-edge-connected) if and only if it has no articulation vertex (no bridge, respectively). The largest k and l such that a graph G is k-connected and l-edge-connected are called the vertex-connectivity and edge-connectivity of G. Here we say that a graph is 1-connected (and 1-edge-connected) if it is connected. A disconnected graph has vertex-connectivity and edge-connectivity zero. The blocks of an undirected graph are its maximal connected subgraphs without articulation vertex. Thus each block is either a maximal 2-connected subgraph, or consists of a bridge or an isolated vertex. Two blocks have at most one vertex in common, and a vertex belonging to more than one block is an articulation vertex. The blocks of an undirected graph can be determined in linear time quite similarly to the STRONGLY CONNECTED COMPONENT ALGORITHM; see Exercise 21. Here we prove a nice structure theorem for 2-connected graphs. We construct graphs from a single vertex by sequentially adding ears:
Definition 2.21. Let G be a graph (directed or undirected). An ear-decomposition of G is a sequence r, P1, . . . , Pk with G = ({r }, ∅)+ P1+· · ·+ Pk, such that each Pi is either a path where exactly the endpoints belong to {r } ∪ V (P1) ∪ · · · ∪ V (Pi−1), or a circuit where exactly one of its vertices belongs to {r } ∪ V (P1) ∪ · · · ∪ V (Pi−1) (i ∈ {1, . . . , k}).
P1, . . . , Pk are called ears. If k ≥ 1, P1 is a circuit of length at least three, and P2, . . . , Pk are paths, then the ear-decomposition is called proper.
Theorem 2.22. (Whitney [1932]) An undirected graph is 2-connected if and only if it has a proper ear-decomposition.
Proof: Evidently a circuit of length at least three is 2-connected. Moreover, if G is 2-connected, then so is G + P, where P is an x-y-path, x, y ∈ V (G) and x = y: deleting any vertex does not destroy connectivity. We conclude that a graph with a proper ear-decomposition is 2-connected. To show the converse, let G be a 2-connected graph. Let G′ be the maximal simple subgraph of G; evidently G′ is also 2-connected. Hence G′ cannot be a tree; i.e. it contains a circuit. Since it is simple, G′, and thus G, contains a circuit of length at least three. So let H be a maximal subgraph of G that has a proper ear-decomposition; H exists by the above consideration. Suppose H is not spanning. Since G is connected, we then know that there exists an edge e = {x, y} ∈ E(G) with x ∈ V (H ) and y ∈/ V (H ). Let z be a vertex in V (H ) \ {x}. Since G − x is connected, there exists a path P from y to z in G − x. Let z′ be the first vertex on this path, when traversed from y, that belongs to V (H ). Then P[y,z′] + e can be added as an ear, contradicting the maximality of H .


Thus H is spanning. Since each edge of E(G) \ E(H ) can be added as an ear, we conclude that H = G.
See Exercise 22 for similar characterizations of 2-edge-connected graphs and strongly connected digraphs.
2.4 Eulerian and Bipartite Graphs
Euler’s work on the problem of traversing each of the seven bridges of Königsberg exactly once was the origin of graph theory. He showed that the problem had no solution by defining a graph, asking for a walk containing all edges, and observing that more than two vertices had odd degree.
Definition 2.23. An Eulerian walk in a graph G is a closed walk containing every edge. An undirected graph G is called Eulerian if the degree of each vertex is even. A digraph G is Eulerian if |δ−(v)| = |δ+(v)| for each v ∈ V (G).
Although Euler neither proved sufficiency nor considered the case explicitly in which we ask for a closed walk, the following famous result is usually attributed to him:
Theorem 2.24. (Euler [1736], Hierholzer [1873]) A connected (directed or undirected) graph has an Eulerian walk if and only if it is Eulerian.
Proof: The necessity of the degree conditions is obvious, as a vertex appearing k times in an Eulerian walk (or k + 1 times if it is the first and the last vertex) must have in-degree k and out-degree k, or degree 2k in the undirected case. For the sufficiency, let W = v1, e1, v2, . . . , vk , ek, vk+1 be a longest walk in G, i.e. one with maximum number of edges. In particular, W must contain all edges leaving vk+1, which implies vk+1 = v1 by the degree conditions. So W is closed. Suppose that W does not contain all edges. As G is connected, we then conclude that there is an edge e ∈ E(G) for which e does not appear in W , but at least one of its endpoints appears in W , say vi . Then e can be combined with vi , ei , vi+1, . . . , ek , vk+1 = v1, e1, v2, . . . , ei−1, vi to a walk which is longer than W .
The following algorithm accepts as input only connected Eulerian graphs. Note that one can check in linear time whether a given graph is connected (Theorem 2.17) and Eulerian (trivial). The algorithm first chooses an initial vertex, then calls a recursive procedure. We first describe it for undirected graphs:
EULER’S ALGORITHM
Input: An undirected connected Eulerian graph G.
Output: An Eulerian walk W in G.


©1 Choose v1 ∈ V (G) arbitrarily. Return W := EULER(v1).
EULER(v1)
©1 Set W := v1 and x := v1.
©2 If δ(x) = ∅ then go to ©4 .
Else let e ∈ δ(x), say e = {x, y}.
©3 Set W := W, e, y and x := y. Set E(G) := E(G) \ {e} and go to ©2 .
©4 Let v1, e1, v2, e2, . . . , vk , ek , vk+1 be the sequence W . For i := 2 to k do: Set Wi := EULER(vi ).
©5 Return W := v1, e1, W2, e2, . . . , Wk , ek , vk+1.
For digraphs, ©2 has to be replaced by:
©2 If δ+(x) = ∅ then go to ©4 .
Else let e ∈ δ+(x), say e = (x, y).
We can analyze both versions (undirected and directed) simultaneously:
Theorem 2.25. EULER’S ALGORITHM works correctly. Its running time is O(m), where m = |E(G)|.
Proof: We show that EULER(v1), if called for an Eulerian graph G and v1 ∈ V (G), returns an Eulerian walk in the connected component G1 of G that contains v1. We use induction on |E(G)|, the case E(G) = ∅ being trivial. Because of the degree conditions, vk+1 = x = v1 when ©4 is reached. So at
this stage W is a closed walk. Let G′ be the graph G at this stage. G′ is also Eulerian. For each edge e ∈ E(G1) ∩ E(G′) there exists a minimum i ∈ {2, . . . , k} such
that e is in the same connected component of G′ as vi (note that v1 = vk+1 is
isolated in G′). Then by the induction hypothesis e belongs to Wi . So the closed walk W composed in ©5 is indeed an Eulerian walk in G1. The running time is linear because each edge is deleted immediately after being examined.
EULER’S ALGORITHM will be used several times as a subroutine in later chapters. Sometimes one is interested in making a given graph Eulerian by adding or contracting edges. Let G be an undirected graph and F a family of unordered pairs
of V (G) (edges or not). F is called an odd join if (V (G), E(G) .∪ F) is Eulerian. F is called an odd cover if the graph which results from G by contracting the vertex set of each connected component of (V (G), F) is Eulerian. Both concepts are equivalent in the following sense.


Theorem 2.26. (Aoshima and Iri [1977]) For any undirected graph we have:
(a) Every odd join is an odd cover. (b) Every minimal odd cover is an odd join.
Proof: Let G be an undirected graph. To prove (a), let F be an odd join. We build a graph G′ by contracting the connected components of (V (G), F) in G. Each of these connected components contains an even number of odd-degree vertices (with respect to F and thus with respect to G, because F is an odd join). So the resulting graph has even degrees only. Thus F is an odd cover. To prove (b), let F be a minimal odd cover. Because of the minimality, (V (G), F) is a forest. We have to show that |δF (v)| ≡ |δG(v)| (mod 2) for each v ∈ V (G). So let v ∈ V (G). Let C1, . . . , Ck be the connected components of (V (G), F) − v that contain a vertex w with {v, w} ∈ F. Since F is a forest, k = |δF (v)|.
As F is an odd cover, contracting X := V (C1) ∪ · · · ∪ V (Ck ) ∪ {v} in G yields a vertex of even degree, i.e. |δG (X )| is even. On the other hand, because of the minimality of F, F \ {{v, w}} is not an odd cover (for any w with {v, w} ∈ F), so |δG (V (Ci ))| is odd for i = 1, . . . , k. Since
k ∑
i =1
|δG (V (Ci ))| = |δG(X )| + |δG (v)|
− 2|EG({v}, V (G) \ X )| + 2
∑
1≤i< j ≤k
|EG(Ci , C j )|,
we conclude that k has the same parity as |δG (v)|.
We shall return to the problem of making a graph Eulerian in Section 12.2. A bipartition of an undirected graph G consists of disjoint sets A and B whose union is V (G) such that every edge has an endpoint in A and an endpoint in B. A graph is called bipartite if it has a bipartition. The simple bipartite graph G with
V (G) = A .∪ B, |A| = n, |B| = m and E(G) = {{a, b} : a ∈ A, b ∈ B} is denoted
by Kn,m (the complete bipartite graph). When we write G = (A .∪ B, E(G)), we
mean that A .∪ B is a bipartition of G.
Proposition 2.27. (König [1916]) An undirected graph is bipartite if and only if it contains no odd circuit (circuit of odd length). There is a linear-time algorithm which, given an undirected graph G, either finds a bipartition or an odd circuit.
Proof: Suppose G is bipartite with bipartition V (G) = A .∪ B, and the closed walk v1, e1, v2, . . . , vk , ek, vk+1 defines some circuit in G. W.l.o.g. v1 ∈ A. But then v2 ∈ B, v3 ∈ A, and so on. We conclude that vi ∈ A if and only if i is odd. But vk+1 = v1 ∈ A, so k must be even. To prove the sufficiency, we may assume that G is connected, since a graph is bipartite iff each connected component is (and the connected components can


be determined in linear time; Proposition 2.17). We choose an arbitrary vertex s ∈ V (G) and apply BFS to (G, s) in order to obtain the distances from s to v for all v ∈ V (G) (see Proposition 2.18). Let T be the resulting BFS-tree. Define A := {v ∈ V (G) : distG (s, v) is even} and B := V (G) \ A. If there is an edge e = {x, y} in G[ A] or G[B], the x-y-path in T together with e forms an odd circuit in G. If there is no such edge, we have a bipartition.
2.5 Planarity
We often draw graphs in the plane. A graph is called planar if it can be drawn such that no pair of edges intersect. To formalize this concept we need the following topological terms:
Definition 2.28. A simple Jordan curve is the image of a continuous injective function φ : [0, 1] → R2; its endpoints are φ(0) and φ(1). A closed Jordan curve is the image of a continuous function φ : [0, 1] → R2 with φ(0) = φ(1) and φ(τ ) = φ(τ ′) for 0 ≤ τ < τ ′ < 1. A polygonal arc is a simple Jordan curve which is the union of finitely many intervals (straight line segments). A polygon is a closed Jordan curve which is the union of finitely many intervals. Let R = R2 \ J , where J is the union of finitely many intervals. We define the connected regions of R as equivalence classes where two points in R are equivalent if they can be joined by a polygonal arc within R.
Definition 2.29. A planar embedding of a graph G consists of an injective mapping ψ : V (G) → R2 and for each e = {x, y} ∈ E(G) a polygonal arc Je with endpoints ψ(x) and ψ(y), such that for each e = {x, y} ∈ E(G):
(Je \ {ψ(x), ψ(y)}) ∩
⎛
⎝{ψ(v) : v ∈ V (G)} ∪ ⋃
e′ ∈ E (G )\{e}
Je′
⎞
⎠ = ∅.
A graph is called planar if it has a planar embedding. Let G be a (planar) graph with some fixed planar embedding = (ψ, (Je)e∈E(G)). After removing the points and polygonal arcs from the plane, the remainder,
R := R2 \
⎛
⎝{ψ(v) : v ∈ V (G)} ∪ ⋃
e∈ E (G )
Je
⎞
⎠,
splits into open connected regions, called faces of .
For example, K4 is obviously planar but it will turn out that K5 is not planar. Exercise 29 shows that restricting ourselves to polygonal arcs instead of arbitrary Jordan curves makes no substantial difference. We will show later that for simple graphs it is indeed sufficient to consider straight line segments only. Our aim is to characterize planar graphs. Following Thomassen [1981], we first prove the following topological fact, a version of the Jordan curve theorem:


Theorem 2.30. If J is a polygon, then R2 \ J splits into exactly two connected regions, each of which has J as its boundary. If J is a polygonal arc, then R2 \ J has only one connected region.
Proof: Let J be a polygon, p ∈ R2 \ J and q ∈ J . Then there exists a polygonal arc in (R2 \ J ) ∪ {q} joining p and q: starting from p, one follows the straight line towards q until one gets close to J , then one proceeds within the vicinity of J . (We use the elementary topological fact that disjoint compact sets, in particular non-adjacent intervals of J , have a positive distance from each other.) We conclude that p is in the same connected region of R2 \ J as some points arbitrarily close to q. J is the union of finitely many intervals; one or two of these intervals contain q. Let > 0 such that the ball with center q and radius intersects no other interval of J ; then clearly this ball intersects at most two connected regions. Since p ∈ R2 \ J and q ∈ J were chosen arbitrarily, we conclude that there are at most two regions and each region has J as its boundary. Since the above also holds if J is a polygonal arc and q is an endpoint of J , R2 \ J has only one connected region in this case. Returning to the case when J is a polygon, it remains to prove that R2 \ J has more than one region. For any p ∈ R2 \ J and any angle α we consider the ray lα starting at p with angle α. J ∩ lα is a set of points or closed intervals. Let cr ( p, lα) be the number of these points or intervals that J enters from a different side of lα than to which it leaves (the number of times J “crosses” lα; e.g. in Figure 2.4 we have cr ( p, lα) = 2). Note that for any angle α,
∣∣∣∣ lim
→0, >0 cr ( p, lα− ) − cr ( p, lα)
∣∣∣∣ and
∣∣∣∣ lim
→0, >0 cr ( p, lα+ ) − cr ( p, lα)
∣∣∣∣
are even integers: twice the number of points and intervals of J ∩lα that J enters from the same side to which it leaves (left side and right side, respectively). Therefore g( p, α) := (cr ( p, lα) mod 2) is a continuous function in α, so it is constant and we denote it by g( p). Clearly g( p) is constant for points p on each straight line
p
lα
J
J
J
Fig. 2.4.


not intersecting J , so it is constant within each region. However, g( p) = g(q) for points p, q such that the straight line segment joining p and q intersects J exactly once. Hence there are indeed two regions.
Exactly one of the faces, the outer face, is unbounded.
Proposition 2.31. Let G be a 2-connected graph with a planar embedding . Then every face is bounded by a circuit, and every edge is on the boundary of exactly two faces. Moreover, the number of faces is |E(G)| − |V (G)| + 2.
Proof: By Theorem 2.30 both assertions are true if G is a circuit. For general 2-connected graphs we use induction on the number of edges, using Theorem 2.22. Consider a proper ear-decomposition of G, and let P be the last ear, a path with endpoints x and y, say. Let G′ be the graph before adding the last ear, and let ′ be the restriction of to G′.
Let = (ψ, ( Je)e∈E(G)). Let F′ be the face of ′ containing ⋃
e∈E(P) Je \
{ψ(x), ψ(y)}. By induction, F′ is bounded by a circuit C. C contains x and y, so C is the union of two x-y-paths Q1, Q2 in G′. Now we apply Theorem 2.30 to each of the circuits Q1 + P and Q2 + P. We conclude that
F′ ∪ {ψ(x ), ψ(y)} = F1
.∪ F2
.∪ ⋃
e∈ E ( P )
Je
and F1 and F2 are two faces of G bounded by the circuits Q1 + P and Q2 + P,
respectively. Hence G has one more face than G′. Using |E(G)\ E(G′)| = |V (G)\ V (G′)| + 1, this completes the induction step.
This proof is due to Tutte. It also implies easily that the circuits bounding the finite faces constitute a cycle basis (Exercise 30). The last statement of Proposition 2.31 is known as Euler’s formula; it holds for general connected graphs:
Theorem 2.32. (Euler [1958], Legendre [1794]) For any planar connected graph G with any embedding, the number of faces is |E(G)| − |V (G)| + 2.
Proof: We have already proved the statement for 2-connected graphs (Proposition 2.31). Moreover, the assertion is trivial if |V (G)| = 1 and follows from Theorem 2.30 if |E(G)| = 1. If |V (G)| = 2 and |E(G)| ≥ 2, then we can subdivide one edge e, thereby increasing the number of vertices and the number of edges by one and making the graph 2-connected, and apply Proposition 2.31. So we may now assume that G has an articulation vertex x; we proceed by induction on the number of vertices. Let be an embedding of G. Let C1, . . . , Ck be the connected components of G − x; and let i be the restriction of to Gi := G[V (Ci ) ∪ {x}] for i = 1, . . . , k.


The set of inner (bounded) faces of is the disjoint union of the sets of inner faces of i , i = 1, . . . , k. By applying the induction hypothesis to (Gi , i ), i = 1, . . . , k, we get that the total number of inner faces of (G, ) is
k ∑
i =1
(|E(Gi )|−|V (Gi )|+1) = |E(G)|−
k ∑
i =1
|V (Gi )\{x}| = |E(G)|−|V (G)|+1.
Taking the outer face into account concludes the proof.
In particular, the number of faces is independent of the embedding. The average degree of a simple planar graph is less than 6:
Corollary 2.33. Let G be a 2-connected simple planar graph whose minimum circuit has length k (we also say that G has girth k). Then G has at most (n −2) k
k−2
edges. Any simple planar graph with n ≥ 3 vertices has at most 3n − 6 edges.
Proof: First assume that G is 2-connected. Let some embedding of G be given, and let r be the number of faces. By Euler’s formula (Theorem 2.32), r = |E(G)| − |V (G)| + 2. By Proposition 2.31, each face is bounded by a circuit, i.e. by at least k edges, and each edge is on the boundary of exactly two faces. Hence kr ≤ 2|E(G)|. Combining the two results we get |E(G)|−|V (G)|+2 ≤ 2
k |E(G)|,
implying |E(G)| ≤ (n − 2) k
k−2 .
If G is not 2-connected we add edges between non-adjacent vertices to make it 2-connected while preserving planarity. By the first part we have at most (n −2) 3
3−2
edges, including the new ones.
Now we show that certain graphs are non-planar:
Corollary 2.34. Neither K5 nor K3,3 is planar.
Proof: This follows directly from Corollary 2.33: K5 has five vertices but 10 >
3·5−6 edges; K3,3 is 2-connected, has girth 4 (as it is bipartite) and 9 > (6−2) 4
4−2
edges.
Figure 2.5 shows these two graphs, which are the smallest non-planar graphs. We shall prove that every non-planar graph contains, in a certain sense, K5 or K3,3. To make this precise we need the following notion:
Fig. 2.5.


Definition 2.35. Let G and H be two undirected graphs. G is a minor of H if there exists a subgraph H ′ of H and a partition V (H ′) = V1
.∪ · · · .∪ Vk of its vertex set into connected subsets such that contracting each of V1, . . . , Vk yields a graph which is isomorphic to G.
In other words, G is a minor of H if it can be obtained from H by a series of operations of the following type: delete a vertex, delete an edge or contract an edge. Since neither of these operations destroys planarity, any minor of a planar graph is planar. Hence a graph which contains K5 or K3,3 as a minor cannot be planar. Kuratowski’s Theorem says that the converse is also true. We first consider 3-connected graphs and start with the following lemma (which is the heart of Tutte’s so-called wheel theorem):
Lemma 2.36. (Tutte [1961], Thomassen [1980]) Let G be a 3-connected graph with at least five vertices. Then there exists an edge e such that G/e is also 3-connected.
Proof: Suppose there is no such edge. Then for each edge e = {v, w} there exists a vertex x such that G − {v, w, x} is disconnected, i.e. has a connected component C with |V (C)| < |V (G)| − 3. Choose e, x and C such that |V (C)| is minimum. x has a neighbour y in C, because otherwise C is a connected component of G − {v, w} (but G is 3-connected). By our assumption, G/{x, y} is not 3connected, i.e. there exists a vertex z such that G − {x, y, z} is disconnected. Since {v, w} ∈ E(G), there exists a connected component D of G − {x, y, z} which contains neither v nor w. But D contains a neighbour d of y, since otherwise D is a connected component of G − {x, z} (again contradicting the fact that G is 3-connected). So d ∈ V (D) ∩ V (C), and thus D is a subgraph of C. Since y ∈ V (C) \ V (D), we have a contradiction to the minimality of |V (C)|.
Theorem 2.37. (Kuratowski [1930], Wagner [1937]) A 3-connected graph is planar if and only if it contains neither K5 nor K3,3 as a minor.
Proof: As the necessity is evident (see above), we prove the sufficiency. Since K4 is obviously planar, we proceed by induction on the number of vertices: let G be a 3-connected graph with more than four vertices but no K5 or K3,3 minor. By Lemma 2.36, there exists an edge e = {v, w} such that G/e is 3-connected.
Let = (ψ, ( Je′ )e′∈E(G/e)
) be a planar embedding of G/e, which exists by induction. Let x be the vertex in G/e which arises by contracting e. Consider (G/e) − x with the restriction of as a planar embedding. Since (G/e) − x is 2-connected, every face is bounded by a circuit (Proposition 2.31). In particular, the face containing the point ψ(x) is bounded by a circuit C. Let y1, . . . , yk ∈ V (C) be the neighbours of v that are distinct from w, numbered in cyclic order, and partition C into edge-disjoint paths Pi , i = 1, . . . , k, such that Pi is a yi -yi+1-path (yk+1 := y1). Suppose there exists an index i ∈ {1, . . . , k} such that (w) ⊆ {v} ∪ V (Pi ). Then a planar embedding of G can be constructed easily by modifying .


(a)
C
(b)
C
yi
yj
(c)
C
yi+ 1
yi
z
z
Fig. 2.6.
We shall prove that all other cases are impossible. First, if w has three neighbours among y1, . . . , yk, we have a K5 minor (Figure 2.6(a)). Next, if (w) = {v, yi , y j } for some i < j , then we must have i + 1 < j and (i, j ) = (1, k) (otherwise yi and y j would both lie on Pi or Pj ); see Figure 2.6(b). Otherwise there is a neighbour z of w in V (Pi ) \ {yi , yi+1} for some
i and another neighbour z′ ∈/ V (Pi ) (Figure 2.6(c)). In both cases, there are four
vertices y, z, y′, z′ on C, in this cyclic order, with y, y′ ∈ (v) and z, z′ ∈ (w). This implies that we have a K3,3 minor.
The proof implies quite directly that every 3-connected simple planar graph has a planar embedding where each edge is embedded by a straight line and each face, except the outer face, is convex (Exercise 33(a)). The general case of Kuratowski’s Theorem can be reduced to the 3-connected case by gluing together planar embeddings of the maximal 3-connected subgraphs, or by the following lemma:
Lemma 2.38. (Thomassen [1980]) Let G be a graph with at least five vertices which is not 3-connected and which contains neither K5 nor K3,3 as a minor. Then there exist two non-adjacent vertices v, w ∈ V (G) such that G + e, where e = {v, w} is a new edge, does not contain a K5 or K3,3 minor either.
Proof: We use induction on |V (G)|. Let G be as above. Without loss of generality, G is simple. If G is disconnected, we can simply add an edge e joining two different connected components. So henceforth we assume that G is connected. Since G is not 3-connected, there exists a set X = {x, y} of two vertices such that G − X is disconnected. (If G is not even 2-connected we may choose x to be an articulation vertex and y a neighbour of x.) Let C be a connected component of G − X , G1 := G[V (C) ∪ X ] and G2 := G − V (C). We first prove the following: Claim: Let v, w ∈ V (G1) be two vertices such that adding an edge e = {v, w} to G creates a K3,3 or K5 minor. Then at least one of G1 + e + f and G2 + f contains a K5 or K3,3 minor, where f is a new edge joining x and y.


To prove this claim, let v, w ∈ V (G1), e = {v, w} and suppose that there are pairwise disjoint connected vertex sets Z1, . . . , Zt of G + e such that after contracting each of them we have a K5 (t = 5) or K3,3 (t = 6) subgraph. Note that it is impossible that Zi ⊆ V (G1) \ X and Z j ⊆ V (G2) \ X for some i, j ∈ {1, . . . , t}: in this case the set of those Zk with Zk ∩ X = ∅ (there are at most two of these) separate Zi and Z j , contradicting the fact that both K5 and K3,3 are 3-connected. Hence there are two cases: If none of Z1, . . . , Zt is a subset of V (G2) \ X , then G1 + e + f also contains a K5 or K3,3 minor: just consider Zi ∩ V (G1) (i = 1, . . . , t).
Analogously, if none of Z1, . . . , Zt is a subset of V (G1) \ X , then G2 + f contains a K5 or K3,3 minor (consider Zi ∩ V (G2) (i = 1, . . . , t)). The claim is proved. Now we first consider the case when G contains an articulation vertex x, and y is a neighbour of x. We choose a second neighbour z of x such that y and z are in different connected components of G − x. W.l.o.g. say that z ∈ V (G1). Suppose that the addition of e = {y, z} creates a K5 or K3,3 minor. By the claim, at least one of G1 + e and G2 contains a K5 or K3,3 minor (an edge {x, y} is already present). But then G1 or G2, and thus G, contains a K5 or K3,3 minor, contradicting our assumption. Hence we may assume that G is 2-connected. Recall that x, y ∈ V (G) were chosen such that G − {x, y} is disconnected. If {x, y} ∈/ E(G) we simply add an edge f = {x, y}. If this creates a K5 or K3,3 minor, the claim implies that G1 + f or G2 + f contains such a minor. Since there is an x-y-path in each of G1, G2 (otherwise we would have an articulation vertex of G), this implies that there is a K5 or K3,3 minor in G which is again a contradiction. Thus we can assume that f = {x, y} ∈ E(G). Suppose now that at least one of the graphs Gi (i ∈ {1, 2}) is not planar. Then this Gi has at least five vertices. Since it does not contain a K5 or K3,3 minor (this would also be a minor of G), we conclude from Theorem 2.37 that Gi is not 3-connected. So we can apply the induction hypothesis to Gi . By the claim, if adding an edge within Gi does not introduce a K5 or K3,3 minor in Gi , it cannot introduce such a minor in G either. So we may assume that both G1 and G2 are planar; let 1 and 2 be planar embeddings. Let Fi be a face of i with f on its boundary, and let zi be another vertex on the boundary of Fi , zi ∈/ {x, y} (i = 1, 2). We claim that adding an edge {z1, z2} (cf. Figure 2.7) does not introduce a K5 or K3,3 minor. Suppose, on the contrary, that adding {z1, z2} and contracting some pairwise disjoint connected vertex sets Z1, . . . , Zt would create a K5 (t = 5) or K3,3 (t = 6) subgraph. First suppose that at most one of the sets Zi is a subset of V (G1) \ {x, y}.
Then the graph G′
2, arising from G2 by adding one vertex w and edges from w
to x, y and z2, also contains a K5 or K3,3 minor. (Here w corresponds to the contracted set Zi ⊆ V (G1) \ {x, y}.) This is a contradiction since there is a planar
embedding of G′
2: just supplement 2 by placing w within F2.


G1 G2
f
x
y
z1 z2
Fig. 2.7.
So we may assume that Z1, Z2 ⊆ V (G1)\{x, y}. Analogously, we may assume that Z3, Z4 ⊆ V (G2) \ {x , y}. W.l.o.g. we have z1 ∈/ Z1 and z2 ∈/ Z3. Then we cannot have a K5, because Z1 and Z3 are not adjacent. Moreover, the only possible common neighbours of Z1 and Z3 are Z5 and Z6. Since in K3,3 two vertices are either adjacent or have three common neighbours, a K3,3 minor is also impossible.
Theorem 2.37 and Lemma 2.38 yield Kuratowski’s Theorem:
Theorem 2.39. (Kuratowski [1930], Wagner [1937]) An undirected graph is planar if and only if it contains neither K5 nor K3,3 as a minor.
Indeed, Kuratowski proved a stronger version (Exercise 34). The proof can be turned into a polynomial-time algorithm quite easily (Exercise 33(b)). In fact, a linear-time algorithm exists:
Theorem 2.40. (Hopcroft and Tarjan [1974]) There is a linear-time algorithm for finding a planar embedding of a given graph or deciding that it is not planar.
2.6 Planar Duality
We shall now introduce an important duality concept. In this section, graphs may contain loops, i.e. edges whose endpoints coincide. In a planar embedding loops are of course represented by polygons instead of polygonal arcs. Note that Euler’s formula (Theorem 2.32) also holds for graphs with loops: this follows from the observation that subdividing a loop e (i.e. replacing e = {v, v} by two parallel edges {v, w}, {w, v} where w is a new vertex) and adjusting the embedding (replacing the polygon Je by two polygonal arcs whose union is Je) increases the number of edges and vertices each by one but does not change the number of faces.
Definition 2.41. Let G be a directed or undirected graph, possibly with loops, and let = (ψ, ( Je)e∈E(G)) be a planar embedding of G. We define the planar


dual G∗ whose vertices are the faces of and whose edge set is {e∗ : e ∈ E(G)}, where e∗ connects the faces that are adjacent to Je (if Je is adjacent to only
one face, then e∗ is a loop). In the directed case, say for e = (v, w), we orient e∗ = (F1, F2) in such a way that F1 is the face “to the right” when traversing Je from ψ(v) to ψ(w).
G∗ is again planar. In fact, there obviously exists a planar embedding (ψ∗,
( Je∗ )e∗∈E(G∗)
) of G∗ such that ψ∗(F) ∈ F for all faces F of and, for each e ∈ E(G),
Je∗ ∩
⎛
⎝{ψ(v) : v ∈ V (G)} ∪ ⋃
f ∈E(G)\{e}
Jf
⎞
⎠ = ∅,
| Je∗ ∩ Je| = 1, and if e∗ is a loop then the face bounded by Je∗ contains exactly
one endpoint of e. Such an embedding is called a standard embedding of G∗. The planar dual of a graph really depends on the embedding: consider the two embeddings of the same graph shown in Figure 2.8. The resulting planar duals are not isomorphic, since the second one has a vertex of degree four (corresponding to the outer face) while the first one is 3-regular.
Proposition 2.42. Let G be an undirected connected planar graph with a fixed embedding. Let G∗ be its planar dual with a standard embedding. Then (G∗)∗ = G.
Proof: Let (ψ, ( Je)e∈E(G)
) be a fixed embedding of G and (ψ∗, ( Je∗ )e∗∈E(G∗)
)
a standard embedding of G∗. Let F be a face of G∗. The boundary of F contains Je∗ for at least one edge e∗, so F must contain ψ(v) for one endpoint v of e. So
every face of G∗ contains at least one vertex of G. By applying Euler’s formula (Theorem 2.32) to G∗ and to G, we get that the number of faces of G∗ is |E(G∗)| − |V (G∗)| + 2 = |E(G)| − (|E(G)| − |V (G)| + 2) + 2 = |V (G)|. Hence each face of G∗ contains exactly one vertex of G. From this we conclude that the planar dual of G∗ is isomorphic to G.
The requirement that G is connected is essential here: note that G∗ is always connected, even if G is disconnected.
(a) (b)
Fig. 2.8.


Theorem 2.43. Let G be a connected planar undirected graph with arbitrary embedding. The edge set of any circuit in G corresponds to a minimal cut in G∗, and any minimal cut in G corresponds to the edge set of a circuit in G∗.
Proof: Let = (ψ, ( Je)e∈E(G)) be a fixed planar embedding of G. Let C be a
circuit in G. By Theorem 2.30, R2 \ ⋃
e∈E(C) Je splits into exactly two connected regions. Let A and B be the set of faces of in the inner and outer region,
respectively. We have V (G∗) = A .∪ B and EG∗(A, B) = {e∗ : e ∈ E(C)}. Since
A and B form connected sets in G∗, this is indeed a minimal cut. Conversely, let δG( A) be a minimal cut in G. Let ∗ = (ψ∗, ( Je)e∈E(G∗)) be
a standard embedding of G∗. Let a ∈ A and b ∈ V (G) \ A. Observe that there is no polygonal arc in
R := R2 \
⎛
⎝{ψ∗(v) : v ∈ V (G∗)} ∪ ⋃
e∈δG ( A)
Je∗
⎞
⎠
which connects ψ(a) and ψ(b): the sequence of faces of G∗ passed by such a polygonal arc would define an edge progression from a to b in G not using any edge of δG(A). So R consists of at least two connected regions. Then, obviously, the boundary of each region must contain a circuit. Hence F := {e∗ : e ∈ δG (A)} contains the
edge set of a circuit C in G∗. We have {e∗ : e ∈ E(C)} ⊆ {e∗ : e ∈ F} = δG (A),
and, by the first part, {e∗ : e ∈ E(C)} is a minimal cut in (G∗)∗ = G (cf. Proposition 2.42). We conclude that {e∗ : e ∈ E(C)} = δG (A).
In particular, e∗ is a loop if and only if e is a bridge, and vice versa. For digraphs the above proof yields:
Corollary 2.44. Let G be a connected planar digraph with some fixed planar embedding. The edge set of any circuit in G corresponds to a minimal directed cut in G∗, and vice versa.
Another interesting consequence of Theorem 2.43 is:
Corollary 2.45. Let G be a connected undirected graph with arbitrary planar embedding. Then G is bipartite if and only if G∗ is Eulerian, and G is Eulerian if and only if G∗ is bipartite.
Proof: Observe that a connected graph is Eulerian if and only if every minimal cut has even cardinality. By Theorem 2.43, G is bipartite if G∗ is Eulerian, and G is Eulerian if G∗ is bipartite. By Proposition 2.42, the converse is also true.
An abstract dual of G is a graph G′ for which there is a bijection χ : E(G) → E(G′) such that F is the edge set of a circuit iff χ(F) is a minimal cut in G′ and vice versa. Theorem 2.43 shows that any planar dual is also an abstract dual. The converse is not true. However, Whitney [1933] proved that a graph has an abstract dual if and only if it is planar (Exercise 40). We shall return to this duality relation when dealing with matroids in Section 13.3.


Exercises
1. Let G be a simple undirected graph on n vertices which is isomorphic to its complement. Show that n mod 4 ∈ {0, 1}. 2. Let G be an undirected graph. For X ⊆ V (G) let f (x) := |E(G[X ])|. Show that f : 2V (G) → Z is supermodular. 3. Prove that every simple undirected graph G with |δ(v)| ≥ 1
2 |V (G)| for all
v ∈ V (G) is Hamiltonian. Hint: Consider a longest path in G and the neighbours of its endpoints. (Dirac [1952])
4. Prove that any simple undirected graph G with |E(G)| > (|V (G)|−1
2
) is connected. 5. Let G be a simple undirected graph. Show that G or its complement is connected. 6. Prove that every simple undirected graph with more than one vertex contains two vertices that have the same degree. Prove that every tree (except a single vertex) contains at least two leaves. 7. Let T be a tree with k leaves. Show that T contains at most k − 2 vertices of degree at least 3. 8. Prove that every tree T contains a vertex v such that no connected component of T − v contains more than |V (T )|
2 vertices. Can you find such a vertex in linear time? 9. Let G be a connected undirected graph, and let (V (G), F) be a forest in G. Prove that there is a spanning tree (V (G), T ) with F ⊆ T ⊆ E(G). 10. Let (V , F1) and (V , F2) be two forests with |F1| < |F2|. Prove that there exists an edge e ∈ F2 \ F1 such that (V , F1 ∪ {e}) is a forest. 11. Let (V , F1) and (V , F2) be two branchings with 2|F1| < |F2|. Prove that there exists an edge e ∈ F2 \ F1 such that (V , F1 ∪ {e}) is a branching. 12. Prove that any cut in an undirected graph is the disjoint union of minimal cuts. 13. Let G be an undirected graph, C a circuit and D a cut. Show that |E(C) ∩ D| is even. 14. Show that any undirected graph has a cut containing at least half of the edges. 15. Let (U, F ) be a cross-free set system with |U | ≥ 2. Prove that F contains at most 4|U | − 4 distinct elements. 16. Let G be a connected undirected graph. Show that there exists an orientation G′ of G and a spanning arborescence T of G′ such that the set of fundamental circuits with respect to T is precisely the set of directed circuits in G′. Hint: Consider a DFS-tree. (Camion [1968]) 17. Describe a linear-time algorithm for the following problem: Given an adjacency list of a graph G, compute an adjacency list of the maximal simple subgraph of G. Do not assume that parallel edges appear consecutively in the input. 18. Given a graph G (directed or undirected), show that there is a linear-time algorithm to find a circuit or decide that none exists.


19. Describe a simple linear-time algorithm that finds a topological order in a given acyclic digraph. (Do not use the STRONGLY CONNECTED COMPONENT ALGORITHM). 20. Let G be a connected undirected graph, s ∈ V (G) and T a DFS-tree resulting from running DFS on (G, s). s is called the root of T . x is an ancestor of y in T if x lies on the (unique) s-y-path in T . x is the parent of y if the edge {x, y} lies on the s-y-path in T . y is a child (successor) of x if x is the parent (an ancestor) of y. Note that with this definition each vertex is an ancestor and a successor of itself. Every vertex except s has exactly one parent. Prove: (a) For any edge {v, w} ∈ E(G), v is an ancestor or a successor of w in T . (b) A vertex v is an articulation vertex of G if and only if • either v = s and |δT (v)| > 1 • or v = s and there is a child w of v such that no edge in G connects a proper ancestor of v (that is, excluding v) with a successor of w. 21. Use Exercise 20 to design a linear-time algorithm which finds the blocks of an undirected graph. It will be useful to compute numbers
α(x) := min{ f (w) : w = x or {w, y} ∈ E(G)\T for some successor y of x}
recursively during the DFS. Here (R, T ) is the DFS-tree (with root s), and the f -values represent the order in which the vertices are added to R (see the GRAPH SCANNING ALGORITHM). If for some vertex x ∈ R \ {s} we have α(x) ≥ f (w), where w is the parent of x, then w must be either the root or an articulation vertex. 22. Prove: (a) An undirected graph is 2-edge-connected if and only if it has at least two vertices and an ear-decomposition. (b) A digraph is strongly connected if and only if it has an ear-decomposition. (c) The edges of an undirected graph G with at least two vertices can be oriented such that the resulting digraph is strongly connected if and only if G is 2-edge-connected. (Robbins [1939]) 23. A tournament is a digraph such that the underlying undirected graph is a (simple) complete graph. Prove that every tournament contains a Hamiltonian path (Rédei [1934]). Prove that every strongly connected tournament is Hamiltonian (Camion [1959]). 24. Let G be an undirected graph. Prove that there exists an orientation G′ of G
such that ||δ+
G′(v)| − |δ−
G′(v)|| ≤ 1 for all v ∈ V (G′). 25. Prove that if a connected undirected simple graph is Eulerian then its line graph is Hamiltonian. What about the converse? 26. Prove that any connected bipartite graph has a unique bipartition. Prove that any non-bipartite undirected graph contains an odd circuit as an induced subgraph. Prove that an undirected graph G is bipartite if and only if E(G) can be partitioned into cuts. 27. Prove that a strongly connected digraph whose underlying undirected graph is non-bipartite contains a (directed) circuit of odd length.


28. Let G be an undirected graph. A tree-decomposition of G is a pair (T, φ), where T is a tree and φ : V (T ) → 2V (G) satisfies the following conditions: • for each e ∈ E(G) there exists a t ∈ V (T ) with e ⊆ φ(t); • for each v ∈ V (G) the set {t ∈ V (T ) : v ∈ φ(t)} is connected in T . We say that the width of (T, φ) is maxt∈V (T ) |φ(t)| − 1. The tree-width of a graph G is the minimum width of a tree-decomposition of G. This notion is due to Robertson and Seymour [1986]. Show that the simple graphs of tree-width at most 1 are the forests. Moreover, prove that the following statements are equivalent for an undirected graph G: (a) G has tree-width at most 2; (b) G does not contain K4 as a minor; (c) G can be obtained from an empty graph by successively adding bridges and doubling and subdividing edges. (Doubling an edge e = {v, w} ∈ E(G) means adding another edge with endpoints v and w; subdividing an edge e = {v, w} ∈ E(G) means adding a vertex x and replacing e by two edges {v, x}, {x, w}.)
Note: Because of the construction in (c) such graphs are called series-parallel. 29. Show that if a graph G has a planar embedding where the edges are embedded by arbitrary Jordan curves, then it also has a planar embedding with polygonal arcs only. 30. Let G be a 2-connected graph with a planar embedding. Show that the set of circuits bounding the finite faces constitutes a cycle basis of G. 31. Can you generalize Euler’s formula (Theorem 2.32) to disconnected graphs? 32. Show that there are exactly five Platonic graphs (corresponding to the Platonic solids; cf. Exercise 11 of Chapter 4), i.e. 3-connected planar regular graphs whose faces are all bounded by the same number of edges. Hint: Use Euler’s formula (Theorem 2.32). 33. Deduce from the proof of Kuratowski’s Theorem 2.39: (a) Every 3-connected simple planar graph has a planar embedding where each edge is embedded by a straight line and each face, except the outer face, is convex. (b) There is a polynomial-time algorithm for checking whether a given graph is planar. 34. Given a graph G and an edge e = {v, w} ∈ E(G), we say that H results from
G by subdividing e if V (H ) = V (G) .∪ {x} and E(H ) = (E(G) \ {e}) ∪ {{v, x}, {x, w}}. A graph resulting from G by successively subdividing edges is called a subdivision of G. (a) Trivially, if H contains a subdivision of G then G is a minor of H . Show that the converse is not true. (b) Prove that a graph containing a K3,3 or K5 minor also contains a subdivision of K3,3 or K5. Hint: Consider what happens when contracting one edge. (c) Conclude that a graph is planar if and only if no subgraph is a subdivision of K3,3 or K5. (Kuratowski [1930])


35. Prove that each of the following statements implies the other: (a) For every infinite sequence of graphs G1, G2, . . . there are two indices i < j such that Gi is a minor of G j . (b) Let G be a class of graphs such that for each G ∈ G and each minor H of G we have H ∈ G (i.e. membership in G is a hereditary graph property). Then there exists a finite set X of graphs such that G consists of all graphs that do not contain any element of X as a minor. Note: The statements have been proved by Robertson and Seymour [2004]; they are a main result of their series of papers on graph minors. Theorem 2.39 and Exercise 28 give examples of forbidden minor characterizations as in (b). 36. Let G be a planar graph with an embedding , and let C be a circuit of G bounding some face of . Prove that then there is an embedding ′ of G such that C bounds the outer face. 37. (a) Let G be disconnected with an arbitrary planar embedding, and let G∗ be the planar dual with a standard embedding. Prove that (G∗)∗ arises from G by successively applying the following operation, until the graph is connected: Choose two vertices x and y which belong to different connected components and which are adjacent to the same face; contract {x, y}.
(b) Generalize Corollary 2.45 to arbitrary planar graphs. Hint: Use (a) and Theorem 2.26. 38. Let G be a connected digraph with a fixed planar embedding, and let G∗ be the planar dual with a standard embedding. How are G and (G∗)∗ related? 39. Prove that if a planar digraph is acyclic (strongly connected), then its planar dual is strongly connected (acyclic). What about the converse? 40. (a) Show that if G has an abstract dual and H is a minor of G then H also has an abstract dual. ∗ (b) Show that neither K5 nor K3,3 has an abstract dual. (c) Conclude that a graph is planar if and only if it has an abstract dual. (Whitney [1933])
References
General Literature
Berge, C. [1985]: Graphs. Second Edition. Elsevier, Amsterdam 1985 Bollobás, B. [1998]: Modern Graph Theory. Springer, New York 1998 Bondy, J.A. [1995]: Basic graph theory: paths and circuits. In: Handbook of Combinatorics; Vol. 1 (R.L. Graham, M. Grötschel, L. Lovász, eds.), Elsevier, Amsterdam 1995 Bondy, J.A., and Murty, U.S.R. [2008]: Graph Theory. Springer, New York 2008 Diestel, R. [2010]: Graph Theory. Fourth Edition. Springer, New York 2010 Wilson, R.J. [2010]: Introduction to Graph Theory. Fifth Edition. Addison-Wesley, Reading 2010


Cited References
Aoshima, K., and Iri, M. [1977]: Comments on F. Hadlock’s paper: finding a maximum cut of a planar graph in polynomial time. SIAM Journal on Computing 6 (1977), 86–87 Camion, P. [1959]: Chemins et circuits hamiltoniens des graphes complets. Comptes Rendus Hebdomadaires des Séances de l’Académie des Sciences (Paris) 249 (1959), 2151–2152 Camion, P. [1968]: Modulaires unimodulaires. Journal of Combinatorial Theory A 4 (1968), 301–362 Dirac, G.A. [1952]: Some theorems on abstract graphs. Proceedings of the London Mathematical Society 2 (1952), 69–81 Edmonds, J., and Giles, R. [1977]: A min-max relation for submodular functions on graphs. In: Studies in Integer Programming; Annals of Discrete Mathematics 1 (P.L. Hammer, E.L. Johnson, B.H. Korte, G.L. Nemhauser, eds.), North-Holland, Amsterdam 1977, pp. 185–204 Euler, L. [1736]: Solutio problematis ad geometriam situs pertinentis. Commentarii Academiae Petropolitanae 8 (1736), 128–140 Euler, L. [1758]: Demonstratio nonnullarum insignium proprietatum quibus solida hedris planis inclusa sunt praedita. Novi Commentarii Academiae Petropolitanae 4 (1758), 140–160 Hierholzer, C. [1873]: Über die Möglichkeit, einen Linienzug ohne Wiederholung und ohne Unterbrechung zu umfahren. Mathematische Annalen 6 (1873), 30–32 Hopcroft, J.E., and Tarjan, R.E. [1974]: Efficient planarity testing. Journal of the ACM 21 (1974), 549–568 Kahn, A.B. [1962]: Topological sorting of large networks. Communications of the ACM 5 (1962), 558–562 Karzanov, A.V. [1970]: An efficient algorithm for finding all the bi-components of a graph. In: Trudy 3-ı ̆ Zimneı ̆ Shkoly po Matematicheskomu Programmirovaniyu i Smezhnym Voprosam (Drogobych, 1970), Issue 2, Moscow Institute for Construction Engineering (MISI) Press, Moscow, 1970, pp. 343–347 [in Russian] Knuth, D.E. [1968]: The Art of Computer Programming; Vol. 1. Fundamental Algorithms. Addison-Wesley, Reading 1968 (third edition: 1997) König, D. [1916]: Über Graphen und ihre Anwendung auf Determinantentheorie und Mengenlehre. Mathematische Annalen 77 (1916), 453–465 König, D. [1936]: Theorie der endlichen und unendlichen Graphen. Teubner, Leipzig 1936; reprint: Chelsea Publishing Co., New York 1950 Kuratowski, K. [1930]: Sur le problème des courbes gauches en topologie. Fundamenta Mathematicae 15 (1930), 271–283 Legendre, A.M. [1794]: Éléments de Géométrie. Firmin Didot, Paris 1794 Minty, G.J. [1960]: Monotone networks. Proceedings of the Royal Society of London A 257 (1960), 194–212 Moore, E.F. [1959]: The shortest path through a maze. Proceedings of the International Symposium on the Theory of Switching; Part II. Harvard University Press 1959, pp. 285–292


Rédei, L. [1934]: Ein kombinatorischer Satz. Acta Litt. Szeged 7 (1934), 39–43 Robbins, H.E. [1939]: A theorem on graphs with an application to a problem of traffic control. American Mathematical Monthly 46 (1939), 281–283 Robertson, N., and Seymour, P.D. [1986]: Graph minors II: algorithmic aspects of tree-width. Journal of Algorithms 7 (1986), 309–322 Robertson, N., and Seymour, P.D. [2004]: Graph minors XX: Wagner’s conjecture. Journal of Combinatorial Theory B 92 (2004), 325–357 Tarjan, R.E. [1972]: Depth first search and linear graph algorithms. SIAM Journal on Computing 1 (1972), 146–160 Thomassen, C. [1980]: Planarity and duality of finite and infinite graphs. Journal of Combinatorial Theory B 29 (1980), 244–271 Thomassen, C. [1981]: Kuratowski’s theorem. Journal of Graph Theory 5 (1981), 225–241 Tutte, W.T. [1961]: A theory of 3-connected graphs. Proceedings of the Koninklijke Nederlandse Akademie van Wetenschappen A 64 (1961), 441–455 Wagner, K. [1937]: Über eine Eigenschaft der ebenen Komplexe. Mathematische Annalen 114 (1937), 570–590 Whitney, H. [1932]: Non-separable and planar graphs. Transactions of the American Mathematical Society 34 (1932), 339–362 Whitney, H. [1933]: Planar graphs. Fundamenta Mathematicae 21 (1933), 73–84


3 Linear Programming
In this chapter we review the most important facts about Linear Programming. Although this chapter is self-contained, it cannot be considered to be a comprehensive treatment of the field. The reader unfamiliar with Linear Programming is referred to the textbooks mentioned at the end of this chapter. The general problem reads as follows:
LINEAR PROGRAMMING
Instance: A matrix A ∈ Rm×n and column vectors b ∈ Rm, c ∈ Rn.
Task: Find a column vector x ∈ Rn such that Ax ≤ b and c x is maximum, decide that {x ∈ Rn : Ax ≤ b} is empty, or decide that for all α ∈ R there is an x ∈ Rn with Ax ≤ b and c x > α.
Here c x denotes the scalar product of the vectors. The notion x ≤ y for vectors x and y (of equal size) means that the inequality holds in each component. If no sizes are specified, the matrices and vectors are always assumed to be compatible in size. We often omit indicating the transposition of column vectors and write e.g. cx for the scalar product. By 0 we denote the number zero as well as all-zero vectors and all-zero matrices (the order will always be clear from the context). A linear program (LP) is an instance of the above problem. We often write a linear program as max{cx : Ax ≤ b}. A feasible solution of an LP max{cx : Ax ≤ b} is a vector x with Ax ≤ b. A feasible solution attaining the maximum is called an optimum solution.
As the problem formulation indicates, there are two possibilities when an LP has no solution: The problem can be infeasible (i.e. P := {x ∈ Rn : Ax ≤ b} = ∅) or unbounded (i.e. for all α ∈ R there is an x ∈ P with cx > α). If an LP is neither infeasible nor unbounded it has an optimum solution:
Proposition 3.1. Let P = {x ∈ Rn : Ax ≤ b} = ∅ and c ∈ Rn with δ := sup{c x : x ∈ P} < ∞. Then there exists a vector z ∈ P with c z = δ.
Proof: Let U be a matrix whose columns are an orthonormal basis of the kernel of A, i.e. U U = I , AU = 0, and rank(A′) = n where A′ :=
(A U
)
. Let b′ := ( b
0
).
We show that for every y ∈ P there exists a subsystem A′′x ≤ b′′ of A′x ≤ b′ such that A′′ is nonsingular, y′ := ( A′′)−1b′′ ∈ P, and c y′ ≥ c y. As there


are only finitely many such subsystems, one of these y′ attains the maximum (c y′ = δ), and the assertion follows. So let y ∈ P, and denote by k(y) the rank of A′′ for the maximal subsystem A′′x ≤ b′′ of A′x ≤ b′ with A′′ y = b′′. Suppose that k(y) < n. We show how to find a y′ ∈ P with c y′ ≥ c y and k(y′) > k(y). After at most n steps we have a vector y′ with k(y′) = n as required. If U y = 0, we set y′ := y − UU y. Since y + λUU c ∈ P for all λ ∈ R we have sup{c (y + λUU c) : λ ∈ R} ≤ δ < ∞ and hence c U = 0 and c y′ = c y. Moreover, Ay′ = Ay − AUU y = Ay and U y′ = U y − U UU y = 0.
Now suppose that U y = 0. Let v = 0 with A′′v = 0. Denote by
ai x ≤ βi the i -th row of Ax ≤ b. Let μ := min
{ βi −ai y
ai v : ai v > 0
}
and
κ := max
{ βi −ai y
ai v : ai v < 0
}
, where min ∅ = ∞ and max ∅ = −∞. We have
κ ≤ 0 ≤ μ, and at least one of κ and μ is finite (because A′v = 0 but U v = 0). For λ ∈ R with κ ≤ λ ≤ μ we have A′′(y + λv) = A′′ y + λA′′v = A′′ y = b′′ and A(y +λv) = Ay +λAv ≤ b, i.e. y +λv ∈ P. Thus, as sup{c x : x ∈ P} < ∞, we have μ < ∞ if c v > 0 and κ > −∞ if c v < 0. Moreover, if c v ≥ 0 and μ < ∞, we have ai (y + μv) = βi for some i . Analogously, if c v ≤ 0 and κ > −∞, we have ai (y + κv) = βi for some i .
Thus in each case we have found a vector y′ ∈ P with c y′ ≥ c y and k(y′) ≥ k(y) + 1.
This justifies the notation max{c x : Ax ≤ b} instead of sup{c x : Ax ≤ b}. Many combinatorial optimization problems can be formulated as LPs. To do this, we encode the feasible solutions as vectors in Rn for some n. In Section 3.5 we show that one can optimize a linear objective function over a finite set S of vectors by solving a linear program. Although the feasible set of this LP contains not only the vectors in S but also all their convex combinations, one can show that among the optimum solutions there is always an element of S. In Section 3.1 we compile some terminology and basic facts about polyhedra, the sets P = {x ∈ Rn : Ax ≤ b} of feasible solutions of LPs. In Sections 3.2 and 3.3 we present the SIMPLEX ALGORITHM, which we also use to derive the Duality Theorem and related results (Section 3.4). LP duality is a most important concept which explicitly or implicitly appears in almost all areas of combinatorial optimization; we shall often refer to the results in Sections 3.4 and 3.5.
3.1 Polyhedra
Linear Programming deals with maximizing or minimizing a linear objective function of finitely many variables subject to finitely many linear inequalities. So the set of feasible solutions is the intersection of finitely many halfspaces. Such a set is called a polyhedron:


Definition 3.2. A polyhedron in Rn is a set of type P = {x ∈ Rn : Ax ≤ b} for some matrix A ∈ Rm×n and some vector b ∈ Rm. If A and b are rational, then P is a rational polyhedron. A bounded polyhedron is also called a polytope. We denote by rank(A) the rank of a matrix A. The dimension dim X of a nonempty set X ⊆ Rn is defined to be
n − max{rank(A) : A is an n × n-matrix with Ax = Ay for all x, y ∈ X }.
A polyhedron P ⊆ Rn is called full-dimensional if dim P = n.
Equivalently, a polyhedron is full-dimensional if and only if there is a point in its interior. For most of this chapter it makes no difference whether we are in the rational or real space. We need the following standard terminology:
Definition 3.3. Let P := {x : Ax ≤ b} be a nonempty polyhedron. If c is a nonzero vector for which δ := max{cx : x ∈ P} is finite, then {x : cx = δ} is called a supporting hyperplane of P. A face of P is P itself or the intersection of P with a supporting hyperplane of P. A point x for which {x} is a face is called a vertex of P, and also a basic solution of the system Ax ≤ b.
Proposition 3.4. Let P = {x : Ax ≤ b} be a polyhedron and F ⊆ P. Then the following statements are equivalent:
(a) F is a face of P. (b) There exists a vector c such that δ := max{cx : x ∈ P} is finite and F = {x ∈ P : cx = δ}. (c) F = {x ∈ P : A′x = b′} = ∅ for some subsystem A′x ≤ b′ of Ax ≤ b.
Proof: (a) and (b) are obviously equivalent. (c)⇒(b): If F = {x ∈ P : A′x = b′} is nonempty, let c be the sum of the rows of A′, and let δ be the sum of the components of b′. Then obviously cx ≤ δ for all x ∈ P and F = {x ∈ P : cx = δ}. (b)⇒(c): Assume that c is a vector, δ := max{cx : x ∈ P} is finite and F = {x ∈ P : cx = δ}. Let A′x ≤ b′ be the maximal subsystem of Ax ≤ b such that A′x = b′ for all x ∈ F. Let A′′x ≤ b′′ be the rest of the system Ax ≤ b. We first observe that for each inequality ai′′x ≤ βi′′ of A′′x ≤ b′′ (i = 1, . . . , k)
there is a point xi ∈ F such that a′′
i xi < β′′
i . Let x ∗ := 1
k
∑k
i=1 xi be the center
of gravity of these points (if k = 0, we can choose an arbitrary x∗ ∈ F); we have x ∗ ∈ F and a′′
i x ∗ < β′′
i for all i .
We have to prove that A′ y = b′ cannot hold for any y ∈ P \ F. So let y ∈ P \ F. We have cy < δ. Now consider z := x∗ + (x∗ − y) for some small > 0; in
particular let be smaller than βi′′−ai′′x∗
ai′′(x∗−y) for all i ∈ {1, . . . , k} with a′′
i x ∗ > a′′
i y.
We have cz > δ and thus z ∈/ P. So there is an inequality ax ≤ β of Ax ≤ b such that az > β. Thus ax∗ > ay. The inequality ax ≤ β cannot belong to A′′x ≤ b′′,
since otherwise we have az = ax ∗ + a(x ∗ − y) < ax ∗ + β−ax∗
a(x∗−y) a(x∗ − y) = β


(by the choice of ). Hence the inequality ax ≤ β belongs to A′x ≤ b′. Since ay = a(x∗ + 1 (x∗ − z)) < β, this completes the proof.
As a trivial but important corollary we remark:
Corollary 3.5. If max{cx : x ∈ P} is bounded for a nonempty polyhedron P and a vector c, then the set of points where the maximum is attained is a face of P.
The relation “is a face of ” is transitive:
Corollary 3.6. Let P be a polyhedron and F a face of P. Then F is again a polyhedron. Furthermore, a set F′ ⊆ F is a face of P if and only if it is a face of F.
The maximal faces distinct from P are particularly important:
Definition 3.7. Let P be a polyhedron. A facet of P is a maximal face distinct from P. An inequality cx ≤ δ is facet-defining for P if cx ≤ δ for all x ∈ P and {x ∈ P : cx = δ} is a facet of P.
Proposition 3.8. Let P ⊆ {x ∈ Rn : Ax = b} be a nonempty polyhedron of dimension n − rank(A). Let A′x ≤ b′ be a minimal inequality system such that P = {x : Ax = b, A′x ≤ b′}. Then each inequality of A′x ≤ b′ is facet-defining for P, and each facet of P is defined by an inequality of A′x ≤ b′.
Proof: If P = {x ∈ Rn : Ax = b}, then there are no facets and the statement is trivial. So let A′x ≤ b′ be a minimal inequality system with P = {x : Ax = b, A′x ≤ b′}, let a′x ≤ β′ be one of its inequalities and A′′x ≤ b′′ be the rest of the system A′x ≤ b′. Let y be a vector with Ay = b, A′′ y ≤ b′′ and a′ y > β′ (such a vector y exists as the inequality a′x ≤ β′ is not redundant). Let x ∈ P such that A′x < b′ (such a vector must exist because dim P = n − rank(A)).
Consider z := x + β′−a′x
a′y−a′x (y − x ). We have a′z = β′, A′′z < b′′, and, since
0 < β′−a′x
a′y−a′x < 1, z ∈ P. Therefore F := {x ∈ P : a′x = β′} = 0 and F = P (as
x ∈ P \ F). We conclude that F is a facet of P. By Proposition 3.4 each facet is defined by an inequality of A′x ≤ b′.
The other important class of faces (beside facets) are minimal faces (i.e. faces not containing any other face). Here we have:
Proposition 3.9. (Hoffman and Kruskal [1956]) Let P = {x : Ax ≤ b} be a polyhedron. A nonempty subset F ⊆ P is a minimal face of P if and only if F = {x : A′x = b′} for some subsystem A′x ≤ b′ of Ax ≤ b.
Proof: If F is a minimal face of P, by Proposition 3.4 there is a subsystem A′x ≤ b′ of Ax ≤ b such that F = {x ∈ P : A′x = b′}. We choose A′x ≤ b′ maximal. Let A′′x ≤ b′′ be a minimal subsystem of Ax ≤ b such that F = {x : A′x = b′, A′′x ≤ b′′}. We claim that A′′x ≤ b′′ does not contain any inequality.


Suppose, on the contrary, that a′′x ≤ β′′ is an inequality of A′′x ≤ b′′. Since it is not redundant for the description of F, Proposition 3.8 implies that F′ := {x : A′x = b′, A′′x ≤ b′′, a′′x = β′′} is a facet of F. By Corollary 3.6 F′ is also a face of P, contradicting the assumption that F is a minimal face of P. Now let ∅ = F = {x : A′x = b′} ⊆ P for some subsystem A′x ≤ b′ of Ax ≤ b. Obviously F has no faces except itself. By Proposition 3.4, F is a face of P. It follows by Corollary 3.6 that F is a minimal face of P.
Corollary 3.5 and Proposition 3.9 imply that LINEAR PROGRAMMING can be solved in finite time by solving the linear equation system A′x = b′ for each subsystem A′x ≤ b′ of Ax ≤ b. A more intelligent way is the SIMPLEX ALGORITHM which is described in the next section. Another consequence of Proposition 3.9 is:
Corollary 3.10. Let P = {x ∈ Rn : Ax ≤ b} be a polyhedron. Then all minimal faces of P have dimension n−rank(A). The minimal faces of polytopes are vertices.
This is why polyhedra {x ∈ Rn : Ax ≤ b} with rank(A) = n are called pointed: their minimal faces are points. Let us close this section with some remarks on polyhedral cones.
Definition 3.11. A (convex) cone is a set C ⊆ Rn for which x, y ∈ C and λ, μ ≥ 0 implies λx + μy ∈ C. A cone C is said to be generated by x1, . . . , xk if x1, . . . , xk ∈ C and for any x ∈ C there are numbers λ1, . . . , λk ≥ 0 with
x = ∑k
i=1 λi xi . A cone is called finitely generated if some finite set of vectors
generates it. A polyhedral cone is a polyhedron of type {x : Ax ≤ 0}.
It is immediately clear that polyhedral cones are indeed cones. We shall now show that polyhedral cones are finitely generated. I always denotes an identity matrix.
Lemma 3.12. (Minkowski [1896]) Let C = {x ∈ Rn : Ax ≤ 0} be a polyhedral cone. Then C is generated by a subset of the set of solutions to the systems My = b′,
where M consists of n linearly independent rows of ( A
I
) and b′ = ±e j for some unit vector e j .
Proof: Let A be an m × n-matrix. Consider the systems My = b′ where M
consists of n linearly independent rows of ( A
I
) and b′ = ±e j for some unit vector e j . Let y1, . . . , yt be those solutions of these equality systems that belong to C. We claim that C is generated by y1, . . . , yt . First suppose C = {x : Ax = 0}, i.e. C is a linear subspace. Write C = {x : A′x = 0} where A′ consists of a maximal set of linearly independent rows of A.
Let I ′ consist of some rows of I such that
(
A′
I′
)
is a nonsingular square matrix.


Then C is generated by the solutions of
( A′
I′
)
x=
(0
b
)
, for b = ±e j , j = 1, . . . , dim C.
For the general case we use induction on the dimension of C. If C is not a linear subspace, then there is a vector z ∈ C such that −z ∈/ C. Then there is a row a of A such that az < 0.
Let A′ consist of any maximal set of rows of A such that (i) the rows of ( A′
a
)
are linearly independent, and (ii) there exists a vector z ∈ C with A′z = 0 and az < 0.
Let y be any vector with A′ y = 0 and ay = −1. We claim that y ∈ C. Let z satisfy (ii), i.e., z ∈ C, A′z = 0 and az < 0. Let B be the set of rows b of A with by > 0. Each b ∈ B must be linearly independent of a and A′: otherwise b = c A′ + δa for a vector c and a number δ, but then 0 ≥ bz = c A′z + δaz = δaz, hence δ ≥ 0, contradicting 0 < by = c A′y + δay = −δ. Suppose that B is nonempty. Let μ := min{ bz
by : b ∈ B}. We have μ ≤ 0. Then
z′ := z − μy ∈ C, A′z′ = A′z − μA′ y = 0, az′ = az − μay < 0, and there is a b′ ∈ B with b′z′ = 0. This contradicts the maximality of A′. So B = ∅, i.e., y ∈ C.
Hence by construction there is an index s ∈ {1, . . . , t} such that A′ ys = 0 and ays = −1.
Now let an arbitrary z ∈ C be given. Let a1, . . . , am be the rows of A and
μ := min
{ ai z
ai ys : i = 1, . . . , m, ai ys < 0
}
. We have μ ≥ 0. Let k be an index where
the minimum is attained. Consider z′ := z − μys. By the definition of μ we have
a j z′ = a j z − ak z
ak ys a j ys for j = 1, . . . , m, and hence z′ ∈ C′ := {x ∈ C : ak x = 0}.
C′ is a cone whose dimension is one less than that of C (because ak ys < 0 and
ys ∈ C). By induction, C′ is generated by a subset of y1, . . . , yt , so z′ = ∑t
i=1 λi yi
for some λ1, . . . , λt ≥ 0. By setting λ′s := λs +μ (observe that μ ≥ 0) and λ′i := λi
(i = s), we obtain z = z′ + μys = ∑t
i=1 λ′i yi .
Thus any polyhedral cone is finitely generated. We shall show the converse at the end of Section 3.4.
3.2 The Simplex Algorithm
The oldest and best-known algorithm for LINEAR PROGRAMMING is Dantzig’s [1951] simplex method. We first assume that the polyhedron has a vertex, and that some vertex is given as input. Later we shall show how general LPs can be solved with this method. For a set J of row indices we write AJ for the submatrix of A consisting of the rows in J only, and bJ for the subvector of b consisting of the components with indices in J . We abbreviate ai := A{i} and βi := b{i}.


SIMPLEX ALGORITHM
Input: A matrix A ∈ Rm×n and column vectors b ∈ Rm, c ∈ Rn. A vertex x of P := {x ∈ Rn : Ax ≤ b}.
Output: A vertex x of P attaining max{cx : x ∈ P} or a vector w ∈ Rn with Aw ≤ 0 and cw > 0 (i.e. the LP is unbounded).
©1 Choose a set of n row indices J such that A J is nonsingular and A J x = bJ .
©2 Compute c (A J )−1 and add zeros in order to obtain a vector y with c = y A such that all entries of y outside J are zero. If y ≥ 0 then stop. Return x and y.
©3 Choose the minimum index i with yi < 0.
Let w be the column of −( A J )−1 with index i , so A J \{i}w = 0 and ai w = −1.
If Aw ≤ 0 then stop. Return w.
©4 Let λ := min
{βj − ajx
a j w : j ∈ {1, . . . , m}, a j w > 0
} ,
and let j be the smallest row index attaining this minimum.
©5 Set J := (J \ {i }) ∪ { j } and x := x + λw. Go to ©2 .
Step ©1 relies on Proposition 3.9 and can be implemented with GAUSSIAN ELIMINATION (Section 4.3). The selection rules for i and j in ©3 and ©4 (often called pivot rule) are due to Bland [1977]. If one just chose an arbitrary i with yi < 0 and an arbitrary j attaining the minimum in ©4 the algorithm would run into cyclic repetitions for some instances. Bland’s pivot rule is not the only one that avoids cycling; another one (the so-called lexicographic rule) was proved to avoid cycling already by Dantzig, Orden and Wolfe [1955]. Before proving the correctness of the SIMPLEX ALGORITHM, let us make the following observation (sometimes known as “weak duality”):
Proposition 3.13. Let x and y be feasible solutions of the LPs
max{cx : Ax ≤ b} and (3.1)
min{yb : y A = c , y ≥ 0}, (3.2)
respectively. Then cx ≤ yb.
Proof: cx = (y A)x = y(Ax) ≤ yb.
Theorem 3.14. (Dantzig [1951], Dantzig, Orden and Wolfe [1955], Bland [1977])
The SIMPLEX ALGORITHM terminates after at most (m
n
) iterations. If it returns x and y in ©2 , these vectors are optimum solutions of the LPs (3.1) and (3.2), respectively, with cx = yb. If the algorithm returns w in ©3 then cw > 0 and the LP (3.1) is unbounded.


Proof: We first prove that the following conditions hold at any stage of the algorithm:
(a) x ∈ P;
(b) AJ x = bJ ;
(c) A J is nonsingular; (d) cw > 0; (e) λ ≥ 0.
(a) and (b) hold initially. ©2 and ©3 guarantee cw = y Aw = −yi > 0. By ©4 , x ∈ P implies λ ≥ 0. (c) follows from the fact that A J \{i}w = 0 and a j w > 0. It remains to show that ©5 preserves (a) and (b). We show that if x ∈ P, then also x + λw ∈ P. For a row index k we have two cases: If akw ≤ 0 then (using λ ≥ 0) ak(x + λw) ≤ ak x ≤ βk. Otherwise
λ ≤ βk−ak x
akw and hence ak(x + λw) ≤ ak x + ak w βk−ak x
akw = βk . (Indeed, λ is chosen
in ©4 to be the largest number such that x + λw ∈ P.)
To show (b), note that after ©4 we have A J \{i}w = 0 and λ = β j −a j x
a j w , so
A J \{i}(x + λw) = A J \{i}x = bJ \{i} and a j (x + λw) = a j x + a j w β j −a j x
ajw = βj.
Therefore after ©5 , A J x = bJ holds again. So we indeed have (a)–(e) at any stage. If the algorithm returns x and y in ©2 , x and y are feasible solutions of (3.1) and (3.2), respectively. x is a vertex of P by (a), (b) and (c). Moreover, cx = y Ax = yb since the components of y are zero outside J . This proves the optimality of x and y by Proposition 3.13. If the algorithm stops in ©3 , the LP (3.1) is indeed unbounded because in this case x + μw ∈ P for all μ ≥ 0, and cw > 0 by (d). We finally show that the algorithm terminates. Let J (k) and x (k) be the set J and the vector x in iteration k of the SIMPLEX ALGORITHM, respectively. If the
algorithm did not terminate after (m
n
) iterations, there are iterations k < l with J (k) = J (l). By (b) and (c), x (k) = x (l). By (d) and (e), cx never decreases, and it strictly increases if λ > 0. Hence λ is zero in all the iterations k, k + 1, . . . , l − 1, and x (k) = x (k+1) = · · · = x (l).
Let h be the highest index leaving J in one of the iterations k, . . . , l − 1, say in iteration p. Index h must also have been added to J in some iteration q ∈ {k, . . . , l − 1}. Now let y′ be the vector y at iteration p, and let w′ be the vector w at iteration q. We have y′ Aw′ = cw′ > 0. So let r be an index for which yr′ ar w′ > 0. Since yr′ = 0, index r belongs to J (p). If r > h, index r would also
belong to J (q) and J (q+1), implying ar w′ = 0. So r ≤ h. But by the choice of i
in iteration p we have yr′ < 0 iff r = h, and by the choice of j in iteration q we
have ar w′ > 0 iff r = h (recall that λ = 0 and ar x (q) = ar x (p) = βr as r ∈ J (p)). This is a contradiction.
Klee and Minty [1972] and Avis and Chvátal [1978] found examples where the SIMPLEX ALGORITHM (with Bland’s rule) needs 2n iterations on LPs with n variables and 2n constraints, proving that it is not a polynomial-time algorithm. It is not known whether there is a pivot rule that leads to a polynomial-time


algorithm. However, Borgwardt [1982] showed that the average running time (for random instances in a certain natural probabilistic model) can be bounded by a polynomial. Spielman and Teng [2004] introduced a so-called smoothed analysis (cf. Section 17.5): for each input they consider the expected running time with respect to small random perturbations of the input. The maximum of all these expectations is polynomially bounded. Kelner and Spielman [2006] proposed a randomized polynomial-time algorithm for LINEAR PROGRAMMING that is similar to the SIMPLEX ALGORITHM. The SIMPLEX ALGORITHM is also quite fast in practice if implemented skilfully; see Section 3.3. We now show how to solve general linear programs with the SIMPLEX ALGORITHM. More precisely, we show how to find an initial vertex. Since there are polyhedra that do not have vertices at all, we put a given LP into a different form first. Let max{cx : Ax ≤ b} be an LP. We substitute x by y − z and write it equivalently in the form
max
{ (c −c) (y
z
)
: (A −A) (y
z
)
≤ b, y, z ≥ 0
} .
So w.l.o.g. we assume that our LP has the form
max{cx : A′x ≤ b′, A′′x ≤ b′′, x ≥ 0} (3.3)
with b′ ≥ 0 and b′′ < 0. We first run the SIMPLEX ALGORITHM on the instance
min{(1l A′′)x + 1ly : A′x ≤ b′, A′′x + y ≥ b′′, x , y ≥ 0}, (3.4)
where 1l denotes a vector whose entries are all 1. Since ( xy
) = 0 defines a vertex, this is possible. The LP is obviously not unbounded since the minimum must be
at least 1lb′′. For any feasible solution x of (3.3), ( x
b′′− A′′ x
) is an optimum solution of (3.4) of value 1lb′′. Hence if the minimum of (3.4) is greater than 1lb′′, then (3.3) is infeasible.
In the contrary case, let ( xy
) be an optimum vertex of (3.4) of value 1lb′′. We claim that x is a vertex of the polyhedron defined by (3.3). To see this, first observe that A′′x + y = b′′. Let n and m be the dimensions of x and y, respectively; then by Proposition 3.9 there is a set S of n + m inequalities of (3.4) satisfied with equality, such that the submatrix corresponding to these n + m inequalities is nonsingular. Let S′ be the inequalities of A′x ≤ b′ and of x ≥ 0 that belong to S. Let S′′ consist of those inequalities of A′′x ≤ b′′ for which the corresponding inequalities of A′′x + y ≥ b′′ and y ≥ 0 both belong to S. Obviously |S′∪S′′| ≥ |S|−m = n, and the inequalities of S′ ∪ S′′ are linearly independent and satisfied by x with equality. Hence x satisfies n linearly independent inequalities of (3.3) with equality; thus x is indeed a vertex. Therefore we can start the SIMPLEX ALGORITHM with (3.3) and x.


3.3 Implementation of the Simplex Algorithm
The previous description of the SIMPLEX ALGORITHM is simple but not suitable for an efficient implementation. As we will see, it is not necessary to solve a linear equation system in each iteration. To motivate the main idea, we start with a proposition (which is actually not needed later): for LPs of the form max{cx : Ax = b, x ≥ 0}, vertices can be represented not only by subsets of rows but also by subsets of columns. For a matrix A and a set J of column indices we denote by AJ the submatrix consisting of the columns in J only. Consequently, AJ
I denotes the submatrix of A with rows in I and columns in J . Sometimes the order of the rows and columns is important: if J = ( j1, . . . , jk) is a vector of row (column) indices, we denote by A J ( A J ) the matrix whose i -th row (column) is the ji -th row (column) of A (i = 1, . . . , k).
Proposition 3.15. Let P := {x : Ax = b, x ≥ 0}, where A is a matrix and b is a vector. Then x is a vertex of P if and only if x ∈ P and the columns of A corresponding to positive entries of x are linearly independent.
Proof: Let A be an m × n-matrix. Let X := ( −I 0
AI
) and b′ := ( 0
b
). Let N := {1, . . . , n} and M := {n +1, . . . , n +m}. For an index set J ⊆ N ∪ M with |J | = n let J ̄ := (N ∪ M) \ J . Then X N
J is nonsingular iff X N∩J ̄
M∩J is nonsingular iff X J ̄
M
is nonsingular. If x is a vertex of P, then – by Proposition 3.9 – there exists a set J ⊆ N ∪ M such that | J | = n, X N
J is nonsingular, and X N
J x = b′J . Then the components of
x corresponding to N ∩ J are zero. Moreover, X J ̄
M is nonsingular, and hence the
columns of AN∩J ̄ are linearly independent. Conversely, let x ∈ P, and let the set of columns of A corresponding to positive entries of x be linearly independent. By adding suitable unit column vectors to these columns we obtain a nonsingular submatrix X B
M with xi = 0 for i ∈ N \ B.
Then X BN ̄ is nonsingular and X BN ̄ x = b′B ̄ . Hence, by Proposition 3.9, x is a vertex
of P.
Corollary 3.16. Let ( xy
) ∈ P := {( xy
) : Ax + y = b, x ≥ 0, y ≥ 0}. Then ( xy
)
is a vertex of P if and only if the columns of (A I ) corresponding to positive
components of ( xy
) are linearly independent. Moreover, x is a vertex of {x : Ax ≤
b, x ≥ 0} if and only if ( x
b−Ax
) is a vertex of P.
We will now analyze the behaviour of the SIMPLEX ALGORITHM when applied to an LP of the form max{cx : Ax ≤ b, x ≥ 0}.
Theorem 3.17. Let A ∈ Rm×n , b ∈ Rm, and c ∈ Rn. Let A′ := ( −I
A
), b′ := ( 0
b
)
and c ̄ := (c , 0). Let B ∈ {1, . . . , n + m}m such that (A I )B is nonsingular. Let J ⊆ {1, . . . , n + m} be the set of the remaining n indices. Let Q B := ((A I )B )−1.


Then:
(a) A′J is nonsingular.
(b) (b′ − A′x )J = 0 and (b′ − A′x )B = Q B b and c x = c ̄B Q Bb, where x :=
( A′
J )−1b′
J.
(c) Let y be the vector with yB = 0 and y A′ = c . Then y = c ̄B Q B (A I ) − c ̄.
(d) Let i ∈ J . Let w be the vector with A′
i w = −1 and A′
J \{i}w = 0. Then
A′Bw = Q B ( A I )i . (e) Define
TB :=
( QB(A I) QBb c ̄B Q B ( A I ) − c ̄ c x
) .
Given B and TB, we can compute B′ and TB′ in O(m(n + m)) time, where
B′ arises from B by replacing j by i , and i and j are given as in ©2 –©4 of the SIMPLEX ALGORITHM (applied to A′, b′, c, and index set J ).
TB is called the simplex tableau with respect to the basis B.
Proof: (a): Let N := {1, . . . , n}. As (A I )B is nonsingular, also (A′)N\J
J \N is
nonsingular, and thus A′J is nonsingular.
(b): The first statement follows directly from A′
J x = b′
J . Then b = Ax +
I (b − Ax ) = ( A I )(b′ − A′x ) = ( A I )B(b′ − A′x )B and c x = c ̄(b′ − A′x ) =
c ̄B(b′ − A′x )B = c ̄B Q B b.
(c): This follows from (c ̄B Q B ( A I ) − c ̄)B = c ̄B Q B ( A I )B − c ̄B = 0 and
(c ̄B Q B ( A I ) − c ̄) A′ = c ̄B Q B ( A I ) A′ − c (−I ) = c .
(d): This follows from 0 = (A I )A′w = (A I )B(A′
B w) + ( A I )J \{i}( A′
J \{i}w) +
( A I )i ( A′i w) = ( A I )B( A′Bw) − ( A I )i .
(e): By (c), y as in ©2 of the SIMPLEX ALGORITHM is given by the last row of TB . If y ≥ 0, we stop (x and y are optimal). Otherwise i is the first index with yi < 0, found in O(n + m) time. If the i -th column of TB has no positive entry, we stop (the LP is unbounded, and w is given by (d)). Otherwise, by (b) and (d), we have that λ in ©4 of the SIMPLEX ALGORITHM is given by
λ = min
{ (QBb)j (Q B ( A I )i ) j
: j ∈ {1, . . . , m}, (Q B (A I )i ) j > 0
} ,
and among the indices attaining this minimum, j is the one for which the j -th component of B is minimum. So we can compute j in O(m) time by considering the i -th and the last column of TB. This yields B′. We can compute the updated tableau TB′ as follows: Divide the j -th row by the entry in row j and column i . Then add a suitable multiple of the j -th row to all other rows, such that the i -th column has zeros only outside row j . Note that these row operations do not destroy the property that the tableau has the form ( Q(A I ) Qb
v(A I ) − c ̄ vb
)


for some nonsingular matrix Q and some vector v, and in addition we have
Q(A I )B′ = I and (v(A I ) − c ̄)B′ = 0. Since there is only one choice for Q and v,
namely Q = Q B′ and v = c ̄B′ Q B′, the updated tableau TB′ is computed correctly by the above operations in O(m(n + m)) time.
To start the SIMPLEX ALGORITHM we consider an LP of the form
max{cx : A′x ≤ b′, A′′x ≤ b′′, x ≥ 0}
with A′ ∈ Rm′×n, A′′ ∈ Rm′′×n, b′ ≥ 0 and b′′ < 0. We first run the SIMPLEX ALGORITHM on the instance
min{(1l A′′)x + 1ly : A′x ≤ b′, A′′x + y ≥ b′′, x , y ≥ 0},
starting with the tableau
⎛
⎝
A′ 0 I 0 b′ − A′′ −I 0 I −b′′ 1lA′′ 1l 0 0 0
⎞
⎠ , (3.5)
corresponding to the basic solution x = 0, y = 0. Then we run the iterations of the SIMPLEX ALGORITHM as in Theorem 3.17(e). If the algorithm terminates with optimum value 1lb, we modify the final simplex tableau as follows. Multiply some rows by −1 such that none of the columns n + m′′+m′+1, . . . , n+m′′+m′+m′′ (the fourth section in (3.5)) is a unit vector, delete the fourth section of the tableau (i.e. columns n+m′′+m′+1, . . . , n+m′′+m′+m′′), and replace the last row by (−c, 0, 0, 0). Then add suitable multiples of the other rows to the last row in order to get zeros at m′ + m′′ places corresponding to columns with distinct unit vectors; these will form our basis. The result is the simplex tableau with respect to the original LP and this basis. Therefore we can continue running the iterations of the SIMPLEX ALGORITHM as in Theorem 3.17(e). In fact, one can often do even more efficiently. Suppose we want to solve an LP min{cx : Ax ≥ b, x ≥ 0} with a very large number of inequalities which are implicitly given in a way that allows us to solve the following problem efficiently: Given a vector x ≥ 0, decide if Ax ≥ b and find a violated inequality otherwise. We apply the SIMPLEX ALGORITHM to the dual LP max{yb : y A ≤ c, y ≥ 0} =
max{by : A y ≤ c, y ≥ 0}. Let b ̄ := (b , 0). For a basis B we set Q B :=
((A I )B )−1 and store only the right-hand part of the simplex tableau
( QB QBc b ̄ B Q B b x
) .
The last row of the full simplex tableau is b ̄ B Q B (A I )−b ̄. To perform an iteration,
we must check if b ̄ B Q B ≥ 0 and b ̄ B Q B A − b ≥ 0, and find a negative component
if one exists. This reduces to solving the above problem for x = (b ̄ B Q B ) . Then we generate the corresponding column of the full simplex tableau, but only for the


current iteration. After updating the reduced tableau we can delete it again. This technique is known under the names revised simplex and column generation. We will see applications later.
3.4 Duality
Theorem 3.14 shows that the LPs (3.1) and (3.2) are related. This motivates the following definition:
Definition 3.18. Given a linear program max{cx : Ax ≤ b}, we define the dual LP to be the linear program min{yb : y A = c, y ≥ 0}.
In this case, the original LP max{cx : Ax ≤ b} is often called the primal LP.
Proposition 3.19. The dual of the dual of an LP is (equivalent to) the original LP.
Proof: Let the primal LP max{cx : Ax ≤ b} be given. Its dual is min{yb : y A = c, y ≥ 0}, or equivalently
− max
⎧⎨
⎩−by :
⎛
⎝
A −A −I
⎞
⎠y ≤
⎛
⎝
c −c 0
⎞
⎠
⎫⎬
⎭.
(Each equality constraint has been split up into two inequality constraints.) So the dual of the dual is
− min
⎧⎨
⎩zc − z′c : (A −A −I )
⎛
⎝
z z′ w
⎞
⎠ = −b, z, z′, w ≥ 0
⎫⎬
⎭
which is equivalent to − min{−cx : −Ax − w = −b, w ≥ 0} (where we have substituted x for z′ − z). By eliminating the slack variables w we see that this is equivalent to the primal LP.
We now obtain the most important theorem in LP theory, the Duality Theorem:
Theorem 3.20. (von Neumann [1947], Gale, Kuhn and Tucker [1951]) If the polyhedra P := {x : Ax ≤ b} and D := {y : y A = c, y ≥ 0} are both nonempty, then max{cx : x ∈ P} = min{yb : y ∈ D}.
Proof: If D is nonempty, it has a vertex y. We run the SIMPLEX ALGORITHM for min{yb : y ∈ D} and y. By Proposition 3.13, the existence of some x ∈ P guarantees that min{yb : y ∈ D} is not unbounded. Thus by Theorem 3.14, the SIMPLEX ALGORITHM returns optimum solutions y and z of the LP min{yb : y ∈ D} and its dual. However, the dual is max{cx : x ∈ P} by Proposition 3.19. We have yb = cz, as required.


We can say even more about the relation between the optimum solutions of the primal and dual LP:
Corollary 3.21. Let max{cx : Ax ≤ b} and min{yb : y A = c, y ≥ 0} be a primal-dual pair of LPs. Let x and y be feasible solutions, i.e. Ax ≤ b, y A = c and y ≥ 0. Then the following statements are equivalent:
(a) x and y are both optimum solutions. (b) cx = yb.
(c) y(b − Ax) = 0.
Proof: The Duality Theorem 3.20 immediately implies the equivalence of (a) and (b). The equivalence of (b) and (c) follows from y(b − Ax) = yb − y Ax = yb −cx.
The property (c) of optimum solutions is often called complementary slackness. It can also be formulated as follows: a point x ∗ ∈ P = {x : Ax ≤ b} is an optimum solution of max{cx : x ∈ P} if and only if c is a nonnegative combination of those rows of A which correspond to inequalities of Ax ≤ b that are satisfied by x∗ with equality. It also implies:
Corollary 3.22. Let P = {x : Ax ≤ b} be a polyhedron and ∅ = Z ⊆ P. Then the set of vectors c for which each z ∈ Z is an optimum solution of max{cx : x ∈ P} is the cone generated by the rows of A′, where A′x ≤ b′ is the maximal subsystem of Ax ≤ b with A′z = b′ for all z ∈ Z .
Proof: There is a z ∈ conv(Z ) that satisfies all other inequalities of Ax ≤ b strictly. Let c be a vector for which each element of Z , and hence also z, is an optimum solution of max{cx : x ∈ P}. Then by Corollary 3.21 there exists an y ≥ 0 with c = y A′, i.e. c is a nonnegative linear combination of the rows of A′. Conversely, for a row a′x ≤ β′ of A′x ≤ b′ and z ∈ Z we have a′z = β′ = max{a′x : x ∈ P}.
Let us write Corollary 3.21 in another form:
Corollary 3.23. Let min{cx : Ax ≥ b, x ≥ 0} and max{yb : y A ≤ c, y ≥ 0} be a primal-dual pair of LPs. Let x and y be feasible solutions, i.e. Ax ≥ b, y A ≤ c and x, y ≥ 0. Then the following statements are equivalent:
(a) x and y are both optimum solutions. (b) cx = yb. (c) (c − y A)x = 0 and y(b − Ax) = 0.
Proof: The equivalence of (a) and (b) is obtained by applying the Duality The
orem 3.20 to max {(−c)x : ( −−IA
) x ≤ ( −b
0
)}.
To prove that (b) and (c) are equivalent, observe that we have y(b − Ax) ≤ 0 ≤ (c − y A)x for any feasible solutions x and y, and that y(b − Ax) = (c − y A)x iff yb = cx.


The two conditions in (c) are sometimes called primal and dual complementary slackness conditions.
The Duality Theorem has many applications in combinatorial optimization. One reason for its importance is that the optimality of a solution can be proved by giving a feasible solution of the dual LP with the same objective value. We shall show now how to prove that an LP is unbounded or infeasible:
Theorem 3.24. There exists a vector x with Ax ≤ b if and only if yb ≥ 0 for each vector y ≥ 0 for which y A = 0.
Proof: If there is a vector x with Ax ≤ b, then yb ≥ y Ax = 0 for each y ≥ 0 with y A = 0. Consider the LP
− min{1lw : Ax − w ≤ b, w ≥ 0}. (3.6)
Writing it in standard form we have
max
{(0 −1l) ( x
w
) :
(A −I 0 −I
)(x
w
) ≤
(b
0
)} .
The dual of this LP is
min
{(b 0) (y
z
) :
(A 0 −I −I
) (y
z
) =
(0
−1l
)
, y, z ≥ 0
} ,
or, equivalently, min{yb : y A = 0, 0 ≤ y ≤ 1l}. (3.7)
Since both (3.6) and (3.7) have a solution (x = 0, w = |b|, y = 0), we can apply Theorem 3.20. So the optimum values of (3.6) and (3.7) are the same. Since the system Ax ≤ b has a solution iff the optimum value of (3.6) is zero, the proof is complete.
So the fact that a linear inequality system Ax ≤ b has no solution can be proved by giving a vector y ≥ 0 with y A = 0 and yb < 0. We mention two equivalent formulations of Theorem 3.24:
Corollary 3.25. There is a vector x ≥ 0 with Ax ≤ b if and only if yb ≥ 0 for each vector y ≥ 0 with y A ≥ 0.
Proof: Apply Theorem 3.24 to the system ( −AI
)x ≤ (b
0
).
Corollary 3.26. (Farkas [1894]) There is a vector x ≥ 0 with Ax = b if and only if yb ≥ 0 for each vector y with y A ≥ 0.
Proof: Apply Corollary 3.25 to the system ( −AA
) x ≤ ( −bb
), x ≥ 0.
Corollary 3.26 is usually known as Farkas’ Lemma. The above results in turn imply the Duality Theorem 3.20 which is interesting since they have quite easy


direct proofs (in fact they were known before the SIMPLEX ALGORITHM); see Exercises 11 and 12. We have seen how to prove that an LP is infeasible. How can we prove that an LP is unbounded? The next theorem answers this question.
Theorem 3.27. If an LP is unbounded, then its dual LP is infeasible. If an LP has an optimum solution, then its dual also has an optimum solution.
Proof: The first statement follows immediately from Proposition 3.13. To prove the second statement, suppose that the (primal) LP max{cx : Ax ≤ b} has an optimum solution x∗, but the dual min{yb : y A = c, y ≥ 0} is infeasible (it cannot be unbounded due to the first statement). In other words, there is no y ≥ 0 with A y = c, and we apply Farkas’ Lemma (Corollary 3.26) to get a vector z with z A ≥ 0 and zc < 0. But then x∗ − z is feasible for the primal, because A(x∗ − z) = Ax∗ − Az ≤ b. The observation c(x∗ − z) > cx∗ therefore contradicts the optimality of x∗.
So there are four cases for a primal-dual pair of LPs: either both have an optimum solution (in which case the optimum values are the same), or one is infeasible and the other one is unbounded, or both are infeasible. We also note:
Corollary 3.28. A feasible LP max{cx : Ax ≤ b} is bounded if and only if c belongs to the cone generated by the rows of A.
Proof: The LP is bounded iff its dual is feasible, i.e. there is a y ≥ 0 with y A = c.
Farkas’ Lemma also enables us to prove that each finitely generated cone is polyhedral:
Theorem 3.29. (Minkowski [1896], Weyl [1935]) A cone is polyhedral if and only if it is finitely generated.
Proof: The only-if direction is given by Lemma 3.12. So consider the cone C generated by a1, . . . , at . We have to show that C is polyhedral. Let A be the matrix whose rows are a1, . . . , at . By Lemma 3.12, the cone D := {x : Ax ≤ 0} is generated by some vectors b1, . . . , bs. Let B be the matrix whose rows are b1, . . . , bs. We prove that C = {x : Bx ≤ 0}.
As b j ai = ai b j ≤ 0 for all i and j , we have C ⊆ {x : B x ≤ 0}. Now suppose there is a vector w ∈/ C with Bw ≤ 0. w ∈ C means that there is no v ≥ 0 such that A v = w. By Farkas’ Lemma (Corollary 3.26) this means that there is a vector y with yw < 0 and Ay ≥ 0. So −y ∈ D. Since D is generated by b1, . . . , bs we have −y = z B for some z ≥ 0. But then 0 < −yw = z Bw ≤ 0, a contradiction.


3.5 Convex Hulls and Polytopes
In this section we collect some more facts on polytopes. In particular, we show that polytopes are precisely those sets that are the convex hull of a finite number of points. We start by recalling some basic definitions:
Definition 3.30. Given vectors x1, . . . , xk ∈ Rn and λ1, . . . , λk ≥ 0 with ∑k
i=1 λi
= 1, we call x = ∑k
i=1 λi xi a convex combination of x1, . . . , xk . A set X ⊆ Rn
is convex if λx + (1 − λ)y ∈ X for all x, y ∈ X and λ ∈ [0, 1]. The convex hull conv(X) of a set X is defined as the set of all convex combinations of points in X. An extreme point of a set X is an element x ∈ X with x ∈/ conv(X \ {x}).
So a set X is convex if and only if all convex combinations of points in X are again in X. The convex hull of a set X is the smallest convex set containing X . Moreover, the intersection of convex sets is convex. Hence polyhedra are convex. Now we prove the “finite basis theorem for polytopes”, a fundamental result which seems to be obvious but is not trivial to prove directly:
Theorem 3.31. (Minkowski [1896], Steinitz [1916], Weyl [1935]) A set P is a polytope if and only if it is the convex hull of a finite set of points.
Proof: (Schrijver [1986]) Let P = {x ∈ Rn : Ax ≤ b} be a nonempty polytope. Obviously,
P=
{
x:
(x
1
)
∈C
}
, where C =
{(x
λ
)
∈ Rn+1 : λ ≥ 0, Ax − λb ≤ 0
} .
C is a polyhedral cone, so by Theorem 3.29 it is generated by finitely many nonzero
vectors, say by ( xλ11
) , . . . , ( λxkk
). Since P is bounded, all λi are nonzero; w.l.o.g. all λi are 1. So x ∈ P if and only if
(x
1
)
= μ1
(x1 1
)
+ · · · + μk
(xk 1
)
for some μ1, . . . , μk ≥ 0. In other words, P is the convex hull of x1, . . . , xk. Now let P be the convex hull of x1, . . . , xk ∈ Rn. Then x ∈ P if and only if
(x 1
) ∈ C, where C is the cone generated by ( x1
1
) , . . . , ( xk
1
). By Theorem 3.29, C is polyhedral, so
C=
{(x
λ
)
: Ax + bλ ≤ 0
} .
We conclude that P = {x ∈ Rn : Ax + b ≤ 0}.
Corollary 3.32. A polytope is the convex hull of its vertices.
Proof: Let P be a polytope. By Theorem 3.31, the convex hull of its vertices is a polytope Q. Obviously Q ⊆ P. Suppose there is a point z ∈ P \ Q. Then there is a vector c with cz > max{cx : x ∈ Q}. The supporting hyperplane


{x : cx = max{cy : y ∈ P}} of P defines a face of P containing no vertex. This is impossible by Corollary 3.10.
The previous two and the following result are the starting point of polyhedral combinatorics; they will be used very often in this book. For a given ground set E and a subset X ⊆ E, the incidence vector of X (with respect to E) is defined as the vector x ∈ {0, 1}E with xe = 1 for e ∈ X and xe = 0 for e ∈ E \ X .
Corollary 3.33. Let (E, F ) be a set system, P the convex hull of the incidence vectors of the elements of F , and c : E → R. Then max{cx : x ∈ P} = max{c(X ) : X ∈ F}.
Proof: Since max{cx : x ∈ P} ≥ max{c(X ) : X ∈ F } is trivial, let x be an optimum solution of max{cx : x ∈ P} (note that P is a polytope by Theorem 3.31). By definition of P, x is a convex combination of incidence vectors y1, . . . , yk of
elements of F: x = ∑k
i=1 λi yi for some λ1, . . . , λk ≥ 0 with ∑k
i=1 λi = 1. Since
cx = ∑k
i=1 λi cyi , we have cyi ≥ cx for at least one i ∈ {1, . . . , k}. This yi is the
incidence vector of a set Y ∈ F with c(Y ) = cyi ≥ cx.
Exercises
1. Let H be a hypergraph, F ⊆ V (H ), and x, y : F → R. The task is to
find x, y : V (H ) \ F → R such that ∑
e∈E(H)(maxv∈e x (v) − minv∈e x (v) +
maxv∈e y(v) − minv∈e y(v)) is minimum. Show that this can be formulated as an LP. Note: This is a relaxation of a placement problem in VLSI design. Here H is called the netlist, and its vertices correspond to modules that need to placed on the chip. Some (those in F) are pre-placed in advance. The main difficulty (ignored in this relaxation) is that modules must not overlap. 2. A set of vectors x1, . . . , xk is called affinely independent if there is no λ ∈
Rk \ {0} with λ 1l = 0 and ∑k
i=1 λi xi = 0. Let ∅ = X ⊆ Rn. Show that the maximum cardinality of an affinely independent set of elements of X equals dim X + 1. 3. Let P, Q ∈ Rn be polyhedra. Prove that the closure of conv(P ∪ Q) is a polyhedron. Show polyhedra P and Q for which conv(P ∪ Q) is not a polyhedron. 4. Show that the problem to compute the largest ball that is a subset of a given polyhedron can be formulated as a linear program. 5. Let P be a polyhedron. Prove that the dimension of any facet of P is one less than the dimension of P. 6. Let F be a minimal face of a polyhedron {x : Ax ≤ b}. Prove that then Ax = Ay for all x, y ∈ F.
7. Let A ∈ Rm×n, b ∈ Rm, c ∈ Rn, and u ∈ Zn. Consider the LP max{cx : Ax ≤ b, 0 ≤ x ≤ u}. Prove: if this LP has an optimum solution, then it has an optimum solution with at most m components that are not integers.


8. Formulate the dual of the LP formulation (1.1) of the JOB ASSIGNMENT PROBLEM. Show how to solve the primal and the dual LP in the case when there are only two jobs (by a simple algorithm). 9. Let G be a digraph, c : E(G) → R+, E1, E2 ⊆ E(G), and s, t ∈ V (G). Consider the following linear program
min
∑
e∈E(G)
c(e)ye
s.t. ye ≥ zw − zv (e = (v, w) ∈ E(G)) zt − zs = 1 ye ≥ 0 (e ∈ E1) ye ≤ 0 (e ∈ E2).
Prove that there is an optimum solution (y, z) and s ∈ X ⊆ V (G) \ {t} with ye = 1 for e ∈ δ+(X ), ye = −1 for e ∈ δ−(X ) \ E1, and ye = 0 for all other edges e. Hint: Consider the complementary slackness conditions for the edges entering or leaving {v ∈ V (G) : zv ≤ zs }. 10. Let Ax ≤ b be a linear inequality system in n variables. By multiplying each row by a positive constant we may assume that the first column of A is a vector with entries 0, −1 and 1 only. So we can write Ax ≤ b equivalently as
ai′ x ′ ≤ bi (i = 1, . . . , m1),
−x1 + a′
j x ′ ≤ b j ( j = m1 + 1, . . . , m2),
x1 + a′
k x ′ ≤ bk (k = m2 + 1, . . . , m),
where x′ = (x2, . . . , xn) and a′
1, . . . , a′m are the rows of A without the first
entry. Then one can eliminate x1: Prove that Ax ≤ b has a solution if and only if the system
ai′ x ′ ≤ bi (i = 1, . . . , m1), a′
j x ′ − b j ≤ bk − a′
k x′ ( j = m1 + 1, . . . , m2, k = m2 + 1, . . . , m)
has a solution. Show that this technique, when iterated, leads to an algorithm for solving a linear inequality system Ax ≤ b (or proving infeasibility). Note: This method is known as Fourier-Motzkin elimination because it was proposed by Fourier and studied by Motzkin [1936]. One can prove that it is not a polynomial-time algorithm. 11. Use Fourier-Motzkin elimination (Exercise 10) to prove Theorem 3.24 directly. (Kuhn [1956]) 12. Show that Theorem 3.24 implies the Duality Theorem 3.20. 13. Prove the decomposition theorem for polyhedra: Any polyhedron P can be written as P = {x + c : x ∈ X, c ∈ C}, where X is a polytope and C is a polyhedral cone. (Motzkin [1936])


14. Let P be a rational polyhedron and F a face of P. Show that
{c : cz = max {cx : x ∈ P} for all z ∈ F}
is a rational polyhedral cone. 15. Prove Carathéodory’s theorem: If X ⊆ Rn and y ∈ conv(X ), then there are x1, . . . , xn+1 ∈ X such that y ∈ conv({x1, . . . , xn+1}). (Carathéodory [1911]) 16. Prove the following extension of Carathéodory’s theorem (Exercise 15): If X ⊆ Rn and y, z ∈ conv(X ), then there are x1, . . . , xn ∈ X such that y ∈ conv({z, x1, . . . , xn}).
17. Prove that the extreme points of a polyhedron are precisely its vertices. 18. Let P be a nonempty polytope. Consider the graph G(P) whose vertices are the vertices of P and whose edges correspond to the 1-dimensional faces of P. Let x be any vertex of P, and c a vector with c x < max{c z : z ∈ P}. Prove that then there is a neighbour y of x in G(P) with c x < c y. 19. Use Exercise 18 to prove that G(P) is n-connected for any n-dimensional polytope P (n ≥ 1). 20. Let P ⊆ Rn be a polytope (not necessarily rational) and y ∈/ P. Prove that there exists a rational vector c with max{cx : x ∈ P} < cy. Show that the statement does not hold for general polyhedra. 21. Let X ⊂ Rn be a nonempty convex set, X ̄ the closure of X , and y ∈/ X . Prove:
(a) There is a unique point in X ̄ that has minimum distance to y. (b) There exists a vector a ∈ Rn \ {0} with a x ≤ a y for all x ∈ X .
(c) If y ∈/ X ̄ , then there exists a vector a ∈ Rn with a x < a y for all x ∈ X .
(d) If X is bounded and y ∈/ X ̄ , then there exists a vector a ∈ Qn with a x < a y for all x ∈ X .
(e) A closed convex set is the intersection of all closed half-spaces containing it.
References
General Literature
Bertsimas, D., and Tsitsiklis, J.N. [1997]: Introduction to Linear Optimization. Athena Scientific, Belmont 1997 Chvátal, V. [1983]: Linear Programming. Freeman, New York 1983 Matoušek, J., and Gärtner, B. [2007]: Understanding and Using Linear Programming. Springer, Berlin 2007 Padberg, M. [1999]: Linear Optimization and Extensions. Second Edition. Springer, Berlin 1999 Schrijver, A. [1986]: Theory of Linear and Integer Programming. Wiley, Chichester 1986


Cited References
Avis, D., and Chvátal, V. [1978]: Notes on Bland’s pivoting rule. Mathematical Programming Study 8 (1978), 24–34 Bland, R.G. [1977]: New finite pivoting rules for the simplex method. Mathematics of Operations Research 2 (1977), 103–107 Borgwardt, K.-H. [1982]: The average number of pivot steps required by the simplex method is polynomial. Zeitschrift für Operations Research 26 (1982), 157–177 Carathéodory, C. [1911]: Über den Variabilitätsbereich der Fourierschen Konstanten von positiven harmonischen Funktionen. Rendiconto del Circolo Matematico di Palermo 32 (1911), 193–217 Dantzig, G.B. [1951]: Maximization of a linear function of variables subject to linear inequalities. In: Activity Analysis of Production and Allocation (T.C. Koopmans, ed.), Wiley, New York 1951, pp. 359–373 Dantzig, G.B., Orden, A., and Wolfe, P. [1955]: The generalized simplex method for minimizing a linear form under linear inequality restraints. Pacific Journal of Mathematics 5 (1955), 183–195 Farkas, G. [1894]: A Fourier-féle mechanikai elv alkalmazásai. Mathematikai és Természettudományi Értesitö 12 (1894), 457–472 Gale, D., Kuhn, H.W., and Tucker, A.W. [1951]: Linear programming and the theory of games. In: Activity Analysis of Production and Allocation (T.C. Koopmans, ed.), Wiley, New York 1951, pp. 317–329 Hoffman, A.J., and Kruskal, J.B. [1956]: Integral boundary points of convex polyhedra. In: Linear Inequalities and Related Systems; Annals of Mathematical Study 38 (H.W. Kuhn, A.W. Tucker, eds.), Princeton University Press, Princeton 1956, pp. 223–246 Kelner, J.A., and Spielman, D.A. [2006]: A randomized polynomial-time simplex algorithm for linear programming. Proceedings of the 38th Annual ACM Symposium on Theory of Computing (2006), 51–60 Klee, V., and Minty, G.J. [1972]: How good is the simplex algorithm? In: Inequalities III (O. Shisha, ed.), Academic Press, New York 1972, pp. 159–175 Kuhn, H.W. [1956]: Solvability and consistency for linear equations and inequalities. The American Mathematical Monthly 63 (1956), 217–232 Minkowski, H. [1896]: Geometrie der Zahlen. Teubner, Leipzig 1896 Motzkin, T.S. [1936]: Beiträge zur Theorie der linearen Ungleichungen (Dissertation). Azriel, Jerusalem 1936 von Neumann, J. [1947]: Discussion of a maximum problem. Working paper. Published in: John von Neumann, Collected Works; Vol. VI (A.H. Taub, ed.), Pergamon Press, Oxford 1963, pp. 27–28 Spielman, D.A., and Teng, S.-H. [2004]: Smoothed analysis of algorithms: why the simplex algorithm usually takes polynomial time. Journal of the ACM 51 (2004), 385–463 Steinitz, E. [1916]: Bedingt konvergente Reihen und konvexe Systeme. Journal für die reine und angewandte Mathematik 146 (1916), 1–52 Weyl, H. [1935]: Elementare Theorie der konvexen Polyeder. Commentarii Mathematici Helvetici 7 (1935), 290–306


4 Linear Programming Algorithms
Three types of algorithms for LINEAR PROGRAMMING had the most impact: the SIMPLEX ALGORITHM (see Section 3.2), interior point algorithms, and the ELLIPSOID METHOD. Each of these has a disadvantage: In contrast to the other two, so far no variant of the SIMPLEX ALGORITHM has been shown to have a polynomial running time. In Sections 4.4 and 4.5 we present the ELLIPSOID METHOD and prove that it leads to a polynomial-time algorithm for LINEAR PROGRAMMING. However, the ELLIPSOID METHOD is too inefficient to be used in practice. Interior point algorithms and, despite its exponential worst-case running time, the SIMPLEX ALGORITHM are far more efficient, and they are both used in practice to solve LPs. In fact, both the ELLIPSOID METHOD and interior point algorithms can be used for more general convex optimization problems, e.g. for so-called semidefinite programs. An advantage of the SIMPLEX ALGORITHM and the ELLIPSOID METHOD is that they do not require the LP to be given explicitly. It suffices to have an oracle (a subroutine) which decides whether a given vector is feasible and, if not, returns a violated constraint. We shall discuss this in detail with respect to the ELLIPSOID METHOD in Section 4.6, because it implies that many combinatorial optimization problems can be solved in polynomial time; for some problems this is in fact the only known way to show polynomial solvability. This is the reason why we discuss the ELLIPSOID METHOD but not interior point algorithms in this book. A prerequisite for polynomial-time algorithms is that there exists an optimum solution that has a binary representation whose length is bounded by a polynomial in the input size. We prove in Section 4.1 that this condition holds for LINEAR PROGRAMMING. In Sections 4.2 and 4.3 we review some basic algorithms needed later, including the well-known Gaussian elimination method for solving systems of equations.
4.1 Size of Vertices and Faces
Instances of LINEAR PROGRAMMING are vectors and matrices. Since no strongly polynomial-time algorithm for LINEAR PROGRAMMING is known we have to restrict attention to rational instances when analyzing the running time of algorithms.


We assume that all numbers are coded in binary. To estimate the size (number of bits) of this representation we define size(n) := 1 + log(|n| + 1) for integers
n ∈ Z and size(r ) := size( p) + size(q) for rational numbers r = p
q , where
p, q are relatively prime integers (i.e. their greatest common divisor is 1). For vectors x = (x1, . . . , xn) ∈ Qn we store the components and have size(x) :=
n + size(x1) + . . . + size(xn). For a matrix A ∈ Qm×n with entries aij we have
size(A) := mn + ∑
i, j size(aij ).
Of course these precise values are a somewhat random choice, but remember that we are not really interested in constant factors. For polynomial-time algorithms it is important that the sizes of numbers do not increase too much by elementary arithmetic operations. We note:
Proposition 4.1. If r1, . . . , rn are rational numbers, then
size(r1 · · · rn) ≤ size(r1) + · · · + size(rn);
size(r1 + · · · + rn) ≤ 2(size(r1) + · · · + size(rn)).
Proof: For integers s1, . . . , sn we obviously have size(s1 · · · sn) ≤ size(s1) + · · · + size(sn) and size(s1 + · · · + sn) ≤ size(s1) + · · · + size(sn).
Let now ri = pi
qi , where pi and qi are nonzero integers (i = 1, . . . , n). Then
size(r1 · · · rn) ≤ size( p1 · · · pn) + size(q1 · · · qn) ≤ size(r1) + · · · + size(rn). For the second statement, observe that the denominator q1 · · · qn has size at most size(q1) + · · · + size(qn). The numerator is the sum of the numbers q1 · · · qi−1 pi qi+1 · · · qn (i = 1, . . . , n), so its absolute value is at most (| p1| + · · · + | pn|)|q1 · · · qn|. Therefore the size of the numerator is at most size(r1) + · · · + size(rn).
The first part of this proposition also implies that we can often assume w.l.o.g. that all numbers in a problem instance are integers, since otherwise we can multiply each of them with the product of all denominators. For addition and inner product of vectors we have:
Proposition 4.2. If x, y ∈ Qn are rational vectors, then
size(x + y) ≤ 2(size(x) + size(y));
size(x y) ≤ 2(size(x) + size(y)).
Proof: Using Proposition 4.1 we have size(x + y) = n + ∑n
i=1 size(xi + yi ) ≤
n +2 ∑n
i=1 size(xi )+2 ∑n
i=1 size(yi ) = 2(size(x )+size(y))−3n and size(x y) =
size (∑n
i=1 xi yi
) ≤ 2 ∑n
i=1 size(xi yi ) ≤ 2 ∑n
i=1 size(xi ) + 2 ∑n
i=1 size(yi ) =
2(size(x) + size(y)) − 4n.
Even under more complicated operations the numbers involved do not grow fast. Recall that the determinant of a matrix A = (aij )1≤i, j≤n is defined by
det A := ∑ π ∈Sn
sgn(π )
n ∏
i =1
ai,π(i), (4.1)


where Sn is the set of all permutations of {1, . . . , n} and sgn(π) is the sign of the permutation π (defined to be 1 if π can be obtained from the identity map by an even number of transpositions, and −1 otherwise).
Proposition 4.3. For any rational square matrix A we have size(det A) ≤ 2 size(A).
Proof: We write aij = pij
qij with relatively prime integers pij , qij . Now let det A =
p
q where p and q are relatively prime integers. Then |det A| ≤ ∏
i, j (| pij | + 1)
and |q| ≤ ∏
i, j |qij |. We obtain size(q) ≤ size(A) and, using | p| = |det A||q| ≤
∏
i, j (| pij | + 1)|qij |,
size( p) ≤ ∑
i, j
(size( pij ) + 1 + size(qij )) = size(A).
With this observation we can prove:
Theorem 4.4. Suppose the rational LP max{cx : Ax ≤ b} has an optimum solution. Then it also has an optimum solution x with size(x) ≤ 4n(size(A) + size(b)), with components of size at most 4(size(A)+size(b)). If b = ei or b = −ei
for some unit vector ei , then there is a nonsingular submatrix A′ of A and an
optimum solution x with size(x) ≤ 4n size(A′).
Proof: By Corollary 3.5, the maximum is attained in a face F of {x : Ax ≤ b}. Let F′ ⊆ F be a minimal face. By Proposition 3.9, F′ = {x : A′x = b′} for some subsystem A′x ≤ b′ of Ax ≤ b. W.l.o.g., we may assume that the rows of A′ are linearly independent. We then take a maximal set of linear independent columns (call this matrix A′′) and set all other components to zero. Then x = (A′′)−1b′, filled up with zeros, is an optimum solution to our LP. By Cramer’s rule the entries of x
are given by x j = det A′′′
det A′′ , where A′′′ arises from A′′ by replacing the j -th column
by b′. By Proposition 4.3 we obtain size(x ) ≤ n + 2n(size(A′′′) + size(A′′)) ≤ 4n(size(A′′) + size(b′)). If b = ±ei then | det(A′′′)| is the absolute value of a
subdeterminant of A′′.
The encoding length of the faces of a polytope given by its vertices can be estimated as follows:
Lemma 4.5. Let P ⊆ Rn be a rational polytope and T ∈ N such that size(x) ≤ T for each vertex x. Then P = {x : Ax ≤ b} for some inequality system Ax ≤ b, each of whose inequalities ax ≤ β satisfies size(a) + size(β) ≤ 75n2T .
Proof: First assume that P is full-dimensional. Let F = {x ∈ P : ax = β} be a facet of P, where P ⊆ {x : ax ≤ β}. Let y1, . . . , yt be the vertices of F (by Proposition 3.6 they are also vertices of P). Let c be the solution of Mc = e1, where M is a t × n-matrix whose i -th row


is yi − y1 (i = 2, . . . , t) and whose first row is some unit vector that is linearly independent of the other rows. Observe that rank(M) = n (because dim F = n −1). So we have c = κa for some κ ∈ R \ {0}. By Theorem 4.4 size(c) ≤ 4n size(M′), where M′ is a nonsingular n × nsubmatrix of M. By Proposition 4.2 we have size(M′) ≤ 4nT and size(c y1) ≤ 2(size(c) + size(y1)). So the inequality c x ≤ δ (or c x ≥ δ if κ < 0), where δ := c y1 = κβ, satisfies size(c) + size(δ) ≤ 3 size(c) + 2T ≤ 48n2T + 2T ≤ 50n2T . Collecting these inequalities for all facets F yields a description of P. If P = ∅, the assertion is trivial, so we now assume that P is neither fulldimensional nor empty. Let V be the set of vertices of P. For s = (s1, . . . , sn) ∈ {−1, 1}n let Ps be the convex hull of V ∪ {x + si ei : x ∈ V , i = 1, . . . , n}. Each Ps is a full-dimensional polytope (Theorem 3.31), and the size of any of its vertices is at most T + n (cf. Corollary 3.32). By the above, Ps can be described by inequalities of size at most 50n2(T + n) ≤ 75n2T (note that T ≥ 2n). Since
P =⋂
s∈{−1,1}n Ps , this completes the proof.
4.2 Continued Fractions
When we say that the numbers occurring in a certain algorithm do not grow too fast, we often assume that for each rational p
q the numerator p and the denominator q are relatively prime. This assumption causes no problem if we can easily find the greatest common divisor of two natural numbers. This is accomplished by one of the oldest algorithms:
EUCLIDEAN ALGORITHM
Input: Two natural numbers p and q.
Output: The greatest common divisor d of p and q, i.e. p
d and q
d are relatively prime integers.
©1 While p > 0 and q > 0 do:
If p < q then set q := q − q
p p else set p := p − p
q q.
©2 Return d := max{ p, q}.
Theorem 4.6. The EUCLIDEAN ALGORITHM works correctly. The number of iterations is at most size( p) + size(q).
Proof: The correctness follows from the fact that the set of common divisors of p and q does not change throughout the algorithm, until one of the numbers becomes zero. One of p or q is reduced by at least a factor of two in each iteration, hence there are at most log p + log q + 1 iterations.
Since no number occurring in an intermediate step is greater than p and q, we have a polynomial-time algorithm.


A similar algorithm is the so-called CONTINUED FRACTION EXPANSION. This can be used to approximate any number by a rational number whose denominator is not too large. For any positive real number x we define x0 := x and xi+1 := 1
xi − xi
for i = 1, 2, . . ., until xk ∈ N for some k. Then we have
x = x0 = x0 + 1
x1
= x0 + 1
x1 + 1
x2
= x0 + 1
x1 + 1
x2 + 1
x3
= ···
We claim that this sequence is finite if and only if x is rational. One direction follows immediately from the observation that xi+1 is rational if and only if xi is
rational. The other direction is also easy: If x = p
q , the above procedure is equivalent to the EUCLIDEAN ALGORITHM applied to p and q. This also shows that for a given rational number p
q with p, q > 0 the (finite) sequence x1, x2, . . . , xk as above can be computed in polynomial time. The following algorithm is almost identical to the EUCLIDEAN ALGORITHM except for the computation of the numbers gi
and hi ; we shall prove that the sequence
( gi hi
)
i∈N converges to x .
CONTINUED FRACTION EXPANSION
Input: Natural numbers p and q (let x := p
q ).
Output: The sequence
(
xi = pi
qi
)
i=0,1,... with x0 = p
q and xi+1 := 1
xi− xi .
©1 Set i := 0, p0 := p and q0 := q. Set g−2 := 0, g−1 := 1, h−2 := 1, and h−1 := 0.
©2 While qi = 0 do:
Set ai := pi
qi .
Set gi := ai gi−1 + gi−2. Set hi := ai hi−1 + hi−2. Set qi+1 := pi − ai qi . Set pi+1 := qi . Set i := i + 1.
We claim that the sequence gi
hi yields good approximations of x. Before we
can prove this, we need some preliminary observations:
Proposition 4.7. The following statements hold for all iterations i in the above algorithm:
(a) ai ≥ 1 (except possibly for i = 0) and hi ≥ hi−1. (b) gi−1hi − gi hi−1 = (−1)i .
(c) pi gi−1 + qi gi−2
pi hi−1 + qi hi−2
= x.
(d) gi
hi ≤ x if i is even and gi
hi ≥ x if i is odd.


Proof: (a) is obvious. (b) is easily shown by induction: For i = 0 we have gi−1hi − gi hi−1 = g−1h0 = 1, and for i ≥ 1 we have
gi−1hi − gi hi−1 = gi−1(ai hi−1 + hi−2) − hi−1(ai gi−1 + gi−2)
= gi−1hi−2 − hi−1 gi−2.
(c) is also proved by induction: For i = 0 we have
pi gi−1 + qi gi−2
pi hi−1 + qi hi−2
= pi · 1 + 0
0 + qi · 1 = x.
For i ≥ 1 we have
pi gi−1 + qi gi−2
pi hi−1 + qi hi−2
= qi−1(ai−1gi−2 + gi−3) + ( pi−1 − ai−1qi−1)gi−2
qi−1(ai−1hi−2 + hi−3) + ( pi−1 − ai−1qi−1)hi−2 = qi−1gi−3 + pi−1gi−2
qi−1hi−3 + pi−1hi−2
.
We finally prove (d). We note g−2
h−2 = 0 < x < ∞ = g−1
h−1 and proceed by
induction. The induction step follows easily from the fact that the function f (α) :=
αgi −1 +gi −2
αhi−1+hi−2 is monotone for α > 0, and f ( pi
qi ) = x by (c).
Theorem 4.8. (Khintchine [1956]) Given a rational number α and a natural number n, a rational number β with denominator at most n such that |α − β| is minimum can be found in polynomial time (polynomial in size(n) + size(α)).
Proof: We run the CONTINUED FRACTION EXPANSION with x := α. If the
algorithm stops with qi = 0 and hi−1 ≤ n, we can set β = gi−1
hi−1 = α by
Proposition 4.7(c). Otherwise let i be the last index with hi ≤ n, and let t be the maximum integer such that thi + hi−1 ≤ n (cf. Proposition 4.7(a)). Since ai+1hi + hi−1 = hi+1 > n, we have t < ai+1. We claim that
y := gi
hi
or z := tgi + gi−1
thi + hi−1
is an optimum solution. Both numbers have denominators at most n. If i is even, then y ≤ x < z by Proposition 4.7(d). Similarly, if i is odd,
we have y ≥ x > z. We show that any rational number p
q between y and z has denominator greater than n. Observe that
|z − y| = |hi gi−1 − hi−1gi |
hi (thi + hi−1) = 1
hi (thi + hi−1)
(using Proposition 4.7(b)). On the other hand,
|z − y| =
∣∣∣∣z − p
q
∣∣∣∣ +
∣∣∣∣
p
q −y
∣∣∣∣ ≥ 1
(thi + hi−1)q + 1
hi q = hi−1 + (t + 1)hi
qhi (thi + hi−1) ,
so q ≥ hi−1 + (t + 1)hi > n.
The above proof is from the book of Grötschel, Lovász and Schrijver [1988], which also contains important generalizations.


4.3 Gaussian Elimination
The most important algorithm in linear algebra is the so-called Gaussian elimination. It has been applied by Gauss but was known much earlier (see Schrijver [1986] for historical notes). Gaussian elimination is used to determine the rank of a matrix, to compute the determinant and to solve a system of linear equations. It occurs very often as a subroutine in linear programming algorithms; e.g. in ©1 of the SIMPLEX ALGORITHM. Given a matrix A ∈ Qm×n, our algorithm for Gaussian Elimination works with an extended matrix Z = (B C) ∈ Qm×(n+m); initially B = A and C = I . The
algorithm transforms B to the form ( I R
00
) by the following elementary operations: permuting rows and columns, adding a multiple of one row to another row, and (in the final step) multiplying rows by nonzero constants. At each iteration C is modified accordingly, such that the property C A ̃ = B is maintained throughout
where A ̃ results from A by permuting rows and columns. The first part of the algorithm, consisting of ©2 and ©3 , transforms B to an upper triangular matrix. Consider for example the matrix Z after two iterations; it has the form ⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
z11 = 0 z12 z13 · · · z1n 1 0 0 · · · 0 0 z22 = 0 z23 · · · z2n z2,n+1 1 0 · · · 0
0 0 z33 · · · z3n z3,n+1 z3,n+2 1 0 · · 0 · · · · · ·0 · · · · · · ·· I · · · · · · ·· 0 0 0 zm3 · · · zmn zm,n+1 zm,n+2 0 · · 0 1
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
If z33 = 0, then the next step just consists of subtracting zi3
z33 times the third row
from the i -th row, for i = 4, . . . , m. If z33 = 0 we first exchange the third row and/or the third column with another one. Note that if we exchange two rows, we have to exchange also the two corresponding columns of C in order to maintain the property C A ̃ = B. To have A ̃ available at each point we store the permutations of the rows and columns in variables r ow(i ), i = 1, . . . , m and col( j ), j = 1, . . . , n.
Then A ̃ = ( Arow(i),col( j ))i∈{1,...,m}, j ∈{1,...,n}.
The second part of the algorithm, consisting of ©4 and ©5 , is simpler since no rows or columns are exchanged anymore.
GAUSSIAN ELIMINATION
Input: A matrix A = (aij ) ∈ Qm×n.
Output: Its rank r ,
a maximal nonsingular submatrix A′ = (arow(i),col( j))i, j∈{1,...,r} of A,
its determinant d = det A′, and its inverse ( A′)−1 = (zi,n+ j )i, j∈{1,...,r}.


©1 Set r := 0 and d := 1.
Set zij := aij , r ow(i ) := i and col( j ) := j (i = 1, . . . , m, j = 1, . . . , n). Set zi,n+ j := 0 and zi,n+i := 1 for 1 ≤ i, j ≤ m, i = j .
©2 Let p ∈ {r + 1, . . . , m} and q ∈ {r + 1, . . . , n} with z pq = 0. If no such p and q exist, then go to ©4 . Set r := r + 1. If p = r then exchange z pj and zr j ( j = 1, . . . , n + m), exchange zi,n+p and zi,n+r (i = 1, . . . , m), and exchange r ow( p) and r ow(r ). If q = r then exchange ziq and zir (i = 1, . . . , m), and exchange col(q) and col(r ).
©3 Set d := d · zrr .
For i := r + 1 to m do: Set α := zir
zrr .
For j := r to n + r do: zij := zij − αzr j . Go to ©2 .
©4 For k := r down to 2 do: For i := 1 to k − 1 do: Set α := zik
zkk .
For j := k to n + r do zij := zij − αzkj .
©5 For k := 1 to r do: Set α := 1
zkk .
For j := 1 to n + r do zkj := αzkj .
Theorem 4.9. GAUSSIAN ELIMINATION works correctly and terminates after O(mnr ) steps.
Proof: First observe that each time before ©2 we have zii = 0 for i ∈ {1, . . . , r } and zij = 0 for all j ∈ {1, . . . , r } and i ∈ { j + 1, . . . , m}. Hence
det ((zi j )i, j ∈{1,2,...,r}
) = z11z22 · · · zrr = d = 0.
Since adding a multiple of one row to another row of a square matrix does not change the value of the determinant (this well-known fact follows directly from the definition (4.1)) we have
det ((zi j )i, j ∈{1,2,...,r}
) = det ((arow(i),col( j ))i, j ∈{1,2,...,r}
)
at any stage before ©5 , and hence the determinant d is computed correctly. A′ is a nonsingular r × r -submatrix of A. Since (zij )i∈{1,...,m}, j∈{1,...,n} has rank r at termination and the operations did not change the rank, A has also rank r .
Moreover, ∑m
j =1 zi,n+ j arow( j ),col(k) = zik for all i ∈ {1, . . . , m} and k ∈
{1, . . . , n} (i.e. C A ̃ = B in our above notation) holds throughout. (Note that for j = r + 1, . . . , m we have at any stage z j,n+ j = 1 and zi,n+ j = 0 for i = j .)
Since (zij )i, j∈{1,2,...,r} is the unit matrix at termination this implies that ( A′)−1 is


also computed correctly. The number of steps is obviously O(r mn + r 2(n + r )) = O(mnr ).
In order to prove that GAUSSIAN ELIMINATION is a polynomial-time algorithm we have to guarantee that all numbers that occur are polynomially bounded by the input size. This is not trivial but can be shown:
Theorem 4.10. (Edmonds [1967]) GAUSSIAN ELIMINATION is a polynomialtime algorithm. Each number occurring in the course of the algorithm can be stored with O(m(m + n) size(A)) bits.
Proof: We first show that in ©2 and ©3 all numbers are 0, 1, or quotients of subdeterminants of A. First observe that entries zij with i ≤ r or j ≤ r are not modified anymore. Entries zij with j > n +r are 0 (if j = n +i ) or 1 (if j = n +i ). Furthermore, we have for all s ∈ {r + 1, . . . , m} and t ∈ {r + 1, . . . , n + m}
zst = det ((zi j )i∈{1,2,...,r,s}, j ∈{1,2,...,r,t}
)
det ((zi j )i, j ∈{1,2,...,r}
).
(This follows from evaluating the determinant det ((zij )i∈{1,2,...,r,s}, j∈{1,2,...,r,t}
)
along the last row because zsj = 0 for all s ∈ {r +1, . . . , m} and all j ∈ {1, . . . , r }.) We have already observed in the proof of Theorem 4.9 that
det ((zi j )i, j ∈{1,2,...,r}
) = det ((arow(i),col( j ))i, j ∈{1,2,...,r}
),
because adding a multiple of one row to another row of a square matrix does not change the value of the determinant. By the same argument we have
det ((zi j )i∈{1,2,...,r,s}, j ∈{1,2,...,r,t}
) = det ((arow(i),col( j ))i∈{1,2,...,r,s}, j ∈{1,2,...,r,t}
)
for s ∈ {r + 1, . . . , m} and t ∈ {r + 1, . . . , n}. Furthermore,
det ((zi j )i∈{1,2,...,r,s}, j ∈{1,2,...,r,n+t}
) = det ((arow(i),col( j ))i∈{1,2,...,r,s}\{t}, j ∈{1,2,...,r}
)
for all s ∈ {r + 1, . . . , m} and t ∈ {1, . . . , r }, which is checked by evaluating the left-hand side determinant (after ©1 ) along column n + t. We conclude that at any stage in ©2 and ©3 all numbers zij are 0, 1, or quotients of subdeterminants of A. Hence, by Proposition 4.3, each number occurring in ©2 and ©3 can be stored with O(size(A)) bits. Finally observe that ©4 is equivalent to applying ©2 and ©3 again, choosing p and q appropriately (reversing the order of the first r rows and columns). Hence
each number occurring in ©4 can be stored with O (size ((zij )i∈{1,...,m}, j∈{1,...,m+n}
))
bits, which is O(m(m + n) size(A)). The easiest way to keep the representations of the numbers zij small enough is to guarantee that the numerator and denominator of each of these numbers are relatively prime at any stage. This can be accomplished by applying the EUCLIDEAN ALGORITHM after each computation. This gives an overall polynomial running time.


In fact, we can easily implement GAUSSIAN ELIMINATION to be a strongly polynomial-time algorithm (Exercise 4). So we can check in polynomial time whether a set of vectors is linearly independent, and we can compute the determinant and the inverse of a nonsingular matrix in polynomial time (exchanging two rows or columns changes just the sign of the determinant). Moreover we get:
Corollary 4.11. Given a matrix A ∈ Qm×n and a vector b ∈ Qm we can in polynomial time find a vector x ∈ Qn with Ax = b or decide that no such vector exists.
Proof: We compute a maximal nonsingular submatrix A′= (arow(i),col( j))i, j∈{1,...,r}
of A and its inverse ( A′)−1 = (zi,n+ j )i, j ∈{1,...,r} by GAUSSIAN ELIMINATION.
Then we set xcol( j) := ∑r
k=1 z j,n+k brow(k) for j = 1, . . . , r and xk := 0 for
k ∈/ {col(1), . . . , col(r )}. We obtain for i = 1, . . . r :
n ∑
j =1
arow(i), j x j =
r ∑
j =1
arow(i),col( j )xcol( j )
=
r ∑
j =1
arow(i),col( j )
r ∑
k=1
z j,n+k brow(k)
=
r ∑
k=1
br ow(k )
r ∑
j =1
arow(i),col( j )z j,n+k
= brow(i).
Since the other rows of A with indices not in {r ow(1), . . . , r ow(r )} are linear combinations of these, either x satisfies Ax = b or no vector satisfies this system of equations.
4.4 The Ellipsoid Method
In this section we describe the so-called ellipsoid method, developed by Iudin and Nemirovskii [1976] and Shor [1977] for nonlinear optimization. Khachiyan [1979] observed that it can be modified in order to solve LPs in polynomial time. Most of our presentation is based on (Grötschel, Lovász and Schrijver [1981]), (Bland, Goldfarb and Todd [1981]) and the book of Grötschel, Lovász and Schrijver [1988], which is also recommended for further study. The idea of the ellipsoid method is very roughly the following. In Proposition 4.16 we will show that it is sufficient to find a feasible solution to an LP or decide that none exists. We start with an ellipsoid that we know a priori to contain the basic solutions (e.g. a large ball). At each iteration k, we check if the center xk of the current ellipsoid is a feasible solution. Otherwise, we take a hyperplane containing xk such that all the solutions lie on one side of this hyperplane. Now we


have a half-ellipsoid which contains all solutions. We take the smallest ellipsoid completely containing this half-ellipsoid and continue.
Definition 4.12. An ellipsoid is a set E(A, x) = {z ∈ Rn : (z − x) A−1(z − x) ≤ 1} for some symmetric positive definite n × n-matrix A.
Note that B(x, r ) := E(r 2 I, x) (with I being the n × n unit matrix) is the n-dimensional Euclidean ball with center x and radius r . The volume of an ellipsoid E(A, x) is known to be
volume (E(A, x)) = √det A volume (B(0, 1))
(see Exercise 7). Given an ellipsoid E(A, x) and a hyperplane {z : az = ax}, the smallest ellipsoid E(A′, x′) containing the half-ellipsoid E′ = {z ∈ E(A, x) : az ≥ ax} is called the Löwner-John ellipsoid of E′ (see Figure 4.1). It can be computed by the following formulas:
A′ = n2
n2 − 1
(
A− 2
n + 1bb
) ,
x′ = x + 1
n + 1b,
b= 1
√a Aa Aa.
One difficulty of the ellipsoid method is caused by the square root in the computation of b. Because we have to tolerate rounding errors, it is necessary to increase the radius of the next ellipsoid a little bit. Here is an algorithmic scheme that takes care of this problem:
ELLIPSOID METHOD
Input: A number n ∈ N, n ≥ 2. A number N ∈ N. x0 ∈ Qn and R ∈ Q+, R ≥ 2. Output: An ellipsoid E(AN , xN ).
©1 Set p := 6N + log(9n3) . Set A0 := R2 I , where I is the n × n unit matrix. Set k := 0. ©2 Choose any ak ∈ Qn \ {0}.
©3 Set bk := 1
√
ak Ak ak
Ak ak .
Set xk+1 :≈ x ∗
k+1 := xk + 1
n + 1 bk.
Set Ak+1 :≈ A∗
k+1 := 2n2 + 3
2n2
(
Ak − 2
n + 1 bkbk
) .
(Here :≈ means computing the entries up to p binary digits behind the point, taking care that Ak+1 is symmetric).


Fig. 4.1.
©4 Set k := k + 1.
If k < N then go to ©2 else stop.
So in each of the N iterations an approximation E(Ak+1, xk+1) of the smallest ellipsoid containing E(Ak, xk) ∩ {z : akz ≥ ak xk} is computed. Two main issues, how to obtain the ak and how to choose N, will be addressed in the next section. But let us first prove some lemmas. Let ||x|| denote the Euclidean norm of vector x, while ||A|| := max{||Ax|| : ||x|| = 1} shall denote the norm of the matrix A. For symmetric matrices, ||A|| is the maximum absolute value of an eigenvalue and ||A|| = max{x Ax : ||x|| = 1}. The first lemma says that each Ek := E(Ak, xk) is indeed an ellipsoid. Furthermore, the absolute values of the numbers involved remain smaller than R22N + 2size(x0). Therefore each iteration of the ELLIPSOID METHOD consists of O(n2) computational steps, each involving numbers with O( p + size(ak) + size(R) + size(x0)) bits.
Lemma 4.13. (Grötschel, Lovász and Schrijver [1981]) Let k ∈ {0, 1, . . . , N}. Then Ak is positive definite, and we have
||xk|| ≤ ||x0|| + R2k , || Ak|| ≤ R22k , and || A−1
k || ≤ R−24k .
Proof: We use induction on k. For k = 0 all the statements are obvious. Assume that they are true for some k ≥ 0. By a straightforward computation one verifies


that
( A∗
k+1)−1 = 2n2
2n2 + 3
(
A−1
k+ 2
n−1
ak ak
ak Akak
)
. (4.2)
So ( A∗
k+1)−1 is the sum of a positive definite and a positive semidefinite matrix;
thus it is positive definite. Hence A∗
k+1 is also positive definite.
Note that for positive semidefinite matrices A and B we have ||A|| ≤ ||A + B||. Therefore
|| A∗
k+1|| = 2n2 + 3
2n2
∣∣∣∣
∣∣∣∣Ak − 2
n + 1 bkbk
∣∣∣∣
∣∣∣∣ ≤ 2n2 + 3
2n2 ||Ak|| ≤ 11
8 R22k.
Since the n × n all-one matrix has norm n, the matrix Ak+1 − A∗
k+1, each of whose
entries has absolute value at most 2−p, has norm at most n2−p. We conclude
|| Ak+1|| ≤ || A∗
k+1|| + || Ak+1 − A∗
k+1|| ≤ 11
8 R22k + n2−p ≤ R22k+1
(here we used the very rough estimate 2−p ≤ 1
n ).
It is well-known from linear algebra that for any symmetric positive definite n × n-matrix A there exists a symmetric positive definite matrix B with A = B B. Writing Ak = B B with B = B we obtain
||bk|| = || Akak||
√
ak Akak
=
√
ak A2
k ak
ak Akak
=
√
(Bak) Ak(Bak)
(Bak) (Bak) ≤ √|| Ak|| ≤ R2k−1.
Using this (and again the induction hypothesis) we get
||xk+1|| ≤ ||xk|| + 1
n + 1 ||bk|| + ||xk+1 − x ∗
k+1||
≤ ||x0|| + R2k + 1
n + 1 R2k−1 + √n2−p ≤ ||x0|| + R2k+1.
Using (4.2) and ||akak || = ak ak we compute
∣∣∣
∣∣∣( A∗
k+1)−1∣∣∣
∣∣∣ ≤ 2n2
2n2 + 3
(∣∣∣
∣∣∣ A−1
k
∣∣∣
∣∣∣ + 2
n−1
ak ak
ak Ak ak
)
(4.3)
= 2n2
2n2 + 3
(∣∣∣
∣∣∣ A−1
k
∣∣∣
∣∣∣ + 2
n−1
ak B A−1
k Bak
ak B Bak
)
≤ 2n2
2n2 + 3
(∣∣∣
∣∣∣ A−1
k
∣∣∣
∣∣∣ + 2
n−1
∣∣∣
∣∣∣ A−1
k
∣∣∣
∣∣∣
)
< n+1
n−1
∣∣∣
∣∣∣ A−1
k
∣∣∣
∣∣∣
≤ 3R−24k .
Let λ be the smallest eigenvalue of Ak+1, and let v be a corresponding eigen
vector with ||v|| = 1. Then – writing A∗
k+1 = CC for a symmetric matrix C – we


have
λ = v Ak+1v = v A∗
k+1v + v ( Ak+1 − A∗
k+1)v = v CCv
v C (A∗
k+1
)−1 Cv + v ( Ak+1 − A∗
k+1)v
≥
∣∣∣
∣∣∣( A∗
k+1)−1∣∣∣
∣∣∣−1 − || Ak+1 − A∗
k+1|| > 1
3 R24−k − n2−p ≥ R24−(k+1),
where we used 2−p ≤ 1
3n 4−k. Since λ > 0, Ak+1 is positive definite. Furthermore,
∣∣∣
∣∣∣( Ak+1)−1∣∣∣
∣∣∣ = 1
λ ≤ R−24k+1.
Next we show that in each iteration the ellipsoid contains the intersection of E0 and the previous half-ellipsoid:
Lemma 4.14. For k = 0, . . . , N −1 we have Ek+1 ⊇ {x ∈ Ek ∩ E0 : ak x ≥ ak xk}.
Proof: Let x ∈ Ek ∩ E0 with ak x ≥ ak xk. We first compute (using (4.2))
(x − x∗
k+1) ( A∗
k+1)−1(x − x ∗
k +1 )
= 2n2
2n2 +3
(
x − xk − 1
n +1 bk
)(
A−1
k+ 2
n−1
ak ak
ak Ak ak
)(
x − xk − 1
n +1 bk
)
= 2n2
2n2 + 3
(
(x − xk ) A−1
k (x − xk) + 2
n − 1 (x − xk) akak
ak Akak
(x − xk)
+1
(n + 1)2
(
bk A−1
k bk + 2
n−1
bk akak bk
ak Ak ak
)
− 2(x − xk)
n+1
(
A−1
k bk + 2
n−1
akak bk
ak Akak
))
= 2n2
2n2 + 3
(
(x − xk ) A−1
k (x − xk) + 2
n − 1 (x − xk) akak
ak Akak
(x − xk) +
1
(n + 1)2
(
1+ 2
n−1
)
−2
n+1
(x − xk) ak
√
ak Ak ak
(
1+ 2
n−1
)) .
Since x ∈ Ek , we have (x −xk) A−1
k (x −xk) ≤ 1. By abbreviating t := ak (x−xk)
√
ak Ak ak
we obtain
(x − x∗
k+1) ( A∗
k+1)−1(x − x ∗
k+1) ≤ 2n2
2n2 + 3
(
1+ 2
n − 1t2 + 1
n2 − 1 − 2
n − 1t
) .


Since bk A−1
k bk = 1 and bk A−1
k (x − xk) = t, we have
1 ≥ (x − xk) A−1
k (x − xk)
= (x − xk − tbk ) A−1
k (x − xk − tbk) + t2
≥ t2,
because A−1
k is positive definite. So (using ak x ≥ ak xk) we have 0 ≤ t ≤ 1 and obtain
(x − x∗
k+1) ( A∗
k+1)−1(x − x ∗
k+1) ≤ 2n4
2n4 + n2 − 3 .
It remains to estimate the rounding error
Z :=
∣∣∣(x − xk+1) ( Ak+1)−1(x − xk+1) − (x − x ∗
k+1) ( A∗
k+1)−1(x − x ∗
k+1)
∣∣∣
≤
∣∣∣(x − xk+1) ( Ak+1)−1(x ∗
k+1 − xk+1)
∣∣∣
+
∣∣∣(x ∗
k+1 − xk+1) ( Ak+1)−1(x − x ∗
k+1)
∣∣∣
+
∣∣∣(x − x∗
k+1)
(
( Ak+1)−1 − ( A∗
k+1)−1)
(x − x∗
k+1)
∣∣∣
≤ ||x − xk+1|| ||( Ak+1)−1|| ||x ∗
k+1 − xk+1||
+||x ∗
k+1 − xk+1|| ||( Ak+1)−1|| ||x − x ∗
k+1||
+||x − x∗
k+1||2 ||( Ak+1)−1|| ||( A∗
k+1)−1|| || A∗
k+1 − Ak+1||.
Using Lemma 4.13 and x ∈ E0 we get ||x − xk+1|| ≤ ||x − x0|| + ||xk+1 − x0|| ≤
R + R2N and ||x − x∗
k+1|| ≤ ||x − xk+1|| + √n2−p ≤ R2N+1. We also use (4.3) and obtain
Z ≤ 2(R2N+1)(R−24N )(√n2−p) + (R24N+1)(R−24N )(3R−24N−1)(n2−p)
= 4 R−123N √n2−p + 3 R−226N n2−p
≤ 26N n2−p
≤1
9n2 ,
by definition of p. Altogether we have
(x − xk+1) ( Ak+1)−1(x − xk+1) ≤ 2n4
2n4 + n2 − 3 + 1
9n2 ≤ 1.
The volumes of the ellipsoids decrease by a constant factor in each iteration:
Lemma 4.15. For k = 0, . . . , N − 1 we have volume (Ek+1)
volume (Ek ) < e− 1
5n .