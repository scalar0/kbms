Informatique théorique : mathématiques
discrètes et logique mathématique
Marc Aiguier Pascale Le Gall
16 septembre 2024


Table des matières
I Mathématiques discrètes 7
1 Introduction 9
Sous partie 1 Théorie de l’ordre et treillis 11
2 Introduction 13
3 Relations d’ordre, ensembles ordonnés, ensembles bien fondés 15 3.1 Relations d’ordre et ensembles ordonnés . . . . . . . . . . . . . . . . . . . . . . . 15 3.2 Éléments remarquables : chaînes et antichaînes, majorants et minorants, et ensembles inductifs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 3.3 Ensembles bien fondés et induction mathématique . . . . . . . . . . . . . . . . . 19 Ensembles bien fondés . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 Induction mathématique . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 Induction sans ordre bien fondé : lemme de Zorn et ses avatars . . . . . . . . . . 23
4 Treillis, définitions inductives et preuve par induction structurelle 27 4.1 Treillis et points fixes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 Définition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 Treillis complets et fonctions continues . . . . . . . . . . . . . . . . . . . . . . . . 28 4.2 Définitions inductives et induction structurelle . . . . . . . . . . . . . . . . . . . . 31 Définitions inductives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 Preuve par induction structurelle . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 Définition récursive d’une fonction . . . . . . . . . . . . . . . . . . . . . . . . . . 34 4.3 Application : sémantique dénotationnelle des langages de programmation . . . . . 36 Un langage de programmation simple . . . . . . . . . . . . . . . . . . . . . . . . . 36 Sémantique dénotationnelle du langage WHILE . . . . . . . . . . . . . . . . . . . 38
Sous partie 2 Calculabilité 43
5 Introduction 45
6 Fonctions primitives récursives et récursives 47 6.1 Codage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 6.2 Les fonctions primitives récursives . . . . . . . . . . . . . . . . . . . . . . . . . . 49 Définition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 Ensemble primitif récursif . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 6.3 Les fonctions récursives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 Récursion primitive et algorithmique . . . . . . . . . . . . . . . . . . . . . . . . . 51 Minimisation non bornée . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
2


6.4 Un interpréteur de programmes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 Indécidabilité du problème de l’arrêt . . . . . . . . . . . . . . . . . . . . . . . . . 59 6.5 Thèse de Church . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
7 Les machines de Turing 63 7.1 Définitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 7.2 Les fonctions T-calculables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 7.3 Machines de Turing universelles . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 7.4 Exemples de problèmes indécidables . . . . . . . . . . . . . . . . . . . . . . . . . 71 Problème du mot dans les systèmes semi-Thuien . . . . . . . . . . . . . . . . . . 71 Correspondance de Post . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 Théorème de Rice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
8 Complexité 77 8.1 Complexité en temps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 8.2 Les classes P et NP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78 8.3 Structure de la classe NP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 Problèmes NP-complets et NP-durs . . . . . . . . . . . . . . . . . . . . . . . . . . 81 Un premier problème NP-complet . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
Sous partie 3 Théorie des langages 89
9 Introduction 91
10 Langages rationnels 93 10.1 Monoide . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 10.2 Langages réguliers et automates . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 Définitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 Lemme de pompage et langage réguliers . . . . . . . . . . . . . . . . . . . . . . . 97 Expressions régulières . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98 10.3 Opérations sur les automates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 Déterminisation d’un automate . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 Minimisation d’un automate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
11 Langages algébriques 103 11.1 Grammaire algébrique . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 Définitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 11.2 Formes normales de grammaires . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 Algorithme CYK . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108 11.3 Arbre de dérivation et itération . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108 11.4 Propriétés de clôture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 11.5 Automates à pile . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
II Logiques mathématiques 117
12 Introduction 119 12.1 Logique mathématique et informatique . . . . . . . . . . . . . . . . . . . . . . . . 119 12.2 Structure d’une logique . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
3


13 Systèmes formels 123 13.1 Définition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123 13.2 Déductions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124 13.3 Théorèmes et arbres de preuve . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125 13.4 Correction et complétude des systèmes formels au regard d’une interprétation . . 127 Propriétés syntaxiques, algorithmiques . . . . . . . . . . . . . . . . . . . . . . . . 127 Propriétés sémantiques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
Sous partie 4 Logique propositionnelle 129
14 Introduction 131
15 Logique propositionnelle : syntaxe 133 15.1 Formules propositionnelles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 15.2 Quelques fonctions définies sur les formules propositionnelles . . . . . . . . . . . . 134
16 Sémantique 137 16.1 Validation des formules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137 16.2 Quelques propriétés remarquables . . . . . . . . . . . . . . . . . . . . . . . . . . . 139 16.3 Conséquence sémantique . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141 16.4 Théorème de la compacité . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141 16.5 Décidabilité da la logique propositionnelle . . . . . . . . . . . . . . . . . . . . . . 142 16.6 Méthode des tableaux . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144 Formules α et formules β . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144 Tableaux . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145 Construction d’un tableau . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146 Exemple de tableau . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147 Correction de la méthode des tableaux . . . . . . . . . . . . . . . . . . . . . . . . 148 Complétude de la méthode des tableaux . . . . . . . . . . . . . . . . . . . . . . . 149
17 Calculs pour la logique propositionnelle 151 17.1 Système de preuves à la Hilbert . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151 17.2 Système de la déduction naturelle . . . . . . . . . . . . . . . . . . . . . . . . . . . 152 17.3 Calcul des séquents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153 Quelques résultats utiles. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155 17.4 Calcul des séquents (complet) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
18 Méthodes algorithmiques 161 18.1 Mise sous forme normale . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161 Rappel : Formules équivalentes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161 Formules équisatisfiables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164 18.2 Algorithme DPLL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164 Valuations partielles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165 Stratégie de choix des variables et des valeurs booléennes . . . . . . . . . . . . . . 166 Algorithme DPLL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167 Vers les SAT solveurs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168 18.3 Résolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168 Quelques rappels et notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168 Satisfiabilité des formules de Horn . . . . . . . . . . . . . . . . . . . . . . . . . . 170
4


Résolvante et preuve par résolution . . . . . . . . . . . . . . . . . . . . . . . . . . 171 Complétude de la réfutation par résolution . . . . . . . . . . . . . . . . . . . . . . 173
Sous partie 5 Logique des prédicats 177
19 Introduction 179
20 Logique des prédicats : Syntaxe 181 20.1 Signatures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181 20.2 Termes et formules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
21 Sémantique 185 21.1 Modèles ou structures du premier ordre . . . . . . . . . . . . . . . . . . . . . . . 185 21.2 Validation des formules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186 21.3 Vers une procédure de semi-décision . . . . . . . . . . . . . . . . . . . . . . . . . 189 Formules prénexes et universelles . . . . . . . . . . . . . . . . . . . . . . . . . . . 189 Théorème d’Herbrand . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190 Skolémisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191 21.4 Indécidabilité de la logique des prédicats du premier ordre . . . . . . . . . . . . . 192 21.5 Pouvoir d’expression de la logique des prédicats . . . . . . . . . . . . . . . . . . . 193 Logique des prédicats avec égalité . . . . . . . . . . . . . . . . . . . . . . . . . . . 193 Théorème de Loweinheim et Skolem . . . . . . . . . . . . . . . . . . . . . . . . . 194
22 Calcul pour la logique des prédicats 197 22.1 Calcul à la Hilbert . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197 Correction et complétude . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198 22.2 Déduction naturelle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202 22.3 Calcul des séquents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202 Correction et complétude du calcul des séquents . . . . . . . . . . . . . . . . . . . 203 Élimination des coupures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205 22.4 Résolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210 Problème d’unification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210 Calcul par résolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215 Langage Prolog . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217 22.5 Méthode des tableaux . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
23 Logique équationnelle et réécriture 219 23.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219 23.2 Logique équationnelle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219 Syntaxe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219 Sémantique . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226 Le calcul équationnel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229 23.3 Réécriture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237 Systèmes de réécriture et leurs propriétés . . . . . . . . . . . . . . . . . . . . . . . 237 Décider de la confluence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242 Complétion de Knuth-Bendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
24 Logique de Hoare 249 24.1 Correction totale et partielle d’un algorithme . . . . . . . . . . . . . . . . . . . . 249
5


Terminaison des algorithmes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249 Correction partielle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252 24.2 Logique de Hoare : définition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253 Langage While . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254 Sémantique . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255 Calcul de Hoare . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255 Calcul de précondition la plus faible . . . . . . . . . . . . . . . . . . . . . . . . . 257
Sous partie 6 Logique modale 261
25 Introduction 263
26 Logique modale : syntaxe et sémantique 265 26.1 Syntaxe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265 26.2 Sémantique : modèle et satisfaction . . . . . . . . . . . . . . . . . . . . . . . . . . 265 26.3 Propriété de modèle fini et décidabilité . . . . . . . . . . . . . . . . . . . . . . . . 268 Propriété de modèle fini . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268 Décidabilté de la validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270 26.4 Traduire la logique modale dans la logique des prédicats . . . . . . . . . . . . . . 271 26.5 Bissimulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272 Définition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273 Logique modale et bissimulation . . . . . . . . . . . . . . . . . . . . . . . . . . . 276
27 Le μ-calcul 279 27.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279 27.2 Syntaxe et sémantique . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280 27.3 Quelques exemples de formules à point fixe . . . . . . . . . . . . . . . . . . . . . 282
28 Calculs pour la logique modale 285 28.1 Calcul à la Hilbert . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285 28.2 Calcul des séquents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290 28.3 Méthode des tableaux . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291
29 Application : raisonnement spatial qualitatif 293 29.1 RCC-8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293 29.2 Le modèle 9-intersection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294 Sémantique topologique . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295 Formalisation du modèle 9-intersection . . . . . . . . . . . . . . . . . . . . . . . . 296
6


Première partie .
Mathématiques discrètes
7




1. Introduction
Comme toute discipline scientifique, l’informatique fait appel aux mathématiques pour formaliser et modéliser les objets qu’elle manipule tels que les machines, programmes et systèmes. À l’inverse des sciences de la nature (physique, mécanique, thermique, etc.), les mathématiques que l’informatique utilise ne sont pas exactement les mêmes. Elles se font "discrètes". Le sens de l’adjectif "discret" est sans rapport avec son sens courant. Il signifie la présence d’objets énumérables, c’est-à-dire que l’on peut construire par des procédés récurrents, les seules choses que savent manipuler les ordinateurs. Nous allons donc aborder dans cette partie les principes fondamentaux et les outils formels (i.e. mathématiquement fondés) à la base de toutes les méthodes de conception et d’implantation des systèmes informatiques. Il sera alors abordé dans cette partie les notions fondamentales aux fondements : — de l’induction et la récurrence (théorie des treillis, ensemble bien-fondé et équivalence avec l’induction mathématique). L’objectif est de formaliser les notions fondamentales d’induction et récursivité sous-jacentes à toutes les mathématiques discrètes. — de l’algorithmique (fonctions récursives de Godel/Herbrand, machine de Turing et tous les théorèmes montrant l’équivalence de ces modèles de calcul ainsi que les résultats d’indécidabilité associés). L’idée est de définir formellement (i.e. mathématiquement) ce qu’est un problème de décision et donner une dénotation formelle à la notion d’algorithme (thèse de Church). — de la théorie de la complexité (classe de complexité P et NP, et structuration de la classe NP - problèmes NP-complets et NP-durs). — de la conception des systèmes (langages rationnels et algébriques, automates et automates à pile). L’objectif est d’étudier un outil formel à la base de toutes les méthodes de modélisation des systèmes informatiques, de l’étude des langues naturelles ou encore de la compilation. Cette partie est composé de trois parties qui sont les suivantes :
1. Fondements de l’induction et la récursivité. On y abordera plus particulièrement les notions de :
— Ordre et préordre ; — Majorants et minorants ; — Ensembles bien fondés et induction ; — Treillis et points fixes (treillis complets et fonctions continues, points fixes et fonctions monotones) avec une application à la sémantique des langages de programmation.
2. Fondements de l’algorithmique (Fonctions calculables et problèmes décidables, et théorie de la complexité). On verra plus précisément :
— Les fonctions récursives (FR) : Fonctions primitives récursives, Fonctions récursives (Problème de la fonction d’Ackermann), définition d’un problème décidable, indécidabilité du problème de l’arrêt, Fonction récursive universelle (interpréteur) ; — Le calcul pas à pas (autre modèle de calcul) : Les machines de Turing (MT), théorèmes d’équivalence (FR ≡ MT), thèse de Church ;
9


— Théorie de la complexité (complexité en temps, classes P et NP, structuration de la classe NP - problèmes NP-complets et NP-durs, un premier problème NP-complet).
3. Langages rationnels et algébriques, automates et automates à pile. On abordera dans cette dernière partie les notions suivantes :
— Monoide libre ; — Langages rationnels (lemme du pompage), et caractérisation par automates finis ; — Opérations sur les automates finis (complétion, détermination, minimalité, opérations algébriques standards) ; — Langages algébriques (lemme de pompage) et caractérisation par automates à pile ; — Automates à pile déterministes et propriétés.
10


Sous partie 1
Théorie de l’ordre et treillis
11




2. Introduction
Il y a un type de relations qui jouent un rôle très important en informatique (mais aussi en mathématiques) : Les relations d’ordre. La raison en est simple : les relations d’ordre sont fortement liées à la notion d’induction (terme générique regroupant à la fois la définition inductive d’un ensemble et la preuve par induction) qui comme nous le verrons dans la seconde partie de ce document fonde la notion de calculabilité. En effet, le fait d’utiliser l’induction est fortement lié à une propriété particulière des relations d’ordre : ordres bien fondés. De plus, sur les relations d’ordre, on peut définir une structure algébrique, les treillis, qui ont montré leur importance dans la formalisation de l’induction. En effet, les treillis permettent de définir des endofonctions F (i.e. des fonctions ayant pour domaine et co-domaine le même treillis) possédant des points fixes (i.e. les solutions satisfaisant une équation de la forme F(x) = x). Quand ils existent, l’ensemble des points fixes de F peut même être ordonné et ainsi dénoter le plus petit (aussi le plus grand) point fixe dont on montrera encore le lien avec l’induction.
Dans cette sous-partie, nous allons formaliser cette notion importante d’induction, i.e. de définition inductive et de preuve par induction. Toutes ces notions bien que supposées "innées" chez la plupart des scientifiques (sans doute à tort), sont rarement (voire jamais) exposées de manière détaillée dans la plupart des livres. Nous profitons de ce cours pour les présenter formellement et permettre ainsi d’appréhender la construction récursive d’objets finis (structures de données, langages des mots finis reconnus par un automate, ensemble des chemins finis dans un graphe - plus petit point fixe) et infinis (flots de données, langages des mots infinis reconnus par un automate, ensemble des chemins infinis dans un graphe - plus grand point fixe) que l’on a l’habitude de manipuler en informatique. Nous commençerons alors par rappeler quelques notions mathématiques de base sur les relations d’ordre. Puis, nous définirons au-dessus de ces relations d’ordre la structure algébrique de treillis ainsi que les notions de complétude et de fonctions continues à partir desquels nous obtiendrons deux théorèmes de point fixe. Sur ces théorèmes de point fixe, nous aborderons les deux notions fondamentales de définition inductive et de preuve par induction. Pour illustrer les treillis, nous aborderons aussi la sémantique des langages de programmation. Nous verrons là encore, l’intérêt des théorèmes de point fixe pour dénoter le comportement des boucles et programmes récursifs, ce qui renforcera encore le lien entre récursivité et calculabilité que nous détaillerons dans la seconde sous-partie de ce partie sur les mathématiques discrètes.
13




3. Relations d’ordre, ensembles ordonnés,
ensembles bien fondés
3.1. Relations d’ordre et ensembles ordonnés
Une relation binaire r sur un ensemble E est un sous-ensemble de E × E. Dans la suite, on utilisera la notation infixe pour les relations binaires : pour tout (x, y) ∈ r, on notera plus simplement x r y. De plus, x r y sera appelée une séquence de réduction. Étant donnée une relation binaire r, son inverse, notée r−1, est la relation binaire sur E définie par : ∀x ∈ E, ∀y ∈ E, x r y ⇔ y r−1 x.
De même, on note r0 et r+ les fermetures réflexive et transitive de r. Ainsi, r0 =
r ∪ {(x, x)|x ∈ E}, et r+ =
⋃
i∈N
ri où :
r0 = r ∀i, i ≥ 1, ri = ri−1 ∪ {(x, z)|∃y ∈ E, (x, y) ∈ ri−1, (y, z) ∈ ri−1}.
On note alors r∗ = r0 ∪ r+.
Définition 1 (Relation binaire).
Parmi, les relations binaires, nous sommes principalement intéressés par celles définissant un ordre sur E.
Soit E un ensemble. Une relation d’ordre sur E est une relation binaire sur E satisfaisant aux conditions suivantes : pour tout e, e′, e′′ ∈ E — réflexivité : e e — anti-symétrie : si e e′ et e′ e alors e = e′ — transitivité : si e e′ et e′ e′′ alors e e′′
est dite relation d’ordre strict, et dans ce cas-là est notée ≺, si, et seulement si elle est irréflexive (i.e. ∀x ∈ E, non(x ≺ x)) et transitive, et elle est dite totale si elle satisfait la condition suivante :
∀(a, b) ∈ E × E, a b ou b a
L’ordre est dit partiel, s’il n’est pas total. On note (resp. ) la relation inverse de (resp. ≺).
Définition 2 (Relation d’ordre).
15


— L’ordre habituel sur les réels est un ordre total. — La relation de divisibilité ≤div sur les entiers naturels définie par :
∀a, b ∈ N, a ≤div b ⇐⇒ ∃c ∈ N, b = a.c
est un ordre partiel. — La relation d’inclusion est un ordre partiel sur P(E) a si |E| > 1, et un ordre total si |E| ≤ 1.
a. P(E) est l’ensemble des parties d’un ensemble E, et |E| désigne la cardinalité de l’ensemble E.
Exemple 1.
Un ensemble ordonné (E, ) est un ensemble E muni d’une relation d’ordre .
Définition 3 (Ensemble ordonné).
Un même ensemble peut être muni de plusieurs relations d’ordre. Par exemple, sur N, on a la relation d’ordre usuelle ≤ ainsi que l’ordre de divisibilité défini plus haut.
Soient (E1, 1) et (E2, 2) deux ensembles ordonnés. Une application f : E1 → E2 est dite monotone si, et seulement si :
∀x, y ∈ E1, x 1 y =⇒ f (x) 2 f (y)
On dira de plus que f est un mono (resp. épi)morphisme si f est injective (resp. surjective). Enfin, f est un isomorphisme si f est une bijection et f −1 est aussi monotone.
Définition 4 (Applications monotones).
L’application identité de (N, ≤div) dans (N, ≤) est monotone.
Exemple 2.
Les applications monotones définissent ainsi les homomorphismes entre les ensembles ordonnés.
Il ne suffit pas qu’une bijection entre deux ensembles ordonnés soit monotone pour être un isomorphisme. En effet, l’application identité de (N, ≤div) dans (N, ≤) est une bijection monotone, mais n’est pas un isomorphisme. La raison vient du fait que ≤ est totale tandis que ≤div est partielle.
Remarque 3.
16


3.2. Éléments remarquables : chaînes et antichaînes, majorants
et minorants, et ensembles inductifs
Soit (E, ) un ensemble ordonné. Un sous-ensemble ordonné de (E, ) est un ensemble ordonné (E′, ′) tel que E′ ⊆ E et ′= ∩(E′ × E′).
Définition 5 (Sous-ensemble ordonné).
Soit (E, ) un ensemble ordonné. — On appelle chaîne de (E, ) tout sous-ensemble totalement ordonné (X, X ) de (E, ).
— On appelle antichaîne de (E, ) tout sous-ensemble X de E tel que (X × X)∩ = IdX où IdX est la relation identité sur X (i.e. (∀x, y ∈ X, x y ⇒ x = y).
Si (X, X ) est une chaîne (resp. X est une antichaîne), nous dirons que (X, X ) (resp. X) est maximale si pour toute chaîne (Y, Y ) (resp. antichaîne Y ) de (E, ), on a : X ⊂Y ⇒X =Y.
Définition 6 (Chaîne et antichaîne).
Ainsi, dans une antichaîne, deux éléments quelconques sont incomparables.
Soit E = P({a, b, c}) muni de l’inclusion. — X = {{a}, {a, b}, {a, b, c}} est une chaîne non maximale. — X = {{a}, {b, c}} est une antichaîne. — X = {∅, {a}, {a, b}, {a, b, c}} est une chaîne maximale. — X = {{a}, {b}, {c}} est une antichaîne maximale. — X = {{a}} est à la fois une chaîne et une antichaîne non maximale.
Exemple 4.
17


Soient (E, ) un ensemble ordonné, X ⊆ E, et α ∈ E. On dit que α est : — un majorant (resp. minorant) de X si :
∀x ∈ X, x α (resp. α x)
Notons M aj(X) (resp. M in(X)) l’ensemble des majorants (resp. des minorants) de X.
— un élément maximal ou plus grand élément (resp. minimal ou plus petit élément) de X, si α ∈ X et :
∀x ∈ X, α x ⇒ α = x (resp. x α ⇒ α = x)
— une borne supérieure (resp. inférieure) de X s’il est le plus petit des majorants (resp. le plus grand des minorants), i.e. si m est un majorant (resp. un minorant) de X, alors on a α m (resp. m α). Si X admet une borne supérieure (resp. inférieure), cette dernière est unique et on la note Sup(X) (resp. Inf (X)).
Définition 7 (Majorant, minorant, plus grand et plus petit élément).
Soit (E, ) un ensemble ordonné. Soit X ⊆ E. M aj(X) ∩ X et M in(X) ∩ X ont au plus un élément.
Proposition 5.
Démonstration : Supposons que M aj(X) ∩ X a deux éléments x et y. Par définition, on a alors x y et y x, et donc par anti-symétrie, x = y. La démonstration est analogue pour M in.
Si M aj(X) ∩ X n’est pas vide, l’unique élément est appelé maximum de X. De même, si M in(X) ∩ X n’est pas vide, l’unique élément est appelé minimum de X.
Tout ordre totalement ordonné, quand il possède un élément maximal (resp. minimal), ce dernier est unique, et donc est maximum (resp. minimum). Comme simple conséquence de l’anti-symétrie des relations d’ordre, si un sous-ensemble X de (E, ) admet un plus grand ou un plus petit élément, il est unique. De même si α est une borne supérieure (resp. inférieure), elle est unique.
Remarque 6.
Un ensemble ordonné (E, ) est dit inductif si chacune de ses chaînes admet un majorant.
Définition 8 (Ensemble inductif).
18


Les ensembles inductifs ne doivent pas être confondus avec les ensembles définis de façon explicite inductivement et dont on verra les fondements mathématiques dans le chapître sur les treillis. Néanmoins, un lien unit tout ça, l’induction mathématique. Nous verrons alors que pour les ensembles définis inductivement, ce principe de preuve peut être explicitement appliqué, tandis que dans le cas des ensembles inductifs, on peut contourner ce principe pour démontrer une propriété sur tous les éléments de l’ensemble, en utilisant un lemme puissant de la théorie des ensembles, le lemme de Zorn (cf. Section 3.3).
3.3. Ensembles bien fondés et induction mathématique
Ensembles bien fondés
Certaines relations d’ordre possèdent une propriété caractéristique, celle d’être bien-fondée ou encore Nothérienne. L’intérêt de telles relations est qu’elles permettent de raisonner par induction généralisée sur les éléments de l’ensemble.
Soit E un ensemble. Une relation d’ordre sur E est dite bien-fondée si ,et seulement si toute suite (xi)i∈N d’éléments de E décroissante selon (c-à-d., xi+1 xi) est stationnaire, i.e. il existe un j ∈ N tel que pour tout i > j, nous avons xi = xj. Un ensemble E muni d’une relation d’ordre bien fondée est dit bien fondé. est un bon ordre si il est total et bien fondé.
Définition 9 (Ordre bien-fondé et bon ordre).
Une définition équivalente à celle ci-dessus peut aussi être trouvée dans les livres traitant de ce sujet comme l’établit le résultat suivant :
(E, ) est bien fondé si, et seulement si tout sous-ensemble non vide de E admet un élément minimal vis-à-vis de .
Proposition 7.
Démonstration : Pour les deux sens de l’équivalence, la preuve se fait par l’absurde.
(⇒) Hypothèse : (E, ) est bien fondé et S est un sous-ensemble non vide de E qui n’admet pas d’élément minimal. Soit x0 ∈ S un élément quelconque (cet élément existe car S 6= ∅). Par hypothèse, il existe x1 ≺ x0. On peut recommencer sur x1 et construire par récurrence une suite décroissante (xi)i∈N à partir des éléments de S. Puisque S n’admet pas d’élément minimal, cette suite est infinie et non stationnaire, ce qui contredit l’hypothèse de E d’être bien fondé.
Soit (⇐) Hypothèse : Tout sous-ensemble non vide S de E admet un élément minimal, et il existe
une suite (xi)i∈N décroissante selon l’ordre non stationnaire. Par définition, l’ensemble des xi de la suite est un sous-ensemble de E. Il admet donc un élément minimal ce qui contredit le fait que la suite n’est pas stationnaire.
19


— L’ordre naturel est un bon ordre sur N mais pas sur Z. — L’ordre lexicographique sur N × N est défini par (n, m) (n′, m′) si et seulement si : (n < n′) ou (n = n′ et m ≤ m′). On remarque que si n > 0 alors (n, m) a une infinité de minorants (tous les éléments (i, p) avec 0 ≤ i ≤ n − 1 et p ∈ N). Néanmoins, l’ordre lexicographique est un bon ordre sur N × N. En effet, soit X un sous-ensemble non vide de N × N. Soit n = min{p|∃q ∈ N, (p, q) ∈ X} et m = min{q|(n, q) ∈ X}. On vérifie aisément que (n, m) est le plus petit élément de X. — Soit (A, ) un ensemble ordonné. Notons A∗ l’ensemble de toutes les suites finies sur A, i.e., A∗ = {(a1, . . . , an)|n ∈ N, ∀i, 1 ≤ i ≤ n, ai ∈ A}. A∗ est appelé le monoide libre sur A et sera étudié plus en détail dans la troisième partie de ce document. Pour simplifier les notations, une suite (a1, . . . , an) de A∗ sera notée a1 . . . an.
Là encore, on peut définir l’ordre lexicographique lex sur A∗ de la façon suivante : soient l = a1 . . . an et l′ = b1 . . . bp deux mots de A∗. l lex l′ si, et seulement si pour r = min{n, p}, il existe i ≤ r tel que ai ≺ bi et pour tout j < i, aj = bj. lex n’est pas bien fondé. En effet, considérons l’alphabet {a, b} avec a b, et considérons la suite (an.b)n∈N où an = a . . . a
} {{ }
n fois
. Cette suite est selon
l’ordre lexicographique décroissante, et pourtant elle n’est pas stationnaire.
Exemple 8.
Les bons ordres ont une propriété remarquable qui signifie que l’on peut ordonner à isomorphisme près et selon la relation "être un début de" tous les bons ordres, où la notion d’"être un début de" est définie par :
Soit (E, ) un ensemble ordonné. Soit X une chaîne de E. On dit qu’un sous-ensemble strict I de X est un début de X si tous les éléments de X \I sont supérieurs strictement à tous les éléments de I.
Définition 10.
Un ensemble ordonné (E, ) tel que est un bon ordre ne peut pas être mis en correspondance par un homomorphisme (i.e. une fonction monotone) avec un de ses débuts.
Lemme 9.
Démonstration : Soit X un début non vide. Supposons un homomorphisme f : E → X. Par définition des bons ordres, E \ X possède un plus petit élément e, et donc X = {x ∈ E|x ≺ e}. On a alors f (e) ≺ e. Posons alors Y = {x ∈ E|f (x) ≺ x}. Y est non vide, il contient au moins e. Y possède alors un élément minimal m. Puisque que m ∈ Y , on a f (m) ≺ m. f étant un homomorphisme, on a alors f (f (m)) ≺ f (m). Maintenant, f (m)6∈Y puisque m est minimal dans Y . Donc, par définition
20


de Y , on a f (f (m)) f (m) contredisant ce qui précède. C’est donc que Y est vide, ce qui interdit que f soit monotone.
A fortiori, (E, ) ne peut pas être isomorphe à un de ses débuts. La propriété en question s’énonce ainsi :
Soient (E, ) et (E′, ′) deux ensembles bien ordonnés tels que et ′ sont des bons ordres. Alors, soit E et E′ sont isomorphes, soit l’un est début de l’autre à isomorphisme près.
Proposition 10.
Démonstration : Définissons la correspondance f de E dans E′ par :
f (e) = e′ ssi {x ∈ E|x ≺ e} est isomorphe à {x′ ∈ E′|x′ ≺′ e′}
Tout d’abord montrons que f est une fonctionnelle, i.e. que pour tout e ∈ E quand f (e) est définie, alors elle est unique. Supposons alors qu’il existe e′1, e′2 ∈ E′ tels que f (e) = e′1 et f (e) = e′2. Par
définition, ceci veut dire que l’ensemble {x ∈ E|x ≺ e} est à la fois isomorphe à {x′ ∈ E′|x′ ≺′ e′1}
et à {x′ ∈ E′|x′ ≺′ e′2}, et donc {x′ ∈ E′|x′ ≺′ e′1} et {x′ ∈ E′|x′ ≺′ e′2} sont aussi isomorphes.
Maintenant, ces deux ensembles sont des sous-ensembles de E′ totalement ordonnés par ′. Ainsi, tous les éléments de {x′ ∈ E′|x′ ≺′ e′1} ∪ {x′ ∈ E′|x′ ≺′ e′2} sont comparables entre eux. Supposons
alors que e′1 ≺′ e′2, ceci veut dire que e′1 ∈ {x′ ∈ E′|x′ ≺′ e′2}, ce qui implique que {x′ ∈ E′|x′ ≺′ e′1}
est un début de {x′ ∈ E′|x′ ≺ e′2}. Or par le lemme 9, on en déduit une contradiction. On peut
faire le même raisonnement et arriver à la même conclusion si l’on suppose que e′2 ≺′ e′1.
Montrons maintenant que f est injective, i.e. pour tout e′ ∈ E′ s’il existe e ∈ E tel que f (e) = e′ alors e est l’unique élément satisfaisant cette équation. Supposons deux éléments e1, e2 ∈ E tels que f (e1) = f (e2) = e′. Par définition, ceci veut dire que les deux ensembles {x ∈ E|x ≺ e1} et {x ∈ E|x ≺ e2} sont isomorphes à {x′ ∈ E′|x′ ≺′ e′}, et donc {x ∈ E|x ≺ e1} et {x ∈ E|x ≺ e2} sont aussi isomorphes. En appliquant, le même raisonnement que précédemment, on démontre alors que e1 et e2 sont nécessairement égaux. Il nous reste alors à montrer que f est strictement croissante ce qui entrainera qu’elle est un isomorphisme de son domaine de définition vers son image. Soient e1, e2 ∈ E tels que e1 ≺ e2, et tels que f (e1) = e′1 et f (e2) = e′2. Par définition, il existe un isomorphisme g de {x ∈ E|x ≺ e2} dans
{x′ ∈ E′|x′ ≺′ e′2}. Par hypothèse, e1 ∈ {x ∈ E|x ≺ e2}, et donc on a g(e1) ≺′ e′2. La restriction de
g à {x ∈ E|x e1} dans {x ∈ E′|x ≺′ g(e1)} est encore un isomorphisme. Ainsi, on a par définition que f (e1) = g(e1) d’où l’on peut conclure directement que e′1 ≺′ e′2.
À partir de là, 4 cas sont à considérer :
1. Dom(f ) = E et Im(f ) = E′, (E, ) et (E′, ′) sont isomorphes.
2. Dom(f ) = E et Im(f ) est un début de E′, (E, ) est isomorphe à un début de (E′, ′).
3. Dom(f ) est un début de E et Im(f ) = E′, (E′, ′) est isomorphe à un début de (E, ).
4. Dom(f ) et Im(f ) sont des débuts de E et E′, respectivement. On montre que ce cas n’est pas possible. Par hypothèse, ceci veut dire qu’il existe e ∈ E et e′ ∈ E′ tels que Dom(f ) = {x ∈ E|x ≺ e} et Im(f ) = {x′ ∈ E′|x′ ≺′ e′}. Dom(f ) et Im(f ) étant isomorphe, par définition on a f (e) = e′ et donc e ∈ Dom(f ) et e′ ∈ Im(f ), ce qui est une contradiction.
La preuve ci-dessus peut laisser penser que la proposition 10 est aussi correcte quand (E, ) et (E′, ′) sont des ensembles totalement ordonnés mais dont les ordres ne sont pas forcément bons. Le problème est que dans ce cas-là, nous ne pourrions pas appliquer le lemme 9 que nous utilisons pour montrer que la correspondance f définie dans la preuve est une fonction injective.
21


En fait, dans le cas où (E, ) et (E′, ′) sont des ensembles totalement ordonnés mais dont les ordres ne sont pas forcément bons, on ne pourrait rien conclure. Par exemple, si on considère (Z, ≤) et (Q+, ≤), le domaine et l’image de la correspondance f sont vides ; aucun début de (Z, ≤) n’est isomorphe à un début de (Q+, ≤).
Induction mathématique
L’intérêt des ensembles bien-fondés est qu’ils sont intimement liés à la notion d’induction mathématique. En effet, l’induction mathématique sur un ensemble ordonné (E, ) est définie par l’équivalence : Soit P une propriété portant sur les éléments de E,
∀e ∈ E, P (e) ⇐⇒ [∀e ∈ E, (∀e′ ∈ E, e′ ≺ e ⇒ P (e′)) ⇒ P (e)]
On peut être intrigué par le fait que ce principe d’induction mathématique ne comporte pas de cas de base. En fait, celui-ci est caché dans la partie droite de l’équivalence. En effet, soit e un élément minimal de E. Si on a vérifié la partie droite de l’équivalence ci-dessus, on a entre autres vérifié
(∀e′ ∈ E, e′ ≺ e ⇒ P (e′)) ⇒ P (e)
Or, P (e) est vraie si ∀e′ ∈ E, e′ ≺ e ⇒ P (e′) est vraie. Mais il n’y a pas d’élément e′ ∈ E tel que e′ ≺ e quand e est minimal dans E pour ≺, et donc ∀e′ ∈ E, e′ ≺ e ⇒ P (e′) est toujours vraie, et donc P (e) l’est aussi. Bien entendu, l’induction mathématique n’est pas valide pour tout les ensembles ordonnés. Dans le cas contraire, ceci voudrait dire que nous pourrions faire de la preuve par récurrence sur l’ensemble R muni de la relation d’ordre classique ≤. En fait, seuls les ensembles bien-fondés admettent l’induction mathématique comme l’établit le théorème suivant :
(E, ) est bien fondé si, et seulement s’il admet l’induction mathématique comme valide.
Théorème 11.
Démonstration : Pour un ensemble bien fondé (E, ) l’équivalence suivante est satisfaite pour tout sous-ensemble S de E :
S 6= ∅ ⇐⇒ (∃e ∈ E, (e ∈ S) et (∀e′ ∈ E, e′ ≺ e ⇒ e′6∈ S))
Soit P une propriété sur E. Par définition, P dénote un sous-ensemble de E. Notons alors S le sous-ensemble de E tel que S = {e | nonP (e)}. Par définition, si P admet une preuve par induction, nous avons alors S = ∅. Par une simple application des équivalences logiques suivantes : — ∃x.P (x) ⇔ non(∀x.nonP (x))
— non(A et B) ⇔ nonA ou nonB — (A ⇒ B) ⇔ (nonA ou B) les équivalences suivantes sont donc satisfaites :
S 6= ∅ ⇐⇒ (∃e ∈ E, (e ∈ S) et (∀e′ ∈ E, e′ ≺ e ⇒ e′6∈ S)) m
S = ∅ ⇐⇒ (∀e ∈ E, (e6∈S) ou non(∀e′ ∈ E, e′ ≺ e ⇒ e′6∈ S)) m
∀e ∈ E, P (e) ⇐⇒ (∀e ∈ E, P (e) ou non(∀e′ ∈ E, e′ ≺ e ⇒ P (e′))) m
∀e ∈ E, P (e) ⇐⇒ (∀e ∈ E, (∀e′ ∈ E, e′ ≺ e ⇒ P (e′)) ⇒ P (e))
22


Le théorème ci-dessus généralise la récurrence usuelle sur les entiers naturels à tout ensemble bien-fondé. En effet, rappelons le principe de preuve par induction sur les entiers naturels muni de l’ordre standard ≤. Ce principe peut s’énoncer de deux façons. La première connue sous le nom de principe de récurrence mathématique s’énonce de la façon suivante : Soit P (n) une propriété dont l’énoncé dépend de n ∈ N, si
(B) P (0) est vraie.
(I) ∀n ∈ N, (P (n) ⇒ P (n + 1))
alors : ∀n ∈ N, P (n) est vraie.
Parfois, dans des cas plus complexes où pour établir P (n + 1) on ait besoin d’utiliser explicitement le fait que P soit vraie aux étapes 0, 1, . . . , n−1, n, on utilise le second principe d’induction dont on reconnaitra aisément une application directe du principe d’induction mathématique généralisé énoncé ci-dessus. Il s’énonce de la façon suivante : Soit P (n) une propriété dont l’énoncé dépend de n ∈ N, si
(I’) ∀n ∈ N, ((∀k < n, P (k)) ⇒ P (n))
alors, ∀n ∈ N, P (n) est vraie. On verra dans la suite de ce chapître que l’induction mathématique généralisée s’applique aussi à un autre principe de preuve très important en informatique, l’induction structurelle (cf. chapitre 4).
Induction sans ordre bien fondé : lemme de Zorn et ses avatars
Comme nous venons de le voir, l’induction mathématique ne marche que pour les ensembles bien fondés. Mais quel type de preuve utilisons-nous pour démontrer une propriété sur tous les éléments d’un ensemble qui n’est pas bien fondé ? On dispose alors d’un lemme puissant, le lemme de Zorn dont on verra qu’il est équivalent à un autre lemme de la théorie des ensembles, le théorème de Zermelo qui justement énonce l’existence d’un bon ordre pour tout ensemble si l’on accepte l’axiome du choix. Ainsi, on peut a priori faire de la récurrence sur tous les ensembles. Le problème est que le théorème de Zermelo énonce l’existence de ce bon ordre mais ne dit pas comment le construire. C’est pourquoi, l’axiome du choix, le lemme de Zorn ou encore le théorème de Zermelo (tous les trois équivalents en fait) sont utilisés dans les preuves non constructives, i.e. on prouve l’existence d’un ensemble vérifiant certaines propriétés sans pour autant caractériser aucun ensemble vérifiant cette propriété, et ainsi contourner le manque d’induction explicite.
Énonçons ce résultat 1.
Tout ensemble inductif (E, ) admet un élément maximal.
Théorème 12 (Lemme de Zorn).
Démonstration : Commençons d’abord par introduire quelques notions que nous utiliserons par la suite pour démontrer ce résultat.
1. La preuve de ce lemme est très fortement inspirée de celle que l’on peut trouver dans le document de Rémi Peyre sur le lemme de Zorn.
23


Soit (E, ) un ensemble ordonné. Nous dirons qu’une chaîne X de E est ultime si elle n’admet aucun majorant strict (i.e. s’il n’y a aucun α ∈ E \ X pour lequel ∀x ∈ X, x ≺ α), et continuable dans le cas contraire.
Définition 11 (Chaîne ultime et continuable).
On observe alors qu’une chaîne ultime X d’un ensemble inductif E a nécessairement un plus grand élément, qui est maximal dans E. En effet, une chaîne ultime est une chaîne dont on a poussé la progression par la relation ≺ jusqu’à ne plus pouvoir continuer. Là, la chaîne X peut se terminer de deux façons a priori
1. il y a en fin de chaîne une infinité d’éléments tous plus grands les uns que les autres,
2. il y a un élément de la chaîne qui est le dernier.
Le premier cas est impossible. En effet, s’il se produisait, la chaîne ayant un majorant par définition des ensembles inductifs, nous pourrions toujours l’ajouter à la fin de la chaîne ce qui contredirait l’ultimité de celle-ci. Donc, X a un plus grand élément m ∈ X. Ce dernier est alors nécessairement maximal dans E. En effet, si ce n’était pas le cas, par définition des ensembles inductifs, il existerait x ∈ E tel que m x ce qui contredirait encore l’ultimité de X car l’on pourrait une nouvelle fois ajouter ce x à la fin de la chaîne X. m est donc l’élément maximal de E.
La démonstration du lemme de Zorn se ramène alors à démontrer que tout ensemble ordonné admet une chaîne ultime. En effet, si E est de plus inductif, on vient de démontrer ci-dessus que E a alors nécessairement un élément maximal. Ce dernier énoncé est connu sous le nom du Principe de maximalité de Hausdorff.
Pour commençer, on choisit pour toute chaîne continuable X de E un majorant strict noté m(X). 2 Commençons d’abord par définir la notion de bonne chaîne qui nous sera utile dans la suite de la preuve.
Soit (E, ) un ensemble ordonné. Soit X une chaîne de E. X est dite être une bonne chaîne quand, pour tout début I de X, m(I) appartient à X et est le plus petit élément de X \ I.
Définition 12 (Bonne chaîne).
Montrons alors pour toutes bonnes chaînes distinctes X1 et X2 de E que l’une est un début de l’autre 3. Notons X∗ l’ensemble de tous les x ∈ X1 ∩ X2 tel que l’ensemble {y ∈ X1|y ≺ x} est un début de X2. 4 Cet ensemble n’est pas vide. En effet, la chaîne vide ∅ est début de toute bonne chaîne et donc de X1 et X2. Par définition, on a alors que m(∅) ∈ X1 ∩ X2. X∗ peut éventuellement coincider avec X1 ou X2 mais pas les deux car X1 et X2 sont distinctes. Supposons alors que X∗ ⊂ X1 (inclusion stricte). Dans ce cas-là, X∗ est un début de X1. En effet, soit x ∈ X∗, et soit y ∈ X1 \ X∗. Si y ≺ x (par définition, y ∈ X2 alors), alors y ∈ X1 ∩ X2 et donc l’ensemble {y′ ∈ X1|y′ ≺ y} est aussi un début de X2, d’où nous pouvons déduire que y ∈ X∗ ce qui est une contradiction. Ainsi, on a nécessairement que x ≺ y. Montrons alors que X∗ = X2 ce qui concluera cette première partie de preuve. Si X∗ était strictement incluse dans X2, pour les
2. En fait, ce choix est possible par l’axiome du choix de la théorie des ensembles et dont l’énoncé dit que pour toute famille d’ensembles (Xi)i∈I non vides, il existe une fonction de choix c = I →
⋃
i∈I
Xi telle que pour
tout i ∈ I, c(i) ∈ Xi. Cet axiome s’utilise en mathématique quand on ne sait pas comment choisir un élément particulier dans des ensembles (e.g. le plus petit élément d’une classe d’équivalence) ou que l’on veuille encore effectuer une infinité de choix simultanément. Dans notre cas, I est l’ensemble de toutes les chaînes continuables et pour i ∈ I, Xi est l’ensemble des majorants stricts de la chaîne continuable i. 3. On retrouve la proposition 10 mais ici étendue aux bonnes chaînes. 4. On aurait pu de façon équivalente choisir l’ensemble {y ∈ X2|y ≺ x} comme début de X1.
24


mêmes raisons que précédemment elle en serait un début. Comme les chaînes X1 et X2 sont bonnes, m(X∗) serait alors dans chacune des chaînes Xi pour i = 1, 2, et serait par définition le plus petit élément de Xi \ X∗. Ainsi, l’ensemble {y ∈ X1|y ≺ m(X∗} est un début de X2, et donc, d’après la définition de X∗, on aurait m(X∗) ∈ X∗ ce qui est une contradiction. Notons alors X ⊆ E l’ensemble défini comme la réunion de toutes les bonnes chaînes. Montrons alors que X est une bonne chaîne. Soient x et y deux éléments de X. Par définition, il existe deux bonnes chaînes Xi et Xj telles que x ∈ Xi et y ∈ Xj. L’on sait que soient Xi et Xj coincident ou bien l’une est début de l’autre, mais dans les deux cas une est incluse dans l’autre. Supposons alors que Xi ⊆ Xj. x et y sont alors tous les deux éléments de Xj et donc sont comparables par . Ainsi, X est une chaîne. Montrons qu’elle est en plus une bonne chaîne. Soit I un début de X. Par définition, tous les points x de I appartiennent à des bonnes chaînes Xx, et donc pour tous points x 6= y ∈ I, Xx ou Xy est début l’une de l’autre. Ainsi, x et y sont comparables et donc I est une chaîne. I est de plus une bonne chaîne. En effet, soit Xm(I) la bonne chaîne contenant m(I). Par hypothèse, pour tout x ∈ I, la bonne chaîne Xx est un début de Xm(I), et donc I est aussi un début de Xm(I). Soit I′ un début de I. I′ est aussi un début de Xm(I), et donc m(I′) est le plus petit élément de Xm(I) \ I′. Mais I \ I′ ⊂ Xm(I) \ I′ d’où nous pouvons déduire que m(I′) est aussi le plus petit élément de I \ I′. I étant continuable, I ∪ {m(I)} est encore une bonne chaîne, d’où nous déduisons que m(I) ∈ X. Montrons que c’est le plus petit élément de X \ I. Considérons l’ensemble J = {x ∈ X|x ≺ m(I)}. Par le fait que tout point de X appartient à une bonne chaîne, et que les bonnes chaînes sont ordonnées par la relation "être un début de", J est un début d’une bonne chaîne et donc un début de X. En suivant le même processus de preuve que pour I ci-dessus, on peut montrer que J est aussi une bonne chaîne dont I par définition est un début. Or, par définition, m(I)6∈J ce qui est une contradiction avec le fait pour J d’être une bonne chaîne.
Pour finir, X est ultime, sinon (i.e. m(X)6∈X), X ∪ {m(X)} serait une bonne chaîne, et donc m(X) ∈ X ce qui serait absurde.
Pour ce qui nous intéresse dans cette première partie du cours, l’intérêt du lemme Zorn est qu’il permet de montrer le théorème suivant, plus connu sous le nom de Théorème de Zermelo 5 :
Tout ensemble X peut être muni d’un bon ordre.
Théorème 13.
Démonstration : Notons S l’ensemble défini par :
S = {(Y, )|Y ⊆ X, est un bon ordre sur Y }
Par défintion, cet ensemble n’est pas vide car il contient au moins le couple (∅, ∅). Définissons sur S la relation R par :
(Y, ) R (Y ′, ′) ssi Y ⊂ Y ′ et Y est début de Y ′ avec = ′
|Y .
Il est façile de vérifier que R est une relation d’ordre strict sur S. Elle est de plus inductive car toute chaîne de (S, R) est majorée par sa réunion. Par le lemme de Zorn, notons (Y , ≤) un élément maximal de (S, R). Si Y ⊂ X, alors en prenant n’importe quel x ∈ X \ Y , on peut définir le bon ordre ≤′ sur Y ∪ {x} en gardant pour tous les éléments de Y l’ordre ≤ et en posant pour tout y ∈ Y , y ≤′ x. Mais alors, par définition, ≤′ prolonge ≤ ce qui est exclu par maximalité, et donc Y = X, et ainsi le bon ordre sur Y est un bon ordre sur X.
La démonstration que nous avons effectuée dans la preuve du théorème de Zermelo est caractéristique de la façon dont on utilise généralement le lemme de Zorn.
5. En fait, l’énoncé du théorème de Zermelo est équivalent au lemme de Zorn et donc à l’axiome du choix.
25




4. Treillis, définitions inductives et preuve
par induction structurelle
Les définitions inductives/récursives d’ensembles sont omniprésentes en informatique. Par exemple, toutes les structures de données que l’on manipule en programmation se définissent de cette façon : une liste d’éléments est soit la liste vide, soit l’ajout d’un élément dans une liste déjà construite, de même, un arbre binaire est soit l’arbre vide, soit l’ajout d’une racine à deux arbres binaires qui dénotent respectivement l’arbre gauche et l’arbre droit. L’on pourrait bien sûr définir de tels ensembles explicitement comme il est d’usage en mathématiques. Ainsi, les listes dont les éléments sont pris dans un ensemble E seraient définies par toutes les applications f : I → E où I = {0, 1, , . . . , n} est un segment de N. De la même façon, les arbres binaires pourraient être définis comme l’ensemble des graphes connexes acycliques, tel que le degré de chaque noeud soit au plus 3. Bien sûr, de telles définitions explicites ne sont pas suffisantes quand l’on veut "programmer" sur ces structures de données. Seules leurs définitions inductives par la considération de cas de base et de cas plus géneraux obtenus à partir de cas plus simples, permettent de "programmer" sur ces structures. Cette construction des objets manipulés permet de plus un type de raisonnement puissant qui fait écho à la section précédente puisqu’il est un cas particulier d’induction, le raisonnement par induction structurelle. Ici, nous allons nous intéresser à définir formellement cette notion de définition inductive à partir de théorèmes de point fixe obtenus dans le cadre de la théorie des treillis. Nous étudierons aussi une autre application des treillis : la sémantique dénotationelle des langages de programmation.
4.1. Treillis et points fixes
Définition
Les treillis sont des ensembles ordonnés avec la contrainte supplémentaire que toute paire d’éléments possède à la fois une borne supérieure et une borne inférieure.
Un ensemble ordonné (E, ) est un treillis si, et seulement si toute paire d’éléments {x, y} de E possède une borne supérieure, noté x t y, et une borne inférieure, notée x u y.
Définition 13 (Treillis).
Si E est un treillis, on peut considérer qu’il est muni de deux opérateurs binaires t : E×E → E et u : E × E → E.
27


— N muni de l’ordre de divisibilité est un treillis, les opérations binaires t et u étant respectivement le ppcm et le pgcd. — Soit E un ensemble quelconque. P(E) ordonné par inclusion est un treillis où t et u sont respectivement ∪ et ∩.
Exemple 14.
Treillis complets et fonctions continues
Un treillis (E, , t, u) est complet si tout sous-ensemble S de E admet une borne supérieure et une borne inférieure. On les note respectivement tS et uS.
Définition 14 (Treillis complet).
Soit E un ensemble quelconque. P(E) ordonné par inclusion est un treillis complet où
t{Ei|i ∈ I} =
⋃
i∈I
Ei et u{Ei|i ∈ I} =
⋂
i∈I
Ei.
Exemple 15.
Par définition, si E est un treillis complet alors la borne inférieure (resp. supérieure) de E est majorée (resp. minorée) par tous les éléments de E. Ainsi, E possède un élément minimum ⊥ et maximum >. Mais ça on pouvait s’y attendre car les treillis complets sont avant tout des ensembles inductifs, même beaucoup plus que des ensembles inductifs car tous les sous-ensembles (et pas seulement les chaînes) possèdent une borne supérieure (et pas simplement un majorant) qui est le plus petit des majorants. Donc, par application du lemme de Zorn, l’ensemble E possède un élément maximal, ici >.
Remarque 16.
Comme on le verra dans la suite de ce chapitre, les définitions inductives d’un ensemble se définissent par une équation à point fixe, i.e. une équation de la forme F (E) = E, et parmi tous les ensembles E, points fixes de l’équation, l’ensemble recherché sera le plus petit. Comme il est d’usage en analyse réelle, pour résoudre/calculer les points fixes d’une fonction f : R → R, on dispose de la méthode itérative qui consiste pour toute fonction continue f de considérer la suite :
x0 = une certaine valeur initiale xi = f (xi−1)
L’on sait que si cette suite possède une limite x, on peut montrer aisément que x est un point fixe de la fonction f . 1 C’est de ce type de méthode dont on va s’inspirer pour résoudre les équations
1. En aucun cas, on assure qu’un point fixe existe, mais quand il existe on sait alors le calculer. Il existe d’autres résultats qui assurent l’existence et l’unicité d’un tel point fixe quand l’application f est dite contractante.
28


à points fixes sur les treillis. Étant donnés un treillis complet (E, , t, u) et une application f : E → E, l’ensemble des points fixes de f est un sous-ensemble de E. Si ce sous-ensemble admet un minimum, on l’appellera le plus petit point fixe, et s’il admet un maximum, on l’appellera le plus grand point fixe. Nous avons un premier résultat qui assure l’existence d’un plus petit et d’un plus grand point fixe quand l’application f est un homomorphisme sur (E, ), i.e. elle est monotone pour l’ordre .
Soit (E, , t, u) un treillis complet. Soit f : E → E une application monotone. Alors, f possède un plus petit et un plus grand point fixe.
Théorème 17.
Démonstration : Soit X l’ensemble défini par :
X = {x ∈ E|x f (x)}
Triviallement, ⊥ ∈ X. Par définition, pour tout x ∈ X, on a x tX, et donc par monotonie, f (x) f (tX). Comme x f (x), on a x f (tX), et donc f (tX) est un majorant de X. On a alors tX f (tX). Comme f est monotone, on a aussi f (tX) f (f (tX)), et donc f (tX) ∈ X, d’où l’on peut déduire f (tX) tX. Par antisymétrie de , on conclut alors que f (tX) = tX. Ainsi, tX est un point fixe de f . Si f a un autre point fixe y. Par défintion de X, y ∈ X, et donc y tX.
On montre de même que u{x ∈ E|x f (x)} est le plus petit point fixe de f .
29


Soit G = (V, E) un graphe orienté où V est l’ensemble des noeuds du graphe et E ⊆ V × V l’ensemble des arcs. Pour tout X ⊆ V , on note P red(X) l’ensemble des prédécesseurs des sommets de X :
P red(X) = {v ∈ V |∃v′ ∈ X, (v, v′) ∈ E}
Soit X0 ⊆ V . L’application f : P(V ) → P(V ) définie par X 7→ X0 ∪ pred(X) est monotone pour l’inclusion. Elle admet donc un plus petit et un plus grand point fixe. Le plus petit point fixe de f est l’ensemble Y des sommets depuis lesquels on peut atteindre un sommet de X0 (définition inductive de la co-accessibilité : Y est le plus petit ensemble contenant X0 et tel que pred(Y ) ⊆ Y - cf. la section 4.2). Le plus grand point fixe de f est l’ensemble Z = Y ∪ I où I est l’ensemble des sommets depuis lesquels il existe dans G un chemin infini. En effet, on a trivialement que I ⊆ pred(I) sinon ceci signifierait qu’il existe un sommet v prédécesseur d’un sommet v′ de I qui ne serait pas dans I. Mais, alors on peut mener un chemin infini à partir de v puisque l’on peut en mener un à partir de v′. Donc, v ∈ I. De plus, on a Y = f (Y ) (Y est son plus petit point fixe). Donc, on a aussi Y ∪ I ⊆ f (Y ) ∪ f (I) , i.e. Z ⊆ f (Z).
Montrons maintenant que Z est le plus grand ensemble vérifiant Z ⊆ f (Z). Soit X ⊆ V tel que X ⊆ f (X), et soit v un sommet de X. Nous avons alors 2 possibilités :
1. si v ∈ Y , alors on a a fortiori v ∈ Z.
2. sinon (i.e. v6∈Y et donc v6∈X0 et v6∈P red(Y )), alors v ∈ pred(X), i.e. il existe un sommet v1 ∈ X tel que (v, v1) ∈ E. Mais du fait que pred(Y ) ⊆ Y , on en déduit que v16∈Y . On peut alors par récurrence construire un chemin infini (v, v1, v2, . . .). On a ainsi v ∈ I, et donc là encore v ∈ Z.
Exemple 18.
Pour permettre de calculer un des points fixes de f (ici ce sera le plus petit), il nous faut maintenant disposer d’une méthode itérative qui itération après itération, tende vers ce point fixe. Les éléments étant ordonnés, chaque itération doit avoir pour effet d’améliorer la solution selon l’ordre , i.e. si x est la solution obtenue à l’itération i, à i + 1 on doit avoir que x f (x). Ceci nous donne alors la forme récursive de notre schéma itératif. Il nous manque le cas de base. Un candidat naturel se présente, l’élément minimal du treillis complet ⊥. ⊥ étant l’élément minimal dans E et f étant monotone, on a ⊥ f (⊥) f (f (⊥)) . . . f n(⊥) . . .. Il nous reste alors à vérifier que t{f n(⊥)|n ∈ N} avec f 0(⊥) = ⊥ est un point fixe de f , i.e. f (t{f n(⊥)|n ∈ N}) = t{f n(⊥)|n ∈ N}. Par définition, tout sous-ensemble Ek = {f i(⊥)|i ≥ k} avec k ∈ N a la même borne supérieure que {f n(⊥)|n ∈ N}. De plus, on a pour tout k ∈ N, Ek = f (Ek−1). On a alors l’équivalence suivante :
f (tE0) = tE0 ⇐⇒ f (tE0) = tf (E0)
Toute application f : E → E qui vérifierait la condition à droite de l’équivalence mais sur tout sous-ensemble E′ de E (et non pas seulement la chaîne E0) est dite continue. En fait, la continuité est plus générale, car elle n’impose pas que l’application f soit définie sur le même ensemble.
30


Soit (E, ) et (E′, ′) deux ensembles ordonnés. Une application f : E → E′ est continue (on dit aussi sup-continue) si elle préserve les bornes supérieures des sousensembles non vides de E, i.e. pour tout S ⊆ E tel que S 6= ∅ si tS existe alors f (tS) = tf (S).
Définition 15.
Si f est une application continue d’un treillis complet sur lui-même, alors le plus petit point fixe de f est égal à t{f n(⊥)|n ∈ N}.
Théorème 19.
Démonstration : On a vu ci-dessus qu’il était bien un point fixe. Il nous reste à montrer qu’il est le plus petit. Si y est un autre point fixe de f , on montre par induction sur n que f n(⊥) y. ⊥ étant l’élément minimal, on a f 0(⊥) y. Supposons que f n(⊥) y. f étant continue, elle est monotone. On a donc aussi f n+1(⊥) f (y). y étant un point fixe de f , on a alors f n+1(⊥) y. Ainsi, y est aussi un majorant de {f n(⊥)|n ∈ N}, et donc t{f n(⊥)|n ∈ N} y.
Dans la preuve ci-dessus, on n’utilise pas la structure de treillis. En fait, les seules conditions qui nous intéressent sont que l’ensemble ordonné (E, ), domaine et co-domaine de la fonction f , possède un élément minimal, et que chaque chaîne admette une borne supérieure. Enfin, la fonction a besoin d’être continue uniquement sur les chaînes. On parle alors d’ordre partiel complet (CPO en anglais pour Complete Partial Order) et de continuité de Scott du nom du logicien qui le premier caractérisa les CPOs. C’est à partir de ce type de structure algébrique que la sémantique dénotationnelle des langages de programmation a été définie, et non pas celle de treillis complet (cf. la section 4.3).
4.2. Définitions inductives et induction structurelle
Définitions inductives
La définition inductive d’un ensemble E consiste à construire chacun des éléments à partir d’éléments de l’ensemble E que l’on aurait déjà construits. Pour que ceci ait un sens, il faut se donner explicitement un ensemble B d’éléments que l’on suppose appartenir à E sans les construire, et des règles de construction. Comme il est d’usage en mathématiques, pour ne pas tomber dans les paradoxes de la théorie des ensembles, on définira toujours inductivement un sous-ensemble X d’un ensemble déjà connu E.
31


Soit E un ensemble. Une définition inductive d’un sous-ensemble X de E est la donnée : — d’un sous-ensemble B de E appelé ensemble de base, et — d’un ensemble Γ de fonctions (qui peuvent être partielles) Fi : Eni → E où ni ∈ N dénote l’arité de la fonction Fi. On appelle souvent les fonctions Fi les règles de construction de l’ensemble inductif.
telle que X est le plus petit ensemble (au sens de l’inclusion) vérifiant : (B) B ⊆ X
(I) ∀Fi : Eni → E, ∀(x1, . . . , xn) ∈ Xni, Fi(x1, . . . , xn) ∈ X
Définition 16 (Définition inductive).
— Le sous-ensemble X de N défini inductivement par : (B) 0 ∈ X (I) x ∈ X ⇒ x + 1 ∈ X n’est autre que N. Ainsi, (B) et (I) constituent une définition inductive de N. — Soit A un ensemble. Notons X le sous-ensemble de A∗ défini inductivement par : a (B) ε ∈ X (ε est le mot vide équivalent à la suite s : ∅ → A dans A∗). (I) α ∈ X ⇒ ∀a ∈ A, a.α ∈ X X n’est autre que A∗. — Soit A = {(, )} l’ensemble (ou alphabet) constitué de deux paranthèses (ouvrante et fermante). Le sous-ensemble D de A∗ des parenthésages bien formés, appelé aussi langage de Dyck, est inductivement défini par : (B) ε ∈ D
(I) x, y ∈ D ⇒ (x) ∈ D et xy ∈ D
— Soit A un ensemble. L’ensemble AB des arbres binaires étiquetés sur A est le sous-ensemble de (A ∪ {∅, (, ), ; })∗ défini inductivement par : (B) ∅ ∈ AB (∅ dénote alors l’arbre vide) (I) g, d ∈ AB ⇒ ∀a ∈ A, (a; g; d) ∈ AB (l’arbre binaire de racine a, d’arbre gauche g et d’arbre droit d)
a. A∗ est l’ensemble de toutes les séquences finies (aussi appelées mots) que nous pouvons construire sur l’ensemble A pris comme un alphabet. Nous définirons plus précisément cet ensemble dans la troisème partie de ce cours.
Exemple 20.
Pour montrer qu’il existe toujours un plus petit sous-ensemble X de E contenant B et satisfaisant la condition (I), définissons la fonction F : P(E) → P(E) par :
F (Y ) = {y ∈ E|(y ∈ B) ou (∃Fi, ∃(y1, . . . , yni) ∈ Y ni, y = Fi(y1, . . . , yni)}
F est trivialement croissante pour l’inclusion. Elle est même continue sur les chaines (le montrer en exercice). Par le théorème 17, on définit X comme le plus petit point fixe de la fonction F . Par définition, X est le plus petit ensemble vérifiant F (X) = X, et d’après la preuve du théorème 17, c’est le plus petit ensemble vérifiant la propriété F (X) ⊆ X. X est donc bien le plus
32


petit ensemble contenant B et fermé par les fonctions Fi. Ainsi, X = u{Y |F (Y ) ⊆ Y } =
⋂
Y ∈F
Y
où F = {Y |F (Y ) ⊆ Y } qui est une autre façon de caractériser l’ensemble X. Par le théorème 19, X peut aussi se définir par la suite suivante :
X0 = ∅ Xi = Xi−1 ∪ {y ∈ E | (y ∈ B) ou ∃Fi, ∃y1, . . . , yni ∈ Xn−1, y = Fi(y1, . . . , yni)}
Et donc X =
⋃
i∈N
Xi. 2
Nous pouvons alors donner un premier principe d’induction.
Soit E un ensemble. Soit X ⊆ E un ensemble défini inductivement à partir d’un ensemble de base B et d’un ensemble Γ de fonctions Fi : Eni → E. Si Y ⊆ X contient B et est aussi clos par les fonctions de Γ, alors Y = X.
Corollaire 1.
Démonstration : Par hypothèse, on a F (Y ) ⊆ Y , et donc X ⊆ Y . Or par hypothèse Y ⊆ X ce qui nous permet de conclure que X = Y .
Preuve par induction structurelle
Par définition, les ensembles X inductivement définis possèdent la propriété que chacun de leurs éléments est obtenu par composition licite de fonctions de Γ. Ainsi, associons à chaque fonction Fi ∈ Γ un nom de fonction F i et notons Γ = {F i|Fi ∈ Γ}. Considérons alors l’ensemble A = B ∪ Γ ∪ {(, ), , }. On peut considérer l’ensemble des expressions TΓ(B) ⊆ A∗ comme défini inductivement par : (B) B ⊆ TΓ(B)
(I) t1, . . . , tni ∈ TΓ(B) ⇒ (∀Fi : Eni → E, F i(t1, . . . , tni) ∈ TΓ(B)) On peut définir une application _X : TΓ(B) → X par :
x ∈ B 7→ x
F i(t1, . . . , tni ) 7→ Fi(t1X , . . . , tnXi )
X étant inductivement défini à partir de B et des fonctions dans Γ, l’application _X est surjective. Maintenant, sur TΓ(B) on peut définir la relation binaire X comme la fermeture réflexive et transitive de :
t X t′ ⇔ t′ = F i(. . . , t, . . .)
X est la relation "être une sous-expression de".
X est un ordre bien fondé sur TΓ(B).
Théorème 21.
2. On suppose ici que tout élément x ∈ B se définit par une fonction constante x :→ E (i.e. une fonction x : E0 → E) qui dénote l’élément x dans E.
33


Démonstration : Montrons que X est anti-symétrique. Par le théorème 19 appliquée à la fonction F : P(E) → P(E), l’ensemble TΓ(B) =
⋃
j∈N
T j où pour tout j ∈ N, T j est l’ensemble défini par 3 :
T0 = ∅ T j = T j−1 ∪ {F i(t1 . . . , tni )|∀k, 1 ≤ k ≤ ni, tk ∈ T j−1}
Ainsi, pour tout j > 0 et pour toute expression F i(t1 . . . , tni ) ∈ T j \ Tj−1, toutes les sousexpressions tk pour k = 1, . . . , ni appartiennent à des ensembles T l avec l < j. On en déduit alors que deux expressions t et t′ de TΓ(B) qui vérifieraient à la fois t X t′ et t′ X t ne peuvent être qu’égaux (une expression ne peut pas être à la fois sur et sous-expression d’une autre). Montrons que X est de plus bien fondé. Par la définition inductive de TΓ(B), étant donnée une expression F i(t1, . . . , tni ), l’ensemble de ses sous-expressions est forcément de cardinalité finie et a pour éléments minimaux tous les éléments appartenant à B.
On peut alors appliquer l’induction mathématique sur les ensembles bien fondés. Ainsi, étant donnée une propriété P portant sur les éléments de TΓ(B), ce principe de preuve s’exprime ici par :
∀t ∈ TΓ(B), P (t) ⇐⇒ [∀t ∈ TΓ(B), (∀t′ ∈ TΓ(B)t′ ≺X t ⇒ P (t′)) ⇒ P (t)]
Par le théorème 19 appliqué à la fonction F : P(E) → P(E), on a vu que l’ensemble TΓ(B) =
⋃
j∈N
T j où pour tout j ∈ N, T j est l’ensemble défini par :
T0 = ∅ T j = T j−1 ∪ {F i(t1 . . . , tni)|Fi ∈ Γ, ∀k, 1 ≤ k ≤ ni, tk ∈ T j−1}
Le principe d’induction ci-dessus s’exprime aussi de la façon suivante : Si les deux conditions sont vérifiées
1. P (x) est vraie pour tout x ∈ B
2. ∀Fi ∈ Γ, (∀k, 1 ≤ k ≤ ni, P (tk)) ⇒ P (F i(t1, . . . , tni))
alors, P (t) est vraie pour tout t ∈ TΓ(B). Ce type de preuve s’appelle induction structurelle. Il s’étend naturellement à X. En effet, soit P une propriété portant sur les éléments de X. On peut définir la propriété P ′ sur TΓ(B) par : ∀t ∈ TΓ(B), P ′(t) ⇔ P (tX ). L’application _X étant surjective, on a alors nécessairement :
(∀t ∈ TΓ(B), P ′(t)) ⇔ (∀x ∈ X, P (x))
Définition récursive d’une fonction
Sur les ensembles définis inductivement, nous pouvons profiter de la structure constructive des éléments de l’ensemble pour définir récursivement des fonctions sur cet ensemble. C’est l’approche suivie quand on écrit un programme implantant une fonction f . En effet, on utilise la structure inductive des structures de données en entrée de la fonction f pour décrire de façon récursive son comportement. Comme nous le verrons dans la seconde partie de ce document, c’est à partir de cette notion de récursivité que l’on définit rigoureusement la notion d’algorithme. Bien sûr, étant donnés un ensemble X défini inductivement et une fonction f : X → A, pour
3. Là encore, nous considérons que tous les éléments x ∈ B sont des fonctions constantes x :→ E, et donc T1 = B.
34


assurer la propriété pour f d’être fonctionnelle, il faut que l’on s’assure qu’à tout élément x ∈ X, on ne peut associer qu’une unique valeur par f . La fonction f étant définie récursivement à partir de la structure inductive des éléments de X (i.e. des éléments dans TΓ(B)), il faut que l’on s’assure alors que toutes les expressions t dans TΓ(B) s’évaluent par _X de façon unique. Quand l’ensemble X vérifie une telle propriété, on dira qu’il est non ambigu.
Une définition inductive d’un ensemble X est dite non ambigue si l’application _X est bijective, i.e. pour tout x ∈ X, il existe une unique expression t ∈ TΓ(B) telle que tX = x.
Définition 17 (Définition inductive non ambigue).
— La définition des arbres binaires donnée dans l’exemple 20 est non ambigue. Nous n’avons qu’un choix unique de construction d’un arbre binaire à chaque étape. — À l’inverse, la définition inductive suivante de N × N est ambigue : (B) (0, 0) ∈ N × N
(I) (n, m) ∈ N × N et n ≥ m ⇒ (n + 1, m) ∈ N × N et (n, m + 1) ∈ N × N
Il suffit de considérer n’importe quel couple d’entiers positifs (n, m) avec n 6= 0 et m 6= 0 qui peut s’obtenir à partir du couple (n − 1, m − 1) en ajoutant une unité au premier argument puis au second, ou l’inverse. Une définition inductive non ambigue de N × N peut être la suivante :
(B) (0, 0) ∈ N × N et ∀n, m ∈ N, n < m ⇒ (n, m) ∈ N × N (I) (n, m) ∈ N × N ⇒ (n + m, m) ∈ N × N
Cette définition de N×N n’est pas la plus naturelle. En fait, comme on pouvait s’y attendre, elle est dictée par la définition récursive d’une fonction sur les couples d’entiers positifs, ici la fonction modulo
n modulo m =
{ n si n < m (n - m) modulo m sinon
Exemple 22.
Intuitivement, pour définir récursivement une fonction sur les éléments d’un ensemble non ambigu, on commence par définir la fonction sur les éléments de l’ensemble de base puis inductivement sur les éléments nouveaux construits à partir d’éléments déjà connus.
Soit X ⊆ E un ensemble défini inductivement et non ambigu. Soit A un ensemble quelconque. La définition récursive d’une application f : X → A est la donnée d’une application g : B → A et pour tout Fi : Eni → E ∈ Γ d’une application hFi : Eni × Ani → A telle que : — f (x) = g(x) pour tout x ∈ B
— f (Fi(x1, . . . , xni)) = hFi(x1, . . . , xni, f (x1), . . . , f (xni))
Définition 18 (Définition récursive d’une fonction).
35


Reprenons l’application modulo définie sur la définition non ambigue de N × N. Dans cette définition, l’ensemble B = {(0, 0)} ∪ {(n, m)|n < m} et Γ = {F : N × N → N × N} où F est la fonction définie par (n, m) 7→ (n + m, m) pour tout couple (n, m) ∈ N × N et n, m 6= 0, et les fonctions g et hF se définissent par : g : (n, m) 7→ n et hF : ((n, m), p) 7→ p. La définition récursive de modulo (i.e. la fonction f ) s’écrit alors : — f (n, m) = g(n, m) = n pour tout (n, m) ∈ B — f (F (n, m)) = f (n + m, m) = hF ((n, m), f (n, m)) = n modulo m = f (n, m)
Exemple 23.
Nous verrons dans la seconde partie de ce document que les fonctions qui se définissent selon la définition 18 caractérisent bien la notion intuitive d’algorithme.
4.3. Application : sémantique dénotationnelle des langages de
programmation
Les programmes écrits dans n’importe quel langage de programmation sont des objets syntaxiques auxquels il faut pouvoir donner une signification pour permettre d’analyser leur comportement, cette signification pouvant aussi bien être dénotationnelle (caractérisation abstraite du comportement) qu’opérationnelle (caractérisation concrête du comportement). L’ensemble des programmes que l’on peut écrire dans tout langage de programmation a une particularité, il suit une définition inductive non ambigue. On peut donc définir des fonctions récursives sur cet ensemble. Les sémantiques dénotationnelle et opérationnelle se définissent justement par de telles fonctions 4. Ici, nous allons nous intéresser plus particulièrement à la sémantique dénotationnelle d’un langage de programmation jouet (donc très simple dans sa syntaxe) mais suffisamment puissant pour exprimer tous les problèmes calculables (cf. la seconde partie de ce document pour une définition formelle de la notion de calculabilité). Pourquoi présenter la sémantique dénotationnelle d’un tel langage ? parce qu’elle est une application directe des treillis et des résultats de points fixes associés. Enfin, l’intérêt d’une sémantique formelle (i.e. mathématiquement définie) des langages de programmation, est de permettre de raisonner sur les programmes générés pour démontrer rigoureusement leur correction partielle, i.e. montrer sans les exécuter que les programmes font bien ce que l’on attend d’eux.
Un langage de programmation simple
Ici, nous donnons la syntaxe de notre langage de programmation d’étude que nous appelons WHILE. WHILE est un langage dit impératif, i.e. que l’instruction de base est l’affectation. Les langages PYTHON, C, C++, COBOL, FORTRAN, JAVA, etc. sont des langages impératifs. Il existe aussi deux autres paradigmes de programmation, celle dite fonctionnelle où l’on évalue des fonctions (e.g. le langage CAML) et celle dite logique où l’on infère des faits (e.g. PROLOG - cf. le cours électif en S8 "Fondements logiques de l’informatique"). Comme tout langage de programmation, WHILE est composé de types de données prédéfinis, d’expressions sur ces types et d’instructions. À la fois, les expressions et les instructions vont
4. Il existe aussi une sémantique axiomatique des langages de programmation qui définit non pas une fonction mais un prédicat de façon récursive sur les programmes.
36


suivre une définition inductive. On supposera que l’on ne peut pas définir d’autres types à l’exception de ceux donnés explicitement. En ce sens, le langage proposé n’est pas réaliste.
Les types autorisés dans le langage sont les suivants : les entiers int, les booléens bool, les caractères ASCII char, les chaînes de caractères string, et les listes list[t] dont tous les éléments sont d’un même type donné t, ce dernier pouvant être un des types précédents. Entre autres, il peut être un type list[t′]. Formellement, l’ensemble des types T est l’ensemble défini inductivement par :
— T0 = {int, bool, char, string} — Tn = Tn−1 ∪ {list[t]|t ∈ Tn−1}
Sur ces types on peut écrire des expressions. Comme il est d’usage dans les langages de programmation, on va utiliser la notation à la BNF (Backus-Naur Form) pour donner la définition inductive de ces expressions. — les expressions arithmétiques : a ::= n|v|a+a|a×a|−a (n ∈ Z utilisé comme une constante, et v est une variable de type int) — les expressions booléennes : b ::= true|f alse|v|not b|b and b|e == e|e < e|e ≤ e où v est une variable de type bool et e est une expression arithmétique, un caractère ou une expression sur les chaînes de caractère. Bien sûr les expressions prises dans une égalité ou une inégalité doivent être de même type. — les expressions sur les chaînes de cacactères : c ::= []|[char1, . . . , charn]|v|c[i]|c + c|size(c) où charj est un des 256 caractères ASCII, v une variable de type string et i est une expression arithmétique qui dénote une position dans la chaîne c. On supposera que les positions dans une chaîne de caractères commençent à 1. — les expressions sur les listes : l ::= []|[e1; . . . ; en]|v|l[i]|l + l|size(l) où tous les ej sont des expressions de même type, et v est une variable de type list[t]. Comme pour les chaînes de caractères, on supposera que les positions dans une liste commençent à 1. Enfin, on définit les instructions de notre langage WHILE. Là encore, nous allons utiliser une notation à la BNF. On suppose s’être donnée une famille indéxée par les types d’ensembles de variables V = (Vt)t∈T.
I ::== skip|x = e|c[i] = char|l[i] = e|I; I|if b then I else I|while b do I
où x, c et l sont des variables de type t ∈ T, string et list[t], respectivement, e est une expression de même type que la variable x, i est une variable ou expression de type int et dénote la position dans la chaîne de caractère c ou la liste l, et b est une expression boolénne. Nous aurions aussi pu introduire la récursivité, mais ceci aurait demandé au préalable d’introduire des notations fonctionnelles qui auraient alourdi la présentation du langage sans apporter de connaissances supplémentaires à la sémantique dénotationnelle d’un langage de programmation.
Un programme P dans le langage WHILE se définit alors :
Prog P(x_1:t_1;...;x_n:t_n):y_1:t’_1;...;y_m:t’_m z_1:t’’_1;...;z_p:t’’_p begin I
end
Les variables x_i définissent les arguments en entrée du programme P , tandis que les variables y_j sont les variables de sorties. Les variables z_l sont des variables locales utiles aux calculs.
37


Donnons le programme qui recherche un élément dans une liste.
Prog Recherche(e:t;l:list[t]):b:bool i:int begin i = 1; if (l == []) then b = false else b = (l[i] == e); while not b i = i + 1; b = (i > size(l)) or (l[i] == e); b = (i < size(l)) end
Exemple 24.
Sémantique dénotationnelle du langage WHILE
Un programme dans notre langage WHILE transforme les états du programme définis par les contenus des variables. Ainsi, la sémantique dénotationnelle d’un programme sera alors définie par une fonction partielle sur l’ensemble de ces états. La fonction est partielle car il se peut que le programme ne termine pas. Ainsi, la sémantique d’un programme P est une fonction JP K : S → S où S est l’ensemble des états du programme, i.e. des fonctions de V dans la sémantique associée aux types. Donc, commençons par associer une sémantique JtK à tout type t ∈ T. L’ensemble T étant défini de façon inductive, nous allons définir leur sémantique profitant de cette structure. 
JintK = Z, JboolK = B = {0, 1}, JcharK = 256 caractères ASCII, JstringK = JcharK
∗

Jlist[t]K = JtK
∗
Soit un programme P dont les variables d’entrée, de sortie et locales sont choisies parmi un ensemble de variables V = (Vt)t∈T. Par définition, pour un type donné t ∈ T, le contenu des variables dans Vt doit être choisi parmi les valeurs dans JtK. Ainsi, chaque état du programme se définit par une famille indéxée par T d’applications Vt → JtK. On notera ces états σ. Pour toute variable x ∈ Vt, σ(x) ∈ JtK est la valeur de x dans cet état. Enfin, σ[x/val] où x ∈ Vt et val ∈ JtK désigne le nouvel état σ′ tel que pour tout y 6= x ∈ V , σ′(y) = σ(y) et σ′(x) = val. L’ensemble S des états est alors l’ensemble des applications σ de V dans JTK, noté aussi JTK
V,
qui respectent le typage (i.e. pour tout x ∈ Vt, σ(x) ∈ JtK).
À partir d’un état σ, nous pouvons maintenant évaluer les expressions. Soit e une expression du langage et σ un état du programme. On note JeKσ son évaluation, et on la définit de la façon suivante : — Évaluation des expressions arithmétiques : JnKσ = n, JvKσ = σ(v), Je1@e2Kσ = Je1Kσ@Je2Kσ où @ ∈ {+, ×}, et J−eKσ = −JeKσ
— Évaluation des expressions booléennes : JtrueKσ = 1, Jf alseKσ = 0, JvKσ = σ(v), Jnot bKσ =
1−JbKσ, Jb1 and b2Kσ = Jb1Kσ ×Jb2Kσ, Je1 == e2Kσ = (Je1Kσ = Je2Kσ), Je1 < e2Kσ = (Je1Kσ <
Je2Kσ), Je1 ≤ e2Kσ = (Je1Kσ ≤ Je2Kσ). Dans le cas où e1et e2 sont des chaînes de caractères, < et ≤ désignent les ordres lexicographiques strict et total, respectivement.
38


— Évaluation des expressions sur les chaînes de cacactères : J[]Kσ = ε (le mot vide), J[char1, . . . , charn]Kσ = char1 . . . charn, JvKσ = σ(v), Jc[i]Kσ = ci quand JcKσ = c1 · · · cn et 1 ≤ JiKσ ≤ n, sinon Jc[i]Kσ = ε, Jc1 + c2Kσ = Jc1Kσ · Jc2Kσ, et Jsize(c)Kσ = n si JcKσ = c1 . . . cn.
— Évaluation des expressions sur les listes : J[]Kσ = ε, J[e1, . . . , en]Kσ = Je1Kσ . . . JenKσ, JvKσ = σ(v), Jl[i]Kσ = ai quand JlKσ = a1 · · · an et 1 ≤ JiKσ ≤ n, sinon Jl[i]Kσ = ε, Jl1 + l2Kσ = Jl1Kσ · Jl2Kσ, et Jsize(l)Kσ = n si JlKσ = l1 . . . ln.
Évaluons maintenant les instructions toujours en profitant de la structure inductive de leur définition. Chaque instruction ι va donc définir une fonction partielle JιK : S → S. Ci-dessous, nous allons donner la sémantique des instructions à l’exception de la boucle "while", celle-ci demandant un traitement particulier. 
JskipK = IdS (l’application identité sur S) 
Jx = eK : σ 7→ σ[x/JeKσ] 
Jc[i] = eK : σ 7→ σ[c/α] quand σ(c) = c1 . . . cn et 1 ≤ JiKσ ≤ n, et dans ce cas-là α = c′1 · · · c′n
tel que pour tout j, 1 ≤ j ≤ n, j 6= JiKσ ⇒ c′
j = cj et j = JiKσ ⇒ c′
j = JeKσ, sinon Jc[i] = eK est indéfini pour σ. 
Jl[i] = eK : σ 7→ σ[l/α] quand σ(l) = l1 . . . ln et 1 ≤ JiKσ ≤ n, et dans ce cas-là α = l′1 · · · l′n
tel que pour tout j, 1 ≤ j ≤ n, j 6= JiKσ ⇒ l′
j = lj et j = JiKσ ⇒ l′
j = JeKσ, sinon Jl[i] = eK est indéfini pour σ. 
JI; I′
K = JI′
K ◦ JIK

Jif b then I else I′
K : σ 7→
{
JIK(σ) si JbKσ = 1 JI ′
K(σ) sinon Terminons la sémantique de notre langage WHILE en traitant le cas de la boucle "while". Par définition, la boucle while satisfait le comportement suivant :
while b do I ≡ if b then I; while b do I else skip
Sémantiquement, on a alors :
Jwhile b do IK : σ 7→
{
Jwhile b do IK(JIK(σ)) si JbKσ = 1 σ sinon
Considérons alors la fonction f : SpS → SpS où SpS est l’ensemble des fonctions partielles de S dans S :
w 7→ (σ 7→
{ w(JIK(σ)) si JbKσ = 1
σ sinon )
La sémantique de la boucle "while" se réécrit alors :
Jwhile b do IK = f (Jwhile b do IK)
Ainsi, la sémantique de la boucle "while" se définit par un point fixe de la fonction f . On attend d’une boucle "while" qu’elle ait un nombre fini d’étapes d’exécution. Le point fixe recherché est alors le plus petit. Par le théorème 19, l’on sait exprimer (voir calculer) ce point fixe. Mais ceci demande à ce que SpS et f soit respectivement, un ordre partiel complet 5 et une application
continue. Associons à SpS l’ordre suivant :
g g′ ⇔ dom(g) ⊆ dom(g′) et ∀σ ∈ dom(g), g(σ) = g′(σ)
5. En fait, il n’est pas possible d’associer une structure de treillis à (SS
p , ). En effet, à deux fonctions g et g′ il n’est pas possible d’associer une borne supérieure car elles ne sont pas nécessairement en accord sur leur intersection des domaines dom(g) et dom(g′). Ceci est aussi vrai pour leur borne inférieure.
39


où dom(g) désigne le sous-ensemble de S pour lequel g est définie. est un ordre car défini à partir de l’inclusion. (SpS, ) possède un élément minimal, la fonction partielle ⊥ : S → S définie nul part. Enfin, toute chaîne de fonctions partielles (gi) possède une borne supérieure qui n’est
autre que
⋃
i
gi. Ainsi, (SpS, ) est un ordre partiel complet.
Pour pouvoir appliquer le théorème 19, il suffit de monter que f préserve la borne supérieure des chaînes de fonctions dans SpS. Soit (gi) une chaîne dont la borne supérieure est g. On observe que f est une fonction monotone. Ainsi, l’ensemble (f (gi)) est aussi une chaîne dont la borne supérieure est h. On doit montrer que f (g) = h. Par définition, on a
f (g)(σ) =
{ g(JIK(σ)) si JbKσ = 1 σ sinon
Si JbKσ = 0, alors f (g)(σ) = σ, et donc ∀i, f (gi)(σ) = σ, d’où l’on déduit aussi que h(σ) = σ. Si JbKσ = 1, alors on a f (g)(σ) = g(JIK(σ)). Ici, on a alors deux possibilités :
1. g(JIK(σ)) est indéfini. Ceci veut dire que pour tout i, gi(JIK(σ)) est indéfini, et donc f (gi)(σ) aussi, d’où l’on déduit que h(σ) est indéfini.
2. g(JIK(σ)) est défini. Ceci veut dire qu’il existe i tel que gi(JIK(σ)) est défini, et donc f (gi)(σ) aussi. On a alors h(σ) = f (gi)(σ) = gi(JIK(σ)) = g(JIK(σ)) = f (g)(σ).
On peut appliquer le théorème 19 et donner la dénotation suivante aux boucles "while" :
Jwhile b do IK = t{f n(⊥)|n ∈ N}
40


En reprenant notre exemple 24, la méthode itérative calcule pour la boucle while la solution g : S → S de l’équation à point fixe g = f (g) comme la limite de la suite de fonctions g0, g1, . . . , gn, . . . définie inductivement par :
g0 = ⊥ gn = f (gn−1)
Ainsi, par définition, g0 est la fonction partielle définie nulle part. g1 est alors la fonction définie par :
σ 7→
{ ⊥(Ji = i + 1; b = (i > size(l))or(l[i] == e)K(σ)) si σ(b) = 0 σ sinon
⊥ étant la fonction définie nulle part, la fonction g1 est seulement définie pour tout état < e, l1 . . . lm, 1, i > tel que m ≥ i, ∀j, 1 ≤ j ≤ i − 1, lj 6= e et li = e. La fonction g1 se comporte alors comme l’identité sur ces états. Si nous examinons g2, nous avons
g1(Ji = i + 1; b = (i > size(l))or(l[i] == e)K(σ)) si σ(b) = 0 σ sinon
Donc, g2 est définie pour tout état σ =< e, l1 . . . lm, b, i > tel que : — ∀j, 1 ≤ j ≤ i, lj 6= e, m ≥ i, et
— soit m ≥ i + 1 et li+1 = e, soit m = i Pour tous ces états σ, g2 est définie par :
g2(σ) =
{ < e, l1 . . . lm, 1, i + 1 > si m = i ou li+1 = e < e, l1 . . . lm, 0, i + 1 > sinon
En procédant de façon récursive, la fonction gn : S → S est la fonction partielle définie pour tout état σ =< e, l1 . . . lm, i, b > tel que i ≤ n, ∀j, 1 ≤ j ≤ i, lj 6= e, m ≥ n, de la façon suivante :
gn(σ) =



< e, l1 . . . lm, 1, k > si i + 1 ≤ k ≤ i + n ≤ m et ((∀j, i + 1 ≤ j ≤ k − 1, lj 6= e et lk = e) ou m = i + (k − 1) < e, l1 . . . lm, 0, m + 1 > sinon
Par le théorème 19, la fonction g associée à la boucle while dans le programme de recherche d’un élément dans une liste, n’est autre que la fonction définie pour tout état σ =< e, l, i, b > tel que 1 ≤ i ≤ |l|, ∀j, 1 ≤ j ≤ i, lj 6= e, de la façon suivante : Pour tous ces états σ, g est défini par :
g(σ) =

  
  
< e, l, 0, 1 > si l = ε < e, l, 1, k > si i + 1 ≤ k ≤ |l| et ((∀j, i + 1 ≤ j ≤ k − 1, lj 6= e et lk = e) ou m = i + (k − 1)) < e, l, 0, |l| + 1 > sinon
Exemple 25.
41




Sous partie 2
Calculabilité
43




5. Introduction
La formalisation du raisonnement occupe les esprits depuis longtemps. En inventant/découvrant la logique, les Grecs furent les premiers à émettre l’idée que tout raisonnement – de nature scientifique ou philosophique – pouvait être réduit à un calcul, c’est-à-dire à une technique permettant de prouver un énoncé en respectant des règles autorisées. Leur but était de mettre fin aux désaccords lors des discussions. Au XVIIème siècle, G. Leibniz affirma même qu’il était possible de réduire tout raisonnement et toute théorie à une combinaison d’éléments de base tels que les nombres, les lettres, les couleurs, etc. Il ébaucha alors une algèbre de pensée et définit la première table de calcul binaire. C’est cette algèbre de pensée qui inspira d’ailleurs G. Boole lorsqu’il réduisit le raisonnement logique au calcul algébrique à base binaire, que l’on appelle algèbre de Boole.
Depuis E. Kant, on savait que seules les questions scientifiques pouvaient être prouvées par un raisonnement rigoureux et non ambigu, et que les énoncés métaphysiques comme "Dieu existe-til ?” ou "L’âme est-elle immortelle ?" amenaient nécessairement à un désaccord persistant sur leur valeur de vérité. Sachant qu’un raisonnement est rigoureux et non ambigu s’il peut être traduit dans un langage mathématique, les mathématiques devenaient donc le langage des sciences. Il était alors nécessaire d’étudier les objets mathématiques, à plus forte raison parce que certains objets mathématiques incongrus venaient d’apparaître (par exemple une courbe sans tangente ou les géométries non euclidiennes). Le XIXème siècle et le premier tiers du XXème siècle furent l’époque de la réflexion sur les objets mathématiques. De cette réflexion sont nées, au cours du XXème siècle, la théorie de la démonstration et la notion de calculabilité. Le but de ces recherches était de mettre fin à la "crise des fondements", principalement après la découverte de paradoxes dans le raisonnement mathématique. Pour l’ensemble des participants, l’idée était de ramener les mathématiques à la seule arithmétique et à l’utilisation de procédés finitistes (L. Kronecker, L. Brower, D. Hilbert). À la même époque, A. Thue posait un problème linguistique qui eut un grand retentissement dans le monde mathématique mais aussi dans un monde qui n’existait pas encore : celui de l’informatique. Son problème s’énonçait ainsi : étant donnés un alphabet et une grammaire (c’est-à-dire un ensemble de règles), un mot quelconque sur l’alphabet peut-il être dérivé de l’alphabet à l’aide de la grammaire ? Ce problème, connu sous le nom de problème du mot pour les systèmes formels, motiva de nombreux chercheurs. En effet, le résoudre équivalait à systématiser le raisonnement, c’est-à-dire à l’automatiser. L’équivalence est évidente lorsqu’on pose le problème de la façon suivante : étant donnés un ensemble d’hypothèses et une démarche de raisonnement, une assertion quelconque est-elle une conséquence logique des hypothèses de départ ? Dès 1931, la thèse finitiste fut mise en échec par les théorèmes d’incomplétude de K. Godel. Comme conséquence logique, il fut démontré que le problème du mot était indécidable pour de nombreux systèmes formels (par A. Church ou A. Turing pour l’arithmétique en 1936, par E. Post et A. Markov en parallèle pour les semi-groupes en 1947, par W. Boone pour les groupes en 1957, etc.). Cependant, de cet échec est née la notion d’indécidabilité. On savait maintenant montrer que certains problèmes mathématiques ne pouvaient être résolus par aucun algorithme ; il devenait alors intéressant de chercher ceux pour lesquels un algorithme existait. L’apparition des ordinateurs donna une motivation supplémentaire à cette recherche : même partielle, l’au
45


tomatisation du raisonnement trouvait là toute sa justification. La partie automatique pourrait être confiée à l’ordinateur : il suffirait de lui fournir les hypothèses, les règles de déduction et le mot à prouver, et d’attendre sa réponse. Toutefois, ceci ne suffisait pas. En effet, rien implique que l’algorithme soit utilisable pratiquement : le temps et l’espace mémoire nécessaires à son exécution pourraient largement dépasser ce dont on peut espérer disposer. Ainsi, un algorithme utilisable se doit d’être relativement efficace (i.e il doit n’utiliser qu’une quantité "raisonnable" de ressources nécessaires au calcul en temps et en espace mémoire). Pour cela, nous devons être capable de distinguer un algorithme efficace à ceux qui ne le sont pas. Une théorie permettant de répondre à cette attente a été développée dans les années 70 et continue à occuper un nombre important de chercheurs en informatique théorique dans le monde : la complexité. Dans cette partie, nous allons donner une définition rigoureuse à ces deux notions de décidabilité et complexité. Nous allons alors aborder la formalisation de la décidabilité de deux façons. Tout d’abord, du point de vue des mathématiques standards où l’on a l’habitude de manipuler des fonctions. On parlera alors de fonctions récursives. Enfin, on caractérisera la propriété pour un problème d’être décidable par une version mathématique plus opérationnelle des machines à calculer ou des ordinateurs, les machines de Turing. On montrera alors que ces deux notions qui au premier abord peuvent sembler différentes, sont en faites équivalentes. En fait, il existe une multitude de définitions mathématiques de la calculabilité traduisant pour certains les différents paradigmes de programmation (λ-calcul pour la programmation fonctionnelle, machine à registre pour la programmation impérative, et la résolution pour la programmation logique), mais tous ont été démontrés équivalents, ce qui peut nous laisser supposer très fortement que les formalisations de la calculabilité telle qu’elles ont été définies il y a maintenant presque un siècle et dont nous en présentons deux ici, ont cerné parfaitement ce qui peut être calculable par un ordinateur (thèse de Church). Nous finirons cette partie par la présentation de la théorie de la complexité. Cette théorie sera étudiée dans le cadre des machines de Turing plus aptes à prendre en compte la notion de calcul. Nous donnerons alors la définition des deux grandes classes de complexité, P et NP, qui contiennent respectivement les algorithmes dont le temps d’exécution est "humainement" abordable et ceux qui ne le sont pas. On finira alors par étudier la sousclasse dans NP des algorithmes dits canoniques (i.e. tels que tous les autres de la classe NP peuvent se ramener par une transformation raisonnable à n’importe lequel de ces problèmes) : les algorithmes NP-complets.
46


6. Fonctions primitives récursives et
récursives
6.1. Codage
Le calcul est une activité qui manipule généralement des mots de longueur finie définis sur un alphabet donné. Ces mots peuvent aussi bien représenter des entiers, des arbres, ou tout autres structures de données mais aussi des programmes. On a vu dans la première partie de ce document, que l’ensemble de ces structures acceptait une définition inductive. Une propriété simple des définitions inductives est que chacun de leurs éléments peut être obtenu par une application finie des règles de production/construction. Ainsi, l’ensemble des mots finis que l’on manipule dans une activité calculatoire a la propriété d’être dénombrable. Or, on sait depuis Cantor que tous les ensembles dénombrables sont en bijection avec l’ensemble des entiers naturels. Ainsi, il existe une fonction bijective, appelée codage, qui permet de faire une correspondance un à un entre ces objets manipulés par tout programme et les entiers naturels. Un exemple de codage peut alors être le suivant : Soit A∗ l’ensemble des mots finis construits sur un alphabet fini ou de cardinalité dénombrable A (ce que nous avons l’habitude de manipuler en programmation pour représenter les structures de données). L’alphabet A étant au plus de cardinalité dénombrable, on peut numéroter ses éléments et donc établir une bijection entre A et N. On a alors le résultat suivant :
Pour chaque entier naturel p, il existe une fonction αp :
p
∏
i=1
N → N qui est une bijection
de
p
∏
i=1
N dans N.
Proposition 26.
Démonstration On commence par construire α2. Pour cela, on va suivre la méthode qui a été utilisée par Cantor pour démontrer que Q est en bijection avec N. On va numéroter les couples d’entiers naturels (x, y) en suivant les diagonales x + y = k pour k un entier naturel croissant (i.e. en suivant l’ordre des entiers k dans N). On commence par la diagonale x + y = 0 (qui ne contient qu’un seul couple), puis on passe à la diagonale x + y = 1 en commençant par le bas (i.e. par le couple (1, 0)), etc. La diagonale x + y = n a exactement n + 1 éléments. Donc avant le couple (p + n, 0) il y a exactement 1 + 2 + . . . + (n + p) = 1
2 (n + p)(n + p + 1). Le couple (p, n) qui se trouve sur la même diagonale que (p + n, 0), a exactement n places après lui. On peut donc poser α2 : (p, n) 7→ 1
2 (n + p)(n + p + 1) + n. Par définition, cette fonction est bijective de
N × N dans N. On en déduit immédiatement pour chaque p ≥ 1 une bijection αp :
p
∏
i=1
N→N
définie par récurrence sur p par :
47


α1 : n 7→ n αp+1 : (n1, . . . , np, np+1) 7→ α2(αp((n1, . . . , np)), np+1)
On peut alors coder toute suite d’entiers à partir de ces fonctions αp pour p ∈ N. En effet,
soit S∗ =
⋃
p∈N
Np. On définit l’application α : S∗ → N pour toute suite d’entiers σ de longueur p
par :
α(σ) = α2(p, αp(σ))
α est une bijection.
Théorème 27.
Démonstration Composition de deux fonctions bijectives.
La notion de calculabilité dont je rappelle qu’elle caractérise ce qui peut être calculé par un
ordinateur, peut alors se définir par des fonctions f : Nk → N où Nk =
k
∏
i=1
N avec pour convention
que N0 est l’ensemble ne contenant que la suite vide. Ces fonctions peuvent être partielles (i.e. indéfinies sur certaines valeurs) afin de prendre en compte le fait qu’un programme puissent ne jamais s’arrêter sur certaines valeurs. La question que l’on peut se poser alors, est "est-ce que toute fonction de ce type est effectivement calculable par un ordinateur ?". Depuis les travaux de Cantor, nous pouvons répondre non à cette question. La raison est que l’ensemble des fonctions f : N → N a déjà une cardinalité strictement supérieure à N. C’est une conséquence triviale du théorème fondamental de Cantor qui énonce pour tout ensemble E, l’inéquation suivante : |E| < |P(E)| où P(E) est l’ensemble des parties de E et |E| dénote la cardinalité de E. En effet, pour l’ensemble des fonctions f : N → N, il y a toutes les fonctions f : N → {0, 1}. Ce sous-ensemble de fonctions est isomorphe à l’ensemble des parties de N (connu aussi comme isomorphe à R). Or on vient de voir que la cardinalité de ce sous-ensemble est plus grand que N. Maintenant, par leur définition récursive, l’ensemble des programmes que l’on peut écrire dans un langage de programmation donné est forcément dénombrable. Ainsi, l’ensemble F =
⋃
k∈N
Fk
où Fk est l’ensemble de toutes les fonctions f : Nk → N contient nécessairement des fonctions non calculables par un ordinateur. Existe-t-il alors un moyen de caractériser le sous-ensemble des fonctions qui sont effectivement calculables ? Oui, et ceci a été fait au siècle dernier par d’éminents logiciens et mathématiciens tels que K. Godel, A. Turing, A. Church, J. Herbrand ou encore W. Ackermann. Cette caractérisation s’est faite en deux étapes. Tout d’abord, un premier sous -ensemble a été défini les fonctions primitives récursives. Cependant, cet ensemble, bien que fort riche, ne traduit pas complètement l’ensemble des fonctions pour lesquelles on peut effectivement trouver un algorithme qui les calcule. En effet, W. Ackermann a défini une telle fonction, la fonction d’Ackermann, dont on démontrera qu’elle n’est pas primitive récursive. En fait, avec le recul que nous avons maintenant sur les langages de programmation, ce que l’on constate alors est que les fonctions primitives récursives traduisent les fonctions que l’on peut calculer avec une boucle For. Cependant, il existe aussi une multitude de fonctions qui ont besoin d’une boucle While et dont le comportement peut aussi ne pas s’arrêter. C’est justement, ce que
48


permettront de caractériser les fonctions récursives en introduisant un schéma de minimisation non bornée et donc la notion de fonctions entières (dites aussi numériques) partielles.
6.2. Les fonctions primitives récursives
Définition
Le mécanisme de récursion comme moyen de définition de fonctions numériques est connu depuis Dedekind et Peano. Par exemple, l’addition peut se définir récursivement de la façon suivante :
x+0=x x + (y + 1) = (x + y) + 1
Dans l’exemple ci-dessus, la récursion porte alors sur le second argument de l’addition. La multiplication aussi peut se définir avec un schéma récursif identique :
x×0=0 x × (y + 1) = (x × y) + x
Là encore, la récursion porte sur le second argument de la multiplication. Ce schéma récursif se généralise au travers de la notion de fonction primitive récursive dont la définition formelle est la suivante :
L’ensemble des fonctions primitives récursives est le plus petit des sous-ensembles E de F (l’ensemble de toutes les fonctions dans N aussi appelées fonctions numériques) satisfaisant les conditions suivantes :
1. E contient toutes les fonctions constantes de Nk dans N pour tout entier k ∈ N.
2. E contient toutes les projections πi
k : (x1, . . . , xk) 7→ xi pour tous entiers i ∈ N et k ∈ N tels que 1 ≤ i ≤ k.
3. E contient la fonction successeur s : x 7→ x + 1 .
4. E est clos par composition, ce qui veut dire que si n et p sont des entiers naturels, que f1, . . . , fn sont des fonctions de E définies de Np dans N et g : Nn → N est aussi une fonction de E, alors la fonction g(f1, . . . , fn) : Np → N qui à tout (x1, . . . , xp) ∈ Np associe g(f1(x1, . . . , xp), . . . , fn(x1, . . . , xp)), est une fonction de E.
5. E est clos par récurrence, ce qui veut dire que, si p est un entier naturel, g : Np → N et h : Np+2 → N sont deux fonctions de E, alors la fonction f : Np+1 → N définie pour tout (x1, . . . , xp) ∈ Np et tout y ∈ N par :
a) f (x1, . . . , xp, 0) = g(x1, . . . , xp)
b) f (x1, . . . , xp, y + 1) = h(x1, . . . , xp, y, f (x1, . . . , xp, y))
est une fonction de E.
Définition 19 (Fonction primitive recursive).
49


Dans la fermeture par récurrence, la fonction g dénote la condition initiale et h l’étape de récurrence de la définition par récurrence de la fonction f .
Pour montrer qu’une fonction est primitive récursive, il suffit de montrer comment l’obtenir à l’aide des clauses 4. et 5., et à partir des fonctiones décrites en 1., 2. et 3., ou plus généralement, par la définition 19, à partir de fonctions dont on sait déjà qu’elles sont primitives récursives. Ainsi, la définition de l’ensemble des fonctions primitives récursives suit bien une définition inductive.
Montrons que l’addition est primitive récursive. On a vu ci-dessus que nous pouvions la définir par récurrence sur son second argument. Il suffit alors de trouver deux fonctions primitives récursives g et h définissant respectivement la condition initiale et l’étape de récurrence de sa définition par récurrence. Selon la définition récursive de l’addition donnée ci-dessus, on doit avoir respectivement : — g(x) = x, et
— h(x, y, x + y) = (x + y) + 1
Pour g, seule la fonction constante π11 peut convenir. La fonction h s’obtient en composant la fonction successeur avec la projection de la façon suivante : h(x, y, x + y) = s(π33(x, y, x + y)). On obtient ainsi la définition suivante :
x + 0 = π11(x)
x + (y + 1) = s(π33(x, y, x + y))
Exemple 28.
Ensemble primitif récursif
Tout ensemble A se définit par une fonction caractéristique χA : A → {0, 1} dont la définition est la suivante :
χA(a) = 1 si a ∈ A χA(a) = 0 sinon
De la même façon que pour les fonctions primitives récursives, si A est un sous-ensemble de Nk pour un entier naturel k donné, la fonction caractéristique χA peut aussi se définir selon un schéma récursif. On peut alors étendre la propriété d’être primitif récursif aux fonctions caractéristiques et donc à tout sous-ensemble A ⊆ Nk pour un entier naturel k donné.
Un ensemble A ⊆ Nk pour un entier naturel k donné est primitif récursif si, et seulement si sa fonction caractéristique χA : Nk → {0, 1} est primitive récursive.
Définition 20 (Ensemble primitif récursif).
50


Définissons d’abord la fonction sg : N → {0, 1} par :
sg(m) =
{ 0 si m = 0 1 sinon
Cette fonction est primitive récursive. En effet, on peut la définir par récurrence de la façon suivante :
sg(0) = 0 sg(m + 1) = 1
Ce qui peut se définir par :
sg(0) = cste00
sg(m + 1) = cste11(π21(m, sg(m)))
où cstej
i : Nj → N pour i, j ∈ N est la fonction constante qui à tout tuple (n1, . . . , nj) fait correspondre l’entier i. À partir de cette fonction, on peut alors montrer que le sous-ensemble A ⊆ N × N défini par :
(n, m) ∈ A ⇐⇒ n < m
est primitif récursif. En effet, on peut définir sa fonction caractéristique par le schéma suivant :
χA(n, m) = sg(m − n)
Or, nous avons monté ci-dessus qu’à la fois les fonctions sg et la soustraction étaient primitives récursives.
Exemple 29.
6.3. Les fonctions récursives
Récursion primitive et algorithmique
Comme nous l’avons déjà dit plus haut, d’un point de vue de la programmation séquentielle, les fonctions et les ensembles primitifs récursifs se caractérisent exactement par des programmes dont la seule structure d’itération est la boucle finie. Ceci se voit bien par les exemples de fonctions récursives primitives qui montrent que l’ensemble E défini dans la définition 19 est stable par certaines opérations faisant intervenir des ensembles bornés. Ainsi, si g : Nk+1 → N est une fonction primitive récursive, alors la fonction f : Nk+1 → N définie par :
f (x1, . . . , xk, n) =
n
∑
i=0
g(x1, . . . , xk, i)
est primitive récursive. En effet, nous avons :
f (x1, . . . , xk, 0) = g(x1, . . . , xk, 0) f (x1, . . . , xk, m + 1) = f (x1, . . . , xk, m) + g(x1, . . . , xk, m + 1)
51


En utilisant la forme stricte de la définition de la primitive récursion, on écrirait :
f (x1, . . . , xk, m + 1) = h(x1, . . . , xk, m, f (x1, . . . , xk, m))
avec h(~n) = g(π1
k+2(~n), . . . , πk
k+2(~n)), s(πk+1
k+2(~n))) + πk+2
k+2(~n) où ~n = (n1, . . . , nk, nk+1, nk+2)
Il en est de même pour la quantification bornée. La quantification bornée universelle (resp. existentielle) permet de préciser qu’une propriété (définie par sa fonction caractéristique) est vraie pour toutes les (resp. pour certaines) valeurs d’un de ses arguments inférieures à une certaine borne. Précisément,
∀i ≤ m, χ(~x, i)
est vrai si χ(~x, i) = 1 pour tout i ≤ m. Si la fonction χ : Nk+1 → {0, 1} est primitive récursive, alors la fonction caractéristique de ∀i ≤ m, χ : Nk+1 → {0, 1} l’est aussi. En effet, la fonction caractéristique de ∀i ≤ m, χ(~x, i) est définie par :
m
∏
i=1
χ(~x, i)
qui vaut bien 1 quand tous les χ(~x, i) = 1 pour i ≤ m. Nous avons déjà montré que la multiplication est primitive récursive, et en suivant la même démarche que pour la somme bornée ci-dessus, il est très facile de montrer que le produit borné est aussi primitif récursif, ce qui nous
permet de déduire que
m
∏
i=1
χ(~x, i) l’est aussi.
La dernière opération que nous allons voir pour laquelle l’ensemble des fonctions primitives récursives est aussi stable, est l’opération dite de minimisation bornée. La minimisation bornée sera utilisée pour représenter les programmes effectuant une recherche sur une structure bornée mais dont on peut arréter la recherche dès que l’on a trouvé le premier élément satisfaisant la condition d’arrêt, et donc sans parcourir la structure complète. Un exemple est la recherche d’un élément tel qu’un entier, dans un tableau trié. Ainsi, la minimisation bornée définira les programmes utilisant une boucle while dont la condition d’arrêt porte sur une structure de donnée dont on connait la taille. Cette opération est importante car c’est en retirant la borne dans le schéma de mnimisation que nous pourrons définir exactement ce qui peut être calculable par un ordinateur. La minimisation bornée est le moyen de rechercher le plus petit élément d’un ensemble A ⊆ Nk+1. Cette fonction notée souvent μA
≤ : Nk+1 → N se définit par : Soit A un ensemble primitif récursif de Nk+1,
μA
≤(x1, . . . , xk, n) = 0 s’il n’existe pas de p ≤ n tel que (x1, . . . , xk, p) ∈ A μA
≤(x1, . . . , xk, n) = p si p ≤ n est le plus petit entier tel que (x1, . . . , xk, p) ∈ A
Pour démontrer que cette fonction est primitive récursive, nous allons utiliser la définition par cas qui peut être vue comme une conditionnelle imbriquée classique dans les langages de programmation. La définition d’une fonction f : Nk → N par cas est la suivante : Soit g1, . . . , gn, gn+1 des fonctions de Nk dans N et χ1, . . . , χn des fonctions caractéristiques de sousensembles de Nk (les conditions)
52


f (~x) =

       
       
g1(~x) si χ1(~x) = 1
...
gn(~x) si
∧
i≤n−1
χi(~x) = 0 et χn(~x) = 1
gn+1(~x) si
∧
i≤n
χi(~x) = 0
Si les fonctions g1, . . . , gn, gn+1 et χ1, . . . , χn sont primitives récursives alors f l’est aussi. En effet, on a
f (~x) =

                
                
g1(~x) × χ1(~x)+ ...+
gi(~x) × (χi(~x) −
i−1
∑
j=1
χj (~x))+
...+
gn(~x) × (χn(~x) −
n−1
∑
j=1
χj (~x))+
gn+1(~x) × (1 −
n
∑
j=1
χj (~x))
À partir de là, on peut montrer que le schéma de minimisation bornée est primitive récursive. En effet, on a la définition par récurrence suivante de μA
≤:
μA
≤(x1, . . . , xk, 0) = 0
μA
≤(x1, . . . , xk, n + 1) =

      
      
μA
≤(x1, . . . , xk, n) si
n
∑
i=1
χA(x1, . . . , xk, i) ≥ 1
n + 1 si
n
∑
i=1
χA(x1, . . . , xk, i) = 0 et χA(x1, . . . , xk, n + 1) = 1
0 pour tous les autres cas
Comme nous l’avons déjà vu précedemment, l’ensemble des fonctions numériques (i.e. définies sur les entiers) n’est pas primitive récursive. La raison est que la cardinalité de l’ensemble des fonctions primitives récursives du fait de leur définition inductive à partir d’un ensemble fini d’opérations de base et de schémas de clôture, est nécessairement dénombrable, mais la cardinalité de l’ensemble des fonctions numériques n’est pas dénombrables (une simple application du théorème de Cantor). Maintenant, la question que l’on peut se poser est "existe-t-il des fonctions que l’on sait calculer (par un programme informatique) mais qui ne seraient pas primitives récursives ?". Malheureusement, la réponse à cette question est "oui" comme le prouve le théorème suivant :
Il existe des fonctions calculables qui ne sont pas primitives récursives.
Théorème 30.
53


Démonstration Pour démontrer ce théorème, nous allons utiliser la méthode de la diagonale introduite pas Cantor pour montrer que les réels étaient en nombre plus grand que les entiers, et utilisée après par Turing et Godel pour démontrer l’existence de problèmes non calculables. Comme l’ensemble des fonctions primitives récursives est de cardinalité dénombrable, nous pouvons ordonner/énumérer ses éléments. On peut en effet représenter les fonctions primitives récursives sous formes de chaînes de caractères dans un alphabet bien choisi et utiliser le codage que nous avons défini précédement et dont nous avons vu qu’il était lui même primitive récursive, en ayant au préalable numéroté les lettres de l’alphabet. Soit donc f0, f1, . . . , fn, . . . une telle énumération. Par définition, le nombre d’arguments de ces fonctions varie. Avec le codage des suites d’entiers que nous avons donné dans la section 26, nous pouvons sans perte de généralité, nous restreindre au cas des fonctions de N dans N. Considérons alors le tableau suivant :
A 0 1 2 ... j ... f0 f0(0) f0(1) f0(2) . . . f0(j) . . . f1 f1(0) f1(1) f1(2) . . . f1(j) . . . f2 f2(0) f2(1) f2(2) . . . f2(j) . . .
... ... ... ... . . . ...
fi fi(0) fi(1) fi(2) . . . fi(j) . . .
... ... ... ... ... . . .
Chaque case A(i, j) de ce tableau contient un entier naturel qui est fi(j). Définissons alors la nouvelle fonction g : N → N de la façon suivante :
g(n) = fn(n) + 1 = A(n, n) + 1
Cette fonction ne peut pas être primitive récursive sinon elle serait identique à une des fonctions fk apparaissant dans le tableau A ci-dessus ce qui n’est pas possible. En effet, si ceci se pouvait, g(k) devrait être égale à fk(k) mais de par sa définition, g(k) = fk(k) + 1. Ainsi, g ne peut pas apparaître dans le tableau A et donc n’est pas primitive récursive. Maintenant, la fonction g est calculable. En effet, pour calculer g(n) on peut procéder ainsi :
1. On énumère les fonctions primitives récursives jusqu’à fn. Ceci peut se faire algorithmiquement en énumérant selon le codage utilisé pour représenter les fonctions primitives récursives jusqu’à la fonction recherchée.
2. Une fois la fonction fn obtenue, on évalue fn. Ce qui est calculable puisque fn est primitive récursive.
3. Finalement, on évalue fn(n) + 1. Ici aussi, ceci est calculable car fn(n) + 1 = s(fn(n)).
Ce théorème montre qu’il existe plus de fonctions que l’on sait calculer que de fonctions primitives récursives. Maintenant, on peut se demander s’il existe vraiment de telles fonctions ou bien le résultat ci-dessus n’est que purement théorique. En fait, il en existe que l’on sait construire. La première a été définie par W. Ackermann et fait l’objet de l’exercice suivant.
Minimisation non bornée
Avec le recul que nous avons des langages de programmation, nous savons bien que les boucles For ou plus généralement des boucles dont on connaît la borne d’arrêt, sont insuffisantes pour décrire l’ensemble des algorithmes, et beaucoup de programmes utilisent d’autres boucles non
54


bornées ou enore bornées par des conditions (utilisation de la boucle While) que nous n’arrivons pas toujours à satisfaire ce qui fait qu’ils ne terminent pas. Que faut-il alors pour prendre en compte l’ensemble des fonctions calculables ? Il nous faut le moyen de pouvoir parcourir l’ensemble des entiers jusqu’à trouver le plus petit élément satisfaisant une propriété P sans pour cela imposer à chaque fois un segment de N (i.e. une borne n). Le schéma de minimisation tel qu’il a été présenté dans la section précédente mais en retirant la borne n permettrait de caractériser un tel schéma. Maintenant, l’élément recherché peut ne pas exister. Comment prendre en compte un tel cas ? En considérant des fonctions partielles, c’est-à-dire pas nécessairement définies partout.
Une fonction partielle de Np dans N est un couple (A, f ) tel que A ⊆ Np et f : A → N est une application. A est appelé le domaine de définition de f . On notera Fp∗
l’ensemble des fonctions partielles de Np dans N.
Définition 21 (Fonctions partielles).
Étendons cette notion de fonctions partielles aux schémas de clôture (i.e. la composition et la définition par récurrence).
Soient f1, f2, . . . , fn ∈ Fp∗ et g ∈ Fn∗. La fonction composée h = g(f1, . . . , fn) est
l’élément de Fp∗ définie par : — h(x1, . . . , xp) n’est pas définie si l’une des fi(x1, . . . , xp)n’est pas définie ou bien si toutes le sont, g(f1(x1, . . . , xp), . . . , fn(x1, . . . , xp)) n’est pas définie. — Dans le cas contraire, h(x1, . . . , xp) est définie et est égale à g(f1(x1, . . . , xp), . . . , fn(x1, . . . , xp)).
Définition 22 (Composition partielle).
Soient g : Fp∗ et h ∈ Fp∗+2. Alors, la fonction f ∈ Fp∗+1 est définie pour tout
(x1, . . . , xp) ∈ Np et tout y ∈ N par :
1. f (x1, . . . , xp, 0) = g(x1, . . . , xp) si g(x1, . . . , xp) est définie, sinon f (x1, . . . , xp, 0) est indéfinie.
2. f (x1, . . . , xp, y + 1) = h(x1, . . . , xp, y, f (x1, . . . , xp, y)) si h(x1, . . . , xp, y, f (x1, . . . , xp, y)) est définie, sinon f (x1, . . . , xp, y + 1) est indéfinie.
La fonction f est définie par récurrence à partir de g et h.
Définition 23 (Récurrence partielle).
55


Soit A ⊆ Np+1. La fonction de minimisation non bornée μA ∈ Fp∗ est définie pour tout (x1, . . . , xp) par : — μA(x1, . . . , xp) = k si k est le plus petit entier tel que (x1, . . . , xp, k) ∈ A — μA(x1, . . . , xp) est indéfinie sinon.
Définition 24 (Minimisation non bornée).
On peut maintenant définir les fonctions récursives.
L’ensemble des fonctions récursives est le plus petit sous-ensemble de F∗ =
⋃
p∈N
F∗
p
qui : — contient toutes les fonctions (totales) constantes, les projections et la fonction successeur, — est clos pour la composition, les définitions par récurrence partielles et la minimisation non bornée. Un ensemble A ⊆ Np est récursif si, et seulement si sa fonction caractéristique χA est récursive.
Définition 25 (Fonctions récursives).
56


Reprenons l’exemple de la fonction d’Ackermann. Ce qui fait que cette fonction n’est pas primitive récursive est qu’elle se définit par une double récurrence sur chacun de ses deux arguments et dont on ne peut ramener la condition d’arrêt sur une simple équation sur les entiers en entrée de la fonction. En fait, on peut montrer que cette fonction termine mais avec assez de difficulté ce qui n’est jamais le cas avec les fonctions primitives récursives. Dans un précédent exercice, nous avons montré que la fonction d’Ackermann n’était pas primitive récursive. Cependant, est-elle récursive ? On peut répondre par l’affirmative à cette question. En effet, si nous considérons le graphe de chacune des fonctions An pour tout n ∈ N défini par :
A0 = {(x, y)|y = 2x} An+1 = {(0, 1)} ∪ {(x, y)|∃y′ ∈ N, (x − 1, y′) ∈ An+1 et (y′, y) ∈ An}
nous voyons bien dans la définition de l’ensemble An+1 l’utilisation de la minisation non bornée. En effet, nous avons :
χA0(x, y) =
{ 1 si y = 2x 0 sinon
χAn+1 (0, y) =
{ 1 si y = 1 0 sinon
χAn+1 (x + 1, y) = χAn (μAn+1 (x), y)
la fonction 2x ainsi que l’égalité sont primitives récursives. La définition ci-dessus de la fonction d’Ackermann est donc bien récursive.
Exemple 31.
6.4. Un interpréteur de programmes
Une question intéressante est de se demander s’il existe une fonction récursive qui peut simuler/interpréter n’importe quelle fonction récursive. Précisément, on voudrait une fonction récursive Int à laquelle on fournirait la description d’une fonction quelconque f de même qu’un tuple de valeurs intiales pour f et qui simulerait l’exécution de f sur ce tuple de valeurs intiales. De telles fonctions existent et sont appelées des fonctions récursives universelles ou interpréteur. Une fonction récursive f par définition, est la composition récursive des opérations de composition, schéma de récurrence et minimisation non bornée à partir des fonctions de bases (constantes, projections, et successeur). Ceci définit un arbre caractérisant le progamme définissant le comportement de la fonction f . Formellement, les programmes sont définis de la façon suivante :
57


L’ensemble des programme d’arité p ∈ N est le plus petit ensemble inductivement défini par : — (0, p) est un programme d’arité p. Ce programme représente la fonction qui à tout tuple (x1, . . . , xp) de Np associe 0. C’est la seule fonction constante que nous considérons, les autres fonctions constantes pouvant toujours être obtenues avec la fonction successeur par composition. — s est un programme d’arité 1. — pour tout i ≤ p ∈ N, (π, i, p) est un programme d’arité p. — si f1, . . . , fn sont des programmes d’arité p et h est un programme d’arité n, alors h(f1, . . . , fn) est un programme d’arité p. — si g est un programme d’arité p et h est un programme d’arité p+2, alors rec(g, h) est un programme d’arité p + 1. — si χ est un programme d’arité p + 1 à valeurs d’arrivée dans {0, 1}, alors μ(χ) est un programme d’arité p + 1. L’ensemble des programmes contient tous les programmes d’arité p pour tout p ∈ N. Une exécution est tout terme de la forme App(f, ~x) où f est un programme d’arité p et ~x ∈ Np.
Définition 26 (Programmes).
Pour pouvoir définir une fonction qui interprèterait tous ces programmes, nous devons tout d’abord associer un entier à chacun d’eux. Les programmes pouvant être représentés par des arbres (de part leur définition récursive), nous pouvons aussi les représenter sous la forme d’une chaîne de caractères qui représente la notation fonctionnelle du programme sous sa forme préfixe et bien parenthésée. Ainsi, chaque caractère est soit le nom d’une fonction de base (constante, projections, successeur), une opération de clôture (composition, récurrence, minimisation), une parenthèse ouvrante ou fermante, ou la virgule. En associant un entier unique à ces différents caractères, on peut alors, par le codage que nous avons défini dans la section 26, associer un entier unique à chacune des chaînes représentant nos programmes et nos exécutions. Pour les exécutions, nous aurons pris soin de représenter tout entier x par le terme s(. . . (s((0, 0)) . . .) avec x occurrences du symbole s. Dans la suite, nous représenterons le code associé à un programme f (resp. une exécution App(f, ~x)) par f (resp. App(f, ~x)). On peut maintenant définir notre interpréteur. Ce dernier a pour fonction d’interpréter des exécutions de la forme App(f, ~x). L’ensemble des programmes suit une définition inductive non ambigue. Nous pouvons alors au codage près, définir notre interpréteur comme une fonction récursive sur la structures des programmes.
58


On définit la fonction Int : N → N par cas de la façon suivante : supposons que x = App(f, x1, . . . , xp)
— Si f est (0, p), alors Int(x) = 0. — Si f est s, alors Int(x) = x1 + 1. — Si f est (π, i, p), alors Int(x) = xi.
— Si f est h(f1, . . . , fn), alors si pour chaque i, 1 ≤ i ≤ n, Int(App(f1, x1, . . . , xp))
est défini et Int(App(h, (Int(App(f1, x1, . . . , xp)), . . . , Int(App(fn, x1, . . . , xp)))) est aussi défini
Int(x) = Int(App(h, (Int(App(f1, x1, . . . , xp)), . . . , Int(App(fn, x1, . . . , xp))))
Int(x) est indéfini sinon. — Si f est rec(g, h), alors
Int(x) =

       
       
Int(App(g, x1, . . . , xp−1)) si xp = 0 et Int(App(g, x1, . . . , xp−1)) est défini Int(App(h, (x1, . . . , xp, Int(App(f, x1, . . . , xp − 1))) si xp 6= 0,
et Int(App(h, x1, . . . , xp − 1, Int(App(f, x1, . . . , xp − 1))) est défini Indéfini sinon
— si f est μ(χ), alors — Int(App((μ(χ)), x1, . . . , xp)) = k si k est le plus petit entier tel que
Int(App(χ, x1, . . . , xp, k)) = 1 — Int(App((μ(χ)), x1, . . . , xp)) est indéfinie sinon.
Définition 27 (Interpréteur).
Int est une fonction récursive.
Théorème 32.
Démonstration Int est défini par cas à partir de la fonction récursive α−1 et par composition à partir d’elle-même. Enfin, elle est bien partielle car si Int(x) n’est pas un entier, alors Int n’est pas définie : l’interpréteur ne termine pas pour un programme qui ne termine pas.
Indécidabilité du problème de l’arrêt
Dans les sections précédentes, la notion de procédure effective a été définie en termes de problèmes portant sur les entiers pour lesquels nous pouvions définir des fonctions récursives (une par problème) calculant une solution à ces problèmes. Avec ces éléments, il est possible de démontrer que certains problèmes ne sont pas solubles par une procédure effective. Parmi ces problèmes non solubles effectivement, un particulièrement intéressant pour la programmation en particulier et l’informatique en général, est celui du problème de l’arrêt, c’est-à-dire de savoir s’il existe une fonction récursive qui est capable de décider pour toute fonction récursive et tout tuple d’entiers si cette fonction est définie ou non sur ce tuple. Pour démontrer qu’il n’existe pas
59


une telle fonction, nous allons encore utiliser un raisonnement par diagonalisation à la Cantor à partir de notre interpréteur défini dans la section précédente.
Un programme f d’arité p est dit terminant si, et seulement si pour tout tuple ~x ∈ Np, Int(App(f, ~x)) ∈ N.
Définition 28 (Programme terminant).
Soit A un sous-ensemble récursif de l’ensemble des programmes qui terminent toujours. Alors, il existe une fonction récursive totale qui n’est représentée par aucun programme de A.
Proposition 33.
Démonstration Soit G : N × N → N la fonction récurisive suivante : pour tout programme f d’arité p de A et tout entier n ∈ N,
G(f , n) = Int(App(f, ~x))
où ~x = n.
La fonction G est trivialement récursive et totale puisque définie directement à partir d’une fonction dans A. De plus, par définition, c’est un interpréteur des programmes de A. Définissons alors la fonction G′ par : G′(n) = G(n, n)+1. Par définition, G′ : N → N est une fonction récursive totale car G est une fonction récursive totale. Supposons qu’il y ait dans A un programme codé par l’entier x qui représente la fonction G′. On a alors G(x, k) = G′(k) = G(k, k) + 1. En particulier, pour k = x, on a G(x, x) = G′(x) = G(x, x) + 1 ce qui n’est pas possible.
Ce que montre la preuve ci-dessus, est qu’un langage de programmation qui ne permettrait d’exprimer que des programmes qui terminent est toujours incomplet, car il ne permet pas d’exprimer son propre interpréteur. C’est l’exemple des langages impératifs formés de la déclaration de variables, de l’affectation, du test et de la boucle for. De la proposition ci-dessus, on a comme corollaire l’indécidabilité du problème de l’arrêt.
L’ensemble des programmes terminant n’est pas récursif.
Théorème 34 (Indécidabilité du problème de l’arrêt).
Démonstration En appliquant la proposition précédente, si l’ensemble des programmes terminant était récursif, il existerait par la proposition ci-dessus, une fonction récursive totale (et donc qui termine) mais qui ne serait représentée par aucun des programmes de cet ensemble, ce qui est une contradiction.
60


6.5. Thèse de Church
Il est clair que toute fonction récursive est calculable (i.e. implémentable par un algorithme). Mais qu’en est-il de l’inverse ? La réponse affirmative à cette question est la thése de Church. Cette affirmation ne se prête pas à une démonstration parce que nous n’avons pas de définition précise à la notion d’algorithme. Cependant, à ce jour, on ne connaît aucun contre-exemple à la thèse de Church. À chaque fois que l’on connaît une fonction dont on avait l’intuition qu’elle est calculable, on a réussi à le prouver. De plus, il existe d’autres formalisations de la calculabilité (λ-calcul, machine à registre, etc.) et toutes ont été montrées équivalentes. Ceci milite fortement en faveur de la thèse de Church. Néanmoins, l’échec de la permière tentative de formalisation de la calculabilité, celles des fonctions primitives récursives, doit nous rendre prudents.
61




7. Les machines de Turing
7.1. Définitions
Les fonctions récursives ne constituent pas un modèle très convaincant de la calculabilité. Elles manquent d’un aspect calculatoire naturel, c’est-à-dire effectuer un calcul pas à pas jusqu’à obtenir le résultat recherché. 1 Pour répondre à ce manque, A. Turing proposa en 1936 un nouveau modèle de calcul appelé les machines de Turing. Les machines de Turing sont un exemple d’automates (que l’on étudiera plus précisément dans la troisième partie de ce document) qui précèdent d’une dizaine d’années les machines de von Neumann. La définition d’une machine de Turing est la suivante :
Une machine de Turing est la donnée de trois ensembles finis (Σ, Q, R) où : on se donne un symbole — Σ est l’alphabet de la machine. On suppose que 6∈Σ. — Q est l’ensemble fini des états de la machine. — R est l’ensemble fini des transitions de la machine. Chaque transition se définit par un 5-uplets (q, s, q′, s′, d) où q, q′ ∈ Q, s, s′ ∈ Σ ∪ { }, et d ∈ {0, +, −}. On
note la transition q s/s′,d
−→ q′.
Définition 29 (Machines de Turing).
Un programme manipule des chaînes de symboles pour les transformer selon des règles de transformation et ce afin de rendre un résultat lui-même représenté par une chaîne de symboles. C’est justement ce que permet de traduire l’ensemble des éléments d’une machine de Turing. Ainsi, une machine de Turing peut se voir comme un automate et donc fonctionne par transitions entre configurations, à partir d’une configuration initiale, vers éventuellement une configuration terminale. Les configurations d’une machine contiennent plus d’information qu’un mot de (Σ ∪ { })∗. Elles contiennent en plus : — une mémoire de taille infinie sous forme d’un ruban divisé en cases. Chaque case contiendra un symbole de Σ ∪ { }. — la position de la tête de lecture. — le mot écrit sur le ruban, élément de (Σ ∪ { })∗ en ommettant les préfixes et suffixes. L’ensemble des règles R définit alors le programme, c’est-à-dire le traitement permettant à la fois de transformer les configurations de la machine en remplaçant le symbole s par s′ et laissant le pointeur sur place (d = 0), ou alors en le déplaçant à droite (d = +) ou à gauche (d = −) à partir de la position courante. Ainsi, une règle s’applique que si dans l’état q, la configuration contient à la position courante de la tête de lecture, le symbole s.
1. Dans le chapître précédent, on a obtenu ce calcul pas à pas mais de façon détournée au travers de la définition de notre interpréteur.
63


Les machines de Turing dérivent alors par réécriture de symbole selon les règles dans R, des configurations. Une configuration sera une configuration terminale quand aucune des règles de transformation de R n’est applicable pour cette configuration. Pour représenter plus formellement cette notion de dérivation, nous devons tout d’abord définir formellement la notion de configuration.
Soit M = (Σ, Q, R) une machine de Turing. Une configuration pour M est un mot de la forme αqβ où q ∈ Q, et α, β ∈ (Σ ∪ { })∗, avec α (resp. β) ne commençant (resp. ne finissant) pas par . On appellera configuration intiale tout mot de la forme qα. Dans la suite, on supposera toujours une configuration initiale pour chaque machine de Turing considérée.
Définition 30 (Configuration).
La configuration αqβ exprime que le contenu de la bande est le mot αβ, que l’état courant est q et que la tête de lecture pointe sur le début de β.
Une étape de dérivation pour M est le triplet (C, ρ, C′) tel que — C = αs′′qsβ et C′ = α′q′β′ sont des configurations pour M, — ρ = (q, s, q′, s′, d) ∈ R, et
— les mots α′ et β′ sont définis comme suit : — α′ = αs′′ et β′ = s′β si d = 0 — α′ = α et β′ = s′′s′β si d = − — α′ = αs′′s′ et β′ = β si d = +
On la note C →ρ C′.
Définition 31 (Étape de dérivation).
Une dérivation pour M est une suite d’étapes de dérivation soit infinie, soit finie et dans ce cas-là se terminant par une configuration d’arrêt. On la note C0
ρ→1 C1
ρ→2
. . . ρ→p Cp. La suite ρ1ρ2 . . . ρp est la trace du calcul.
Définition 32 (Dérivation et calcul).
64


On se propose de donner la machine de Turing qui permet de calculer le successeur d’un entier. Tout d’abord, nous devons représenter les entiers. Pour cela, nous allons utiliser la représentation unaire des entiers a, i.e. celle qui représente les entiers par des "bâtons", autant que l’entier désigné. Nous supposerons alors l’alphabet singleton Σ = {|}. Ainsi, pour représenter le nombre x, on aura sur les cases 1, 2, . . . , x des bâtons, et enfin le symbole sur les autres cases. La configuration initiale est donc notée q || . . . | . . . |
} {{ }
n fois
.
La machine de Turing permettant de calculer le successeur de tout nombre est défini par l’automate :
q qf
|/|, +
/|, +
a. qui est la représentation la plus abstraite car contenant leur définition inductive à partir de la fonction successeur.
Exemple 35.
7.2. Les fonctions T-calculables
Là encore, les seules fonctions qu’une machine de Turing peut calculer sont les fonctions numériques, c’est-à-dire celles dont les domaines sont des tuples d’entiers. Bien sûr, pour qu’une machine de Turing puisse calculer la valeur d’une fonction f : Nk → N il faut évidemment coder les valeurs des tuples (x1, . . . , xk) afin qu’ils puissent être lus par la machine de Turing. Pour cela, nous allons étendre la représentation des entiers sous forme unaire que nous avons donnée dans l’exemple précédent. Il faut maintenant séparer ces différents entiers mis sous forme unaire afin de représenter les tupes. On se propose alors de les séparer par une virgule. Le tuple (x1, . . . , xk) sera donc défini par la configuration
q |...|
} {{ }
x1 fois
,..., |...|
} {{ }
xk fois
Une fonction partielle f : Nk → N est dite T-calculable si, et seulement s’il existe une machine de Turing construite sur l’alphabet Σ = {|,′′ ,′′ } qui à partir de toute configuration initiale q0 | . . . |
} {{ }
x1 fois
,..., |...|
} {{ }
xk fois
a pour comportement :
— de ne jamais s’arrêter si f (x1, . . . , xk) n’est pas défini, — ou bien de rendre la configuration qf | . . . |
} {{ }
y fois
en un temps fini telle que
f (x1, . . . , xk) = y.
Définition 33 (Fonction T-calculable).
65


La question naturelle que l’on se pose est de savoir si les fonctions récursives que nous avons étudiées dans le chapître précédent définissent la même notion de calculabilté que les machines de Turing. La réponse à cette question a été apportée par A. Turing lui-même au travers des deux théorèmes suivants :
Toute fonction récursive est calculable par une machine de Turing ou T-calulable.
Théorème 36.
Démonstration : Pour démontrer ce théorème, nous devons alors tout d’abord montrer que les fonctions de base (i.e. les fonctions constantes, les projections et la fonction successeur) sont Tcalculables. Puis montrer que les fonctions T-calculables sont stables par composition, récurrence et minimisation non bornées. Dans l’exemple 35 et l’exercice ?? ci-dessus, nous avons déjà montré que les fonctions de base sont T -calculables. Il nous reste alors à montrer que l’ensemble des fonctions T-calculables sont stables par la composition, le schéma de récurrence et la minimisation non bornée. Les machines de Turing associées à chacune des opérations de clôture sont lourdes de notations dans leur définition. Nous ne donnerons ici que les grandes étapes de leur description. — Commençons par montrer la clôture des fonctions T-calculables pour la composition. Soient donc f1, f2, . . . , fn ∈ F ∗
k et g ∈ Fn∗ telles que toutes ces fonctions soient T-calculables. On sait alors que pour tout i, 1 ≤ i ≤ n, il existe une machine de Turing Mi qui calcule fi. De même, on sait qu’il existe une machine de Turing N qui calcule g. On peut alors construire la machine de Turing M de la façon suivante : la configuration initiale de la machine M est q0 | . . . |
} {{ }
x1 fois
,..., |...|
} {{ }
xk fois
1. on commence par réécrire la configuration q0 | . . . |
} {{ }
x1 fois
,..., |...|
} {{ }
xk fois
en la configuration
|...|
} {{ }
x1 fois
,..., |...|
} {{ }
xk fois
|...|
} {{ }
x1 fois
,..., |...|
} {{ }
xk fois
q1
2. On revient en arrière jusqu’au 1er symbole rencontré. On a alors la configuration
|...|
} {{ }
x1 fois
,..., |...|
} {{ }
xk fois
q′
1 |...|
} {{ }
x1 fois
,..., |...|
} {{ }
xk fois
3. On applique l’étape ci-dessus n-1 fois pour obtenir la configuration
|...|
} {{ }
x1 fois
,..., |...|
} {{ }
xk fois
... |...|
} {{ }
x1 fois
,..., |...|
} {{ }
xk fois
} {{ }
n fois
qn−1
4. on revient en arrière jusqu’au 1er symbole ” ” que l’on remplace par #, et on applique les règles de la machine Mn. À cette étape, on a alors la configuration
|...|
} {{ }
x1 fois
,..., |...|
} {{ }
xk fois
... |...|
} {{ }
x1 fois
,..., |...|
} {{ }
xk fois
} {{ }
n−1 fois
# |...|
} {{ }
fn(~x) fois
qfMn
5. on revient en arrière jusqu’au 1er symbole rencontré que l’on remplace par #, pour obtenir la configuration
66


|...|
} {{ }
x1 fois
,..., |...|
} {{ }
xk fois
... |...|
} {{ }
x1 fois
,..., |...|
} {{ }
xk fois
} {{ }
n−2 fois
#q0Mn−1 | . . . |
} {{ }
x1 fois
,..., |...|
} {{ }
xk fois
# |...|
} {{ }
fn(~x) fois
6. on applique les règles de la machine Mn−1. À cette étape, on a alors la configuration
|...|
} {{ }
x1 fois
,..., |...|
} {{ }
xk fois
... |...|
} {{ }
x1 fois
,..., |...|
} {{ }
xk fois
} {{ }
n−3 fois
# |...|
} {{ }
fn−1(~x) fois
qfMn−1 # | . . . |
} {{ }
fn(~x) fois
7. On applique ce processus n fois jusqu’à obtenir la configuration | . . . |
} {{ }
f1(~x) fois
qfM1 # . . . # | . . . |
} {{ }
fn(~x) fois
8. On remplace tous les symboles # par virgule et on revient en arrière jusqu’au 1er symbole rencontré. On obtient alors la configuration q0N | . . . |
} {{ }
f1(~x) fois
,..., |...|
} {{ }
fn(~x) fois
9. On applique alors les règle de la machine N pour aboutir à la configuration | . . . |
} {{ }
g(f1(~x),...,fn(~x)) fois
qfN
où qfN devient l’état final de la machine M.
— Montrons maintenant la clôture des fonctions T-calculables pour la récurrence. Soient donc g ∈ Fp∗ et h ∈ Fp∗+2 telles que ces deux fonctions soient T-calculables. On sait alors qu’il existe une machine de Turing Mg qui calcule g. De même, on sait qu’il existe une machine de Turing Mh qui calcule h. On peut alors construire la machine de Turing M de la façon suivante : la configuration initiale de la machine M est q0 | . . . |
} {{ }
x1 fois
,..., |...|
} {{ }
xp fois
, |...|
} {{ }
y fois
1. On commence par réécrire la configuration initiale en la configuration
| ···
}{{}
x1
|, · · · , | · · ·
}{{}
xp
|, | · · ·
}{{}
y−1
| q1
2. On revient en arrière pour aboutir à la configuration :
q1| · · ·
}{{}
x1
|, · · · , | · · ·
}{{}
xp
|, | · · ·
}{{}
y−1
|
3. On réécrit la configuration précédente en la configuration :
| ···
}{{}
x1
|, · · · , | · · ·
}{{}
xp
|, | · · ·
}{{}
y−1
| | ···
}{{}
x1
|, · · · , | · · ·
}{{}
xp
|, | · · ·
}{{}
y−2
|q2
4. On revient en arrière jusqu’à rencontrer le premier . On aboutit à la configuration :
| ···
}{{}
x1
|, · · · , | · · ·
}{{}
xp
|, | · · ·
}{{}
y−1
| q1| · · ·
}{{}
x1
|, · · · , | · · ·
}{{}
xp
|, | · · ·
}{{}
y−2
|
5. On applique les étapes 3 et 4 jusqu’à obtenir la configuration :
| ···
}{{}
x1
|, · · · , | · · ·
}{{}
xp
|, | · · ·
}{{}
y−1
| ··· | ···
}{{}
x1
|, · · · , | · · ·
}{{}
xp
|, | · · ·
}{{}
x1
|, · · · , | · · ·
}{{}
xp
|q2
6. On revient en arrière jusqu’au premier symbole pour obtenir la configuration
| ···
}{{}
x1
|, · · · , | · · ·
}{{}
xp
|, | · · ·
}{{}
y−1
| ··· | ···
}{{}
x1
|, · · · , | · · ·
}{{}
xp
|, q0Mg | · · ·
}{{}
x1
|, · · · , | · · ·
}{{}
xp
|
67


7. On applique les règles de Mg. On obtient
| ···
}{{}
x1
|, · · · , | · · ·
}{{}
xp
|, | · · ·
}{{}
y−1
| ··· | ···
}{{}
x1
|, · · · , | · · ·
}{{}
xp
|, | · · ·
}{{}
g(~x)
|qfMg
8. On revient en arrière jusqu’au premier que l’on remplace par ,. On aboutit à la configuration :
| ···
}{{}
x1
|, · · · , | · · ·
}{{}
xp
|, | · · ·
}{{}
y−1
| ··· | ···
}{{}
x1
|, · · · , | · · ·
}{{}
xp
|, q3, | · · ·
}{{}
g(~x)
|
9. On revient en arrière jusqu’au premier rencontré. On a la configuration :
| ···
}{{}
x1
|, · · · , | · · ·
}{{}
xp
|, | · · ·
}{{}
y−1
| · · · q0Mh | · · ·
}{{}
x1
|, · · · , | · · ·
}{{}
xp
|, , | · · ·
}{{}
g(~x)
|
10. On applique les règles de Mh. On obtient
| ···
}{{}
x1
|, · · · , | · · ·
}{{}
xp
|, | · · ·
}{{}
y−1
| ··· | ···
}{{}
h(~x,0,g(~x))
|qfMh
11. On réapplique l’étape 8, 9 et 10 jusqu’à obtenir la configuration :
| ···
}{{}
f (~x,y)
|qfMh
— On finit en montrant la clôture des fonctions T-calculables pour la minimisation non bornée. Soit A ⊆ Np+1 un ensemble récursif de fonction caractéristique χA : Np+1 → {0, 1}. Soit M la machine de Turing calculant χA. La machine de Turing N effectuant une minimisation non bornée sur A est définit de la façon suivante : la configuration initiale est q0 | . . . |
} {{ }
x1 fois
,..., |...|
} {{ }
xp fois
1. On commence par réécrire à la fin de la séquence, l’entier 0 représenté par une virgule suivie de rien et le symbole #. On obtient la configuration | . . . |
} {{ }
x1 fois
,..., |...|
} {{ }
xp fois
, #q1
2. On revient au début de la séquence que l’on réécrit après le symbole #. On obtient la configuration | . . . |
} {{ }
x1 fois
,..., |...|
} {{ }
xp fois
,# |...|
} {{ }
x1 fois
,..., |...|
} {{ }
xp fois
, q2
3. On se place après le symbole #, pour appliquer les transitions de M. On a alors la configuration | . . . |
} {{ }
x1 fois
,..., |...|
} {{ }
xp fois
, #q0M | . . . |
} {{ }
x1 fois
,..., |...|
} {{ }
xp fois
,
4. On applique les transitions de la MT M. On obtient alors la configuration | . . . |
} {{ }
x1 fois
,..., |...|
} {{ }
xp fois
,# 0
1 qfM
5. On revient en arrière jusqu’au symbole # pour tester si la cellule lue est 0 ou 1. Si elle contient 0, on retourne au symbole #, que l’on remplace par | et que l’on fait suivre par #. On obtient alors configuration | . . . |
} {{ }
x1 fois
,..., |...|
} {{ }
xp fois
, |#q1. On applique les étapes 2., 3., et 4.,
et on répète ce processus tant que nous n’avons pas 1 après le #. Si la cellule lue contient 1, on s’arrête et la solution est l’entier compris entre la dernière virgule et le symbole #.
68


Toute fonction T-calculable est récursive.
Théorème 37.
Démonstration : Soit M une machine de Turing calculant une fonction f : Np → N. Montrons alors qu’il existe une fonction récursive h : Np → N telle que h(x1, . . . , xp) = f (x1, . . . , xp). Pour définir la fonction h, nous allons tout d’abord définir un certain nombre de fonctions primitives récursives qui opèrent sur les configurations de M. Avant cela, nous devons d’abord associer un entier unique à chaque configuration de la machine M. Nous allons alors définir la fonction qui à partir d’une configuration c donne l’entier unique qui la représente. Pour cela, nous allons utiliser le codage gc suivant : on suppose que Q = {q1, q2, . . . , qk} est l’ensemble fini des états de la machine M où q1 (resp. qk) désigne l’état initial (resp. final) de M
— gv(qi) = i pour i = 1, . . . , k — gv( ) = k + 1 — gv(, ) = k + 2 — gv(|) = k + 3
On pose alors
gc(ω) =
l
∑
i=0
(k + 4)l−igc(ωi)
où |ω| = l et ωi désigne l’élément se trouvant à la position i dans ω commençant à la position i. La fonction gc donne alors l’entier dont la représentation en base k + 4 est la configuration ω. Ce codage est effective (i.e. injectif) dans le sens où il associe un unique entier à chaque configuration. Par contre, il existe certains entiers naturels qui ne correspondent à aucune configuration (tous ceux contenant des 0 en base k + 4), mais ce n’est pas grave car nous cherchions une représentation des configurations par des entiers, et non l’inverse.
Les fonctions que nous allons utiliser pour montrer que toutes les fonctions T-calculables sont récursives sont énumérées ci-dessous.
1. init(~x) donne l’entier correspondant par la fonction gc à la configuration initiale de la machine M pour le tuple en entrée ~x. Cette fonction se définit par : si ~x = (x1, . . . , xp) alors
init(~x) =

                 
                 
(k + 4)
p
∑
i=1
xi + (p − 1)
+(
p
∑
i=1
xi
∑
j=0
(k + 4)
(xi −j )+(
p
∑
l=i+1
xl) + (p − (i + 1))
(k + 3))
+
p
∑
i=1
(k + 4)
(
p
∑
l=i+1
xl) + (p − (i + 1))
(k + 2)
Le premier argument de init calcule la valeur en base k +4 de l’état inital q1, le second argument calcule les différentes valeurs des ”bâtons” associés à chacun des entiers xi et le dernier calcule la valeur des ”,” apparaissant dans la configuration intiale associée au tuple ~x. Bien que la fonction init peut sembler compliquée dans sa définition, elle n’est définie qu’à partir de fonctions dont on a déjà vu qu’elles étaient primitives récursives (la somme et l’exponentiation principalement). Cette fonction peut donc être considérée comme primitive récursive elle-même.
69


2. conf ig_suivant(x) a pour valeur la configuration de M, si elle existe, qui suit immédiatement la configuration x définie par un entier qui représente cette configuration par la fonction de codage gc. Si aucune configuration suit x alors conf ig_suivant(x) = 0. Ici, la fonction conf ig_suivant(x) se définit par cas à partir de la fonction caractéristique χ(x1, x2) où x1 (resp. x2) est l’entier dénotant la configuration C1 (resp. C2) qui est vraie si il existe une transi
tion ρ de M tel que gc−1(x1) →ρ gc−1(x2) et 0 sinon. Bien entendu, ceci demande au préalable d’avoir représenté les transitions comme des relations incluses dans N5 selon le codage des états et des symboles donné par la fonction gc. On peut aisément montrer que la fonction χ est primitive récursive. À l’inverse, on ne donnera pas explicitement la définition conf ig_suivant(x) à cause de la lourdeur de sa définition comme peut nous le laisser supposer la définition de la fonction init ci-dessus. L’élève intéressé est invité à se convaincre qu’elle est bien primitive récursive.
3. conf ig(x, n) est la fonction qui donne la configuration après l’exécution de n étapes à partir de la configuration x. Elle est définie par le schéma de récurrence suivant :
conf ig(x, 0) = x conf ig(x, n + 1) = conf ig_suivant(conf ig(x, n))
4. La fonction caractéristique stop qui est vraie pour la configuration x finale et faux pour toutes les autres. Elle se définit par : stop(x) = 1 − sg(conf ig_suivant(x)).
5. sortie(x) donne pour une configuration finale si elle existe, la valeur associée.
La fonction h recherchée est alors définie par :
h(~x) = sortie(conf ig(init(~x)), nb_de_pas(init(~x)))
où nb_de_pas(x) = μA(x) avec A = {(x, n)|stop(conf ig(x, n))}.
La preuve de ces deux théorèmes montrent la robustesse de la formalisation de la calculabilité même si ceci ne peut pas se démontrer rigoureusement (voir la section 6.5). Maintenant, si les machines de Turing donnent une description plus convaincante du calcul que les fonctions récursives, leur influence sur la conception des langages de programmation évolués a été très réduite. Du point de vue théorique, de nombreuses constructions et démonstrations se font via les machines de Turing. En outre, elles permettent la définition précise de nombre de "pas" et de "cases" nécessaires à un calcul, donc de complexités temporelle et spatiale d’un algorithme. Les machines de Turing ont donc été et sont à la base de la plupart des travaux sur la théorie de la complexité et la définition des classes de complexité telles que P et N P et la définition des problèmes dits N P -complets. Enfin, le modèle des machines de Turing est souvent étendu (plusieurs bandes, têtes de lecture, etc.) pour faciliter le traitement de certains problèmes ; sans que cela augmente leur capacité de calcul.
7.3. Machines de Turing universelles
Là encore, une question intéressante est de se demander s’il existe une machine de Turing qui peut simuler n’importe quelle machine de Turing. La réponse à cette question est un simple corollaire des théorèmes 32 et 36. En effet, nous avons vu que toutes les fonctions récursives étaient T-calculables et que justement l’interpréteur de tout programme était récursif et donc T-calculable. La traduction par une machine de Turing de cet interpréteur comme exprimée dans la preuve du théorème 36 est la définition d’une machine de Turing universelle. Bien entendu, par les deux théorèmes qui montrent l’équivalence entre l’ensemble des fonctions T-calculables et des fonctions récursives, le problème de l’arrêt demeure aussi un problème indécidable pour les machines de Turing.
70


7.4. Exemples de problèmes indécidables
À la suite du premier résultat d’indécidabilité sur la terminaison des programmes (qu’ils soient représentés par des fonctions récursives ou des machines de Turing ou tout autre modèle de calcul), assez rapidement d’autres problèmes ont aussi été montrés indécidables. Entre autres, par la possibilité des machines de Turing de manipuler des mots sur un alphabet sans faire de codage au préalable, des résultats d’indécidabilité dans le monde des mots ont été établis. Ces résultats ont été obtenus en réduisant un problème connu comme indécidableau au problème traité sur les mots. Définissons d’abord formellement cette notion de réduction que nous utiliserons sous une autre forme dans le chapître suivant traitant de la complexité théorique des machines de Turing.
Soient A et B deux ensembles de mots définis sur les alphabets fini Σ1 et Σ2, respectivement. Nous dirons que A est réduisible à B si, et seulement si il existe une fonction totale f : Σ∗1 → Σ∗2 telle que : — f est calculable par un algorithme. — pour tout mot α ∈ Σ∗1, α ∈ A ⇔ f (α) ∈ B. La fonction f s’appelle la réduction de A à B.
Définition 34 (Réduction).
Ainsi, s’il existe un algorithme qui décide de l’appartenance ou non d’un mot de Σ2 dans B, alors il existe aussi un algorithme qui décide de l’appartenance ou non d’un mot de Σ1 dans A. On réduit donc la décidabilité de A à celle de B. Inversement, si l’appartenance dans A est indécidable, elle l’est aussi pour B. C’est cette contraposée que l’on utilise pour démontrer l’indécidabilité d’un problème. Dans la suite, on va s’intéresser à deux problèmes sur les mots connus comme indécidables :
1. Problème du mot dans les systèmes semi-Thuien, et
2. Problème de la correspondance de Post.
Ces deux problèmes sont souvent utilisés pour démontrer que d’autres problèmes sont indécidables (e.g. indécidabilité de la validation dans la logique des prédicats du premier ordre à partir de la correspondance de Post - cf. le cours en S8 "fondements logiques de l’informatique").
Problème du mot dans les systèmes semi-Thuien
Les systèmes que nous allons voir ici sont des instances d’un problème plus général appelé le problème du mot dans les systèmes de réécriture. Les systèmes de réécriture sont l’objet de la dernière partie du cours de "fondements logiques de l’informatique" proposé en S8. Définissions ici simplement les systèmes semi-Thuien.
71


Un système semi-Thuien S est la donnée d’un alphabet fini Σ et d’un ensemble fini de couples (α, β) de mots sur Σ tels que α n’est pas le mot vide. Un tel système introduit naturellement les règles de réécriture suivantes (une par couple) : si un mot α′ peut se factoriser en α1.α.α2, alors en appliquant la règle on produit un nouveau mot β′ = α1.β.α2. On note une telle production par α′ →S β′. Ainsi, →S est une
relation binaire sur Σ∗ (i.e. →S ⊆ Σ∗ × Σ∗), et donc on note ∗→S sa fermeture réflexive et transitive.
Définition 35 (Système semi-Thuien).
Étant donné un système semi-Thuien S, on appelle problème du mot, le problème
de décision qui consiste à savoir pour tout couple de mots (α, β) si α ∗→S β.
Définition 36 (Problème du mot).
Le problème du mot pour les systèmes semi-Thuien est indécidable.
Théorème 38.
Démonstration : À toute machine de Turing M, on peut associer un système semi-Thuien S. En effet, les configurations de la machine sont les mots sur l’alphabet Σ ∪ Q ∪ { }, et toute transition
q s/s′,d
−→ q′ peut être transformée en la règle de réécriture suivante :
— qs → qs′ si d = 0 — qs → s′q′ si d = + — xqs → q′xs′ si d = −
La transformation est trivialement calculable. De plus, ce système semi-Thuien simule complètement la machine de Turing M. On part d’une configuration initiale q0α de la machine et l’on réécrit cette configuration en appliquant les règles ci-dessus. Très aisément, la machine M s’arrête si, et
seulement si pour toute configuration initiale q0α ∗ →S α′q′β′ où α′q′β′ est une configuration d’arrêt. Ainsi, le problème de la terminaison pour les MTs est équivalent au problème du mot pour les systèmes semi-Thuien obtenus à partir de la réduction ci-dessus des MTs. Le problème de la terminaison étant indécidable, le problème du mot pour les systèmes semi-Thuien l’est aussi.
Correspondance de Post
Avant de présenter ce problème, on va introduire quelques notations utiles à son énoncé. Soit x = (α1, . . . , αn) une suite de mots tous définis sur un même alphabet V . Posons I = {1, . . . , n}. Soit σ = i1i2 . . . ip un mot dans I+. On note σx le mot sur V :
αi1 αi2 . . . αip
Par exemple, soit x = (bb, ab, b) et σ = 1223. σx est alors égale à bbababb.
72


On appelle problème de la correspondance de Post le problème de décision qui consiste à partir de deux suites finies non vides x et y toutes les deux indéxées par le même ensemble I et dont tous les mots sont définis sur un même alphabet Σ, à chercher un mot σ ∈ I+ tel que σx = σy.
Définition 37 (Correspondance de Post).
Le problème de la correspondance de Post est indécidable.
Théorème 39.
Démonstration : Pour cela, on va réduire le problème du mot pour les systèmes semi-Thuien au problème de la correspondance de Post.
Tout d’abord, tout système semi-Thuien S défini sur un alphabet Σ fini peut se ramener à un système semi-Thuien équivalent dont l’alphabet ne comporte que deux lettres, par exemple {a, b}. En effet, Σ étant de taille finie, on peut numéroter ses éléments, prendre leur notation binaire et répercuter cette notation sur chacun des mots de Σ∗ et donc sur les règles du système S. En renommant 0 par a et 1 par b, on obtient bien un système semi-Thuien équivalent à S mais sur {a, b}.
Donc, soit le système semi-Thuien S sur l’alphabet Σ = {a, b} comportant n règles αi → βi, et soit (α, β) une instance du problème du mot pour S. On construit alors le problème de la correspondance de Post sur l’alphabet Σ′ = {a, b, a′, b′, [, ], >, <}. Pour tout mot α ∈ Σ∗, on écrit α′ le mot sur Σ′ obtenu à partir de α en remplaçant les lettres a et b par a′ et b′, respectivement. On prend I = {1, . . . , 8 + 2n} indexant les deux suites x et y des mots sur Σ′ définies par :
x1 = [α < x2 =] x3 =< x4 => x5 = a x6 = a′ x7 = b x8 = b′ y1 = [ y2 => β] x3 => x4 =< x5 = a′ x6 = a x7 = b′ x8 = b ∀i = 1, 3, 5, . . . , 2n − 1 x8+i = βi y8+i = α′
i x8+i+1 = β′
i y8+i+1 = αi
Soit α = u1 →S u2 →S . . . →S up = β une solution au problème du mot (α, β). Alors, selon que p soit pair ou impair, le mot ω
1. p est pair, ω = [u1 < u′2 > u3 < . . . < u′p > up]
2. p est impair, ω = [u1 < u′2 > u3 < . . . < u′p−1 > up]
est une solution au problème de la correspondance de Post ci-dessus. En effet, nous avons les deux dćompositions suivantes
ω = [u1 < |u′2 > |u3 < | . . . |]
[|u1 < |u′2 > | . . . | > up]
Ainsi, la solution est σ = i1 . . . ip si p est impair, σ = i1 . . . ip+1 si p est pair. Dans les deux cas, pour chaque j, 2 ≤ j ≤ p − 1 (resp. 2 ≤ j ≤ p) pour voir dans le cas où j est pair que u′
j>
correspond à uj1 <, notons que nous pouvons écrire uj−1 = δ1αkδ2 et u′
j = δ′1
beta′
kδ′2. On a alors la correspondance triviale qui consiste à utiliser les indices 5 et 7 autant de fois
que la taille des mots δ1 et δ2 le requièrent pour renommer les a en a′ et b en b′ et l’indice 8 + k + i. Dans le cas où j est impair, la démarche est identique.
Inversement, si σ est une solution au problème de la correspondance de Post, nécessairement le mot σx = σy = ω est de la forme [α < . . . > β]. Il n’est pas possible de commencer et de finir par un indice i autre que 1 et 2 car les mots xi et yi sont différents sur toutes leurs lettres. Là encore, par la forme des mots dans la suite x et y, on a
73


ω = [α < | . . . > β|] [|α < | . . . | > β]
Nous avons alors nécessairement une correspondance du mot α < avec un certain δ′ > et du mot > β avec un certain < τ ′ tels que α →∗
S δ et τ →∗
S β. Ainsi, le mot a la forme plus précise
ω = [α < |δ′ > | . . . | < τ ′| > β|] [|α < |δ′ > | . . . | < τ ′| > β]
On peut réitérer le procédé sur δ′ > et < τ ′. Le mot ω étant de taille finie, ce procédé terminera et donc on aboutira à α →∗
S β.
Théorème de Rice
Le théorème de Rice est un résultat important de la théorie de la calculabilité qui dit (en gros) que toute propriété non triviale (i.e. qui n’est pas toujours vraie ou toujours fausse) sur les programmes est indécidable. Avant d’énoncer ce théorème, nous devons d’abord introduire les notions de langages acceptés et décidés par une machine de Turing (MT). On parlera aussi de langage récursivement énumérable et récursif.
Soit M une MT sur un alphabet Σ. Soit ω un mot de Σ (i.e. ω ∈ Σ∗). ω est accepté par M si l’exécution de la machine M pour reconnaitre ω mène à une configuration dont l’état est final (considéré alors comme état acceptant).
Définition 38 (Mot accepté).
Un mot ω n’est pas accepté par une machine M si soit la machine s’arrête sur un état non final, soit elle ne termine pas.
Soit M une MT sur un alphabet Σ. Soit ω un mot de Σ. Notons L(M) le langage des mots ω ∈ Σ∗ acceptés par M. L(M) est appelé le langage accepté par M. L(M) est dit langage décidé si de plus M n’a pas d”exécution infinie.
Définition 39 (Langage accepté et décidé).
Ainsi, dans un langage L ⊆ Σ∗ décidé par une MT M, M définit une procédure effective qui pour tout mot ω ∈ Σ∗ répond oui si ω ∈ L(M), et non si ω6∈L(M). A l’inverse, dans un langage accepté, on ne sait pas répondre pour certains mots dont l’exécution est infinie.
Un langage L ⊆ Σ∗ est dit récursif (R) (resp. récursivement énumérable (RE)) s’il est décidé (resp. accepté) par une MT.
Définition 40 (Langage récursif et récursivement énumérable).
74


Le complément d’un langage R est aussi R.
Proposition 40.
Démonstration : Soit L un langage décidé par une MT M. Il est facile de définir une MT M′ qui répond oui sur un mot quand M répond non, et inversement. M′ étant définie à partir de M en inversant ses réponses, elle termine pour tout mot ω, et donc le langage L(M′) est récursif.
Si un langage L et son complément sont RE, alors ils sont tous les deux R.
Proposition 41.
Démonstration : Il suffit de composer les deux MTs pour avoir une MT capable de reconnaitre l’appartenance ou non d’un mot à L et donc aussi à son complémenaire en inversant les réponses.
Une conséquence des deux propositions ci-dessus est qu’étant donné un langage L, on a l’un des trois cas suivants : On note L le complémentaire de L
1. L et L sont tous les deux R.
2. L et L ne sont pas RE.
3. L n’est pas RE mais L est RE mais non R.
Ainsi, un langage est indécidable quand lui ou son complémentaire ne sont pas RE.
Un langage important pour notre propos est le langage LU sous-jacent à toute MT universelle. Les mots acceptés par cette machine sont les codifications dans un alphabet approprié que l’on notera < M, ω >, d’une MT M sur un alphabet Σ et d’un mot ω ∈ Σ∗. LU se définit alors :
< M, ω >∈ LU ⇔ ω ∈ L(M)
Par définition, le langage LU est RE car on a montré l’existence de ces MTs universelles. Par contre, il n’est pas récursif car on sait qu’il existe des MTs M qui ne terminent pas sur certains mots ω et donc la MT universelle ne peut pas non plus répondre sur l’appartenance à LU de tels < M, ω >. Ainsi, par les propositions ci-dessus, on a aussi que le complémentaire LU est aussi non R. En fait, comme LU est RE, LU n’est pas non plus RE et donc est indécidable. C’est ce langage que nous allons utiliser dans la preuve du théorème de Rice pour obtenir notre résultat d’indécidabilité recherché.
Toute propriété P non triviale sur les langages RE est indécidable a.
a. Par propriété P non triviale, on entend que P satisfait aux deux conditions suivantes :
1. Pour toutes MTs M, M′, si M ∈ P et L(M) = L(M′), alors M′ ∈ P .
2. Il existe deux MTs M, M′ telles que M ∈ P et M′6∈P .
Théorème 42 (Théorème de Rice).
75


Démonstration : P est un sous-ensemble de MTs. On peut supposer que P ne contient pas le langage vide. En effet, si P le contient, on peut alors considérer le complément de P et montrer son indécidabilité par la construction ci-dessous. En effet, un problème est indécidable si, et seulement si son complémantaire est indécidable. Réduisons le langage LU à P . À toute instance < M, ω > du problème de l’arrêt, on associe la MT M′ qui a le comportement suivant : On se donne une des MTs MP d’un des langages de P (ceci est possible car P est non triviale et donc n’est pas vide). — quelque soit le mot δ qu’elle a entrée, elle simule la MT M sur ω ; — si M accepte ω, alors on exécute MP sur δ ; — si M n’accepte pas ω (i.e. le rejette ou a une exécution infinie), alors M′ n’accepte pas δ (et donc au final le langage L(M′) = ∅) Cette réduction est correcte. En effet, on a : — L(M′) = ∅ et donc ne satisfait pas la propriété P si, et seulement si M n’accepte pas ω et donc < M, ω > 6∈LU ;
— L(M′) = L(MP ) et donc satisfait la propriété P si, et seulement si M accepte ω et donc < M, ω >∈ LU ;
Un corollaire du théorème de Rice est qu’il n’est pas possible de décider algorithmiquement pour un programme donné s’il est correct, i.e. s’il vérifie sa spécification. En effet, cette propriété de correction est non triviale (des programmes sont correctes et d’autres non pour une spécification donnée).
76


8. Complexité
Dans ce chapitre nous étudierons essentiellement la complexité en temps des algorithmes et n’arborderons pas la complexité en espace, cette dernière étant moins contrainte par l’explosion de la taille des mémoires des ordinateurs.
8.1. Complexité en temps
Toute machine de Turing (MT) M peut se voir comme un problême de décision, i.e. un langage L(M) ⊆ Σ∗ sur son alphabet Σ tel que :
L(M) = {α ∈ Σ∗|M s’arrête sur α}
Quand L(M) est récursif, M est appelé un décideur, i.e. toutes les branches de calcul de M s’arrêtent. On va alors mesurer ces branches qui permettra d’avoir une idée sur le comportement d’un algorithme. Cette mesure doit donc être abstraite, i.e. indépendante de la représentation des données en entrée et de la machine (physique) sur lequel il est exécuté.
On est donc intéressé par :
— la taille des données plus que leur valeur — les ordres de grandeur et la croissance de cette mesure pour des données de grande taille.
Ainsi, soit M une Machine de Turing qui est un décideur. La complexité en temps est une fonction τM : N → N telle que τM(n) soit le nombre maximal de pas qu’effectue M lorsqu’on lui soumet un mot sur son alphabet de taille n. Ainsi, si f : N → N est une fonction, on dit que M est en complexité en temps f (n) si τM(n) ≤ f (n) pour tout n. En fait, on s’intéressera à une complexité asymptotique (i.e. pour n grand) en se contentant des ordres de grandeur et de la croissance de la fonction f .
Soit M une MT et f : N → N une fonction. La complexité asymptotique de M est dite en O(f (n)) si, et seulement s’il existe deux constantes c ∈ R+ et n0 ∈ N telles que τM(n) ≤ c.f (n) pour tout n ≥ n0.
Définition 41 (Complexité asymptotique).
Deux classes de complexité ont émergé naturellement, la classe contenant tous les problèmes que l’on peut résoudre de façon déterministe (i.e. sans choix dans le parcours d’arbres des exécutions) en temps dit polynomial (i.e. que la fonction f défini un polynôme en n), et ceux que l’on peut résoudre toujours en temps polynomial mais de façon non déterministe (i.e. à chaque pas de calcul un choix dans la branche à suivre peut se présenter).
77


Une machine de Turing M = (Σ, Q, R) est déterministe si R peut être définie par une fonction δ : Q × Σ → Q × Σ × {0, +, −}, et non déterministe sinon (i.e. δ ⊆ (Q × Σ) × (Q × Σ × {0, +, −}) est une relation).
Définition 42.
Soit f : N → N. La classe de complexité T IM E(f (n)) contient l’ensemble des langages qui sont décidés par des MTs (déterministes) en temps O(f (n)). La classe N T IM E(f (n)) contient l’ensemble des langages qui sont décidés par des MTs non déterministes en temps O(f (n)).
Définition 43 (Complexité d’une MT).
8.2. Les classes P et NP
Succinctement, la classe P contient tous les problèmes résolus par une MT déterministe en temps polynomial tandis que la classe NP contient tous ceux résolus par une MT non déterministe aussi en temps polynomial. Ce qui fait que les problèmes de la classe NP sont considérés comme être résolus de façon non efficace est leur coût exponentiel quand ces derniers sont résolus par des machines déterministes (machines usuelles), et ce provenant des choix dans l’arbre d’exécution comme l’établit le résultat suivant :
Soit f : N → N telle que pour tout n ∈ N, f (n) ≥ n. Soit M une MT non déterministe de complexité O(f (n)). Il existe une MT déterministe M′ qui décide le même langage
en 2O(f(n)).
Théorème 43.
Démonstration La MT M′ doit simuler toutes les exécutions de M. Toutes ces exécutions sont de longueur bornée par O(f (n)). Soit m un majorant du nombre de choix à chaque transition. L’exploration de toutes ces exécutions, jusqu’à leur longueur maximale f (n), demande au plus mf(n). Maintenant, la simulation de chacun de ces calculs est bornée par O(f (n)). Donc le temps de la calcul de M′ est bornée par O(f (n)) × mf(n), et donc par 2log2(m)(f(n)+O(f(n))), i.e. 2O(f(n)).
La définition des classes P et NP est alors la suivante :
Les classes P et NP sont les réunions des classes T IM E(nk) et N T IM E(nk) pour k ≥ 1, respectivement.
Définition 44 (Classes P et NP).
78


Étant donné un graphe G = (X, A) et 2 sommets s, s′ ∈ X, existe-t-il un chemin de s à s′ ? L’on représente classiquement les graphes par leur matrice d’adjacence, i.e. une matrice X × X telle que chaque entrée as,s′ est le nombre d’arêtes reliant s à s′.
PATH ∈ P.
Théorème 45.
Démonstration On marque une et une seule fois chaque sommet rencontré en commençant par s, i.e. que l’on exige que chaque sommet marqué ne sera jamais revisité. Si l’on rencontre s′ on s’arrête et on répond "Oui", sinon (i.e. tous les sommets rencontrés sont marqués et on n’a pas rencontré s′) on répond "Non". Ainsi, à la 1ère étape on va visiter au plus dans la matrice |X| − 1 sommets, à la seconde étape |X| − 2 sommets, etc. Cet algorithme est borné par O(n2) où n est le nombre de sommets (pire cas, celui du graphe en peigne).
Exemple 45 (Un exemple de problème P : PATH).
Autre exemple- Coprimalité. Tester pour 2 nombres entiers a et b si ils sont premiers entre eux. On applique l’algorithme d’Euclide qui calcule de façon déterministe le pgcd de a et b, et dont on sait que la complexité est O(log2sup(a, b)).
Exemple 46.
Ainsi, pour un problème de NP si l’on dispose d’une solution, on doit être capable de la vérifier en temps polynomiale. On parle alors de certificat.
Soit L ⊆ Σ∗ un langage sur l’alphabet Σ. L ∈ N P si, et seulement s’il existe LR ⊆ Σ∗ × Σ∗ t.q. : — LR ∈ P — ∀x ∈ Σ∗, x ∈ L ⇔ ∃y ∈ Σ∗, (x, y) ∈ LR
La MT en temps polynomiale qui décide LR est le certificat.
Définition 45 (Certificat).
79


Ce problème consiste à tester si un nombre est composable (i.e. n’est pas premier). Ce problème de décision se définit par :
COM P OSIT E = {n|∃a, b > 1, n = ab}
De là, on peut définir le langage LR de la façon suivante :
LR = {(x, y)|1 < y < √x, y divise x}
Maintenant, vérifier si y divise x se fait en temps polynomial.
En fait, il est facile de montrer que COMPOSITE est dans P. En effet, la recherche
des diviseurs de n est bornée par √n. Le problème dual défini par le langage PRIME contenant tous les nombres premiers est aussi dans P mais ceci n’a été démontré que récemment (cf. Théorème d’Agrawal, Kayal, et Saxena 2002).
Exemple 47 (Problème COMPOSITE).
Étant donné un graphe G = (X, A), existe-t-il un chemin qui passe par tous les sommets du graphe ? Un chemin vérifiant cette propriété est appelé chemin Hamiltonien du nom de l’astronome et mathématicien anglais William Rowan Hamilton.
HAMIL ∈ NP.
Théorème 49.
Démonstration Le problème HAMIL peut se traduite par le problème de décision représenté par le langage :
HAM IL = {G|∃σX, σX est un chemin}
σX étant une substitution des sommets de X. De là, on peut définir simplement le certificat :
LR = {(G, σX)|σX est un chemin de G}
Maintenant, contrôler qu’une substitution des sommets est un chemin dans G se fait en temps polynomial.
Ce problème est naturellement exponentiel car il demande de vérifier dans le pire cas toutes les substitutions de sommets du graphe dont le nombre est |X|!.
Exemple 49 (Exemple de problème NP : HAMIL).
80


8.3. Structure de la classe NP
On peut démontrer simplement que P ⊆ N P . En effet, l’algorithme de vérification est l’algorithme de décision, ce dernier étant déjà de complexité polynomiale pour une MT déterministe. On n’a donc pas besoin de certificat ou plus exactement le certificat que l’on considère est pour tout langage L ⊆ Σ∗ calculable par une MT déterministe en temps polynomiale, le langage LR ⊆ Σ∗ × Σ∗ suivant :
LR = {(x, ε)|x ∈ L}
où ε désigne le mot vide sur Σ.
Démontrer l’inclusion inverse consisterait à démontrer que P = N P . Or, ceci est une conjecture, plus exactement sa négation, qui fait partie des problèmes à 1M$ de l’institut Clay, et qui n’a toujours pas reçu de réponse. Par contre, on peut démontrer que la classe NP est close par union, intersection, et concaténation (cf. l’exercice énoncé ci-dessous). À l’inverse, on ne sait pas si la classe NP est close par complémentaire, i.e. pour un langage L dans NP, on ne sait pas répondre pour Σ∗ \ L.
Problèmes NP-complets et NP-durs
Face à toutes ces questions, les chercheurs ont tenté de connaître de façon plus précise la structure de la classe NP, et ce dans l’espoir de répondre à ces questions (et il y en a bien d’autres). Parmi ces chercheurs, un particulièrement a oeuvré dans la description de la structure de la classe NP : S.-A. Cook. Ce dernier a en effet défini une sous-classe de NP, de problèmes dont les définitions contiennent toute la difficulté de la classe NP. Cette sous-classe connue sous le nom de la classe des problèmes NP-complets, contient alors tous les problèmes dont la résolution engendre nécessairement, à une transformation polynomiale près, la résolution de tous les problèmes de la classe NP. Ainsi, si pour un de ces problème NP-complet, nous étions capables de trouver une MT déterministe en temps polynomial qui résolvait ce problème, alors nous aurions toujours à une transformation polynomiale près, un algorithme raisonnable qui résolverait tous les problèmes NP et ainsi on aurait démontré que P = NP. Inversement, si nous montrons qu’il ne peut exister une MT déterministe en temps polynomial pour seulement un problème de la classe NP-complet, alors nous aurons montré que P 6= NP. Bien sûr, pour l’instant ni l’un, ni l’autre de ces cas n’ont encore été obtenus. Néanmoins, la définition de cette sous-classe a permis de grandes avancées dans le domaine de la théorie de la complexité. Nous allons donc dans la section suivante définir cette sous-classe des problèmes dits NP-complets. Comme nous l’avons déjà dit ci-dessus, la classe NP-complet contient tous les problèmes dont toute propriété sur leur complexité devient, à une transformation polynomiale près, une propriété de tous les problèmes NP. Que veut bien dire "à une transformation polynomiale près" ? On comprend aisément l’adjectif "polynomial" car on ne veut pas que cette transformation engendre un surcoût de complexité. La question est plutôt de savoir ce que l’on entend par transformation ? Ce que l’on veut obtenir est donc un moyen permettant de montrer qu’un problème n’est pas plus dur qu’un autre, i.e. que toutes instances d’un problème peut à transformation près se traduire en une instance d’un autre problème. Ainsi, si un premier problème se réduit en un second, le premier problème est (au moins) aussi facile que le second - si la réduction est facile, i.e. en temps polynomial. On pourra donc utiliser la réduction de deux façon :
1. pour montrer qu’un problème est NP-complet : si le premier est réputé NP-complet (i.e. dur), le second l’est aussi ;
81


2. pour montrer qu’un problème est facile : si le deuxième est facile, le premier l’est aussi.
C’est en fait surtout le premier raisonnement que l’on utilise généralement. Définissons alors cette notion de réduction ou transformation polynomiale.
Soient L1 ⊆ Σ∗1 et L2 ⊆ Σ∗2 deux langages. Une transformation polynomiale de L1
vers L2 est une fonction f : Σ∗1 → Σ∗2 qui satisfait les conditions suivantes :
1. f est calculable en temps polynomial, i.e. par une MT déterministe en temps polynomial.
2. f (x) ∈ L2 ⇐⇒ x ∈ L1
On note L1 L2.
Définition 46 (Transformation polynomiale).
On remarque simplement que la relation est transitive car définie à partir de la loi ◦, et la composition de deux fonctions polynomiales est trivialement une fonction polynomiale.
82


Soient les deux problèmes suivants :
1. Problème 1 : Un jeu composé de N participants et d’une liste L de paires de participants : les paires d’ennemis. Le but est de créer p équipes de telle sorte qu’aucune équipe ne contienne une paire d’ennemis.
2. Problème 2 : Soit G = (X, A) un graphe et k un nombre de couleurs. G est-il k-coloriable ? a
Le langage associé au problème 1 est alors :
L1 = {((N, L), p)|possibilité de faire p équipes sans paire d’ennemis}
et le langage associé au problème 2 est le suivant :
L2 = {(G, k)|G est k-coloriable}
Deux transformations polynomiales f1 : L1 → L2 et f2 : L2 → L1 peuvent être simplement définies :
1. Pour f1, étant donnée une instance ((N, L), p) de L1, on peut définir le graphe G dont les sommets sont numérotés de 1 à N et dénotent les participants au jeu et les arcs sont toutes les paires de la liste L. On pose alors k = p. On a bien G est k-coloriable si, et seulement si il est possible de faire p équipes. La transformation f1 n’étant qu’un renommage, elle est trivialement polynomiale en N + |L|.
2. De la même manière, pour f2, étant donné un graphe G = (X, A) et un entier positif k, on peut définir le jeu ((N, L), p) en posant N = |X|, L = A et p = k. Là encore, on peut faire p équipes si, et seulement si G est k-coloriable. Enfin, la transformation n’étant qu’un calcul de cardinalité et un renommage, elle est aussi polynomiale en temps de calcul.
a. G est dit k-coloriable si, et seulement s’il existe une application g de X dans {1, 2, . . . , k} telle que pour tout (x, y) ∈ A, g(x) 6= g(y).
Exemple 50.
Si L′ ∈ P et L L′, alors L ∈ P .
Proposition 51.
Démonstration Soit u une instance du problème posé par L. Pour décider si u ∈ L, il suffit d’appliquer la transformation f à u et tester si f (u) ∈ L′. Maintenant, tester cette appartenance est par hypothèse effectuée en temps polynomial et la transformation est elle-même calculable en temps polynomial. Ainsi, tester l’appartenance de u à L consiste à composer deux MTs déterministe en temps de calcul polynomial, qui donne donc une MT déterministe qui reste encore en temps de calcul polynomial.
On peut maintenant définir la classe des problèmes NP-complets.
83


Un langage L est NP-complet si : — L ∈ NP ; — ∀L′ ∈ N P, L′ L.
Définition 47 (NP-complétude).
La classe des problèmes NP-complets impose comme contrainte sur les langages qu’ils soient reconnus par une MT non déterministe en temps polynomial. Or, il existe des problèmes décidables qui ne sont pas reconnus en temps polynomial même sur une machine non déterministe. Pour de tels problèmes possédant aussi la particularité que tous les problèmes NP peuvent se réduire en eux par une transformation polynomiale, on dira qu’ils sont NP-durs. En résumé, démontrer qu’un problème est NP-dur établit qu’il n’a vraisemblablement pas de solution calculable en temps polynomial, démontrer qu’il est NP-complet assure au moins l’existence d’une solution polynomiale sur une MT non déterministe.
Si L est NP-dur et L L′, alors L′ est NP-dur.
Proposition 52.
Démonstration Par hypothèse, L′ contient à la transformation polynomiale près donnée par L L′, un sous-langage permettant de réduire tous les problèmes NP.
Un premier problème NP-complet
La question naturelle que l’on peut se poser est de savoir si de tels problèmes NP-complets (NP-durs) existent ? La réponse est positive et a été apportée par S.-A. Cook en 1970 dans un papier à la base de la théorie de la complexité des algorithmes et a valu à l’auteur le Turing award équivalent du prix Nobel en informatique et informatique théorique. Le premier problème que S.-A. Cook a montré être NP-complet vient du monde de la logique mathématique, et plus précisément de la logique dite propositionnelle. La logique propositionnelle est un formalisme qui étudie les valeurs de vérité de propositions complexes obtenues par composition de propositions élémentaires à partir des connecteurs logiques standards que sont la conjonction (notée ∧), la disjonction (notée ∨), l’implication (⇒), et la négation (notée ¬). Formellement, la logique propositionnelle se définit de la façon suivante : — On se donne un ensemble P dont les éléments sont appelés des variables propositionnelles. Une formule propositionnelle sur P est alors une suite de symboles pris dans P ∪ {⇒, ¬, ∧, ∨, (, )} et construite selon les règles suivantes :
1. toute variable propositionnelle est une formule sur P .
2. si φ et ψ sont des formules sur P alors φ @ ψ est une formule sur P avec @ ∈ {∧, ∨, ⇒}.
3. si φ est une formule sur P alors ¬φ est une formule sur P .
4. toute formule sur P est obtenue par application répétée, un nombre fini de fois des étapes 1. à 3. ci-dessus 1.
1. C’est une définition inductive.
84


Pour le problème qui nous intéresse ici, on se restreindra à un type de formule particulier, les formules en forme normale conjonctive. Ce sont toutes les formules de la forme :
E1 ∧ . . . ∧ Ek
et chaque Ei est une clause, i.e. une disjonction de variables propositionnelles ou de négations de variables propositionnelles, à savoir une formule de la forme :
xi1 ∨ . . . ∨ xil
où xij est soit p, soit ¬p avec p ∈ P . En fait, on peut montrer que toute formule peut être transformée algorithmiquement en une formule équivalente (i.e. avec les même valeurs de vérité - voir juste ci-dessous) en forme normale conjonctive. — Pour calculer la valeur de vérité d’une formule φ, on commence par donner une valeur de vérité aux variables propositionnelles dans P . Puis, on applique les tables de vérité de chacun des connecteurs apparaissant dans φ, les valeurs de vérité données aux variables propositionnelles, appelées modèles, caractérisant une ligne dans le tableau. Donc, étant donné un modèle défini par une application ν : P → {0, 1}, la valeur de vérité d’une formule φ, notée ν∗(φ), se définit inductivement sur la structure de φ de la façon suivante : — si φ ∈ P alors v∗(φ) = v(φ), — si φ = φ1 ∧ φ2 (resp. φ1 ∨ φ2, resp. φ1 ⇒ φ2) alors v∗(φ) = 1 si, et seulement si v∗(φ1) × v∗(φ2) = 1 (resp. v∗(φ1) + v∗(φ2) ≥ 1, resp. v∗(φ1) ≤ v∗(φ2)), — si φ = ¬φ1 alors v∗(φ) = 1 si, et seulement si v∗(φ1) = 0. On dit alors que φ est satisfiable s’il existe un modèle ν tel que ν∗(φ) = 1. Le problème qui a été démontré par Cook comme premier problème NP-complet et connu sous l’acronyme SAT, est celui de la satisfiabilité pour les formules en forme normale conjonctive.
SAT est NP-complet.
Théorème 53.
Démonstration Montrons tout d’abord que SAT est NP. Soit LR = {(φ, ν)|ν∗(φ) = 1} où ν : P → {0, 1} est une valuation des variables. propositionnelles (ν peut se représenter par une séquence de varariables propositionnelles pour dénoter celles mises à 1). Étant donnée une valuation, il est façile en temps polyniomial en la taille de φ (parcours d’arbre) de vérifier sa valeur de vérité.
On va maintenant montrer comment transformer toute MT déterministe polynomiale au problème SAT. Pour cela, on va définir une formule propositionnelle φ dont les variables représentent l’historique du calcul qu’effectuerait la MT, un choix de valeurs des variables fournissant une branche du calcul. On se donne une MT M non déterministe d’alphabet Σ. Soit p(n) la complexité de M pour décider de l’appartenance d’un mot de longueur n au langage calculé. Pour construire les clauses, nous aurons besoin des variables suivantes : — pour q ∈ Q et 0 ≤ k ≤ p(n), Q(q, k) = 1 ssi l’état de la MT à l’étape k est q. — pour 1 ≤ i ≤ p(n) et 0 ≤ k ≤ p(n), T (i, k) = 1 ssi la position de la MT à l’étape k est i. — pour 1 ≤ i ≤ p(n), 0 ≤ k ≤ p(n) et j ∈ Σ, R(i, j, k) = 1 ssi à l’étape k, la case i contient le symbole j.
85


Le nombre de ces variables est O(p(n)2).
La formule φ que nous allons construire va être la conjonction d’une longue liste de clauses qui expriment que chaque étape du calcul obéit aux règles de la machine M.
— Pour tout i ≤ p(n), le symbole j ∈ Σ est l’état initial de la machine à la position i. Ceci s’exprime par la clause R(i, j, 0). — Pour tout état q ∈ Q, on a la clause Q(q, 0) si l’état initial de la machine est q, la clause ¬Q(q, 0) sinon. — À l’état intial, la tête est au début du ruban. Ceci s’exprime par la clause T (1, 0). — À chaque instant, une case du ruban ne peut contenir qu’un unique symbole. Ceci s’exprime pour tout i, k ≤ p(n) et pour tout j 6= j′ ∈ Σ par la clause ¬R(i, j, k) ∨ ¬R(i, j′, k). — À chaque étape k ≤ p(n), la tête de lecture ne peut pas être simultanément à deux positions i 6= i′ ≤ p(n) différentes. Ceci s’exprime par la clause ¬T (i, k) ∨ ¬T (i′, k). — À chaque étape k ≤ p(n), M ne peut pas être simultanément dans deux états q 6= q′ ∈ Q différents. Ceci s’exprime par la clause ¬Q(q, k) ∨ ¬Q(q′, k). — À chaque étape k < p(n), la machine M ne modifie que la case i ≤ p(n) où la tête de lecture est positionnée. Ceci s’exprime par la clause R(i, j, k) = R(i, j, k + 1) ∨ T (i, k) où x = y est l’abbréviation pour (¬x ∧ ¬y) ∨ (x ∧ y). — À chaque étape k ≤ p(n) et selon la position i ≤ p(n) de la machine M, toute transition (q, j, q′, j′, d) de M peut être applicable si M est dans l’état q et le symbole lu à la position i est bien j. Ceci s’exprime par la clause
T (i, k) ∧ Q(q, k) ∧ R(i, j, k) ⇒ T (i + d, k + 1) ∧ Q(q′, k + 1) ∧ R(i, j′, k + 1)
— La clause Q(qf , p(n)) où qf est l’état final de M.
Le temps de calcul de cette transformation correspond à la taille de la formule φ générée. Or, on constate que le nombre de clauses est en O(p(n)3). La transformation est donc polynomiale, comme requis. En outre, sa satisfiabilité est, par construction même, équivalente à l’existence d’une branche du calcul qui accepte le mot passé en argument (celui se trouvant sur le ruban initial).
La NP-complétude de SAT ayant été établie, il est maintenant possible de montrer par transformation polynomiale que d’autres problèmes sont NP-complets.
86


Nous allons montrer la NP-complétude pour une restriction de SAT. En effet, nous allons nous restreindre à des formules en forme normale conjonctive comportant exactement 3 littéraux par clause. Ce problème connu sous l’acronyme 3-SAT va donc manipuler des formules de la forme E1 ∧ . . . ∧ Ek où chaque Ei est la disjonction d’exactement 3 littéraux (variables propositionnelles ou négation de variables propositionnelles). Nous allons alors monter que SAT 3-SAT. Pour cela, on remplace les clauses ne contenant pas 3 littéraux par : — Une clause x1 comportant un seul littéral est remplacée par :
(x1 ∨ y1 ∨ y2) ∧ (x1 ∨ y1 ∨ ¬y2) ∧ (x1 ∨ ¬y1 ∨ y2) ∧ (x1 ∨ ¬y1 ∨ ¬y2)
— Une clause (x1 ∨ x2) contenant 2 littéraux est remplacée par :
(x1 ∨ x2 ∨ y) ∧ (x1 ∨ x2 ∨ ¬y)
— Une clause (x1 ∨ . . . ∨ xl) avec l ≥ 4 est remplacée par :
(x1 ∨ x2 ∨ y1) ∧ (
∧
3≤i≤l
(¬yi2 ∨ xi ∨ yi−1)) ∧ (¬yl−3 ∨ xl−1 ∨ xl)
où y, y1, . . . , yl−3 sont des variables nouvelles.
La construction que nous venons de décrire est aisément implantée par un algorithme polynomiale. Par la proposition 52 ci-dessus, nous avons donc démontré que 3-SAT est NP-dur. Maintenant, en suivant le même raisonnement que pour SAT, 3-SAT appartient à NP. Ainsi, 3-SAT est aussi NP-complet.
Exemple 54.
87




Sous partie 3
Théorie des langages
89




9. Introduction
Dans la partie sur la calculabilité, nous avons vu qu’un moyen simple de représenter un algorithme était par un graphe dont les noeuds définissent les états du système et les transitions, les actions de lecture ou d’écriture de caractères et de déplacement de la tête de lecture. C’est le modèle de calcul des Machines de Turing. Ici, nous allons étudier un formalisme plus restrictif permettant de modéliser des procédures effectives de reconnaissance des mots dans un langage, et donc très utilisé dans la modélisation des systèmes discrets comme les systèmes informatiques, les mots étant les exécutions possibles du système. Au départ, cette forme de modélisation est née d’une tentative de représentation formelle des langues naturelles. Elle connut une expansion considérable lorsque l’on s’aperçut de son adéquation à la description à la fois des langages de programmation (description des analyseurs syntaxiques et compilateurs) et des programmes eux-mêmes. Dans le premier cas, le formalisme des automates est utilisé pour la définition de procédures effectives (i.e. calculables) de reconnaissance de langages (i.e. montrer qu’un mot construit sur un alphabet Σ donné appartient ou non à une partie L ⊆ Σ∗, le langage défini). Cette approche, appelée souvent théorie des langages, a reçu beaucoup d’attention de la part des chercheurs en informatique et a eu de nombreuses applications dans la définition des compilateurs, des traitements de texte (correcteur orthographique et grammatical), la compréhension automatique des langues naturelles, etc. Plus récemment, la théorie des langages est aussi très utilisée pour les recherches de caractères dans un texte et a dans ce cadre de nombreuses applications en bio-informatique, entre autres dans l’analyse de séquences qui consiste à reconnaitre des motifs particuliers (i.e. des mots construits sur l’alphabet {A, T, G, C}) dans une protéine à partir de la séquence de celle-ci. Dans le cadre de la modélisation des programmes et des systèmes informatiques, les automates sont utilisés pour modéliser et raisonner sur le comportement de ces derniers. Ainsi, l’automate représente une modélisation formelle du comportement du programme dont le langage reconnu correspond à l’ensemble des exécutions possibles, appelées aussi traces. L’analyse du système consiste alors à vérifier certaines propriétés sur ces traces telles qu’il n’y ait pas de blocage, d’état non atteignable, ou encore l’existence d’exécutions finies et infinies. Ces propriétés s’expriment dans des logiques dites temporelles, et leur vérification s’automatise parfois. On parle alors de "Model-Checking" (le cours "Méthode de Conception Formelle" dans l’option "Ingénierie des Systèmes Informatiques et Avancés" traite de ce sujet). Ici, nous ne traiterons pas de ces applications et extensions de la théorie des automates et des langages. Nous présenterons plutôt les fondements mathématiques et outils formels sous-jacents à l’ensemble de ces différents cas d’application.
91




10. Langages rationnels
10.1. Monoide
Comme nous l’avons déjà vu dans la partie précédente, les langages, que ces derniers soient des expressions ou des instructions d’un langage de programmation, les exécutions d’un programme ou encore un problème de décision (i.e. les mots sur lesquels une machine de Turing donnée s’arrête), sont des ensembles de suites de symboles construits sur un alphabet. Ici, nous allons voir une famille de langages très utilisée en informatique du fait de leur structuration, les langages rationnels ou réguliers.
Les langages étant des ensembles, il existe plusieurs façons de les définir, et nous en avons vu une dans la première partie de ce document au travers des définitions inductives. Ici, nous allons étudier une autre façon plus effective (mais en même temps très proche) que les définitions inductives pour définir un langage. L’intérêt du procédé que nous allons présenter est de donner une façon simple de savoir si un mot appartient ou non au langage défini. Mais avant, comme il est d’usage en mathématique, les langages que nous allons considérer seront toujours des parties d’une structure algébrique particulière, les monoides.
Un monoide est un ensemble M muni d’une loi interne . : M × M → M associative, et d’un élément neutre e (i.e. ∀m ∈ M, e.m = m.e = m).
Définition 48 (Monoide).
Comme toute structure algébrique, les monoides peuvent être comparés au travers de la notion d’homomorphisme, c’est-à-dire des applications entre les ensembles qui préservent l’élément neutre et la loi interne.
Un homomorphisme entre deux monoides (M1, ., e1) et (M2, ×, e2) est une application f : M1 → M2 telle que : — f (e1) = e2 — ∀α, β ∈ M1, f (α.β) = f (α) × f (β)
Définition 49 (Homomorphisme).
Parmi les monoides, un particulièrement nous intéresse, le monoide Σ∗ des mots finis défini inductivement sur l’alphabet Σ par :
— ε ∈ Σ∗ — si α, β ∈ Σ∗, alors α.β ∈ Σ∗
93


Ainsi, (Σ∗, ., ε) est bien un monoide. La loi interne s’appelle la concaténation, et l’élément neutre ε le mot vide. Ce monoide particulier est aussi appelé le monoide libre engendré par Σ. La propriété d’être libre lui vient de la propriété suivante :
Soient Σ un ensemble, (M, ., e) un monoide et f : Σ → M une application. Il existe un unique homomorphisme h : Σ∗ → M tel que pour tout a ∈ Σ, h(a) = f (a).
Théorème 55.
Démonstration : Il suffit de poser h(e) = ε et h(a1 . . . an) = f (a1) . . . f (an). h est bien un homomorphisme. L’unicité est une simple conséquence du fait que h(a) = f (a) pour tout a ∈ Σ.
10.2. Langages réguliers et automates
Définitions
Un langage va donc être une partie de Σ∗ pour un alphabet Σ donné, i.e. un élément de P(Σ∗). Il est façile de voir que l’ensemble des langages P(Σ∗) sur une même signature Σ est stable pour un certains nombre d’opérations telles que l’union, l’intersection et le passage au complémentaire. Cet ensemble est aussi stable pour le produit et l’itération où :
— Produit de langages. Soit L, L′ ∈ P(Σ∗), L.L′ = {α.β|α ∈ L, β ∈ L′}.
— Itération d’un langage. Soit L ∈ P(Σ∗). Notons L∗ le langage obtenu à partir de L de la façon suivante :
L0 = {ε} L1 = L Li+1 = L.Li L∗ =
⋃
i≥0
Li. On note aussi L+ =
⋃
i>0
Li
On a donc :
L+ = L.L∗ = L∗.L et L∗ = {ε} ∪ L+
Comme on l’a vu dans la partie précédente, une machine de Turing est un moyen effectif pour engendrer un langage, le problème de décision qu’elle définit. Ici, nous allons suivre le même procédé en définissant aussi un moyen effectif pour engendrer un langage au travers de la notion d’automate fini qui s’abstrait des actions d’écriture et de lecture, et des têtes de lecture, pour ne garder que les symboles lus à chaque transition. Formellement, les automates finis se définissent ainsi :
94


Soit Σ un alphabet. Un automate fini A sur Σ est un tuple (Q, F, q0, τ ) où : — Q est un ensemble fini dont les éléments sont appelés les états de l’automate ; — F ⊆ Q est l’ensemble des états dits finaux ; — q0 ∈ Q est l’état initial ;
— τ ⊆ Q×Σ∪{ε}×Q est une relation dont les éléments s’appellent des transitions.
Dans la suite, une transition (q, a, q′) ∈ τ sera notée q →a q′. L’automate A est dit déterministe quand τ : Q × Σ ∪ {ε} → Q est une fonction. Quand τ est une application alors l’automate est dit complet.
Définition 50 (Automate fini).
Soit A = (Q, F, q0, τ ) un automate fini sur un alphabet Σ. Le langage reconnu par A est le langage L(A) ⊆ Σ∗ défini par : ∀a1 . . . an ∈ Σ∗,
a1 . . . an ∈ L(A) ⇔ (∃q1, . . . , qn ∈ Q, (∀i, 0 ≤ i ≤ n − 1, qi
a→i qi+1) et qn ∈ F )
Définition 51 (Langage reconnu par un automate).
Il est façile de compléter un automate sans changer le langage reconnu. Soit A = (Q, F, q0, τ ). On définit l’automate complété A′ = (Q′, F ′, q′0, τ ′) comme suit :
— Q′ = Q ∪ {p}, F ′ = F , et q′0 = q0 ;
— τ ′ = {(q, a, p)|6 ∃q′ ∈ Q, q →a q′} ∪ {(p, a, p)|a ∈ Σ}
Par construction A′ est complet. De plus, p n’étant pas un état terminal, on a L(A) = L(A′). Dans la suite, on ne considèrera que des automates complets.
Étant donné un automate A sur un alphabet Σ, il est façile d’écrire un algorithme qui pour un mot de Σ∗ répond par l’affirmative si le mot appartient à L(A). Ainsi, par l’exercice ci-dessus, les langages reconnus par un automate sont calculables. On appellera de tels langages des langages réguliers.
Soit Σ un alphabet. Un langage L ⊆ Σ∗ est dit régulier si, et seulement s’il existe un automate A sur Σ tel que L = L(A).
Définition 52 (Langages réguliers).
Pour prouver qu’un langage est régulier, une méthode évidente consiste à construire explicitement un automate. Bien sûr, on peut imposer d’autres contraintes à l’automate comme avoir le plus petit nombre d’états ou encore être déterministe. On verra dans la suite que pour l’essentiel, ces contraintes sont façiles à obtenir. Néanmoins, pour simplifier la construction des automates, il est souvent utile de savoir que les langages réguliers sont aussi stables pour les opérations ensemblistes que nous avons définies ci-dessus.
95


L’ensemble des langages réguliers est stable par passage au complémentaire, intersection, union, produit et itération.
Théorème 56.
Démonstration : Nous allons donner ci-dessous, les preuves pour chacune des opérations mentionnées : nous rappelons que tous les automates considérés sont complets
— Passage au complémentaire. Soit A = (Q, F, q0, τ ) un automate (complet) et déterministe (on verra dans la suite que tout automate peut se déterminiser - ceci n’est donc pas une restriction). L’automate A qui reconnait le langage complémentaire est défini par (Q, Q \ F, q0, τ ). Nous avons bien que tous les mots qui ne sont pas reconnus par A sont maintenant reconnus par A. — Intersection. L’idée est d’exécuter en parallèle les deux automates sur un même mot et ne reconnaître que ceux pour lesquels les deux automates aboutissent dans un état final. Ceci s’obtient très bien par le produit Cartésien d’automates. Soient A1 = (Q1, F1, q01, τ1) et A2 =
(Q2, F2, q02, τ2) deux automates définis sur une même signature Σ. Le produit Cartésien de A1 et A2, noté A1 × A2, est l’automate fini A = (Q, F, q0, τ ) défini par : — Q = Q1 × Q2 et F = F1 × F2 ; — q0 = (q01, q02) ;
— ∀a ∈ Σ, (q1, q2) a → (q′1, q′2) ∈ τ ⇔ q1
a → q′1 ∈ τ1 et q2
→a q′2 ∈ τ2.
L’automate A reconnait le langage L(A1) ∩ L(A2). — Union. L’union de deux automates A1 = (Q1, F1, q01, τ1) et A2 = (Q2, F2, q02, τ2) est définie par
l’automate A′ = (Q′, F ′, q′0, τ ′) avec :
— Q′ = Q1 ∪ Q2 ∪ {q′0} et F ′ = F1 ∪ F2 — τ ′ = τ1 ∪ τ2 ∪ {q′0
ε→ q01, q′0
ε→ q02}
Une autre construction de l’union existe à partir du produit cartésien de A1 et A2 en posant que l’état intial est (q01, q02) et les états finaux sont F1 × Q2 ∪ Q1 × F2. Cette construction marche parce que l’on a supposé A1 et A2 complets. — Produit. Soient A1 = (Q1, F1, q01, τ1) et A2 = (Q2, F2, q02, τ2) deux automates définis sur une même signature Σ. On ajoute à Σ le mot vide ε dont on rappelle qu’il est neutre pour la concaténation. On définit alors l’automate fini A = (Q, F, q0, τ ) par :
— Q = Q1 ∪ Q2, F = F2, et q0 = q01 ; — τ = τ1 ∪ τ2 ∪ {q1
ε→ q02|q1 ∈ F1}
L’automate A ainsi défini reconnait bien exactement le langage L(A1).L(A2). — Itération. Soit A = (Q, F, q0, τ ) un automate sur Σ. On définit l’automate A∗ = (Q′, F ′, q′0, τ ′)
sur Σ′ = Σ ∪ {ε} de la façon suivante :
— Q′ = Q ∪ {q′0} (q′0 est un nouvel état qui n’apparaissait pas dans Q)
— F ′ = F ∪ {q′0}
— τ ′ = τ ∪ {q ε→ q0|q ∈ F ′}
Remarquons que nous avons ajouté un nouvel état initial et pas considéré q0 comme un nouvel état final. En effet, pourquoi ne pas imposer que q0 soit un nouvel état final ? Si nous faisions un tel choix, nous changeons alors le langage de départ et donc on ne calculerait pas L(A)∗. En effet, supposons l’automate A = ({q0, qf }, {qf }, q0, τ ) défini sur l’alphabet {a, b} et dont les transitions dans τ sont définies par :
q0 qf
a b
Il est aisé de voir que le langage calculé L(A) = {an.b|n ∈ N} où a0 = ε et ∀n ≥ 1, an = a . . . a
} {{ }
n fois
.
Si dans notre construction de l’automate A∗, nous imposions que q0 deviennent un état terminal,
96


nous reconnaitrions alors tous les mots an pour n ∈ N, qui bien sûr n’appartiennent pas à L(A) et donc ne devraient pas être reconnus par A∗.
Lemme de pompage et langage réguliers
Nous allons présenter une propriété importante des langages réguliers. Elle est souvent utilisée pour démontrer qu’un langage n’est pas régulier.
Soit L un langage régulier dans un alphabet Σ. Il existe un entier naturel p (la longueur du pompage) tel que pour tout mot α de L de longueur au moins p, il existe trois mots x, y, z ∈ Σ∗ tels que :
— y 6= ε — le mot x.y a une longueur (son nombre de lettres) inférieure ou égale à p — α = x.y.z — ∀k ∈ N, x.yk.z ∈ L.
Théorème 57 (Lemme de pompage).
Démonstration : Soit un automate A = (Q, F, q0, τ ) qui reconnait L. Posons p = |Q|. Soit α = a1 · · · an un mot de longueur au moins égale à p. Ceci signifie alors que n ≥ p. Soit q1, . . . , qn ∈ Q la séquence d’états telle que pour tout i, 0 ≤ i ≤ n − 1, qi
a→i qi+1. D’après le principe des tiroirs, il y a parmi les p + 1 premiers états, deux endroits i, j, 0 ≤ i < j ≤ p, tels que qi = qj. Posons alors x = a1 . . . ai, y = ai+1 . . . aj et z = aj+1 . . . an. Les deux premières conditions sont vérifiées (x.y = a1 · · · aj et j ≤ p). Vérifions la troisième. Soit k ∈ N. Montrons alors que le mot x.yk.z est reconnu par l’automate A. Il commence par lire x en suivant le chemin q0
a→1 q1
a→2 . . . a→i qi. Puis, il lit y en suivant le chemin qi
ai→+1 qi+1
ai→+2 . . . a→j qj. Comme qj = qi, il peut recommencer le même chemin, et donc relire y, et répéter ainsi ce processus k fois. À partir de là, il peut lire z en suivant le chemin qj
aj→+1 qj+1
aj→+2 . . . a→n qn. L’état qn ∈ F puisque le mot α ∈ L.
On peut se demander pourquoi on appelle le théorème, le lemme de pompage ? C’est simplement parce qu’il indique que chaque mot d’un langage régulier peut être gonflé par la répétition d’une de ses parties.
Example : Montrons que le langage L = {0n.1n|n ∈ N} n’est pas un langage régulier. Supposons le contraire. Par le théorème ci-dessus, il existe p ∈ N tel que pour tout mot 0n.1n ∈ L, il existe x, y, z satisfaisant les conditions du théorème. Supposons alors α = 0p.1p. Par la seconde condition du théorème, x et y sont forcément composés que de 0. Supposons alors que x = 0i, y = 0j avec i + j ≤ p et z = 0l.1p avec l ∈ N tel que i + j + l = p. Il est claire qu’à partir d’un certain k ∈ N, i + kj > p et donc le mot x.yk.z = 0i+jk.0l.1p6∈L car i + jk + l > p.
Les automates finis sont donc un modèle de calcul plus faible que les machines de Turing car ils ne permettent pas de reconnaître les langages de l’exercice précédent ou de l’exemple 10.2 alors que ces derniers sont calculables. La raison est que les MTs disposent d’une mémoire, le ruban de taille infinie, qui permet de conserver un historique des calculs effectués. Une autre forme d’automates a alors été définie comme une extension des automates finis, les automates à pile, qui se rapprochent des MTs par le fait qu’ils manipulent une mémoire non bornée mais à laquelle on accède en suivant une politique LIFO (Last In First Out) (i.e. une pile d’où leur nom). Ce type d’automate permet de reconnaître les langages de l’exercice précédent. Néanmoins, là
97


encore, nous disposons d’un résultat équivalent au lemme du pompage qui montre que certains langages pourtant calculables, ne sont pas modélisables par un automate à pile. La raison est que la pile associée par sa politique LIFO, contraint la façon dont on accède aux éléments dans la pile. Nous ne présenterons pas ce type d’automates qui n’ont pas connus la même notoriété que les automates finis du fait de leur faible application (à part bien sûr la reconnaissance de certains langages) entre autres dans la modélisation des systèmes.
Expressions régulières
Les expressions régulières sont un autre moyen très simple de définir les langages réguliers.
Soit Σ un alphabet. L’ensembles des expressions régulières, noté ER(Σ), est défini inductivement par :
— ∅, ε ∈ ER(Σ) ; — ∀a ∈ Σ, a ∈ ER(Σ) ;
— E1 + E2 ∈ ER(Σ) si E1, E2 ∈ ER(Σ) ; — E1.E2 ∈ ER(Σ) si E1, E2 ∈ ER(Σ) ; — E∗ ∈ ER(Σ) si E ∈ ER(Σ)
Définition 53 (Expressions régulières).
À partir d’une expression régulière, on peut définir le langage qu’elle caractérise.
Soit Σ un alphabet et E ∈ ER(Σ). Le langage L(E) est défini par induction sur la structure de l’expression E par :
— L(∅) = ∅, L(ε) = {ε}, et L(a) = {a} — L(E1 + E2) = L(E1) ∪ L(E2) — L(E1.E2) = L(E1).L(E2) — L(E∗) = L(E)∗
Définition 54.
On a alors le résultat important suivant :
Un langage est régulier si, et seulement s’il existe une expression régulière qui le décrit.
Théorème 58.
Démonstration : Soit E une expression régulière. Montrons par récurrence structurelle sur E que le langage L(E) est régulier. Il est très simple de construire un automate pour les langages ∅, {ε} et {a}. Le pas de récurrence est une conséquence directe du théorème 56 qui nous assure que l’ensemble des langages réguliers est stable par union, produit et itération.
Soit A = (Q, F, q0, τ ) un automate sur un alphabet Σ. Pour démontrer que le langage L(A) peut se décrire par une expression régulière, nous allons appliquer la procédure suivante :
98


— On considère tous les chemins de A allant de l’état initial vers les états finaux de F . — Pour chacun des chemins, on peut construire l’expression régulière en concaténant les lettres apparaissant sur les transitions. Les boucles sont traitées par l’introduction de l’opérateur d’itération ∗. — L’expression régulière recherchée est alors la somme des expressions régulières définissant les chemins.
De façon plus rigoureuse, nous allons définir inductivement les expressions que l’on peut construire sur les chemins. Supposons que l’ensemble des états Q = {q0, . . . , qn}. Étant donnés deux états qi et qj de Q, on va construire l’ensemble des mots que l’on peut obtenir en partant de l’état qi pour finir dans l’état qj. On commence alors par construire l’ensemble des mots que l’on obtient directement sans passer par des états intermédiaires de qi à qj, puis les mots que l’on obtient en passant par q0, puis par q0 et q1, etc. Pour définir l’ensemble de ces mots, on introduit la notation R(i, j, k) comme l’ensemble des mots permettant de passer de l’état qi à l’état qj en passant par les états {q0, q1, . . . , qk−1}, pour tout i, j, k, 0 ≤ i, j ≤ n et 0 ≤ k ≤ n + 1. Définissons alors par récurrence sur k l’ensemble R(i, j, k) :
— R(i, j, 0) =
{ {a|qi
→a qj} si i 6= j {ε} ∪ {a|qi
→a qj} si i = j
— Les mots dans R(i, j, k + 1) qui permettent de passer de qi à qj sont alors : — tous les mots qui permettent de passer de qi à qj en ne passant que par des états dans {q0, . . . , qk−1}, i.e. les mots de R(i, j, k) ; — tous les mots qui permettent de passer qi à qk, puis de qk à qk un certains nombre de fois, puis de qk à qj, i.e. R(i, k, k).R(k, k, k)∗.R(k, j, k). Nous avons donc
R(i, j, k + 1) = R(i, j, k) + R(i, k, k).R(k, k, k)∗.R(k, j, k)
Le langage L(A) s’exprime alors par l’expression régulière
E=
∑
qj ∈F
R(0, j, n + 1)
10.3. Opérations sur les automates
Un certain nombre d’opérations peuvent être appliquées aux automates. Nous en avons déjà présenté deux en section 10.2 : la complétion et le produit Cartésien d’automates. Ici, nous allons présenter deux nouvelles opérations classiquement utilisées dans la reconnaissance des langages réguliers :
1. La déterminisation d’un automate.
2. La minimisation d’un automate.
Déterminisation d’un automate
L’intérêt de la déterminisation d’un automate est qu’il est plus façile de décider de l’appartenance d’un mot dans un langage L quand l’automate qui le définit est déterministe. La construction est assez simple. Étant donné un automate A = (Q, F, q0, τ ) sur un alphabet Σ, nous pouvons représenter la relation de transition τ comme une application τ : Q × Σ → P(Q). Il suffit alors de définir l’automate Ad = (P(Q), F ′, {q0}, τ ′) où :
99


— τ ′ = {(Q1, a, Q2)|Q2 = {q2|∃q1 ∈ Q1, q1
→a q2 ∈ τ }} — F ′ = {Q′|Q′ ∩ F 6= ∅}
Clairement, Ad est un automate déterministe.
L(A) = L(Ad).
Théorème 59.
Démonstration : La preuve est laissée en exercice.
Minimisation d’un automate
Soit L ⊆ Σ∗ un langage régulier. Notons ∼L⊆ Σ∗ × Σ∗ la relation binaire définie par :
α ∼L β ⇐⇒ (∀δ ∈ Σ∗, α.δ ∈ L ⇔ β.δ ∈ L)
∼L est clairement une relation d’équivalence. De plus, pour tous mots α, β ∈ L, si α ∼L β alors pour tout a ∈ Σ, nous avons α.a ∼L β.a. En effet, si α ∼L β alors pour tout δ ∈ Σ∗, α.a.δ ∈ L ⇔ β.a.δ ∈ L, et donc α.a ∼L β.a. Définissons alors l’automate T = (T, F, [ε]∼L, τ ) de la façon suivante :
— T = Σ∗
/∼L
— F = L/∼L
— τ ([α]∼L, a) = [α.a]∼L
τ est définie sous sa forme fonctionnelle car T est par définition déterministe (nous avons vu que si α ∼L β alors α.a ∼L β.a). De plus, elle est une application (i.e. T est complet) car elle est définie pour tous les mots de T (pour les mêmes raisons que ci-dessus). Il nous faut montrer que T est un automate fini. Pour cela, considérons un automate A déterministe et complet reconnaissant L. A étant déterministe et complet, pour tout mot α = a1 . . . an ∈ Σ∗, il existe un unique chemin dans A dont les transitions sont étiquetées par a1 . . . an, et ce chemin a la forme
q0
a→1 q1
a→2 . . . a→n qn (si α ∈ L, alors qn ∈ F ). Notons alors κα = qn, et définissons la relation ∼A⊆ Σ∗ × Σ∗ par :
α ∼A β ⇐⇒ κα = κβ
∼A étant fondée sur l’égalité d’états, elle est clairement une relation d’équivalence. De plus, on a :
1. Le nombre de classes d’équivalence dans ∼A est fini. En effet, à toute classe d’équivalence [α]∼A on peut associer l’unique état κα, et le nombre d’états dans A est fini.
2. α ∼A β =⇒ (∀a ∈ Σ, α.a ∼A β.a) (une conséquence du fait que A est déterministe et complet).
3. α ∼A β et α ∈ L(A) ⇐⇒ β ∈ L(A)
On a alors le résultat suivant :
100